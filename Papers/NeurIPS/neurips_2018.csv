conference,url,title,authors,abstract
neurips,https://proceedings.neurips.cc/paper/2018/file/00ac8ed3b4327bdd4ebbebcb2ba10a00-Paper.pdf,Synthesized Policies for Transfer and Adaptation across Tasks and Environments,"Hexiang Hu, Liyu Chen, Boqing Gong, Fei Sha",
neurips,https://proceedings.neurips.cc/paper/2018/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf,Self-Supervised Generation of Spatial Audio for 360° Video,"Pedro Morgado, Nuno Nvasconcelos, Timothy Langlois, Oliver Wang",
neurips,https://proceedings.neurips.cc/paper/2018/file/0172d289da48c48de8c5ebf3de9f7ee1-Paper.pdf,On GANs and GMMs,"Eitan Richardson, Yair Weiss","In this paper, we examine the utility of GANs in learning statistical models of images by comparing them to perhaps the simplest statistical model, the Gaussian Mixture Model. First, we present a simple method to evaluate generative models based on relative proportions of samples that fall into predetermined bins. Unlike previous automatic methods for evaluating models, our method does not rely on an additional neural network nor does it require approximating intractable computations. Second, we compare the performance of GANs to GMMs trained on the same datasets. While GMMs have previously been shown to be successful in modeling small patches of images, we show how to train them on full sized images despite the high dimensionality. Our results show that GMMs can generate realistic samples (although less sharp than those of GANs) but also capture the full distribution, which GANs fail to do. Furthermore, GMMs allow efficient inference and explicit representation of the underlying statistical structure. Finally, we discuss how GMMs can be used to generate sharp images."
neurips,https://proceedings.neurips.cc/paper/2018/file/018b59ce1fd616d874afad0f44ba338d-Paper.pdf,Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks,"Hyeonseob Nam, Hyo-Eun Kim",
neurips,https://proceedings.neurips.cc/paper/2018/file/018dd1e07a2de4a08e6612341bf2323e-Paper.pdf,Hierarchical Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies,"Sungryull Sohn, Junhyuk Oh, Honglak Lee",
neurips,https://proceedings.neurips.cc/paper/2018/file/019d385eb67632a7e958e23f24bd07d7-Paper.pdf,KDGAN: Knowledge Distillation with Generative Adversarial Networks,"Xiaojie Wang, Rui Zhang, Yu Sun, Jianzhong Qi",
neurips,https://proceedings.neurips.cc/paper/2018/file/01a0683665f38d8e5e567b3b15ca98bf-Paper.pdf,Contour location via entropy reduction leveraging multiple information sources,"Alexandre Marques, Remi Lam, Karen Willcox",
neurips,https://proceedings.neurips.cc/paper/2018/file/01e9565cecc4e989123f9620c1d09c09-Paper.pdf,Contextual bandits with surrogate losses: Margin bounds and efficient algorithms,"Dylan J. Foster, Akshay Krishnamurthy","We use surrogate losses to obtain several new regret bounds and new algorithms for contextual bandit learning. Using the ramp loss, we derive a new margin-based regret bound in terms of standard sequential complexity measures of a benchmark class of real-valued regression functions. Using the hinge loss, we derive an efficient algorithm with a
√
d
T
d
-type mistake bound against benchmark policies induced by
d
d
-dimensional regressors. Under realizability assumptions, our results also yield classical regret bounds."
neurips,https://proceedings.neurips.cc/paper/2018/file/01eee509ee2f68dc6014898c309e86bf-Paper.pdf,Adaptive Sampling Towards Fast Graph Representation Learning,"Wenbing Huang, Tong Zhang, Yu Rong, Junzhou Huang",
neurips,https://proceedings.neurips.cc/paper/2018/file/0245952ecff55018e2a459517fdb40e3-Paper.pdf,Analytic solution and stationary phase approximation for the Bayesian lasso and elastic net,Tom Michoel,
neurips,https://proceedings.neurips.cc/paper/2018/file/024677efb8e4aee2eaeef17b54695bbe-Paper.pdf,Identification and Estimation of Causal Effects from Dependent Data,"Eli Sherman, Ilya Shpitser",
neurips,https://proceedings.neurips.cc/paper/2018/file/02ed812220b0705fabb868ddbf17ea20-Paper.pdf,Streamlining Variational Inference for Constraint Satisfaction Problems,"Aditya Grover, Tudor Achim, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2018/file/033cc385728c51d97360020ed57776f0-Paper.pdf,A Spectral View of Adversarially Robust Features,"Shivam Garg, Vatsal Sharan, Brian Zhang, Gregory Valiant",
neurips,https://proceedings.neurips.cc/paper/2018/file/037a595e6f4f0576a9efe43154d71c18-Paper.pdf,Dimensionality Reduction has Quantifiable Imperfections: Two Geometric Bounds,"Kry Lui, Gavin Weiguang Ding, Ruitong Huang, Robert McCann","In this paper, we investigate Dimensionality reduction (DR) maps in an information retrieval setting from a quantitative topology point of view. In particular, we show that no DR maps can achieve perfect precision and perfect recall simultaneously. Thus a continuous DR map must have imperfect precision. We further prove an upper bound on the precision of Lipschitz continuous DR maps. While precision is a natural measure in an information retrieval setting, it does not measure `how' wrong the retrieved data is. We therefore propose a new measure based on Wasserstein distance that comes with similar theoretical guarantee. A key technical step in our proofs is a particular optimization problem of the
L
2
L
-Wasserstein distance over a constrained set of distributions. We provide a complete solution to this optimization problem, which can be of independent interest on the technical side."
neurips,https://proceedings.neurips.cc/paper/2018/file/03b2ceb73723f8b53cd533e4fba898ee-Paper.pdf,Learning SMaLL Predictors,"Vikas Garg, Ofer Dekel, Lin Xiao",
neurips,https://proceedings.neurips.cc/paper/2018/file/03bfc1d4783966c69cc6aef8247e0103-Paper.pdf,ResNet with one-neuron hidden layers is a Universal Approximator,"Hongzhou Lin, Stefanie Jegelka",
neurips,https://proceedings.neurips.cc/paper/2018/file/03c6b06952c750899bb03d998e631860-Paper.pdf,How Many Samples are Needed to Estimate a Convolutional Neural Network?,"Simon S. Du, Yining Wang, Xiyu Zhai, Sivaraman Balakrishnan, Russ R. Salakhutdinov, Aarti Singh","A widespread folklore for explaining the success of Convolutional Neural Networks (CNNs) is that CNNs use a more compact representation than the Fully-connected Neural Network (FNN) and thus require fewer training samples to accurately estimate their parameters. We initiate the study of rigorously characterizing the sample complexity of estimating CNNs. We show that for an
m
m
-dimensional convolutional filter with linear activation acting on a
d
d
-dimensional input, the sample complexity of achieving population prediction error of
ϵ
ϵ
is $\widetilde{O(m/\epsilon^2)
,
w
h
e
r
e
a
s
t
h
e
s
a
m
p
l
e
−
c
o
m
p
l
e
x
i
t
y
f
o
r
i
t
s
F
N
N
c
o
u
n
t
e
r
p
a
r
t
i
s
l
o
w
e
r
b
o
u
n
d
e
d
b
y
,
\Omega(d/\epsilon^2)
s
a
m
p
l
e
s
.
S
i
n
c
e
,
i
n
t
y
p
i
c
a
l
s
e
t
t
i
n
g
s
s
m \ll d
,
t
h
i
s
r
e
s
u
l
t
d
e
m
o
n
s
t
r
a
t
e
s
t
h
e
a
d
v
a
n
t
a
g
e
o
f
u
s
i
n
g
a
C
N
N
.
W
e
f
u
r
t
h
e
r
c
o
n
s
i
d
e
r
t
h
e
s
a
m
p
l
e
c
o
m
p
l
e
x
i
t
y
o
f
e
s
t
i
m
a
t
i
n
g
a
o
n
e
−
h
i
d
d
e
n
−
l
a
y
e
r
C
N
N
w
i
t
h
l
i
n
e
a
r
a
c
t
i
v
a
t
i
o
n
w
h
e
r
e
b
o
t
h
t
h
e
,
m
−
d
i
m
e
n
s
i
o
n
a
l
c
o
n
v
o
l
u
t
i
o
n
a
l
f
i
l
t
e
r
a
n
d
t
h
e
−
r
−
d
i
m
e
n
s
i
o
n
a
l
o
u
t
p
u
t
w
e
i
g
h
t
s
a
r
e
u
n
k
n
o
w
n
.
F
o
r
t
h
i
s
m
o
d
e
l
,
w
e
s
h
o
w
t
h
a
t
t
h
e
s
a
m
p
l
e
c
o
m
p
l
e
x
i
t
y
i
s
−
\widetilde{O}\left((m+r)/\epsilon^2\right)$ when the ratio between the stride size and the filter size is a constant. For both models, we also present lower bounds showing our sample complexities are tight up to logarithmic factors. Our main tools for deriving these results are a localized empirical process analysis and a new lemma characterizing the convolutional structure. We believe that these tools may inspire further developments in understanding CNNs."
neurips,https://proceedings.neurips.cc/paper/2018/file/03cf87174debaccd689c90c34577b82f-Paper.pdf,Objective and efficient inference for couplings in neuronal networks,"Yu Terada, Tomoyuki Obuchi, Takuya Isomura, Yoshiyuki Kabashima",
neurips,https://proceedings.neurips.cc/paper/2018/file/03e7ef47cee6fa4ae7567394b99912b7-Paper.pdf,Unsupervised Adversarial Invariance,"Ayush Jaiswal, Rex Yue Wu, Wael Abd-Almageed, Prem Natarajan",
neurips,https://proceedings.neurips.cc/paper/2018/file/045cf83ab0722e782cf72d14e44adf98-Paper.pdf,Critical initialisation for deep signal propagation in noisy rectifier neural networks,"Arnu Pretorius, Elan van Biljon, Steve Kroon, Herman Kamper",
neurips,https://proceedings.neurips.cc/paper/2018/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf,Learning sparse neural networks via sensitivity-driven regularization,"Enzo Tartaglione, Skjalg Lepsøy, Attilio Fiandrotti, Gianluca Francini",
neurips,https://proceedings.neurips.cc/paper/2018/file/051928341be67dcba03f0e04104d9047-Paper.pdf,Cooperative neural networks (CoNN): Exploiting prior independence structure for improved classification,"Harsh Shrivastava, Eugene Bart, Bob Price, Hanjun Dai, Bo Dai, Srinivas Aluru",
neurips,https://proceedings.neurips.cc/paper/2018/file/059fdcd96baeb75112f09fa1dcc740cc-Paper.pdf,Data center cooling using model-predictive control,"Nevena Lazic, Craig Boutilier, Tyler Lu, Eehern Wong, Binz Roy, MK Ryu, Greg Imwalle",
neurips,https://proceedings.neurips.cc/paper/2018/file/05a624166c8eb8273b8464e8d9cb5bd9-Paper.pdf,Generalization Bounds for Uniformly Stable Algorithms,"Vitaly Feldman, Jan Vondrak","Uniform stability of a learning algorithm is a classical notion of algorithmic stability introduced to derive high-probability bounds on the generalization error (Bousquet and Elisseeff, 2002). Specifically, for a loss function with range bounded in
[
0
,
1
]
[
, the generalization error of
γ
γ
-uniformly stable learning algorithm on
n
n
samples is known to be at most
O
(
(
γ
+
1
/
n
)
√
n
log
(
1
/
δ
)
)
O
with probability at least
1
−
δ
1
. Unfortunately, this bound does not lead to meaningful generalization bounds in many common settings where
γ
≥
1
/
√
n
γ
. At the same time the bound is known to be tight only when
γ
=
O
(
1
/
n
)
γ
. Here we prove substantially stronger generalization bounds for uniformly stable algorithms without any additional assumptions. First, we show that the generalization error in this setting is at most
O
(
√
(
γ
+
1
/
n
)
log
(
1
/
δ
)
)
O
with probability at least
1
−
δ
1
. In addition, we prove a tight bound of
O
(
γ
2
+
1
/
n
)
O
on the second moment of the generalization error. The best previous bound on the second moment of the generalization error is
O
(
γ
+
1
/
n
)
O
. Our proofs are based on new analysis techniques and our results imply substantially stronger generalization guarantees for several well-studied algorithms."
neurips,https://proceedings.neurips.cc/paper/2018/file/05a70454516ecd9194c293b0e415777f-Paper.pdf,COLA: Decentralized Linear Learning,"Lie He, An Bian, Martin Jaggi",
neurips,https://proceedings.neurips.cc/paper/2018/file/0609154fa35b3194026346c9cac2a248-Paper.pdf,Leveraging the Exact Likelihood of Deep Latent Variable Models,"Pierre-Alexandre Mattei, Jes Frellsen",
neurips,https://proceedings.neurips.cc/paper/2018/file/060afc8a563aaccd288f98b7c8723b61-Paper.pdf,A General Method for Amortizing Variational Filtering,"Joseph Marino, Milan Cvitkovic, Yisong Yue",
neurips,https://proceedings.neurips.cc/paper/2018/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf,One-Shot Unsupervised Cross Domain Translation,"Sagie Benaim, Lior Wolf","Given a single image
x
x
from domain
A
A
and a set of images from domain
B
B
, our task is to generate the analogous of
x
x
in
B
B
. We argue that this task could be a key AI capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised domain translation methods fail on this task. Our method follows a two step process. First, a variational autoencoder for domain
B
B
is trained. Then, given the new sample
x
x
, we create a variational autoencoder for domain
A
A
by adapting the layers that are close to the image in order to directly fit
x
x
, and only indirectly adapt the other layers. Our experiments indicate that the new method does as well, when trained on one sample
x
x
, as the existing domain transfer methods, when these enjoy a multitude of training samples from domain
A
A
. Our code is made publicly available at https://github.com/sagiebenaim/OneShotTranslation"
neurips,https://proceedings.neurips.cc/paper/2018/file/0655f117444fc1911ab9c6f6b0139051-Paper.pdf,Query K-means Clustering and the Double Dixie Cup Problem,"I Chien, Chao Pan, Olgica Milenkovic","We consider the problem of approximate
K
K
-means clustering with outliers and side information provided by same-cluster queries and possibly noisy answers. Our solution shows that, under some mild assumptions on the smallest cluster size, one can obtain an
(
1
+
ϵ
)
(
-approximation for the optimal potential with probability at least
1
−
δ
1
, where
ϵ
>
0
ϵ
and
δ
∈
(
0
,
1
)
δ
, using an expected number of
O
(
K
3
ϵ
δ
)
O
noiseless same-cluster queries and comparison-based clustering of complexity
O
(
n
d
K
+
K
3
ϵ
δ
)
O
; here,
n
n
denotes the number of points and
d
d
the dimension of space. Compared to a handful of other known approaches that perform importance sampling to account for small cluster sizes, the proposed query technique reduces the number of queries by a factor of roughly
O
(
K
6
ϵ
3
)
O
, at the cost of possibly missing very small clusters. We extend this settings to the case where some queries to the oracle produce erroneous information, and where certain points, termed outliers, do not belong to any clusters. Our proof techniques differ from previous methods used for
K
K
-means clustering analysis, as they rely on estimating the sizes of the clusters and the number of points needed for accurate centroid estimation and subsequent nontrivial generalizations of the double Dixie cup problem. We illustrate the performance of the proposed algorithm both on synthetic and real datasets, including MNIST and CIFAR
10
10
."
neurips,https://proceedings.neurips.cc/paper/2018/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf,Probabilistic Neural Programmed Networks for Scene Generation,"Zhiwei Deng, Jiacheng Chen, YIFANG FU, Greg Mori",
neurips,https://proceedings.neurips.cc/paper/2018/file/069654d5ce089c13f642d19f09a3d1c0-Paper.pdf,Escaping Saddle Points in Constrained Optimization,"Aryan Mokhtari, Asuman Ozdaglar, Ali Jadbabaie","In this paper, we study the problem of escaping from saddle points in smooth nonconvex optimization problems subject to a convex set
C
C
. We propose a generic framework that yields convergence to a second-order stationary point of the problem, if the convex set
C
C
is simple for a quadratic objective function. Specifically, our results hold if one can find a
ρ
ρ
-approximate solution of a quadratic program subject to
C
C
in polynomial time, where
ρ
<
1
ρ
is a positive constant that depends on the structure of the set
C
C
. Under this condition, we show that the sequence of iterates generated by the proposed framework reaches an
(
ϵ
,
γ
)
(
-second order stationary point (SOSP) in at most
O
(
max
{
ϵ
−
2
,
ρ
−
3
γ
−
3
}
)
O
iterations. We further characterize the overall complexity of reaching an SOSP when the convex set
C
C
can be written as a set of quadratic constraints and the objective function Hessian has a specific structure over the convex
C
C
. Finally, we extend our results to the stochastic setting and characterize the number of stochastic gradient and Hessian evaluations to reach an
(
ϵ
,
γ
)
(
-SOSP."
neurips,https://proceedings.neurips.cc/paper/2018/file/074177d3eb6371e32c16c55a3b8f706b-Paper.pdf,Adversarial Text Generation via Feature-Mover's Distance,"Liqun Chen, Shuyang Dai, Chenyang Tao, Haichao Zhang, Zhe Gan, Dinghan Shen, Yizhe Zhang, Guoyin Wang, Ruiyi Zhang, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2018/file/07f75d9144912970de5a09f5a305e10c-Paper.pdf,On gradient regularizers for MMD GANs,"Michael Arbel, Danica J. Sutherland, Mikołaj Bińkowski, Arthur Gretton","We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). We show that controlling the gradient of the critic is vital to having a sensible loss function, and devise a method to enforce exact, analytical gradient constraints at no additional cost compared to existing approximate techniques based on additive regularizers. The new loss function is provably continuous, and experiments show that it stabilizes and accelerates training, giving image generation models that outperform state-of-the art methods on
160
×
160
160
CelebA and
64
×
64
64
unconditional ImageNet."
neurips,https://proceedings.neurips.cc/paper/2018/file/08040837089cdf46631a10aca5258e16-Paper.pdf,Differentially Private Bayesian Inference for Exponential Families,"Garrett Bernstein, Daniel R. Sheldon",
neurips,https://proceedings.neurips.cc/paper/2018/file/08048a9c5630ccb67789a198f35d30ec-Paper.pdf,Stochastic Primal-Dual Method for Empirical Risk Minimization with O(1) Per-Iteration Complexity,"Conghui Tan, Tong Zhang, Shiqian Ma, Ji Liu",
neurips,https://proceedings.neurips.cc/paper/2018/file/0829424ffa0d3a2547b6c9622c77de03-Paper.pdf,Answerer in Questioner's Mind: Information Theoretic Approach to Goal-Oriented Visual Dialog,"Sang-Woo Lee, Yu-Jung Heo, Byoung-Tak Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/08aac6ac98e59e523995c161e57875f5-Paper.pdf,Learning Plannable Representations with Causal InfoGAN,"Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart J. Russell, Pieter Abbeel",
neurips,https://proceedings.neurips.cc/paper/2018/file/08c5433a60135c32e34f46a71175850c-Paper.pdf,Interactive Structure Learning with Structural Query-by-Committee,"Christopher Tosh, Sanjoy Dasgupta",
neurips,https://proceedings.neurips.cc/paper/2018/file/08f90c1a417155361a5c4b8d297e0d78-Paper.pdf,The streaming rollout of deep networks - towards fully model-parallel execution,"Volker Fischer, Jan Koehler, Thomas Pfeil",
neurips,https://proceedings.neurips.cc/paper/2018/file/08fc80de8121419136e443a70489c123-Paper.pdf,Contextual Stochastic Block Models,"Yash Deshpande, Subhabrata Sen, Andrea Montanari, Elchanan Mossel",
neurips,https://proceedings.neurips.cc/paper/2018/file/09060616068d2b9544dc33f2fbe4ce2d-Paper.pdf,Unsupervised Learning of Artistic Styles with Archetypal Style Analysis,"Daan Wynen, Cordelia Schmid, Julien Mairal",
neurips,https://proceedings.neurips.cc/paper/2018/file/0937fb5864ed06ffb59ae5f9b5ed67a9-Paper.pdf,Generalisation in humans and deep neural networks,"Robert Geirhos, Carlos R. M. Temme, Jonas Rauber, Heiko H. Schütt, Matthias Bethge, Felix A. Wichmann",
neurips,https://proceedings.neurips.cc/paper/2018/file/093b60fd0557804c8ba0cbf1453da22f-Paper.pdf,Distributed Weight Consolidation: A Brain Segmentation Case Study,"Patrick McClure, Charles Y. Zheng, Jakub Kaczmarzyk, John Rogers-Lee, Satra Ghosh, Dylan Nielson, Peter A. Bandettini, Francisco Pereira",
neurips,https://proceedings.neurips.cc/paper/2018/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf,IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis,"Huaibo Huang, zhihang li, Ran He, Zhenan Sun, Tieniu Tan",
neurips,https://proceedings.neurips.cc/paper/2018/file/09779bb7930c8a0a44360e12b538ae3c-Paper.pdf,MixLasso: Generalized Mixed Regression via Convex Atomic-Norm Regularization,"Ian En-Hsu Yen, Wei-Cheng Lee, Kai Zhong, Sung-En Chang, Pradeep K. Ravikumar, Shou-De Lin",
neurips,https://proceedings.neurips.cc/paper/2018/file/09a5e2a11bea20817477e0b1dfe2cc21-Paper.pdf,A Dual Framework for Low-rank Tensor Completion,"Madhav Nimishakavi, Pratik Kumar Jawanpuria, Bamdev Mishra",
neurips,https://proceedings.neurips.cc/paper/2018/file/09d37c08f7b129e96277388757530c72-Paper.pdf,Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer,"David Madras, Toni Pitassi, Richard Zemel",
neurips,https://proceedings.neurips.cc/paper/2018/file/0a113ef6b61820daa5611c870ed8d5ee-Paper.pdf,Enhancing the Accuracy and Fairness of Human Decision Making,"Isabel Valera, Adish Singla, Manuel Gomez Rodriguez","In this paper, we address the above problem from the perspective of sequential decision making and show that, for different fairness notions from the literature, it reduces to a sequence of (constrained) weighted bipartite matchings, which can be solved efficiently using algorithms with approximation guarantees. Moreover, these algorithms also benefit from posterior sampling to actively trade off exploitation---selecting expert assignments which lead to accurate and fair decisions---and exploration---selecting expert assignments to learn about the experts' preferences and biases. We demonstrate the effectiveness of our algorithms on both synthetic and real-world data and show that they can significantly improve both the accuracy and fairness of the decisions taken by pools of experts."
neurips,https://proceedings.neurips.cc/paper/2018/file/0a1bf96b7165e962e90cb14648c9462d-Paper.pdf,Computing Higher Order Derivatives of Matrix and Tensor Expressions,"Soeren Laue, Matthias Mitterreiter, Joachim Giesen",
neurips,https://proceedings.neurips.cc/paper/2018/file/0a348ede8ac3768875037baca5de6e26-Paper.pdf,Efficient online algorithms for fast-rate regret bounds under sparsity,"Pierre Gaillard, Olivier Wintenberger","We consider the problem of online convex optimization in two different settings: arbitrary and i.i.d. sequence of convex loss functions. In both settings, we provide efficient algorithms whose cumulative excess risks are controlled with fast-rate sparse bounds. First, the excess risks bounds depend on the sparsity of the objective rather than on the dimension of the parameters space. Second, their rates are faster than the slow-rate
1
/
√
T
1
under additional convexity assumptions on the loss functions. In the adversarial setting, we develop an algorithm BOA+ whose cumulative excess risks is controlled by several bounds with different trade-offs between sparsity and rate for strongly convex loss functions. In the i.i.d. setting under the Łojasiewicz's assumption, we establish new risk bounds that are sparse with a rate adaptive to the convexity of the risk (ranging from a rate
1
/
√
T
1
for general convex risk to
1
/
T
1
for strongly convex risk). These results generalize previous works on sparse online learning under weak assumptions on the risk."
neurips,https://proceedings.neurips.cc/paper/2018/file/0a7d83f084ec258aefd128569dda03d7-Paper.pdf,Human-in-the-Loop Interpretability Prior,"Isaac Lage, Andrew Ross, Samuel J. Gershman, Been Kim, Finale Doshi-Velez",
neurips,https://proceedings.neurips.cc/paper/2018/file/0aa1883c6411f7873cb83dacb17b0afc-Paper.pdf,Deep Non-Blind Deconvolution via Generalized Low-Rank Approximation,"Wenqi Ren, Jiawei Zhang, Lin Ma, Jinshan Pan, Xiaochun Cao, Wangmeng Zuo, Wei Liu, Ming-Hsuan Yang",
neurips,https://proceedings.neurips.cc/paper/2018/file/0ae3f79a30234b6c45a6f7d298ba1310-Paper.pdf,Regret Bounds for Robust Adaptive Control of the Linear Quadratic Regulator,"Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, Stephen Tu",
neurips,https://proceedings.neurips.cc/paper/2018/file/0b1ec366924b26fc98fa7b71a9c249cf-Paper.pdf,Binary Rating Estimation with Graph Side Information,"Kwangjun Ahn, Kangwook Lee, Hyunseung Cha, Changho Suh",
neurips,https://proceedings.neurips.cc/paper/2018/file/0b9e57c46de934cee33b0e8d1839bfc2-Paper.pdf,A Bayesian Nonparametric View on Count-Min Sketch,"Diana Cai, Michael Mitzenmacher, Ryan P. Adams",
neurips,https://proceedings.neurips.cc/paper/2018/file/0c0a7566915f4f24853fc4192689aa7e-Paper.pdf,Provable Variational Inference for Constrained Log-Submodular Models,"Josip Djolonga, Stefanie Jegelka, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2018/file/0c215f194276000be6a6df6528067151-Paper.pdf,Middle-Out Decoding,"Shikib Mehri, Leonid Sigal",
neurips,https://proceedings.neurips.cc/paper/2018/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf,Maximum-Entropy Fine Grained Classification,"Abhimanyu Dubey, Otkrist Gupta, Ramesh Raskar, Nikhil Naik",
neurips,https://proceedings.neurips.cc/paper/2018/file/0cd60efb5578cd967c3c23894f305800-Paper.pdf,Deep State Space Models for Unconditional Word Generation,"Florian Schmidt, Thomas Hofmann",
neurips,https://proceedings.neurips.cc/paper/2018/file/0d59701b3474225fca5563e015965886-Paper.pdf,Total stochastic gradient algorithms and applications in reinforcement learning,Paavo Parmas,
neurips,https://proceedings.neurips.cc/paper/2018/file/0e64a7b00c83e3d22ce6b3acf2c582b6-Paper.pdf,Neural Arithmetic Logic Units,"Andrew Trask, Felix Hill, Scott E. Reed, Jack Rae, Chris Dyer, Phil Blunsom",
neurips,https://proceedings.neurips.cc/paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf,Implicit Bias of Gradient Descent on Linear Convolutional Networks,"Suriya Gunasekar, Jason D. Lee, Daniel Soudry, Nati Srebro","We show that gradient descent on full-width linear convolutional networks of depth
L
L
converges to a linear predictor related to the
ℓ
2
/
L
ℓ
bridge penalty in the frequency domain. This is in contrast to linearly fully connected networks, where gradient descent converges to the hard margin linear SVM solution, regardless of depth."
neurips,https://proceedings.neurips.cc/paper/2018/file/0ebcc77dc72360d0eb8e9504c78d38bd-Paper.pdf,On Binary Classification in Extreme Regions,"Hamid JALALZAI, Stephan Clémençon, Anne Sabourin","In pattern recognition, a random label Y is to be predicted based upon observing a random vector X valued in
R
d
R
with d>1 by means of a classification rule with minimum probability of error. In a wide variety of applications, ranging from finance/insurance to environmental sciences through teletraffic data analysis for instance, extreme (i.e. very large) observations X are of crucial importance, while contributing in a negligible manner to the (empirical) error however, simply because of their rarity. As a consequence, empirical risk minimizers generally perform very poorly in extreme regions. It is the purpose of this paper to develop a general framework for classification in the extremes. Precisely, under non-parametric heavy-tail assumptions for the class distributions, we prove that a natural and asymptotic notion of risk, accounting for predictive performance in extreme regions of the input space, can be defined and show that minimizers of an empirical version of a non-asymptotic approximant of this dedicated risk, based on a fraction of the largest observations, lead to classification rules with good generalization capacity, by means of maximal deviation inequalities in low probability regions. Beyond theoretical results, numerical experiments are presented in order to illustrate the relevance of the approach developed."
neurips,https://proceedings.neurips.cc/paper/2018/file/0f0ee3310223fe38a989b2c818709393-Paper.pdf,Adaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical Models,"Alexander Neitz, Giambattista Parascandolo, Stefan Bauer, Bernhard Schölkopf",
neurips,https://proceedings.neurips.cc/paper/2018/file/0f2818101a7ac4b96ceeba38de4b934c-Paper.pdf,Learning to Specialize with Knowledge Distillation for Visual Question Answering,"Jonghwan Mun, Kimin Lee, Jinwoo Shin, Bohyung Han",
neurips,https://proceedings.neurips.cc/paper/2018/file/0f49c89d1e7298bb9930789c8ed59d48-Paper.pdf,A Model for Learned Bloom Filters and Optimizing by Sandwiching,Michael Mitzenmacher,
neurips,https://proceedings.neurips.cc/paper/2018/file/0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf,Random Feature Stein Discrepancies,"Jonathan Huggins, Lester Mackey",
neurips,https://proceedings.neurips.cc/paper/2018/file/0fe473396242072e84af286632d3f0ff-Paper.pdf,Nonparametric Bayesian Lomax delegate racing for survival analysis with competing risks,"Quan Zhang, Mingyuan Zhou",
neurips,https://proceedings.neurips.cc/paper/2018/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf,Hessian-based Analysis of Large Batch Training and Robustness to Adversaries,"Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, Michael W. Mahoney",
neurips,https://proceedings.neurips.cc/paper/2018/file/10a5ab2db37feedfdeaab192ead4ac0e-Paper.pdf,Adaptive Online Learning in Dynamic Environments,"Lijun Zhang, Shiyin Lu, Zhi-Hua Zhou","In this paper, we study online convex optimization in dynamic environments, and aim to bound the dynamic regret with respect to any sequence of comparators. Existing work have shown that online gradient descent enjoys an
O
(
√
T
(
1
+
P
T
)
)
O
dynamic regret, where
T
T
is the number of iterations and
P
T
P
is the path-length of the comparator sequence. However, this result is unsatisfactory, as there exists a large gap from the
Ω
(
√
T
(
1
+
P
T
)
)
Ω
lower bound established in our paper. To address this limitation, we develop a novel online method, namely adaptive learning for dynamic environment (Ader), which achieves an optimal
O
(
√
T
(
1
+
P
T
)
)
O
dynamic regret. The basic idea is to maintain a set of experts, each attaining an optimal dynamic regret for a specific path-length, and combines them with an expert-tracking algorithm. Furthermore, we propose an improved Ader based on the surrogate loss, and in this way the number of gradient evaluations per round is reduced from
O
(
log
T
)
O
to
1
1
. Finally, we extend Ader to the setting that a sequence of dynamical models is available to characterize the comparators."
neurips,https://proceedings.neurips.cc/paper/2018/file/10c66082c124f8afe3df4886f5e516e0-Paper.pdf,Learning in Games with Lossy Feedback,"Zhengyuan Zhou, Panayotis Mertikopoulos, Susan Athey, Nicholas Bambos, Peter W. Glynn, Yinyu Ye",
neurips,https://proceedings.neurips.cc/paper/2018/file/10ff0b5e85e5b85cc3095d431d8c08b4-Paper.pdf,Statistical Optimality of Stochastic Gradient Descent on Hard Learning Problems through Multiple Passes,"Loucas Pillaud-Vivien, Alessandro Rudi, Francis Bach",
neurips,https://proceedings.neurips.cc/paper/2018/file/1102a326d5f7c9e04fc3c89d0ede88c9-Paper.pdf,Multimodal Generative Models for Scalable Weakly-Supervised Learning,"Mike Wu, Noah Goodman",
neurips,https://proceedings.neurips.cc/paper/2018/file/1141938ba2c2b13f5505d7c424ebae5f-Paper.pdf,Multi-Class Learning: From Theory to Algorithm,"Jian Li, Yong Liu, Rong Yin, Hua Zhang, Lizhong Ding, Weiping Wang",
neurips,https://proceedings.neurips.cc/paper/2018/file/11704817e347269b7254e744b5e22dac-Paper.pdf,Learning and Inference in Hilbert Space with Quantum Graphical Models,"Siddarth Srinivasan, Carlton Downey, Byron Boots",
neurips,https://proceedings.neurips.cc/paper/2018/file/11e2ad6bf99300cd3808bb105b55d4b8-Paper.pdf,Bayesian Structure Learning by Recursive Bootstrap,"Raanan Y. Rohekar, Yaniv Gurwicz, Shami Nisimov, Guy Koren, Gal Novik",
neurips,https://proceedings.neurips.cc/paper/2018/file/12092a75caa75e4644fd2869f0b6c45a-Paper.pdf,Efficient Convex Completion of Coupled Tensors using Coupled Nuclear Norms,"Kishan Wimalawarne, Hiroshi Mamitsuka",
neurips,https://proceedings.neurips.cc/paper/2018/file/125b93c9b50703fe9dac43ec231f5f83-Paper.pdf,Stein Variational Gradient Descent as Moment Matching,"Qiang Liu, Dilin Wang",
neurips,https://proceedings.neurips.cc/paper/2018/file/128ac9c427302b7a64314fc4593430b2-Paper.pdf,Data-Driven Clustering via Parameterized Lloyd's Families,"Maria-Florina F. Balcan, Travis Dick, Colin White","In this paper, we define an infinite family of algorithms generalizing Lloyd's algorithm, with one parameter controlling the the initialization procedure, and another parameter controlling the local search procedure. This family of algorithms includes the celebrated k-means++ algorithm, as well as the classic farthest-first traversal algorithm. We design efficient learning algorithms which receive samples from an application-specific distribution over clustering instances and learn a near-optimal clustering algorithm from the class. We show the best parameters vary significantly across datasets such as MNIST, CIFAR, and mixtures of Gaussians. Our learned algorithms never perform worse than k-means++, and on some datasets we see significant improvements."
neurips,https://proceedings.neurips.cc/paper/2018/file/12b1e42dc0746f22cf361267de07073f-Paper.pdf,Semi-Supervised Learning with Declaratively Specified Entropy Constraints,"Haitian Sun, William W. Cohen, Lidong Bing",
neurips,https://proceedings.neurips.cc/paper/2018/file/13168e6a2e6c84b4b7de9390c0ef5ec5-Paper.pdf,Bayesian Inference of Temporal Task Specifications from Demonstrations,"Ankit Shah, Pritish Kamath, Julie A. Shah, Shen Li",
neurips,https://proceedings.neurips.cc/paper/2018/file/136f951362dab62e64eb8e841183c2a9-Paper.pdf,Stochastic Nested Variance Reduction for Nonconvex Optimization,"Dongruo Zhou, Pan Xu, Quanquan Gu","We study finite-sum nonconvex optimization problems, where the objective function is an average of
n
n
nonconvex functions. We propose a new stochastic gradient descent algorithm based on nested variance reduction. Compared with conventional stochastic variance reduced gradient (SVRG) algorithm that uses two reference points to construct a semi-stochastic gradient with diminishing variance in each iteration, our algorithm uses
K
+
1
K
nested reference points to build a semi-stochastic gradient to further reduce its variance in each iteration. For smooth nonconvex functions, the proposed algorithm converges to an
ϵ
ϵ
-approximate first-order stationary point (i.e.,
∥
∇
F
(
x
)
∥
2
≤
ϵ
‖
) within
~
O
(
n
∧
ϵ
−
2
+
ϵ
−
3
∧
n
1
/
2
ϵ
−
2
)
O
\footnote{
~
O
(
⋅
)
O
hides the logarithmic factors, and
a
∧
b
a
means
min
(
a
,
b
)
min
.} number of stochastic gradient evaluations. This improves the best known gradient complexity of SVRG
O
(
n
+
n
2
/
3
ϵ
−
2
)
O
and that of SCSG
O
(
n
∧
ϵ
−
2
+
ϵ
−
10
/
3
∧
n
2
/
3
ϵ
−
2
)
O
. For gradient dominated functions, our algorithm also achieves better gradient complexity than the state-of-the-art algorithms. Thorough experimental results on different nonconvex optimization problems back up our theory."
neurips,https://proceedings.neurips.cc/paper/2018/file/1371bccec2447b5aa6d96d2a540fb401-Paper.pdf,On Markov Chain Gradient Descent,"Tao Sun, Yuejiao Sun, Wotao Yin",
neurips,https://proceedings.neurips.cc/paper/2018/file/139c3c1b7ca46a9d4fd6d163d98af635-Paper.pdf,The Limit Points of (Optimistic) Gradient Descent in Min-Max Optimization,"Constantinos Daskalakis, Ioannis Panageas",
neurips,https://proceedings.neurips.cc/paper/2018/file/13f320e7b5ead1024ac95c3b208610db-Paper.pdf,Empirical Risk Minimization in Non-interactive Local Differential Privacy Revisited,"Di Wang, Marco Gaboardi, Jinhui Xu","In this paper, we revisit the Empirical Risk Minimization problem in the non-interactive local model of differential privacy. In the case of constant or low dimensions (
p
≪
n
p
), we first show that if the loss function is
(
∞
,
T
)
(
-smooth, we can avoid a dependence of the sample complexity, to achieve error
α
α
, on the exponential of the dimensionality
p
p
with base
1
/
α
1
({\em i.e.,}
α
−
p
α
), which answers a question in \cite{smith2017interaction}. Our approach is based on polynomial approximation. Then, we propose player-efficient algorithms with
1
1
-bit communication complexity and
O
(
1
)
O
computation cost for each player. The error bound is asymptotically the same as the original one. With some additional assumptions, we also give an efficient algorithm for the server. In the case of high dimensions (
n
≪
p
n
), we show that if the loss function is a convex generalized linear function, the error can be bounded by using the Gaussian width of the constrained set, instead of
p
p
, which improves the one in \cite{smith2017interaction}."
neurips,https://proceedings.neurips.cc/paper/2018/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf,Greedy Hash: Towards Fast Optimization for Accurate Hash Coding in CNN,"Shupeng Su, Chao Zhang, Kai Han, Yonghong Tian",
neurips,https://proceedings.neurips.cc/paper/2018/file/13f9896df61279c928f19721878fac41-Paper.pdf,Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients?,Boris Hanin,
neurips,https://proceedings.neurips.cc/paper/2018/file/1415db70fe9ddb119e23e9b2808cde38-Paper.pdf,Learning a High Fidelity Pose Invariant Model for High-resolution Face Frontalization,"Jie Cao, Yibo Hu, Hongwen Zhang, Ran He, Zhenan Sun",
neurips,https://proceedings.neurips.cc/paper/2018/file/142c65e00f4f7cf2e6c4c996e34005df-Paper.pdf,Provably Correct Automatic Sub-Differentiation for Qualified Programs,"Sham M. Kakade, Jason D. Lee","The \emph{Cheap Gradient Principle}~\citep{Griewank:2008:EDP:1455489} --- the computational cost of computing a
d
d
-dimensional vector of partial derivatives of a scalar function is nearly the same (often within a factor of
5
5
) as that of simply computing the scalar function itself --- is of central importance in optimization; it allows us to quickly obtain (high-dimensional) gradients of scalar loss functions which are subsequently used in black box gradient-based optimization procedures. The current state of affairs is markedly different with regards to computing sub-derivatives: widely used ML libraries, including TensorFlow and PyTorch, do \emph{not} correctly compute (generalized) sub-derivatives even on simple differentiable examples. This work considers the question: is there a \emph{Cheap Sub-gradient Principle}? Our main result shows that, under certain restrictions on our library of non-smooth functions (standard in non-linear programming), provably correct generalized sub-derivatives can be computed at a computational cost that is within a (dimension-free) factor of
6
6
of the cost of computing the scalar function itself."
neurips,https://proceedings.neurips.cc/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf,CatBoost: unbiased boosting with categorical features,"Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, Andrey Gulin",
neurips,https://proceedings.neurips.cc/paper/2018/file/1458e7509aa5f47ecfb92536e7dd1dc7-Paper.pdf,Constrained Generation of Semantically Valid Graphs via Regularizing Variational Autoencoders,"Tengfei Ma, Jie Chen, Cao Xiao",
neurips,https://proceedings.neurips.cc/paper/2018/file/148510031349642de5ca0c544f31b2ef-Paper.pdf,"Deep, complex, invertible networks for inversion of transmission effects in multimode optical fibres","Oisín Moran, Piergiorgio Caramazza, Daniele Faccio, Roderick Murray-Smith",
neurips,https://proceedings.neurips.cc/paper/2018/file/14c879f3f5d8ed93a09f6090d77c2cc3-Paper.pdf,Scalable Hyperparameter Transfer Learning,"Valerio Perrone, Rodolphe Jenatton, Matthias W. Seeger, Cedric Archambeau",
neurips,https://proceedings.neurips.cc/paper/2018/file/15212f24321aa2c3dc8e9acf820f3c15-Paper.pdf,Wasserstein Distributionally Robust Kalman Filtering,"Soroosh Shafieezadeh Abadeh, Viet Anh Nguyen, Daniel Kuhn, Peyman Mohajerin Mohajerin Esfahani",
neurips,https://proceedings.neurips.cc/paper/2018/file/1543843a4723ed2ab08e18053ae6dc5b-Paper.pdf,SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path-Integrated Differential Estimator,"Cong Fang, Chris Junchi Li, Zhouchen Lin, Tong Zhang","In this paper, we propose a new technique named \textit{Stochastic Path-Integrated Differential EstimatoR} (SPIDER), which can be used to track many deterministic quantities of interests with significantly reduced computational cost. Combining SPIDER with the method of normalized gradient descent, we propose SPIDER-SFO that solve non-convex stochastic optimization problems using stochastic gradients only. We provide a few error-bound results on its convergence rates. Specially, we prove that the SPIDER-SFO algorithm achieves a gradient computation cost of
O
(
min
(
n
1
/
2
ϵ
−
2
,
ϵ
−
3
)
)
O
to find an
ϵ
ϵ
-approximate first-order stationary point. In addition, we prove that SPIDER-SFO nearly matches the algorithmic lower bound for finding stationary point under the gradient Lipschitz assumption in the finite-sum setting. Our SPIDER technique can be further applied to find an
(
ϵ
,
O
(
\ep
0.5
)
)
(
-approximate second-order stationary point at a gradient computation cost of
~
O
(
min
(
n
1
/
2
ϵ
−
2
+
ϵ
−
2.5
,
ϵ
−
3
)
)
O
."
neurips,https://proceedings.neurips.cc/paper/2018/file/1587965fb4d4b5afe8428a4a024feb0d-Paper.pdf,Generalized Zero-Shot Learning with Deep Calibration Network,"Shichen Liu, Mingsheng Long, Jianmin Wang, Michael I. Jordan",
neurips,https://proceedings.neurips.cc/paper/2018/file/15e122e839dfdaa7ce969536f94aecf6-Paper.pdf,Dual Policy Iteration,"Wen Sun, Geoffrey J. Gordon, Byron Boots, J. Bagnell",
neurips,https://proceedings.neurips.cc/paper/2018/file/16026d60ff9b54410b3435b403afd226-Paper.pdf,Recurrently Controlled Recurrent Networks,"Yi Tay, Anh Tuan Luu, Siu Cheung Hui",
neurips,https://proceedings.neurips.cc/paper/2018/file/160c88652d47d0be60bfbfed25111412-Paper.pdf,"Modelling sparsity, heterogeneity, reciprocity and community structure in temporal interaction data","Xenia Miscouridou, Francois Caron, Yee Whye Teh",
neurips,https://proceedings.neurips.cc/paper/2018/file/161882dd2d19c716819081aee2c08b98-Paper.pdf,Decentralize and Randomize: Faster Algorithm for Wasserstein Barycenters,"Pavel Dvurechenskii, Darina Dvinskikh, Alexander Gasnikov, Cesar Uribe, Angelia Nedich",
neurips,https://proceedings.neurips.cc/paper/2018/file/165a59f7cf3b5c4396ba65953d679f17-Paper.pdf,Heterogeneous Multi-output Gaussian Process Prediction,"Pablo Moreno-Muñoz, Antonio Artés, Mauricio Álvarez",
neurips,https://proceedings.neurips.cc/paper/2018/file/166cee72e93a992007a89b39eb29628b-Paper.pdf,SNIPER: Efficient Multi-Scale Training,"Bharat Singh, Mahyar Najibi, Larry S. Davis",
neurips,https://proceedings.neurips.cc/paper/2018/file/1700002963a49da13542e0726b7bb758-Paper.pdf,Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis,"Haoye Dong, Xiaodan Liang, Ke Gong, Hanjiang Lai, Jia Zhu, Jian Yin",
neurips,https://proceedings.neurips.cc/paper/2018/file/1714726c817af50457d810aae9d27a2e-Paper.pdf,Delta-encoder: an effective sample synthesis method for few-shot object recognition,"Eli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan Harary, Mattias Marder, Abhishek Kumar, Rogerio Feris, Raja Giryes, Alex Bronstein",
neurips,https://proceedings.neurips.cc/paper/2018/file/17326d10d511828f6b34fa6d751739e2-Paper.pdf,A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication,"Peng Jiang, Gagan Agrawal","The large communication overhead has imposed a bottleneck on the performance of distributed Stochastic Gradient Descent (SGD) for training deep neural networks. Previous works have demonstrated the potential of using gradient sparsification and quantization to reduce the communication cost. However, there is still a lack of understanding about how sparse and quantized communication affects the convergence rate of the training algorithm. In this paper, we study the convergence rate of distributed SGD for non-convex optimization with two communication reducing strategies: sparse parameter averaging and gradient quantization. We show that
O
(
1
/
√
M
K
)
O
convergence rate can be achieved if the sparsification and quantization hyperparameters are configured properly. We also propose a strategy called periodic quantized averaging (PQASGD) that further reduces the communication cost while preserving the
O
(
1
/
√
M
K
)
O
convergence rate. Our evaluation validates our theoretical results and shows that our PQASGD can converge as fast as full-communication SGD with only
3
%
−
5
%
3
communication data size."
neurips,https://proceedings.neurips.cc/paper/2018/file/173f0f6bb0ee97cf5098f73ee94029d4-Paper.pdf,Almost Optimal Algorithms for Linear Stochastic Bandits with Heavy-Tailed Payoffs,"Han Shao, Xiaotian Yu, Irwin King, Michael R. Lyu","In linear stochastic bandits, it is commonly assumed that payoffs are with sub-Gaussian noises. In this paper, under a weaker assumption on noises, we study the problem of \underline{lin}ear stochastic {\underline b}andits with h{\underline e}avy-{\underline t}ailed payoffs (LinBET), where the distributions have finite moments of order
1
+
ϵ
1
, for some
ϵ
∈
(
0
,
1
]
ϵ
. We rigorously analyze the regret lower bound of LinBET as
Ω
(
T
1
1
+
ϵ
)
Ω
, implying that finite moments of order 2 (i.e., finite variances) yield the bound of
Ω
(
√
T
)
Ω
, with
T
T
being the total number of rounds to play bandits. The provided lower bound also indicates that the state-of-the-art algorithms for LinBET are far from optimal. By adopting median of means with a well-designed allocation of decisions and truncation based on historical information, we develop two novel bandit algorithms, where the regret upper bounds match the lower bound up to polylogarithmic factors. To the best of our knowledge, we are the first to solve LinBET optimally in the sense of the polynomial order on
T
T
. Our proposed algorithms are evaluated based on synthetic datasets, and outperform the state-of-the-art results."
neurips,https://proceedings.neurips.cc/paper/2018/file/177540c7bcb8db31697b601642eac8d4-Paper.pdf,Learning towards Minimum Hyperspherical Energy,"Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, Le Song",
neurips,https://proceedings.neurips.cc/paper/2018/file/17b3c7061788dbe82de5abe9f6fe22b3-Paper.pdf,Learning a latent manifold of odor representations from neural responses in piriform cortex,"Anqi Wu, Stan Pashkovski, Sandeep R. Datta, Jonathan W. Pillow",
neurips,https://proceedings.neurips.cc/paper/2018/file/17c276c8e723eb46aef576537e9d56d0-Paper.pdf,Global Gated Mixture of Second-order Pooling for Improving Deep Convolutional Neural Networks,"Qilong Wang, Zilin Gao, Jiangtao Xie, Wangmeng Zuo, Peihua Li",
neurips,https://proceedings.neurips.cc/paper/2018/file/17c3433fecc21b57000debdf7ad5c930-Paper.pdf,Neural Code Comprehension: A Learnable Representation of Code Semantics,"Tal Ben-Nun, Alice Shoshana Jakobovits, Torsten Hoefler",
neurips,https://proceedings.neurips.cc/paper/2018/file/1819020b02e926785cf3be594d957696-Paper.pdf,PAC-Bayes Tree: weighted subtrees with guarantees,"Tin D. Nguyen, Samory Kpotufe",
neurips,https://proceedings.neurips.cc/paper/2018/file/1819932ff5cf474f4f19e7c7024640c2-Paper.pdf,Amortized Inference Regularization,"Rui Shu, Hung H. Bui, Shengjia Zhao, Mykel J. Kochenderfer, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2018/file/182be0c5cdcd5072bb1864cdee4d3d6e-Paper.pdf,Structure-Aware Convolutional Neural Networks,"Jianlong Chang, Jie Gu, Lingfeng Wang, GAOFENG MENG, SHIMING XIANG, Chunhong Pan",
neurips,https://proceedings.neurips.cc/paper/2018/file/185c29dc24325934ee377cfda20e414c-Paper.pdf,"Alternating optimization of decision trees, with application to learning sparse oblique trees","Miguel A. Carreira-Perpinan, Pooya Tavallali",
neurips,https://proceedings.neurips.cc/paper/2018/file/185e65bc40581880c4f2c82958de8cfe-Paper.pdf,Gradient Descent for Spiking Neural Networks,"Dongsung Huh, Terrence J. Sejnowski",
neurips,https://proceedings.neurips.cc/paper/2018/file/18903e4430783a191b0cfab439daaef8-Paper.pdf,Statistical and Computational Trade-Offs in Kernel K-Means,"Daniele Calandriello, Lorenzo Rosasco","We investigate the efficiency of k-means in terms of both statistical and computational requirements. More precisely, we study a Nystr\""om approach to kernel k-means. We analyze the statistical properties of the proposed method and show that it achieves the same accuracy of exact kernel k-means with only a fraction of computations. Indeed, we prove under basic assumptions that sampling
√
n
n
Nystr\""om landmarks allows to greatly reduce computational costs without incurring in any loss of accuracy. To the best of our knowledge this is the first result showing in this kind for unsupervised learning."
neurips,https://proceedings.neurips.cc/paper/2018/file/18bb68e2b38e4a8ce7cf4f6b2625768c-Paper.pdf,The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network,"Jeffrey Pennington, Pratik Worah",
neurips,https://proceedings.neurips.cc/paper/2018/file/196f5641aa9dc87067da4ff90fd81e7b-Paper.pdf,Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks,"Grant Rotskoff, Eric Vanden-Eijnden","The performance of neural networks on high-dimensional data distributions suggests that it may be possible to parameterize a representation of a given high-dimensional function with controllably small errors, potentially outperforming standard interpolation methods. We demonstrate, both theoretically and numerically, that this is indeed the case. We map the parameters of a neural network to a system of particles relaxing with an interaction potential determined by the loss function. We show that in the limit that the number of parameters
n
n
is large, the landscape of the mean-squared error becomes convex and the representation error in the function scales as
O
(
n
−
1
)
O
. In this limit, we prove a dynamical variant of the universal approximation theorem showing that the optimal representation can be attained by stochastic gradient descent, the algorithm ubiquitously used for parameter optimization in machine learning. In the asymptotic regime, we study the fluctuations around the optimal representation and show that they arise at a scale
O
(
n
−
1
)
O
. These fluctuations in the landscape identify the natural scale for the noise in stochastic gradient descent. Our results apply to both single and multi-layer neural networks, as well as standard kernel methods like radial basis functions."
neurips,https://proceedings.neurips.cc/paper/2018/file/198dd5fb9c43b2d29a548f8c77e85cf9-Paper.pdf,Uplift Modeling from Separate Labels,"Ikko Yamane, Florian Yger, Jamal Atif, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2018/file/1a0a283bfe7c549dee6c638a05200e32-Paper.pdf,A Bridging Framework for Model Optimization and Deep Propagation,"Risheng Liu, Shichao Cheng, xiaokun liu, Long Ma, Xin Fan, Zhongxuan Luo",
neurips,https://proceedings.neurips.cc/paper/2018/file/1a3f91fead97497b1a96d6104ad339f6-Paper.pdf,Learning filter widths of spectral decompositions with wavelets,"Haidar Khan, Bulent Yener",
neurips,https://proceedings.neurips.cc/paper/2018/file/1a9dcba2349fef2bb823c39e45dd6c96-Paper.pdf,Learning a Warping Distance from Unlabeled Time Series Using Sequence Autoencoders,"Abubakar Abid, James Y. Zou",
neurips,https://proceedings.neurips.cc/paper/2018/file/1b318124e37af6d74a03501474f44ea1-Paper.pdf,Gen-Oja: Simple & Efficient Algorithm for Streaming Generalized Eigenvector Computation,"Kush Bhatia, Aldo Pacchiano, Nicolas Flammarion, Peter L. Bartlett, Michael I. Jordan",
neurips,https://proceedings.neurips.cc/paper/2018/file/1b36ea1c9b7a1c3ad668b8bb5df7963f-Paper.pdf,Heterogeneous Bitwidth Binarization in Convolutional Neural Networks,"Joshua Fromm, Shwetak Patel, Matthai Philipose",
neurips,https://proceedings.neurips.cc/paper/2018/file/1b9f38268c50805669fd8caf8f3cc84a-Paper.pdf,BRUNO: A Deep Recurrent Model for Exchangeable Data,"Iryna Korshunova, Jonas Degrave, Ferenc Huszar, Yarin Gal, Arthur Gretton, Joni Dambre",
neurips,https://proceedings.neurips.cc/paper/2018/file/1bc0249a6412ef49b07fe6f62e6dc8de-Paper.pdf,Asymptotic optimality of adaptive importance sampling,"François Portier, Bernard Delyon","\textit{Adaptive importance sampling} (AIS) uses past samples to update the \textit{sampling policy}
q
t
q
at each stage
t
t
. Each stage
t
t
is formed with two steps : (i) to explore the space with
n
t
n
points according to
q
t
q
and (ii) to exploit the current amount of information to update the sampling policy. The very fundamental question raised in this paper concerns the behavior of empirical sums based on AIS. Without making any assumption on the \textit{allocation policy}
n
t
n
, the theory developed involves no restriction on the split of computational resources between the explore (i) and the exploit (ii) step. It is shown that AIS is asymptotically optimal : the asymptotic behavior of AIS is the same as some
oracle'' strategy that knows the targeted sampling policy from the beginning. From a practical perspective, weighted AIS is introduced, a new method that allows to forget poor samples from early stages."
neurips,https://proceedings.neurips.cc/paper/2018/file/1bc2029a8851ad344a8d503930dfd7f7-Paper.pdf,Phase Retrieval Under a Generative Prior,"Paul Hand, Oscar Leong, Vlad Voroninski","We introduce a novel deep-learning inspired formulation of the \textit{phase retrieval problem}, which asks to recover a signal
y
0
∈
\R
n
y
from
m
m
quadratic observations, under structural assumptions on the underlying signal. As is common in many imaging problems, previous methodologies have considered natural signals as being sparse with respect to a known basis, resulting in the decision to enforce a generic sparsity prior. However, these methods for phase retrieval have encountered possibly fundamental limitations, as no computationally efficient algorithm for sparse phase retrieval has been proven to succeed with fewer than
O
(
k
2
log
n
)
O
generic measurements, which is larger than the theoretical optimum of
O
(
k
log
n
)
O
. In this paper, we sidestep this issue by considering a prior that a natural signal is in the range of a generative neural network
G
:
\R
k
→
\R
n
G
. We introduce an empirical risk formulation that has favorable global geometry for gradient methods, as soon as
m
=
O
(
k
)
m
, under the model of a multilayer fully-connected neural network with random weights. Specifically, we show that there exists a descent direction outside of a small neighborhood around the true
k
k
-dimensional latent code and a negative multiple thereof. This formulation for structured phase retrieval thus benefits from two effects: generative priors can more tightly represent natural signals than sparsity priors, and this empirical risk formulation can exploit those generative priors at an information theoretically optimal sample complexity, unlike for a sparsity prior. We corroborate these results with experiments showing that exploiting generative models in phase retrieval tasks outperforms both sparse and general phase retrieval methods."
neurips,https://proceedings.neurips.cc/paper/2018/file/1c336b8080f82bcc2cd2499b4c57261d-Paper.pdf,Gaussian Process Prior Variational Autoencoders,"Francesco Paolo Casale, Adrian Dalca, Luca Saglietti, Jennifer Listgarten, Nicolo Fusi",
neurips,https://proceedings.neurips.cc/paper/2018/file/1c63926ebcabda26b5cdb31b5cc91efb-Paper.pdf,Deep Predictive Coding Network with Local Recurrent Processing for Object Recognition,"Kuan Han, Haiguang Wen, Yizhen Zhang, Di Fu, Eugenio Culurciello, Zhongming Liu",
neurips,https://proceedings.neurips.cc/paper/2018/file/1c6a0198177bfcc9bd93f6aab94aad3c-Paper.pdf,Exponentiated Strongly Rayleigh Distributions,"Zelda E. Mariet, Suvrit Sra, Stefanie Jegelka",
neurips,https://proceedings.neurips.cc/paper/2018/file/1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf,Variational Inference with Tail-adaptive f-Divergence,"Dilin Wang, Hao Liu, Qiang Liu",
neurips,https://proceedings.neurips.cc/paper/2018/file/1d6408264d31d453d556c60fe7d0459e-Paper.pdf,Modelling and unsupervised learning of symmetric deformable object categories,"James Thewlis, Hakan Bilen, Andrea Vedaldi",
neurips,https://proceedings.neurips.cc/paper/2018/file/1d94108e907bb8311d8802b48fd54b4a-Paper.pdf,Generalizing to Unseen Domains via Adversarial Data Augmentation,"Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C. Duchi, Vittorio Murino, Silvio Savarese",
neurips,https://proceedings.neurips.cc/paper/2018/file/1dba3025b159cd9354da65e2d0436a31-Paper.pdf,Information-theoretic Limits for Community Detection in Network Models,"Chuyang Ke, Jean Honorio",
neurips,https://proceedings.neurips.cc/paper/2018/file/1dc3a89d0d440ba31729b0ba74b93a33-Paper.pdf,Dendritic cortical microcircuits approximate the backpropagation algorithm,"João Sacramento, Rui Ponte Costa, Yoshua Bengio, Walter Senn",
neurips,https://proceedings.neurips.cc/paper/2018/file/1e1d184167ca7676cf665225e236a3d2-Paper.pdf,Structured Local Minima in Sparse Blind Deconvolution,"Yuqian Zhang, Han-wen Kuo, John Wright","Blind deconvolution is a ubiquitous problem of recovering two unknown signals from their convolution. Unfortunately, this is an ill-posed problem in general. This paper focuses on the {\em short and sparse} blind deconvolution problem, where the one unknown signal is short and the other one is sparsely and randomly supported. This variant captures the structure of the unknown signals in several important applications. We assume the short signal to have unit
ℓ
2
ℓ
norm and cast the blind deconvolution problem as a nonconvex optimization problem over the sphere. We demonstrate that (i) in a certain region of the sphere, every local optimum is close to some shift truncation of the ground truth, and (ii) for a generic short signal of length
k
k
, when the sparsity of activation signal
θ
≲
k
−
2
/
3
θ
and number of measurements
m
≳
\poly
\paren
k
m
, a simple initialization method together with a descent algorithm which escapes strict saddle points recovers a near shift truncation of the ground truth kernel."
neurips,https://proceedings.neurips.cc/paper/2018/file/1e4d36177d71bbb3558e43af9577d70e-Paper.pdf,"Solving Non-smooth Constrained Programs with Lower Complexity than
O
(
1
/
ε
)
O
: A Primal-Dual Homotopy Smoothing Approach","Xiaohan Wei, Hao Yu, Qing Ling, Michael Neely","We propose a new primal-dual homotopy smoothing algorithm for a linearly constrained convex program, where neither the primal nor the dual function has to be smooth or strongly convex. The best known iteration complexity solving such a non-smooth problem is
O
(
ε
−
1
)
O
. In this paper, we show that by leveraging a local error bound condition on the dual function, the proposed algorithm can achieve a better primal convergence time of
O
\l
(
ε
−
2
/
(
2
+
β
)
log
2
(
ε
−
1
)
\r
)
O
, where
β
∈
(
0
,
1
]
β
is a local error bound parameter. As an example application, we show that the distributed geometric median problem, which can be formulated as a constrained convex program, has its dual function non-smooth but satisfying the aforementioned local error bound condition with
β
=
1
/
2
β
, therefore enjoying a convergence time of
O
\l
(
ε
−
4
/
5
log
2
(
ε
−
1
)
\r
)
O
. This result improves upon the
O
(
ε
−
1
)
O
convergence time bound achieved by existing distributed optimization algorithms. Simulation experiments also demonstrate the performance of our proposed algorithm."
neurips,https://proceedings.neurips.cc/paper/2018/file/1ea97de85eb634d580161c603422437f-Paper.pdf,Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces,"Yu-An Chung, Wei-Hung Weng, Schrasing Tong, James Glass",
neurips,https://proceedings.neurips.cc/paper/2018/file/1ee3dfcd8a0645a25a35977997223d22-Paper.pdf,Isolating Sources of Disentanglement in Variational Autoencoders,"Ricky T. Q. Chen, Xuechen Li, Roger B. Grosse, David K. Duvenaud",
neurips,https://proceedings.neurips.cc/paper/2018/file/1ef03ed0cd5863c550128836b28ec3e9-Paper.pdf,Learning to Share and Hide Intentions using Information Regularization,"DJ Strouse, Max Kleiman-Weiner, Josh Tenenbaum, Matt Botvinick, David J. Schwab",
neurips,https://proceedings.neurips.cc/paper/2018/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf,Why Is My Classifier Discriminatory?,"Irene Chen, Fredrik D. Johansson, David Sontag",
neurips,https://proceedings.neurips.cc/paper/2018/file/1f36c15d6a3d18d52e8d493bc8187cb9-Paper.pdf,Unsupervised Learning of Object Landmarks through Conditional Image Generation,"Tomas Jakab, Ankush Gupta, Hakan Bilen, Andrea Vedaldi",
neurips,https://proceedings.neurips.cc/paper/2018/file/1f4fe6a4411edc2ff625888b4093e917-Paper.pdf,Scalable Coordinated Exploration in Concurrent Reinforcement Learning,"Maria Dimakopoulou, Ian Osband, Benjamin Van Roy",
neurips,https://proceedings.neurips.cc/paper/2018/file/1fc214004c9481e4c8073e85323bfd4b-Paper.pdf,Bayesian Semi-supervised Learning with Graph Gaussian Processes,"Yin Cheng Ng, Nicolò Colombo, Ricardo Silva",
neurips,https://proceedings.neurips.cc/paper/2018/file/201e5bacd665709851b77148e225b332-Paper.pdf,"Simple, Distributed, and Accelerated Probabilistic Programming","Dustin Tran, Matthew W. Hoffman, Dave Moore, Christopher Suter, Srinivas Vasudevan, Alexey Radul",
neurips,https://proceedings.neurips.cc/paper/2018/file/204da255aea2cd4a75ace6018fad6b4d-Paper.pdf,When do random forests fail?,"Cheng Tang, Damien Garreau, Ulrike von Luxburg",
neurips,https://proceedings.neurips.cc/paper/2018/file/207f88018f72237565570f8a9e5ca240-Paper.pdf,Contextual Combinatorial Multi-armed Bandits with Volatile Arms and Submodular Reward,"Lixing Chen, Jie Xu, Zhuo Lu","In this paper, we study the stochastic contextual combinatorial multi-armed bandit (CC-MAB) framework that is tailored for volatile arms and submodular reward functions. CC-MAB inherits properties from both contextual bandit and combinatorial bandit: it aims to select a set of arms in each round based on the side information (a.k.a. context) associated with the arms. By
volatile arms'', we mean that the available arms to select from in each round may change; and by
submodular rewards'', we mean that the total reward achieved by selected arms is not a simple sum of individual rewards but demonstrates a feature of diminishing returns determined by the relations between selected arms (e.g. relevance and redundancy). Volatile arms and submodular rewards are often seen in many real-world applications, e.g. recommender systems and crowdsourcing, in which multi-armed bandit (MAB) based strategies are extensively applied. Although there exist works that investigate these issues separately based on standard MAB, jointly considering all these issues in a single MAB problem requires very different algorithm design and regret analysis. Our algorithm CC-MAB provides an online decision-making policy in a contextual and combinatorial bandit setting and effectively addresses the issues raised by volatile arms and submodular reward functions. The proposed algorithm is proved to achieve
O
(
c
T
2
α
+
D
3
α
+
D
log
(
T
)
)
O
regret after a span of
T
T
rounds. The performance of CC-MAB is evaluated by experiments conducted on a real-world crowdsourcing dataset, and the result shows that our algorithm outperforms the prior art."
neurips,https://proceedings.neurips.cc/paper/2018/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf,Learning to Reconstruct Shapes from Unseen Classes,"Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Josh Tenenbaum, Bill Freeman, Jiajun Wu",
neurips,https://proceedings.neurips.cc/paper/2018/file/20d135f0f28185b84a4cf7aa51f29500-Paper.pdf,Mixture Matrix Completion,Daniel Pimentel-Alarcon,
neurips,https://proceedings.neurips.cc/paper/2018/file/210f760a89db30aa72ca258a3483cc7f-Paper.pdf,Fighting Boredom in Recommender Systems with Linear Reinforcement Learning,"Romain WARLOP, Alessandro Lazaric, Jérémie Mary",
neurips,https://proceedings.neurips.cc/paper/2018/file/211a7a84d3d5ce4d80347da11e0c85ed-Paper.pdf,Diffusion Maps for Textual Network Embedding,"Xinyuan Zhang, Yitong Li, Dinghan Shen, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2018/file/2151b4c76b4dcb048d06a5c32942b6f6-Paper.pdf,Out-of-Distribution Detection using Multiple Semantic Label Representations,"Gabi Shalev, Yossi Adi, Joseph Keshet",
neurips,https://proceedings.neurips.cc/paper/2018/file/217e342fc01668b10cb1188d40d3370e-Paper.pdf,First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time,"Yi Xu, Rong Jin, Tianbao Yang","(This is a theory paper) In this paper, we consider first-order methods for solving stochastic non-convex optimization problems. The key building block of the proposed algorithms is first-order procedures to extract negative curvature from the Hessian matrix through a principled sequence starting from noise, which are referred to {\it NEgative-curvature-Originated-from-Noise or NEON} and are of independent interest. Based on this building block, we design purely first-order stochastic algorithms for escaping from non-degenerate saddle points with a much better time complexity (almost linear time in the problem's dimensionality). In particular, we develop a general framework of {\it first-order stochastic algorithms} with a second-order convergence guarantee based on our new technique and existing algorithms that may only converge to a first-order stationary point. For finding a nearly {\it second-order stationary point}
\x
\x
such that
∥
∇
F
(
\x
)
∥
≤
ϵ
‖
and
∇
2
F
(
\x
)
≥
−
√
ϵ
I
∇
(in high probability), the best time complexity of the presented algorithms is
˜
O
(
d
/
ϵ
3.5
)
O
, where
F
(
⋅
)
F
denotes the objective function and
d
d
is the dimensionality of the problem. To the best of our knowledge, this is the first theoretical result of first-order stochastic algorithms with an almost linear time in terms of problem's dimensionality for finding second-order stationary points, which is even competitive with existing stochastic algorithms hinging on the second-order information."
neurips,https://proceedings.neurips.cc/paper/2018/file/21ce689121e39821d07d04faab328370-Paper.pdf,cpSGD: Communication-efficient and differentially-private distributed SGD,"Naman Agarwal, Ananda Theertha Suresh, Felix Xinnan X. Yu, Sanjiv Kumar, Brendan McMahan","Distributed stochastic gradient descent is an important subroutine in distributed learning. A setting of particular interest is when the clients are mobile devices, where two important concerns are communication efficiency and the privacy of the clients. Several recent works have focused on reducing the communication cost or introducing privacy guarantees, but none of the proposed communication efficient methods are known to be privacy preserving and none of the known privacy mechanisms are known to be communication efficient. To this end, we study algorithms that achieve both communication efficiency and differential privacy. For
d
d
variables and
n
≈
d
n
clients, the proposed method uses
\cO
(
log
log
(
n
d
)
)
\cO
bits of communication per client per coordinate and ensures constant privacy. We also improve previous analysis of the \emph{Binomial mechanism} showing that it achieves nearly the same utility as the Gaussian mechanism, while requiring fewer representation bits, which can be of independent interest."
neurips,https://proceedings.neurips.cc/paper/2018/file/226d1f15ecd35f784d2a20c3ecf56d7f-Paper.pdf,Factored Bandits,"Julian Zimmert, Yevgeny Seldin",
neurips,https://proceedings.neurips.cc/paper/2018/file/22722a343513ed45f14905eb07621686-Paper.pdf,Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks,"Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, Tom Goldstein","We present an optimization-based method for crafting poisons, and show that just one single poison image can control classifier behavior when transfer learning is used. For full end-to-end training, we present a
watermarking'' strategy that makes poisoning reliable using multiple (approx. 50) poisoned training instances. We demonstrate our method by generating poisoned frog images from the CIFAR dataset and using them to manipulate image classifiers."
neurips,https://proceedings.neurips.cc/paper/2018/file/228b25587479f2fc7570428e8bcbabdc-Paper.pdf,Implicit Probabilistic Integrators for ODEs,"Onur Teymur, Han Cheng Lie, Tim Sullivan, Ben Calderhead",
neurips,https://proceedings.neurips.cc/paper/2018/file/229754d7799160502a143a72f6789927-Paper.pdf,Non-metric Similarity Graphs for Maximum Inner Product Search,"Stanislav Morozov, Artem Babenko",
neurips,https://proceedings.neurips.cc/paper/2018/file/22b1f2e0983160db6f7bb9f62f4dbb39-Paper.pdf,Learning convex polytopes with margin,"Lee-Ad Gottlieb, Eran Kaufman, Aryeh Kontorovich, Gabriel Nivasch",
neurips,https://proceedings.neurips.cc/paper/2018/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf,Distilled Wasserstein Learning for Word Embedding and Topic Modeling,"Hongteng Xu, Wenlin Wang, Wei Liu, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2018/file/23451391cd1399019fa0421129066bc6-Paper.pdf,Inferring Latent Velocities from Weather Radar Data using Gaussian Processes,"Rico Angell, Daniel R. Sheldon",
neurips,https://proceedings.neurips.cc/paper/2018/file/23ce1851341ec1fa9e0c259de10bf87c-Paper.pdf,Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization,"Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan, Xiujun Li, Chris Brockett, Bill Dolan",
neurips,https://proceedings.neurips.cc/paper/2018/file/240c945bb72980130446fc2b40fbb8e0-Paper.pdf,Multi-Agent Generative Adversarial Imitation Learning,"Jiaming Song, Hongyu Ren, Dorsa Sadigh, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2018/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf,Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning,"Supasorn Suwajanakorn, Noah Snavely, Jonathan J. Tompson, Mohammad Norouzi",
neurips,https://proceedings.neurips.cc/paper/2018/file/24b43fb034a10d78bec71274033b4096-Paper.pdf,Variational Learning on Aggregate Outputs with Gaussian Processes,"Ho Chung Law, Dino Sejdinovic, Ewan Cameron, Tim Lucas, Seth Flaxman, Katherine Battle, Kenji Fukumizu",
neurips,https://proceedings.neurips.cc/paper/2018/file/253f7b5d921338af34da817c00f42753-Paper.pdf,Adaptation to Easy Data in Prediction with Limited Advice,"Tobias Sommer Thune, Yevgeny Seldin","We derive an online learning algorithm with improved regret guarantees for
easy'' loss sequences. We consider two types of
easiness'': (a) stochastic loss sequences and (b) adversarial loss sequences with small effective range of the losses. While a number of algorithms have been proposed for exploiting small effective range in the full information setting, Gerchinovitz and Lattimore [2016] have shown the impossibility of regret scaling with the effective range of the losses in the bandit setting. We show that just one additional observation per round is sufficient to circumvent the impossibility result. The proposed Second Order Difference Adjustments (SODA) algorithm requires no prior knowledge of the effective range of the losses,
ε
ε
, and achieves an
O
(
ε
√
K
T
ln
K
)
+
~
O
(
ε
K
4
√
T
)
O
expected regret guarantee, where
T
T
is the time horizon and
K
K
is the number of actions. The scaling with the effective loss range is achieved under significantly weaker assumptions than those made by Cesa-Bianchi and Shamir [2018] in an earlier attempt to circumvent the impossibility result. We also provide a regret lower bound of
Ω
(
ε
√
T
K
)
Ω
, which almost matches the upper bound. In addition, we show that in the stochastic setting SODA achieves an
O
(
∑
a
:
Δ
a
>
0
K
ε
2
Δ
a
)
O
pseudo-regret bound that holds simultaneously with the adversarial regret guarantee. In other words, SODA is safe against an unrestricted oblivious adversary and provides improved regret guarantees for at least two different types of
easiness'' simultaneously."
neurips,https://proceedings.neurips.cc/paper/2018/file/2596a54cdbb555cfd09cd5d991da0f55-Paper.pdf,Maximum Causal Tsallis Entropy Imitation Learning,"Kyungjae Lee, Sungjoon Choi, Songhwai Oh",
neurips,https://proceedings.neurips.cc/paper/2018/file/25db67c5657914454081c6a18e93d6dd-Paper.pdf,Importance Weighting and Variational Inference,"Justin Domke, Daniel R. Sheldon",
neurips,https://proceedings.neurips.cc/paper/2018/file/2668a7105966cae6e23901495176b8f9-Paper.pdf,Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction,"Roei Herzig, Moshiko Raboh, Gal Chechik, Jonathan Berant, Amir Globerson",
neurips,https://proceedings.neurips.cc/paper/2018/file/26e359e83860db1d11b6acca57d8ea88-Paper.pdf,Are ResNets Provably Better than Linear Predictors?,Ohad Shamir,
neurips,https://proceedings.neurips.cc/paper/2018/file/2715518c875999308842e3455eda2fe3-Paper.pdf,Meta-Gradient Reinforcement Learning,"Zhongwen Xu, Hado P. van Hasselt, David Silver",
neurips,https://proceedings.neurips.cc/paper/2018/file/27e8e17134dd7083b050476733207ea1-Paper.pdf,GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration,"Jacob Gardner, Geoff Pleiss, Kilian Q. Weinberger, David Bindel, Andrew G. Wilson",
neurips,https://proceedings.neurips.cc/paper/2018/file/280cf18baf4311c92aa5a042336587d3-Paper.pdf,Spectral Signatures in Backdoor Attacks,"Brandon Tran, Jerry Li, Aleksander Madry","In this paper, we identify a new property of all known backdoor attacks, which we call spectral signatures. This property allows us to utilize tools from robust statistics to thwart the attacks. We demonstrate the efficacy of these signatures in detecting and removing poisoned examples on real image sets and state of the art neural network architectures. We believe that understanding spectral signatures is a crucial first step towards a principled understanding of backdoor attacks."
neurips,https://proceedings.neurips.cc/paper/2018/file/285e19f20beded7d215102b49d5c09a0-Paper.pdf,Uncertainty-Aware Attention for Reliable Interpretation and Prediction,"Jay Heo, Hae Beom Lee, Saehoon Kim, Juho Lee, Kwang Joon Kim, Eunho Yang, Sung Ju Hwang",
neurips,https://proceedings.neurips.cc/paper/2018/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf,Attention in Convolutional LSTM for Gesture Recognition,"Liang Zhang, Guangming Zhu, Lin Mei, Peiyi Shen, Syed Afaq Ali Shah, Mohammed Bennamoun",
neurips,https://proceedings.neurips.cc/paper/2018/file/287e041302f34b11ddfb57afc8048cd8-Paper.pdf,Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger,"Gabriel Synnaeve, Zeming Lin, Jonas Gehring, Dan Gant, Vegard Mella, Vasil Khalidov, Nicolas Carion, Nicolas Usunier",
neurips,https://proceedings.neurips.cc/paper/2018/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf,PacGAN: The power of two samples in generative adversarial networks,"Zinan Lin, Ashish Khetan, Giulia Fanti, Sewoong Oh",
neurips,https://proceedings.neurips.cc/paper/2018/file/28a543c2a9eee8c0d6fbfaff7ca7e224-Paper.pdf,Bayesian multi-domain learning for cancer subtype discovery from next-generation sequencing count data,"Ehsan Hajiramezanali, Siamak Zamani Dadaneh, Alireza Karbalayghareh, Mingyuan Zhou, Xiaoning Qian",
neurips,https://proceedings.neurips.cc/paper/2018/file/28b9f8aa9f07db88404721af4a5b6c11-Paper.pdf,Multilingual Anchoring: Interactive Topic Modeling and Alignment Across Languages,"Michelle Yuan, Benjamin Van Durme, Jordan L. Ying",
neurips,https://proceedings.neurips.cc/paper/2018/file/28dd2c7955ce926456240b2ff0100bde-Paper.pdf,Generalized Inverse Optimization through Online Learning,"Chaosheng Dong, Yiran Chen, Bo Zeng","Inverse optimization is a powerful paradigm for learning preferences and restrictions that explain the behavior of a decision maker, based on a set of external signal and the corresponding decision pairs. However, most inverse optimization algorithms are designed specifically in batch setting, where all the data is available in advance. As a consequence, there has been rare use of these methods in an online setting suitable for real-time applications. In this paper, we propose a general framework for inverse optimization through online learning. Specifically, we develop an online learning algorithm that uses an implicit update rule which can handle noisy data. Moreover, under additional regularity assumptions in terms of the data and the model, we prove that our algorithm converges at a rate of
O
(
1
/
√
T
)
O
and is statistically consistent. In our experiments, we show the online learning approach can learn the parameters with great accuracy and is very robust to noises, and achieves a dramatic improvement in computational efficacy over the batch learning approach."
neurips,https://proceedings.neurips.cc/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf,Sanity Checks for Saliency Maps,"Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, Been Kim",
neurips,https://proceedings.neurips.cc/paper/2018/file/296472c9542ad4d4788d543508116cbc-Paper.pdf,Differentially Private Uniformly Most Powerful Tests for Binomial Data,"Jordan Awan, Aleksandra Slavković",
neurips,https://proceedings.neurips.cc/paper/2018/file/2974788b53f73e7950e8aa49f3a306db-Paper.pdf,Bayesian Alignments of Warped Multi-Output Gaussian Processes,"Markus Kaiser, Clemens Otte, Thomas Runkler, Carl Henrik Ek",
neurips,https://proceedings.neurips.cc/paper/2018/file/29c0605a3bab4229e46723f89cf59d83-Paper.pdf,Semidefinite relaxations for certifying robustness to adversarial examples,"Aditi Raghunathan, Jacob Steinhardt, Percy S. Liang",
neurips,https://proceedings.neurips.cc/paper/2018/file/29c4a0e4ef7d1969a94a5f4aadd20690-Paper.pdf,Compact Representation of Uncertainty in Clustering,"Craig Greenberg, Nicholas Monath, Ari Kobren, Patrick Flaherty, Andrew McGregor, Andrew McCallum",
neurips,https://proceedings.neurips.cc/paper/2018/file/29daf9442f3c0b60642b14c081b4a556-Paper.pdf,DeepPINK: reproducible feature selection in deep neural networks,"Yang Lu, Yingying Fan, Jinchi Lv, William Stafford Noble",
neurips,https://proceedings.neurips.cc/paper/2018/file/2a38a4a9316c49e5a833517c45d31070-Paper.pdf,Supervised autoencoders: Improving generalization performance with unsupervised regularizers,"Lei Le, Andrew Patterson, Martha White",
neurips,https://proceedings.neurips.cc/paper/2018/file/2a845d4d23b883acb632fefd814e175f-Paper.pdf,Understanding Regularized Spectral Clustering via Graph Conductance,"Yilin Zhang, Karl Rohe",
neurips,https://proceedings.neurips.cc/paper/2018/file/2ad9e5e943e43cad612a7996c12a8796-Paper.pdf,Plug-in Estimation in High-Dimensional Linear Inverse Problems: A Rigorous Analysis,"Alyson K. Fletcher, Parthe Pandit, Sundeep Rangan, Subrata Sarkar, Philip Schniter","Estimating a vector
x
x
from noisy linear measurements
A
x
+
w
A
often requires use of prior knowledge or structural constraints on
x
x
for accurate reconstruction. Several recent works have considered combining linear least-squares estimation with a generic or plug-in
denoiser"" function that can be designed in a modular manner based on the prior knowledge about
x
x
. While these methods have shown excellent performance, it has been difficult to obtain rigorous performance guarantees. This work considers plug-in denoising combined with the recently-developed Vector Approximate Message Passing (VAMP) algorithm, which is itself derived via Expectation Propagation techniques. It shown that the mean squared error of this
plug-in"" VAMP can be exactly predicted for a large class of high-dimensional random
\Abf
\Abf
and denoisers. The method is illustrated in image reconstruction and parametric bilinear estimation."
neurips,https://proceedings.neurips.cc/paper/2018/file/2afe4567e1bf64d32a5527244d104cea-Paper.pdf,Multitask Boosting for Survival Analysis with Competing Risks,"Alexis Bellot, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2018/file/2b0aa0d9e30ea3a55fc271ced8364536-Paper.pdf,Deep Dynamical Modeling and Control of Unsteady Fluid Flows,"Jeremy Morton, Antony Jameson, Mykel J. Kochenderfer, Freddie Witherden",
neurips,https://proceedings.neurips.cc/paper/2018/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf,Learning Deep Disentangled Embeddings With the F-Statistic Loss,"Karl Ridgeway, Michael C. Mozer","Deep-embedding methods aim to discover representations of a domain that make explicit the domain's class structure and thereby support few-shot learning. Disentangling methods aim to make explicit compositional or factorial structure. We combine these two active but independent lines of research and propose a new paradigm suitable for both goals. We propose and evaluate a novel loss function based on the
F
F
statistic, which describes the separation of two or more distributions. By ensuring that distinct classes are well separated on a subset of embedding dimensions, we obtain embeddings that are useful for few-shot learning. By not requiring separation on all dimensions, we encourage the discovery of disentangled representations. Our embedding method matches or beats state-of-the-art, as evaluated by performance on recall@
k
k
and few-shot learning tasks. Our method also obtains performance superior to a variety of alternatives on disentangling, as evaluated by two key properties of a disentangled representation: modularity and explicitness. The goal of our work is to obtain more interpretable, manipulable, and generalizable deep representations of concepts and categories."
neurips,https://proceedings.neurips.cc/paper/2018/file/2b346a0aa375a07f5a90a344a61416c4-Paper.pdf,Disconnected Manifold Learning for Generative Adversarial Networks,"Mahyar Khayatkhoei, Maneesh K. Singh, Ahmed Elgammal",
neurips,https://proceedings.neurips.cc/paper/2018/file/2b64c2f19d868305aa8bbc2d72902cc5-Paper.pdf,Automating Bayesian optimization with Bayesian optimization,"Gustavo Malkomes, Roman Garnett",
neurips,https://proceedings.neurips.cc/paper/2018/file/2ba8698b79439589fdd2b0f7218d8b07-Paper.pdf,Leveraged volume sampling for linear regression,"Michal Derezinski, Manfred K. K. Warmuth, Daniel J. Hsu","Surprisingly we show that volume sampling can have poor behavior when we require a very accurate approximation -- indeed worse than some i.i.d. sampling techniques whose estimates are biased, such as leverage score sampling. We then develop a new rescaled variant of volume sampling that produces an unbiased estimate which avoids this bad behavior and has at least as good a tail bound as leverage score sampling: sample size k=O(d log d + d/epsilon) suffices to guarantee total loss at most 1+epsilon times the minimum with high probability. Thus, we improve on the best previously known sample size for an unbiased estimator, k=O(d^2/epsilon)."
neurips,https://proceedings.neurips.cc/paper/2018/file/2c3ddf4bf13852db711dd1901fb517fa-Paper.pdf,Scalable Robust Matrix Factorization with Nonconvex Loss,"Quanming Yao, James Kwok","Robust matrix factorization (RMF), which uses the
ℓ
1
ℓ
-loss, often outperforms standard matrix factorization using the
ℓ
2
ℓ
-loss, particularly when outliers are present. The state-of-the-art RMF solver is the RMF-MM algorithm, which, however, cannot utilize data sparsity. Moreover, sometimes even the (convex)
ℓ
1
ℓ
-loss is not robust enough. In this paper, we propose the use of nonconvex loss to enhance robustness. To address the resultant difficult optimization problem, we use majorization-minimization (MM) optimization and propose a new MM surrogate. To improve scalability, we exploit data sparsity and optimize the surrogate via its dual with the accelerated proximal gradient algorithm. The resultant algorithm has low time and space complexities and is guaranteed to converge to a critical point. Extensive experiments demonstrate its superiority over the state-of-the-art in terms of both accuracy and scalability."
neurips,https://proceedings.neurips.cc/paper/2018/file/2c6a0bae0f071cbbf0bb3d5b11d90a82-Paper.pdf,Pipe-SGD: A Decentralized Pipelined SGD Framework for Distributed Deep Net Training,"Youjie Li, Mingchao Yu, Songze Li, Salman Avestimehr, Nam Sung Kim, Alexander Schwing",
neurips,https://proceedings.neurips.cc/paper/2018/file/2c89109d42178de8a367c0228f169bf8-Paper.pdf,Wasserstein Variational Inference,"Luca Ambrogioni, Umut Güçlü, Yağmur Güçlütürk, Max Hinne, Marcel A. J. van Gerven, Eric Maris",
neurips,https://proceedings.neurips.cc/paper/2018/file/2cbd9c540641923027adb8ab89decc05-Paper.pdf,Bayesian Control of Large MDPs with Unknown Dynamics in Data-Poor Environments,"Mahdi Imani, Seyede Fatemeh Ghoreishi, Ulisses M. Braga-Neto",
neurips,https://proceedings.neurips.cc/paper/2018/file/2cfd4560539f887a5e420412b370b361-Paper.pdf,A Smoothed Analysis of the Greedy Algorithm for the Linear Contextual Bandit Problem,"Sampath Kannan, Jamie H. Morgenstern, Aaron Roth, Bo Waggoner, Zhiwei Steven Wu","We give a smoothed analysis, showing that even when contexts may be chosen by an adversary, small perturbations of the adversary's choices suffice for the algorithm to achieve
no regret'', perhaps (depending on the specifics of the setting) with a constant amount of initial training data. This suggests that in slightly perturbed environments, exploration and exploitation need not be in conflict in the linear setting."
neurips,https://proceedings.neurips.cc/paper/2018/file/2d969e2cee8cfa07ce7ca0bb13c7a36d-Paper.pdf,Lifelong Inverse Reinforcement Learning,"Jorge Mendez, Shashank Shivkumar, Eric Eaton",
neurips,https://proceedings.neurips.cc/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf,Recurrent World Models Facilitate Policy Evolution,"David Ha, Jürgen Schmidhuber",
neurips,https://proceedings.neurips.cc/paper/2018/file/2e2079d63348233d91cad1fa9b1361e9-Paper.pdf,Algorithms and Theory for Multiple-Source Adaptation,"Judy Hoffman, Mehryar Mohri, Ningshan Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/2e855f9489df0712b4bd8ea9e2848c5a-Paper.pdf,On preserving non-discrimination when combining expert advice,"Avrim Blum, Suriya Gunasekar, Thodoris Lykouris, Nati Srebro",
neurips,https://proceedings.neurips.cc/paper/2018/file/2e9f978b222a956ba6bdf427efbd9ab3-Paper.pdf,A Likelihood-Free Inference Framework for Population Genetic Data using Exchangeable Neural Networks,"Jeffrey Chan, Valerio Perrone, Jeffrey Spence, Paul Jenkins, Sara Mathieson, Yun Song",
neurips,https://proceedings.neurips.cc/paper/2018/file/2eace51d8f796d04991c831a07059758-Paper.pdf,The Price of Privacy for Low-rank Factorization,Jalaj Upadhyay,"Our algorithms when private matrix is updated in an arbitrary manner promise differential privacy with respect to two stronger privacy guarantees than previously studied, use space and time \emph{comparable} to the non-private algorithm, and achieve \emph{optimal accuracy}. To complement our positive results, we also prove that the space required by our algorithms is optimal up to logarithmic factors. When data matrices are distributed over multiple servers, we give a non-interactive differentially private algorithm with communication cost independent of dimension. In concise, we give algorithms that incur {\em optimal cost across all parameters of interest}. We also perform experiments to verify that all our algorithms perform well in practice and outperform the best known algorithm until now for large range of parameters."
neurips,https://proceedings.neurips.cc/paper/2018/file/2ecd2bd94734e5dd392d8678bc64cdab-Paper.pdf,Efficient Formal Safety Analysis of Neural Networks,"Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, Suman Jana",
neurips,https://proceedings.neurips.cc/paper/2018/file/2f25f6e326adb93c5787175dda209ab6-Paper.pdf,Inferring Networks From Random Walk-Based Node Similarities,"Jeremy Hoskins, Cameron Musco, Christopher Musco, Babis Tsourakakis","For the effective resistance metric, we show that with just a small subset of measurements, one can learn a large fraction of edges in a social network. We also show that it is possible to learn a graph which accurately matches the underlying network on all other effective resistances. This second observation is interesting from a data mining perspective, since it can be expensive to compute all effective resistances or other random walk-based similarities. As an alternative, our graphs learned from just a subset of effective resistances can be used as surrogates in a range of applications that use effective resistances to probe graph structure, including for graph clustering, node centrality evaluation, and anomaly detection."
neurips,https://proceedings.neurips.cc/paper/2018/file/2f37d10131f2a483a8dd005b3d14b0d9-Paper.pdf,Unsupervised Learning of View-invariant Action Representations,"Junnan Li, Yongkang Wong, Qi Zhao, Mohan S. Kankanhalli",
neurips,https://proceedings.neurips.cc/paper/2018/file/2fd0fd3efa7c4cfb034317b21f3c2d93-Paper.pdf,Extracting Relationships by Multi-Domain Matching,"Yitong Li, michael Murias, geraldine Dawson, David E. Carlson",
neurips,https://proceedings.neurips.cc/paper/2018/file/2fe5a27cde066c0b65acb8f2c1717464-Paper.pdf,"Distributed
k
k
-Clustering for Data with Heavy Noise","Shi Li, Xiangyu Guo","In this paper, we consider the
k
k
-center/median/means clustering with outliers problems (or the
(
k
,
z
)
(
-center/median/means problems) in the distributed setting. Most previous distributed algorithms have their communication costs linearly depending on
z
z
, the number of outliers. Recently Guha et al.[10] overcame this dependence issue by considering bi-criteria approximation algorithms that output solutions with
2
z
2
outliers. For the case where
z
z
is large, the extra
z
z
outliers discarded by the algorithms might be too large, considering that the data gathering process might be costly. In this paper, we improve the number of outliers to the best possible
(
1
+
ϵ
)
z
(
, while maintaining the
O
(
1
)
O
-approximation ratio and independence of communication cost on
z
z
. The problems we consider include the
(
k
,
z
)
(
-center problem, and
(
k
,
z
)
(
-median/means problems in Euclidean metrics. Implementation of the our algorithm for
(
k
,
z
)
(
-center shows that it outperforms many previous algorithms, both in terms of the communication cost and quality of the output solution."
neurips,https://proceedings.neurips.cc/paper/2018/file/300891a62162b960cf02ce3827bb363c-Paper.pdf,"Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic Corrections","Xin Zhang, Armando Solar-Lezama, Rishabh Singh",
neurips,https://proceedings.neurips.cc/paper/2018/file/3070e6addcd702cb58de5d7897bfdae1-Paper.pdf,Diverse Ensemble Evolution: Curriculum Data-Model Marriage,"Tianyi Zhou, Shengjie Wang, Jeff A. Bilmes","We study a new method (
Diverse Ensemble Evolution (DivE
2
2
)'') to train an ensemble of machine learning models that assigns data to models at each training epoch based on each model's current expertise and an intra- and inter-model diversity reward. DivE
2
2
schedules, over the course of training epochs, the relative importance of these characteristics; it starts by selecting easy samples for each model, and then gradually adjusts towards the models having specialized and complementary expertise on subsets of the training data, thereby encouraging high accuracy of the ensemble. We utilize an intra-model diversity term on data assigned to each model, and an inter-model diversity term on data assigned to pairs of models, to penalize both within-model and cross-model redundancy. We formulate the data-model marriage problem as a generalized bipartite matching, represented as submodular maximization subject to two matroid constraints. DivE
2
2
solves a sequence of continuous-combinatorial optimizations with slowly varying objectives and constraints. The combinatorial part handles the data-model marriage while the continuous part updates model parameters based on the assignments. In experiments, DivE
2
2
outperforms other ensemble training methods under a variety of model aggregation techniques, while also maintaining competitive efficiency."
neurips,https://proceedings.neurips.cc/paper/2018/file/309fee4e541e51de2e41f21bebb342aa-Paper.pdf,Q-learning with Nearest Neighbors,"Devavrat Shah, Qiaomin Xie","We consider model-free reinforcement learning for infinite-horizon discounted Markov Decision Processes (MDPs) with a continuous state space and unknown transition kernel, when only a single sample path under an arbitrary policy of the system is available. We consider the Nearest Neighbor Q-Learning (NNQL) algorithm to learn the optimal Q function using nearest neighbor regression method. As the main contribution, we provide tight finite sample analysis of the convergence rate. In particular, for MDPs with a
d
d
-dimensional state space and the discounted factor
γ
∈
(
0
,
1
)
γ
, given an arbitrary sample path with
covering time''
L
L
, we establish that the algorithm is guaranteed to output an
ε
ε
-accurate estimate of the optimal Q-function using
\Ot
(
L
/
(
ε
3
(
1
−
γ
)
7
)
)
\Ot
samples. For instance, for a well-behaved MDP, the covering time of the sample path under the purely random policy scales as
\Ot
(
1
/
ε
d
)
,
\Ot
so the sample complexity scales as
\Ot
(
1
/
ε
d
+
3
)
.
\Ot
Indeed, we establish a lower bound that argues that the dependence of
\Omegat
(
1
/
ε
d
+
2
)
\Omegat
is necessary."
neurips,https://proceedings.neurips.cc/paper/2018/file/310ce61c90f3a46e340ee8257bc70e93-Paper.pdf,Modular Networks: Learning to Decompose Neural Computation,"Louis Kirsch, Julius Kunze, David Barber",
neurips,https://proceedings.neurips.cc/paper/2018/file/314450613369e0ee72d0da7f6fee773c-Paper.pdf,The Convergence of Sparsified Gradient Methods,"Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, Cedric Renggli","This is the question we address in this paper. We prove that, under analytic assumptions, sparsifying gradients by magnitude with local error correction provides convergence guarantees, for both convex and non-convex smooth objectives, for data-parallel SGD. The main insight is that sparsification methods implicitly maintain bounds on the maximum impact of stale updates, thanks to selection by magnitude. Our analysis and empirical validation also reveal that these methods do require analytical conditions to converge well, justifying existing heuristics."
neurips,https://proceedings.neurips.cc/paper/2018/file/31f81674a348511b990af268ca3a8391-Paper.pdf,Deepcode: Feedback Codes via Deep Learning,"Hyeji Kim, Yihan Jiang, Sreeram Kannan, Sewoong Oh, Pramod Viswanath","We break this logjam by integrating information theoretic insights harmoniously with recurrent-neural-network based encoders and decoders to create novel codes that outperform known codes by 3 orders of magnitude in reliability. We also demonstrate several desirable properties in the codes: (a) generalization to larger block lengths; (b) composability with known codes; (c) adaptation to practical constraints. This result also presents broader ramifications to coding theory: even when the channel has a clear mathematical model, deep learning methodologies, when combined with channel specific information-theoretic insights, can potentially beat state-of-the-art codes, constructed over decades of mathematical research."
neurips,https://proceedings.neurips.cc/paper/2018/file/31fefc0e570cb3860f2a6d4b38c6490d-Paper.pdf,Chain of Reasoning for Visual Question Answering,"Chenfei Wu, Jinlai Liu, Xiaojie Wang, Xuan Dong",
neurips,https://proceedings.neurips.cc/paper/2018/file/3202111cf90e7c816a472aaceb72b0df-Paper.pdf,Hamiltonian Variational Auto-Encoder,"Anthony L. Caterini, Arnaud Doucet, Dino Sejdinovic",
neurips,https://proceedings.neurips.cc/paper/2018/file/322f62469c5e3c7dc3e58f5a4d1ea399-Paper.pdf,Unorganized Malicious Attacks Detection,"Ming Pang, Wei Gao, Min Tao, Zhi-Hua Zhou",
neurips,https://proceedings.neurips.cc/paper/2018/file/32b991e5d77ad140559ffb95522992d0-Paper.pdf,Differentially Private k-Means with Constant Multiplicative Error,"Uri Stemmer, Haim Kaplan","Although the problem has been widely studied in the context of differential privacy, all of the existing constructions achieve only super constant approximation factors. We present, for the first time, efficient private algorithms for the problem with constant multiplicative error. Furthermore, we show how to modify our algorithms so they compute private coresets for k-means clustering in both models."
neurips,https://proceedings.neurips.cc/paper/2018/file/32bbf7b2bc4ed14eb1e9c2580056a989-Paper.pdf,Multi-value Rule Sets for Interpretable Classification with Feature-Efficient Representations,Tong Wang,
neurips,https://proceedings.neurips.cc/paper/2018/file/32fdab6559cdfa4f167f8c31b9199643-Paper.pdf,Provable Gaussian Embedding with One Observation,"Ming Yu, Zhuoran Yang, Tuo Zhao, Mladen Kolar, Zhaoran Wang",
neurips,https://proceedings.neurips.cc/paper/2018/file/331316d4efb44682092a006307b9ae3a-Paper.pdf,Contamination Attacks and Mitigation in Multi-Party Machine Learning,"Jamie Hayes, Olga Ohrimenko",
neurips,https://proceedings.neurips.cc/paper/2018/file/3328bdf9a4b9504b9398284244fe97c2-Paper.pdf,Gradient Sparsification for Communication-Efficient Distributed Optimization,"Jianqiao Wangni, Jialei Wang, Ji Liu, Tong Zhang","Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed computational architectures. A key bottleneck is the communication overhead for exchanging information such as stochastic gradients among different workers. In this paper, to reduce the communication cost, we propose a convex optimization formulation to minimize the coding length of stochastic gradients. The key idea is to randomly drop out coordinates of the stochastic gradient vectors and amplify the remaining coordinates appropriately to ensure the sparsified gradient to be unbiased. To solve the optimal sparsification efficiently, several simple and fast algorithms are proposed for an approximate solution, with a theoretical guarantee for sparseness. Experiments on
ℓ
2
ℓ
regularized logistic regression, support vector machines, and convolutional neural networks validate our sparsification approaches."
neurips,https://proceedings.neurips.cc/paper/2018/file/335cd1b90bfa4ee70b39d08a4ae0cf2d-Paper.pdf,The promises and pitfalls of Stochastic Gradient Langevin Dynamics,"Nicolas Brosse, Alain Durmus, Eric Moulines",
neurips,https://proceedings.neurips.cc/paper/2018/file/335d3d1cd7ef05ec77714a215134914c-Paper.pdf,Training Deep Neural Networks with 8-bit Floating Point Numbers,"Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, Kailash Gopalakrishnan",
neurips,https://proceedings.neurips.cc/paper/2018/file/33b3214d792caf311e1f00fd22b392c5-Paper.pdf,ATOMO: Communication-efficient Learning via Atomic Sparsification,"Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary Charles, Dimitris Papailiopoulos, Stephen Wright",
neurips,https://proceedings.neurips.cc/paper/2018/file/34306d99c63613fad5b2a140398c0420-Paper.pdf,Depth-Limited Solving for Imperfect-Information Games,"Noam Brown, Tuomas Sandholm, Brandon Amos",
neurips,https://proceedings.neurips.cc/paper/2018/file/347665597cbfaef834886adbb848011f-Paper.pdf,Causal Inference and Mechanism Clustering of A Mixture of Additive Noise Models,"Shoubo Hu, Zhitang Chen, Vahid Partovi Nia, Laiwan CHAN, Yanhui Geng",
neurips,https://proceedings.neurips.cc/paper/2018/file/3483e5ec0489e5c394b028ec4e81f3e1-Paper.pdf,Adversarial Risk and Robustness: General Definitions and Implications for the Uniform Distribution,"Dimitrios Diochnos, Saeed Mahloujifar, Mohammad Mahmoody","As the current literature contains multiple definitions of adversarial risk and robustness, we start by giving a taxonomy for these definitions based on their direct goals; we identify one of them as the one guaranteeing misclassification by pushing the instances to the error region. We then study some classic algorithms for learning monotone conjunctions and compare their adversarial risk and robustness under different definitions by attacking the hypotheses using instances drawn from the uniform distribution. We observe that sometimes these definitions lead to significantly different bounds. Thus, this study advocates for the use of the error-region definition, even though other definitions, in other contexts with context-dependent assumptions, may coincide with the error-region definition."
neurips,https://proceedings.neurips.cc/paper/2018/file/349f36aa789af083b8e26839bd498af9-Paper.pdf,Analysis of Krylov Subspace Solutions of Regularized Non-Convex Quadratic Problems,"Yair Carmon, John C. Duchi","We provide convergence rates for Krylov subspace solutions to the trust-region and cubic-regularized (nonconvex) quadratic problems. Such solutions may be efficiently computed by the Lanczos method and have long been used in practice. We prove error bounds of the form
1
/
t
2
1
and
e
−
4
t
/
√
κ
e
, where
κ
κ
is a condition number for the problem, and
t
t
is the Krylov subspace order (number of Lanczos iterations). We also provide lower bounds showing that our analysis is sharp."
neurips,https://proceedings.neurips.cc/paper/2018/file/34adeb8e3242824038aa65460a47c29e-Paper.pdf,Efficient Anomaly Detection via Matrix Sketching,"Vatsal Sharan, Parikshit Gopalan, Udi Wieder",
neurips,https://proceedings.neurips.cc/paper/2018/file/34e157766f31db3d2099831d348a7933-Paper.pdf,Backpropagation with Callbacks: Foundations for Efficient and Expressive Differentiable Programming,"Fei Wang, James Decker, Xilun Wu, Gregory Essertel, Tiark Rompf","In this paper we propose an implementation of backpropagation using functions with callbacks, where the forward pass is executed as a sequence of function calls, and the backward pass as a corresponding sequence of function returns. A key realization is that this technique of chaining callbacks is well known in the programming languages community as continuation-passing style (CPS). Any program can be converted to this form using standard techniques, and hence, any program can be mechanically converted to compute gradients."
neurips,https://proceedings.neurips.cc/paper/2018/file/34ffeb359a192eb8174b6854643cc046-Paper.pdf,Constrained Cross-Entropy Method for Safe Reinforcement Learning,"Min Wen, Ufuk Topcu",
neurips,https://proceedings.neurips.cc/paper/2018/file/351869bde8b9d6ad1e3090bd173f600d-Paper.pdf,Graphical model inference: Sequential Monte Carlo meets deterministic approximations,"Fredrik Lindsten, Jouni Helske, Matti Vihola",
neurips,https://proceedings.neurips.cc/paper/2018/file/35309226eb45ec366ca86a4329a2b7c3-Paper.pdf,Playing hard exploration games by watching YouTube,"Yusuf Aytar, Tobias Pfaff, David Budden, Thomas Paine, Ziyu Wang, Nando de Freitas",
neurips,https://proceedings.neurips.cc/paper/2018/file/3569df159ec477451530c4455b2a9e86-Paper.pdf,Improved Algorithms for Collaborative PAC Learning,"Huy Nguyen, Lydia Zakynthinou","We study a recent model of collaborative PAC learning where
k
k
players with
k
k
different tasks collaborate to learn a single classifier that works for all tasks. Previous work showed that when there is a classifier that has very small error on all tasks, there is a collaborative algorithm that finds a single classifier for all tasks and has
O
(
(
ln
(
k
)
)
2
)
O
times the worst-case sample complexity for learning a single task. In this work, we design new algorithms for both the realizable and the non-realizable setting, having sample complexity only
O
(
ln
(
k
)
)
O
times the worst-case sample complexity for learning a single task. The sample complexity upper bounds of our algorithms match previous lower bounds and in some range of parameters are even better than previous algorithms that are allowed to output different classifiers for different tasks."
neurips,https://proceedings.neurips.cc/paper/2018/file/358f9e7be09177c17d0d17ff73584307-Paper.pdf,Scaling provable adversarial defenses,"Eric Wong, Frank Schmidt, Jan Hendrik Metzen, J. Zico Kolter","Recent work has developed methods for learning deep network classifiers that are \emph{provably} robust to norm-bounded adversarial perturbation; however, these methods are currently only possible for relatively small feedforward networks. In this paper, in an effort to scale these approaches to substantially larger models, we extend previous work in three main directly. First, we present a technique for extending these training procedures to much more general networks, with skip connections (such as ResNets) and general nonlinearities; the approach is fully modular, and can be implemented automatically analogously to automatic differentiation. Second, in the specific case of
ℓ
∞
ℓ
adversarial perturbations and networks with ReLU nonlinearities, we adopt a nonlinear random projection for training, which scales \emph{linearly} in the number of hidden units (previous approached scaled quadratically). Third, we show how to further improve robust error through cascade models. On both MNIST and CIFAR data sets, we train classifiers that improve substantially on the state of the art in provable robust adversarial error bounds: from 5.8% to 3.1% on MNIST (with
ℓ
∞
ℓ
perturbations of
ϵ
=
0.1
ϵ
), and from 80% to 36.4% on CIFAR (with
ℓ
∞
ℓ
perturbations of
ϵ
=
2
/
255
ϵ
)."
neurips,https://proceedings.neurips.cc/paper/2018/file/36072923bfc3cf47745d704feb489480-Paper.pdf,Understanding Batch Normalization,"Nils Bjorck, Carla P. Gomes, Bart Selman, Kilian Q. Weinberger",
neurips,https://proceedings.neurips.cc/paper/2018/file/366f0bc7bd1d4bf414073cabbadfdfcd-Paper.pdf,Navigating with Graph Representations for Fast and Scalable Decoding of Neural Language Models,"Minjia Zhang, Wenhan Wang, Xiaodong Liu, Jianfeng Gao, Yuxiong He",
neurips,https://proceedings.neurips.cc/paper/2018/file/36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf,Learning from discriminative feature feedback,"Sanjoy Dasgupta, Akansha Dey, Nicholas Roberts, Sivan Sabato",
neurips,https://proceedings.neurips.cc/paper/2018/file/36d7534290610d9b7e9abed244dd2f28-Paper.pdf,Zeroth-order (Non)-Convex Stochastic Optimization via Conditional Gradient and Gradient Updates,"Krishnakumar Balasubramanian, Saeed Ghadimi",
neurips,https://proceedings.neurips.cc/paper/2018/file/36f4d832825380f102846560a5104c90-Paper.pdf,Coordinate Descent with Bandit Sampling,"Farnood Salehi, Patrick Thiran, Elisa Celis",
neurips,https://proceedings.neurips.cc/paper/2018/file/386854131f58a556343e056f03626e00-Paper.pdf,PAC-Bayes bounds for stable algorithms with instance-dependent priors,"Omar Rivasplata, Emilio Parrado-Hernandez, John S. Shawe-Taylor, Shiliang Sun, Csaba Szepesvari",
neurips,https://proceedings.neurips.cc/paper/2018/file/389bc7bb1e1c2a5e7e147703232a88f6-Paper.pdf,DropMax: Adaptive Variational Softmax,"Hae Beom Lee, Juho Lee, Saehoon Kim, Eunho Yang, Sung Ju Hwang",
neurips,https://proceedings.neurips.cc/paper/2018/file/39027dfad5138c9ca0c474d71db915c3-Paper.pdf,Multi-Layered Gradient Boosting Decision Trees,"Ji Feng, Yang Yu, Zhi-Hua Zhou",
neurips,https://proceedings.neurips.cc/paper/2018/file/39059724f73a9969845dfe4146c5660e-Paper.pdf,Rest-Katyusha: Exploiting the Solution's Structure via Scheduled Restart Schemes,"Junqi Tang, Mohammad Golbabaee, Francis Bach, Mike E. davies",
neurips,https://proceedings.neurips.cc/paper/2018/file/390e982518a50e280d8e2b535462ec1f-Paper.pdf,Automatic Program Synthesis of Long Programs with a Learned Garbage Collector,"Amit Zohar, Lior Wolf",
neurips,https://proceedings.neurips.cc/paper/2018/file/39461a19e9eddfb385ea76b26521ea48-Paper.pdf,Quantifying Learning Guarantees for Convex but Inconsistent Surrogates,"Kirill Struminsky, Simon Lacoste-Julien, Anton Osokin",
neurips,https://proceedings.neurips.cc/paper/2018/file/398475c83b47075e8897a083e97eb9f0-Paper.pdf,Unsupervised Text Style Transfer using Language Models as Discriminators,"Zichao Yang, Zhiting Hu, Chris Dyer, Eric P. Xing, Taylor Berg-Kirkpatrick",
neurips,https://proceedings.neurips.cc/paper/2018/file/39ae2ed11b14a4ccb41d35e9d1ba5d11-Paper.pdf,Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation,"Edward Smith, Scott Fujimoto, David Meger",
neurips,https://proceedings.neurips.cc/paper/2018/file/39e98420b5e98bfbdc8a619bef7b8f61-Paper.pdf,Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions,"Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers, Philip Versteeg, Joris M. Mooij",
neurips,https://proceedings.neurips.cc/paper/2018/file/3a09a524440d44d7f19870070a5ad42f-Paper.pdf,Confounding-Robust Policy Improvement,"Nathan Kallus, Angela Zhou",
neurips,https://proceedings.neurips.cc/paper/2018/file/3a20f62a0af1aa152670bab3c602feed-Paper.pdf,Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes,"Ronan Fruit, Matteo Pirotta, Alessandro Lazaric","While designing the state space of an MDP, it is common to include states that are transient or not reachable by any policy (e.g., in mountain car, the product space of speed and position contains configurations that are not physically reachable). This results in weakly-communicating or multi-chain MDPs. In this paper, we introduce TUCRL, the first algorithm able to perform efficient exploration-exploitation in any finite Markov Decision Process (MDP) without requiring any form of prior knowledge. In particular, for any MDP with
S
c
S
communicating states,
A
A
actions and
Γ
c
≤
S
c
Γ
possible communicating next states, we derive a
O
(
D
c
√
Γ
c
S
c
A
T
)
r
e
g
r
e
t
b
o
u
n
d
,
w
h
e
r
e
O
D^c$ is the diameter (i.e., the length of the longest shortest path between any two states) of the communicating part of the MDP. This is in contrast with optimistic algorithms (e.g., UCRL, Optimistic PSRL) that suffer linear regret in weakly-communicating MDPs, as well as posterior sampling or regularised algorithms (e.g., REGAL), which require prior knowledge on the bias span of the optimal policy to bias the exploration to achieve sub-linear regret. We also prove that in weakly-communicating MDPs, no algorithm can ever achieve a logarithmic growth of the regret without first suffering a linear regret for a number of steps that is exponential in the parameters of the MDP. Finally, we report numerical simulations supporting our theoretical findings and showing how TUCRL overcomes the limitations of the state-of-the-art."
neurips,https://proceedings.neurips.cc/paper/2018/file/3a37abdeefe1dab1b30f7c5c7e581b93-Paper.pdf,Mesh-TensorFlow: Deep Learning for Supercomputers,"Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, Blake Hechtman",
neurips,https://proceedings.neurips.cc/paper/2018/file/3b3dbaf68507998acd6a5a5254ab2d76-Paper.pdf,Foreground Clustering for Joint Segmentation and Localization in Videos and Images,Abhishek Sharma,
neurips,https://proceedings.neurips.cc/paper/2018/file/3b5020bb891119b9f5130f1fea9bd773-Paper.pdf,Privacy Amplification by Subsampling: Tight Analyses via Couplings and Divergences,"Borja Balle, Gilles Barthe, Marco Gaboardi",
neurips,https://proceedings.neurips.cc/paper/2018/file/3b712de48137572f3849aabd5666a4e3-Paper.pdf,The Description Length of Deep Learning models,"Léonard Blier, Yann Ollivier",
neurips,https://proceedings.neurips.cc/paper/2018/file/3c1e4bd67169b8153e0047536c9f541e-Paper.pdf,Semi-crowdsourced Clustering with Deep Generative Models,"Yucen Luo, TIAN TIAN, Jiaxin Shi, Jun Zhu, Bo Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/3d387d2612f9027154ed3b99a7427da1-Paper.pdf,Scalable Laplacian K-modes,"Imtiaz Ziko, Eric Granger, Ismail Ben Ayed",
neurips,https://proceedings.neurips.cc/paper/2018/file/3d863b367aa379f71c7afc0c9cdca41d-Paper.pdf,Early Stopping for Nonparametric Testing,"Meimei Liu, Guang Cheng",
neurips,https://proceedings.neurips.cc/paper/2018/file/3db11d259a9db7fb8965bdf25ec850b9-Paper.pdf,Non-Adversarial Mapping with VAEs,Yedid Hoshen,
neurips,https://proceedings.neurips.cc/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf,Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,"Kurtland Chua, Roberto Calandra, Rowan McAllister, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2018/file/3e441eec3456b703a4fe741005f3981f-Paper.pdf,The challenge of realistic music generation: modelling raw audio at scale,"Sander Dieleman, Aaron van den Oord, Karen Simonyan",
neurips,https://proceedings.neurips.cc/paper/2018/file/3e60e09c222f206c725385f53d7e567c-Paper.pdf,Approximation algorithms for stochastic clustering,"David Harris, Shi Li, Aravind Srinivasan, Khoa Trinh, Thomas Pensyl",
neurips,https://proceedings.neurips.cc/paper/2018/file/3e9e39fed3b8369ed940f52cf300cf88-Paper.pdf,Inexact trust-region algorithms on Riemannian manifolds,"Hiroyuki Kasai, Bamdev Mishra",
neurips,https://proceedings.neurips.cc/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf,Towards Robust Interpretability with Self-Explaining Neural Networks,"David Alvarez Melis, Tommi Jaakkola",
neurips,https://proceedings.neurips.cc/paper/2018/file/3ea2db50e62ceefceaf70a9d9a56a6f4-Paper.pdf,Predictive Uncertainty Estimation via Prior Networks,"Andrey Malinin, Mark Gales",
neurips,https://proceedings.neurips.cc/paper/2018/file/3ec27c2cff04bc5fd2586ca36c62044e-Paper.pdf,"Graph Oracle Models, Lower Bounds, and Gaps for Parallel Stochastic Optimization","Blake E. Woodworth, Jialei Wang, Adam Smith, Brendan McMahan, Nati Srebro",
neurips,https://proceedings.neurips.cc/paper/2018/file/3ef815416f775098fe977004015c6193-Paper.pdf,An Off-policy Policy Gradient Theorem Using Emphatic Weightings,"Ehsan Imani, Eric Graves, Martha White",
neurips,https://proceedings.neurips.cc/paper/2018/file/3f998e713a6e02287c374fd26835d87e-Paper.pdf,Multiple-Step Greedy Policies in Approximate and Online Reinforcement Learning,"Yonathan Efroni, Gal Dalal, Bruno Scherrer, Shie Mannor",
neurips,https://proceedings.neurips.cc/paper/2018/file/3fab5890d8113d0b5a4178201dc842ad-Paper.pdf,Scaling the Poisson GLM to massive neural datasets through polynomial approximations,"David Zoltowski, Jonathan W. Pillow",
neurips,https://proceedings.neurips.cc/paper/2018/file/3fb04953d95a94367bb133f862402bce-Paper.pdf,Hybrid Macro/Micro Level Backpropagation for Training Deep Spiking Neural Networks,"Yingyezhe Jin, Wenrui Zhang, Peng Li",
neurips,https://proceedings.neurips.cc/paper/2018/file/3fc2c60b5782f641f76bcefc39fb2392-Paper.pdf,Differential Properties of Sinkhorn Approximation for Learning with Wasserstein Distance,"Giulia Luise, Alessandro Rudi, Massimiliano Pontil, Carlo Ciliberto",
neurips,https://proceedings.neurips.cc/paper/2018/file/3fd60983292458bf7dee75f12d5e9e05-Paper.pdf,"The Cluster Description Problem - Complexity Results, Formulations and Approximations","Ian Davidson, Antoine Gourru, S Ravi","Consider the situation where you are given an existing
k
k
-way clustering
π
π
. A challenge for explainable AI is to find a compact and distinct explanations of each cluster which in this paper is using instance-level descriptors/tags from a common dictionary. Since the descriptors/tags were not given to the clustering method, this is not a semi-supervised learning situation. We show that the \emph{feasibility} problem of just testing whether any distinct description (not the most compact) exists is generally intractable for just two clusters. This means that unless \textbf{P} = \cnp, there cannot exist an efficient algorithm for the cluster description problem. Hence, we explore ILP formulations for smaller problems and a relaxed but restricted setting that leads to a polynomial time algorithm for larger problems. We explore several extension to the basic setting such as the ability to ignore some instances and composition constraints on the descriptions of the clusters. We show our formulation's usefulness on Twitter data where the communities were found using social connectivity (i.e. \texttt{follower} relation) but the explanation of the communities is based on behavioral properties of the nodes (i.e. hashtag usage) not available to the clustering method."
neurips,https://proceedings.neurips.cc/paper/2018/file/3ffebb08d23c609875d7177ee769a3e9-Paper.pdf,Global Non-convex Optimization with Discretized Diffusions,"Murat A. Erdogdu, Lester Mackey, Ohad Shamir",
neurips,https://proceedings.neurips.cc/paper/2018/file/403ea2e851b9ab04a996beab4a480a30-Paper.pdf,Contextual Pricing for Lipschitz Buyers,"Jieming Mao, Renato Leme, Jon Schneider","We investigate the problem of learning a Lipschitz function from binary feedback. In this problem, a learner is trying to learn a Lipschitz function
f
:
[
0
,
1
]
d
→
[
0
,
1
]
f
over the course of
T
T
rounds. On round
t
t
, an adversary provides the learner with an input
x
t
x
, the learner submits a guess
y
t
y
for
f
(
x
t
)
f
, and learns whether
y
t
>
f
(
x
t
)
y
or
y
t
≤
f
(
x
t
)
y
. The learner's goal is to minimize their total loss
∑
t
ℓ
(
f
(
x
t
)
,
y
t
)
∑
(for some loss function
ℓ
ℓ
). The problem is motivated by \textit{contextual dynamic pricing}, where a firm must sell a stream of differentiated products to a collection of buyers with non-linear valuations for the items and observes only whether the item was sold or not at the posted price. For the symmetric loss
ℓ
(
f
(
x
t
)
,
y
t
)
=
|
f
(
x
t
)
−
y
t
|
ℓ
, we provide an algorithm for this problem achieving total loss
O
(
log
T
)
O
when
d
=
1
d
and
O
(
T
(
d
−
1
)
/
d
)
O
when
d
>
1
d
, and show that both bounds are tight (up to a factor of
√
log
T
log
). For the pricing loss function
ℓ
(
f
(
x
t
)
,
y
t
)
=
f
(
x
t
)
−
y
t
1
{
y
t
≤
f
(
x
t
)
}
ℓ
we show a regret bound of
O
(
T
d
/
(
d
+
1
)
)
O
and show that this bound is tight. We present improved bounds in the special case of a population of linear buyers."
neurips,https://proceedings.neurips.cc/paper/2018/file/411ae1bf081d1674ca6091f8c59a266f-Paper.pdf,Processing of missing data by neural networks,"Marek Śmieja, Łukasz Struski, Jacek Tabor, Bartosz Zieliński, Przemysław Spurek",
neurips,https://proceedings.neurips.cc/paper/2018/file/415185ea244ea2b2bedeb0449b926802-Paper.pdf,Invariant Representations without Adversarial Training,"Daniel Moyer, Shuyang Gao, Rob Brekelmans, Aram Galstyan, Greg Ver Steeg",
neurips,https://proceedings.neurips.cc/paper/2018/file/4172f3101212a2009c74b547b6ddf935-Paper.pdf,Inference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo,"Marton Havasi, José Miguel Hernández-Lobato, Juan José Murillo-Fuentes",
neurips,https://proceedings.neurips.cc/paper/2018/file/41f860e3b7f548abc1f8b812059137bf-Paper.pdf,Regret bounds for meta Bayesian optimization with an unknown Gaussian process prior,"Zi Wang, Beomjoon Kim, Leslie Pack Kaelbling",
neurips,https://proceedings.neurips.cc/paper/2018/file/42299f06ee419aa5d9d07798b56779e2-Paper.pdf,Fully Neural Network Based Speech Recognition on Mobile and Embedded Devices,"Jinhwan Park, Yoonho Boo, Iksoo Choi, Sungho Shin, Wonyong Sung",
neurips,https://proceedings.neurips.cc/paper/2018/file/42998cf32d552343bc8e460416382dca-Paper.pdf,Large Margin Deep Networks for Classification,"Gamaleldin Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, Samy Bengio","We present a formulation of deep learning that aims at producing a large margin classifier. The notion of \emc{margin}, minimum distance to a decision boundary, has served as the foundation of several theoretically profound and empirically successful results for both classification and regression tasks. However, most large margin algorithms are applicable only to shallow models with a preset feature representation; and conventional margin methods for neural networks only enforce margin at the output layer. Such methods are therefore not well suited for deep networks. In this work, we propose a novel loss function to impose a margin on any chosen set of layers of a deep network (including input and hidden layers). Our formulation allows choosing any
l
p
l
norm (
p
≥
1
p
) on the metric measuring the margin. We demonstrate that the decision boundary obtained by our loss has nice properties compared to standard classification loss functions. Specifically, we show improved empirical results on the MNIST, CIFAR-10 and ImageNet datasets on multiple tasks: generalization from small training sets, corrupted labels, and robustness against adversarial perturbations. The resulting loss is general and complementary to existing data augmentation (such as random/adversarial input transform) and regularization techniques such as weight decay, dropout, and batch norm. \footnote{Code for the large margin loss function is released at \url{https://github.com/google-research/google-research/tree/master/large_margin}}"
neurips,https://proceedings.neurips.cc/paper/2018/file/430c3626b879b4005d41b8a46172e0c0-Paper.pdf,Collaborative Learning for Deep Neural Networks,"Guocong Song, Wei Chai",
neurips,https://proceedings.neurips.cc/paper/2018/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf,Multi-Task Learning as Multi-Objective Optimization,"Ozan Sener, Vladlen Koltun",
neurips,https://proceedings.neurips.cc/paper/2018/file/43feaeeecd7b2fe2ae2e26d917b6477d-Paper.pdf,Learning to Exploit Stability for 3D Scene Parsing,"Yilun Du, Zhijian Liu, Hector Basevi, Ales Leonardis, Bill Freeman, Josh Tenenbaum, Jiajun Wu",
neurips,https://proceedings.neurips.cc/paper/2018/file/44968aece94f667e4095002d140b5896-Paper.pdf,Direct Runge-Kutta Discretization Achieves Acceleration,"Jingzhao Zhang, Aryan Mokhtari, Suvrit Sra, Ali Jadbabaie","We study gradient-based optimization methods obtained by directly discretizing a second-order ordinary differential equation (ODE) related to the continuous limit of Nesterov's accelerated gradient method. When the function is smooth enough, we show that acceleration can be achieved by a stable discretization of this ODE using standard Runge-Kutta integrators. Specifically, we prove that under Lipschitz-gradient, convexity and order-
(
s
+
2
)
(
differentiability assumptions, the sequence of iterates generated by discretizing the proposed second-order ODE converges to the optimal solution at a rate of
O
(
N
−
2
s
s
+
1
)
O
, where
s
s
is the order of the Runge-Kutta numerical integrator. Furthermore, we introduce a new local flatness condition on the objective, under which rates even faster than
O
(
N
−
2
)
O
can be achieved with low-order integrators and only gradient information. Notably, this flatness condition is satisfied by several standard loss functions used in machine learning. We provide numerical experiments that verify the theoretical rates predicted by our results."
neurips,https://proceedings.neurips.cc/paper/2018/file/44feb0096faa8326192570788b38c1d1-Paper.pdf,Communication Compression for Decentralized Training,"Hanlin Tang, Shaoduo Gan, Ce Zhang, Tong Zhang, Ji Liu","Optimizing distributed learning systems is an art of balancing between computation and communication. There have been two lines of research that try to deal with slower networks: {\em communication compression} for low bandwidth networks, and {\em decentralization} for high latency networks. In this paper, We explore a natural question: {\em can the combination of both techniques lead to a system that is robust to both bandwidth and latency?} Although the system implication of such combination is trivial, the underlying theoretical principle and algorithm design is challenging: unlike centralized algorithms, simply compressing {\rc exchanged information, even in an unbiased stochastic way, within the decentralized network would accumulate the error and cause divergence.} In this paper, we develop a framework of quantized, decentralized training and propose two different strategies, which we call {\em extrapolation compression} and {\em difference compression}. We analyze both algorithms and prove both converge at the rate of
O
(
1
/
√
n
T
)
O
where
n
n
is the number of workers and
T
T
is the number of iterations, matching the convergence rate for full precision, centralized training. We validate our algorithms and find that our proposed algorithm outperforms the best of merely decentralized and merely quantized algorithm significantly for networks with {\em both} high latency and low bandwidth."
neurips,https://proceedings.neurips.cc/paper/2018/file/4559912e7a94a9c32b09d894f2bc3c82-Paper.pdf,Neural Voice Cloning with a Few Samples,"Sercan Arik, Jitong Chen, Kainan Peng, Wei Ping, Yanqi Zhou",
neurips,https://proceedings.neurips.cc/paper/2018/file/45a766fa266ea2ebeb6680fa139d2a3d-Paper.pdf,Low-Rank Tucker Decomposition of Large Tensors Using TensorSketch,"Osman Asif Malik, Stephen Becker",
neurips,https://proceedings.neurips.cc/paper/2018/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf,But How Does It Work in Theory? Linear SVM with Random Features,"Yitong Sun, Anna Gilbert, Ambuj Tewari","We prove that, under low noise assumptions, the support vector machine with
N
≪
m
N
random features (RFSVM) can achieve the learning rate faster than
O
(
1
/
√
m
)
O
on a training set with
m
m
samples when an optimized feature map is used. Our work extends the previous fast rate analysis of random features method from least square loss to 0-1 loss. We also show that the reweighted feature selection method, which approximates the optimized feature map, helps improve the performance of RFSVM in experiments on a synthetic data set."
neurips,https://proceedings.neurips.cc/paper/2018/file/470e7a4f017a5476afb7eeb3f8b96f9b-Paper.pdf,Designing by Training: Acceleration Neural Network for Fast High-Dimensional Convolution,"Longquan Dai, Liang Tang, Yuan Xie, Jinhui Tang","The high-dimensional convolution is widely used in various disciplines but has a serious performance problem due to its high computational complexity. Over the decades, people took a handmade approach to design fast algorithms for the Gaussian convolution. Recently, requirements for various non-Gaussian convolutions have emerged and are continuously getting higher. However, the handmade acceleration approach is no longer feasible for so many different convolutions since it is a time-consuming and painstaking job. Instead, we propose an Acceleration Network (AccNet) which turns the work of designing new fast algorithms to training the AccNet. This is done by: 1, interpreting splatting, blurring, slicing operations as convolutions; 2, turning these convolutions to
g
g
CP layers to build AccNet. After training, the activation function
g
g
together with AccNet weights automatically define the new splatting, blurring and slicing operations. Experiments demonstrate AccNet is able to design acceleration algorithms for a ton of convolutions including Gaussian/non-Gaussian convolutions and produce state-of-the-art results."
neurips,https://proceedings.neurips.cc/paper/2018/file/473447ac58e1cd7e96172575f48dca3b-Paper.pdf,A Probabilistic U-Net for Segmentation of Ambiguous Images,"Simon Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey De Fauw, Joseph R. Ledsam, Klaus Maier-Hein, S. M. Ali Eslami, Danilo Jimenez Rezende, Olaf Ronneberger",
neurips,https://proceedings.neurips.cc/paper/2018/file/47fd3c87f42f55d4b233417d49c34783-Paper.pdf,Bandit Learning in Concave N-Person Games,"Mario Bravo, David Leslie, Panayotis Mertikopoulos",
neurips,https://proceedings.neurips.cc/paper/2018/file/48000647b315f6f00f913caa757a70b3-Paper.pdf,Fast Approximate Natural Gradient Descent in a Kronecker Factored Eigenbasis,"Thomas George, César Laurent, Xavier Bouthillier, Nicolas Ballas, Pascal Vincent",
neurips,https://proceedings.neurips.cc/paper/2018/file/482db0ecc10b8a9984ae850c9ada9899-Paper.pdf,A convex program for bilinear inversion of sparse vectors,"Alireza Aghasi, Ali Ahmed, Paul Hand, Babhru Joshi",
neurips,https://proceedings.neurips.cc/paper/2018/file/485843481a7edacbfce101ecb1e4d2a8-Paper.pdf,Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks,"Yusuke Tsuzuku, Issei Sato, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2018/file/486fbd761bfa5400722324fdc9822adc-Paper.pdf,Robust Learning of Fixed-Structure Bayesian Networks,"Yu Cheng, Ilias Diakonikolas, Daniel Kane, Alistair Stewart","We investigate the problem of learning Bayesian networks in a robust model where an
ϵ
ϵ
-fraction of the samples are adversarially corrupted. In this work, we study the fully observable discrete case where the structure of the network is given. Even in this basic setting, previous learning algorithms either run in exponential time or lose dimension-dependent factors in their error guarantees. We provide the first computationally efficient robust learning algorithm for this problem with dimension-independent error guarantees. Our algorithm has near-optimal sample complexity, runs in polynomial time, and achieves error that scales nearly-linearly with the fraction of adversarially corrupted samples. Finally, we show on both synthetic and semi-synthetic data that our algorithm performs well in practice."
neurips,https://proceedings.neurips.cc/paper/2018/file/488e4104520c6aab692863cc1dba45af-Paper.pdf,3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data,"Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, Taco S. Cohen",
neurips,https://proceedings.neurips.cc/paper/2018/file/48ab2f9b45957ab574cf005eb8a76760-Paper.pdf,Toddler-Inspired Visual Object Learning,"Sven Bambach, David Crandall, Linda Smith, Chen Yu",
neurips,https://proceedings.neurips.cc/paper/2018/file/48db71587df6c7c442e5b76cc723169a-Paper.pdf,Reducing Network Agnostophobia,"Akshay Raj Dhamija, Manuel Günther, Terrance Boult",
neurips,https://proceedings.neurips.cc/paper/2018/file/49182f81e6a13cf5eaa496d51fea6406-Paper.pdf,Deep Functional Dictionaries: Learning Consistent Semantic Structures on 3D Models from Functions,"Minhyuk Sung, Hao Su, Ronald Yu, Leonidas J. Guibas",
neurips,https://proceedings.neurips.cc/paper/2018/file/4928e7510f45da6575b04a28519c09ed-Paper.pdf,Teaching Inverse Reinforcement Learners via Features and Demonstrations,"Luis Haug, Sebastian Tschiatschek, Adish Singla",
neurips,https://proceedings.neurips.cc/paper/2018/file/496e05e1aea0a9c4655800e8a7b9ea28-Paper.pdf,Learning to Decompose and Disentangle Representations for Video Prediction,"Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li F. Fei-Fei, Juan Carlos Niebles",
neurips,https://proceedings.neurips.cc/paper/2018/file/498f2c21688f6451d9f5fd09d53edda7-Paper.pdf,Maximizing acquisition functions for Bayesian optimization,"James Wilson, Frank Hutter, Marc Deisenroth",
neurips,https://proceedings.neurips.cc/paper/2018/file/4996dcc43b5be197b5887a4e60817b1c-Paper.pdf,Nonparametric Density Estimation under Adversarial Losses,"Shashank Singh, Ananya Uppal, Boyue Li, Chun-Liang Li, Manzil Zaheer, Barnabas Poczos",
neurips,https://proceedings.neurips.cc/paper/2018/file/49af6c4e558a7569d80eee2e035e2bd7-Paper.pdf,Weakly Supervised Dense Event Captioning in Videos,"Xuguang Duan, Wenbing Huang, Chuang Gan, Jingdong Wang, Wenwu Zhu, Junzhou Huang",
neurips,https://proceedings.neurips.cc/paper/2018/file/49b8b4f95f02e055801da3b4f58e28b7-Paper.pdf,Moonshine: Distilling with Cheap Convolutions,"Elliot J. Crowley, Gavin Gray, Amos J. Storkey",
neurips,https://proceedings.neurips.cc/paper/2018/file/4a1590df1d5968d41b855005bb8b67bf-Paper.pdf,Exploiting Numerical Sparsity for Efficient Learning : Faster Eigenvector Computation and Regression,"Neha Gupta, Aaron Sidford","In this paper, we obtain improved running times for regression and top eigenvector computation for numerically sparse matrices. Given a data matrix
\mat
A
∈
\R
n
×
d
\mat
where every row
a
∈
\R
d
a
has
∥
a
∥
2
2
≤
L
‖
and numerical sparsity
≤
s
≤
, i.e.
∥
a
∥
2
1
/
∥
a
∥
2
2
≤
s
‖
, we provide faster algorithms for these problems for many parameter settings. For top eigenvector computation, when
\gap
>
0
\gap
is the relative gap between the top two eigenvectors of
\mat
A
⊤
\mat
A
\mat
and
r
r
is the stable rank of
\mat
A
\mat
we obtain a running time of
\otilde
(
n
d
+
r
(
s
+
√
r
s
)
/
\gap
2
)
\otilde
improving upon the previous best unaccelerated running time of
O
(
n
d
+
r
d
/
\gap
2
)
O
. As
r
≤
d
r
and
s
≤
d
s
our algorithm everywhere improves or matches the previous bounds for all parameter settings. For regression, when
μ
>
0
μ
is the smallest eigenvalue of
\mat
A
⊤
\mat
A
\mat
we obtain a running time of
\otilde
(
n
d
+
(
n
L
/
μ
)
√
s
n
L
/
μ
)
\otilde
improving upon the previous best unaccelerated running time of
\otilde
(
n
d
+
n
L
d
/
μ
)
\otilde
. This result expands when regression can be solved in nearly linear time from when
L
/
μ
=
\otilde
(
1
)
L
to when
L
/
μ
=
\otilde
(
d
2
/
3
/
(
s
n
)
1
/
3
)
L
. Furthermore, we obtain similar improvements even when row norms and numerical sparsities are non-uniform and we show how to achieve even faster running times by accelerating using approximate proximal point \cite{frostig2015regularizing} / catalyst \cite{lin2015universal}. Our running times depend only on the size of the input and natural numerical measures of the matrix, i.e. eigenvalues and
ℓ
p
ℓ
norms, making progress on a key open problem regarding optimal running times for efficient large-scale learning."
neurips,https://proceedings.neurips.cc/paper/2018/file/4ad13f04ef4373992c9d3046200aa350-Paper.pdf,The Everlasting Database: Statistical Validity at a Fair Price,"Blake E. Woodworth, Vitaly Feldman, Saharon Rosset, Nati Srebro","The problem of handling adaptivity in data analysis, intentional or not, permeates a variety of fields, including test-set overfitting in ML challenges and the accumulation of invalid scientific discoveries. We propose a mechanism for answering an arbitrarily long sequence of potentially adaptive statistical queries, by charging a price for each query and using the proceeds to collect additional samples. Crucially, we guarantee statistical validity without any assumptions on how the queries are generated. We also ensure with high probability that the cost for
M
M
non-adaptive queries is
O
(
log
M
)
O
, while the cost to a potentially adaptive user who makes
M
M
queries that do not depend on any others is
O
(
√
M
)
O
."
neurips,https://proceedings.neurips.cc/paper/2018/file/4aeae10ea1c6433c926cdfa558d31134-Paper.pdf,Learning Conditioned Graph Structures for Interpretable Visual Question Answering,"Will Norcliffe-Brown, Stathis Vafeias, Sarah Parisot",
neurips,https://proceedings.neurips.cc/paper/2018/file/4aec1b3435c52abbdf8334ea0e7141e0-Paper.pdf,Exponentially Weighted Imitation Learning for Batched Historical Data,"Qing Wang, Jiechao Xiong, Lei Han, peng sun, Han Liu, Tong Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/4b0250793549726d5c1ea3906726ebfe-Paper.pdf,Temporal Regularization for Markov Decision Process,"Pierre Thodoroff, Audrey Durand, Joelle Pineau, Doina Precup",
neurips,https://proceedings.neurips.cc/paper/2018/file/4b4edc2630fe75800ddc29a7b4070add-Paper.pdf,Explaining Deep Learning Models -- A Bayesian Non-parametric Approach,"Wenbo Guo, Sui Huang, Yunzhe Tao, Xinyu Xing, Lin Lin",
neurips,https://proceedings.neurips.cc/paper/2018/file/4ba3c163cd1efd4c14e3a415fa0a3010-Paper.pdf,Optimization of Smooth Functions with Noisy Observations: Local Minimax Rates,"Yining Wang, Sivaraman Balakrishnan, Aarti Singh",
neurips,https://proceedings.neurips.cc/paper/2018/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf,Measures of distortion for machine learning,"Leena Chennuru Vankadara, Ulrike von Luxburg","Given data from a general metric space, one of the standard machine learning pipelines is to first embed the data into a Euclidean space and subsequently apply out of the box machine learning algorithms to analyze the data. The quality of such an embedding is typically described in terms of a distortion measure. In this paper, we show that many of the existing distortion measures behave in an undesired way, when considered from a machine learning point of view. We investigate desirable properties of distortion measures and formally prove that most of the existing measures fail to satisfy these properties. These theoretical findings are supported by simulations, which for example demonstrate that existing distortion measures are not robust to noise or outliers and cannot serve as good indicators for classification accuracy. As an alternative, we suggest a new measure of distortion, called
σ
σ
-distortion. We can show both in theory and in experiments that it satisfies all desirable properties and is a better candidate to evaluate distortion in the context of machine learning."
neurips,https://proceedings.neurips.cc/paper/2018/file/4c5bde74a8f110656874902f07378009-Paper.pdf,Sparse DNNs with Improved Adversarial Robustness,"Yiwen Guo, Chao Zhang, Changshui Zhang, Yurong Chen","Deep neural networks (DNNs) are computationally/memory-intensive and vulnerable to adversarial attacks, making them prohibitive in some real-world applications. By converting dense models into sparse ones, pruning appears to be a promising solution to reducing the computation/memory cost. This paper studies classification models, especially DNN-based ones, to demonstrate that there exists intrinsic relationships between their sparsity and adversarial robustness. Our analyses reveal, both theoretically and empirically, that nonlinear DNN-based classifiers behave differently under
l
2
l
attacks from some linear ones. We further demonstrate that an appropriately higher model sparsity implies better robustness of nonlinear DNNs, whereas over-sparsified models can be more difficult to resist adversarial examples."
neurips,https://proceedings.neurips.cc/paper/2018/file/4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf,e-SNLI: Natural Language Inference with Natural Language Explanations,"Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, Phil Blunsom",
neurips,https://proceedings.neurips.cc/paper/2018/file/4d19b37a2c399deace9082d464930022-Paper.pdf,Synaptic Strength For Convolutional Neural Network,"CHEN LIN, Zhao Zhong, Wu Wei, Junjie Yan",
neurips,https://proceedings.neurips.cc/paper/2018/file/4de754248c196c85ee4fbdcee89179bd-Paper.pdf,Meta-Reinforcement Learning of Structured Exploration Strategies,"Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2018/file/4e2a6330465c8ffcaa696a5a16639176-Paper.pdf,Gamma-Poisson Dynamic Matrix Factorization Embedded with Metadata Influence,"Trong Dinh Thac Do, Longbing Cao",
neurips,https://proceedings.neurips.cc/paper/2018/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Paper.pdf,A Block Coordinate Ascent Algorithm for Mean-Variance Optimization,"Tengyang Xie, Bo Liu, Yangyang Xu, Mohammad Ghavamzadeh, Yinlam Chow, Daoming Lyu, Daesub Yoon",
neurips,https://proceedings.neurips.cc/paper/2018/file/4e4e53aa080247bc31d0eb4e7aeb07a0-Paper.pdf,MetaGAN: An Adversarial Approach to Few-Shot Learning,"Ruixiang ZHANG, Tong Che, Zoubin Ghahramani, Yoshua Bengio, Yangqiu Song",
neurips,https://proceedings.neurips.cc/paper/2018/file/4e5046fc8d6a97d18a5f54beaed54dea-Paper.pdf,Efficient High Dimensional Bayesian Optimization with Additivity and Quadrature Fourier Features,"Mojmir Mutny, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2018/file/4e62e752ae53fb6a6eebd0f6146aa702-Paper.pdf,The Physical Systems Behind Optimization Algorithms,"Lin Yang, Raman Arora, Vladimir braverman, Tuo Zhao",
neurips,https://proceedings.neurips.cc/paper/2018/file/4e8412ad48562e3c9934f45c3e144d48-Paper.pdf,Unsupervised Learning of Shape and Pose with Differentiable Point Clouds,"Eldar Insafutdinov, Alexey Dosovitskiy",
neurips,https://proceedings.neurips.cc/paper/2018/file/4e87337f366f72daa424dae11df0538c-Paper.pdf,Unsupervised Attention-guided Image-to-Image Translation,"Youssef Alami Mejjati, Christian Richardt, James Tompkin, Darren Cosker, Kwang In Kim",
neurips,https://proceedings.neurips.cc/paper/2018/file/4ebccfb3e317c7789f04f7a558df4537-Paper.pdf,Learning Concave Conditional Likelihood Models for Improved Analysis of Tandem Mass Spectra,"John T. Halloran, David M. Rocke",
neurips,https://proceedings.neurips.cc/paper/2018/file/4efb80f630ccecb2d3b9b2087b0f9c89-Paper.pdf,Beyond Grids: Learning Graph Representations for Visual Recognition,"Yin Li, Abhinav Gupta",
neurips,https://proceedings.neurips.cc/paper/2018/file/4f164cf233807fc02da06599a1264dee-Paper.pdf,Approximate Knowledge Compilation by Online Collapsed Importance Sampling,"Tal Friedman, Guy Van den Broeck",
neurips,https://proceedings.neurips.cc/paper/2018/file/4fb8a7a22a82c80f2c26fe6c1e0dcbb3-Paper.pdf,Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation,"Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen, Tie-Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2018/file/4fe5149039b52765bde64beb9f674940-Paper.pdf,A Lyapunov-based Approach to Safe Reinforcement Learning,"Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, Mohammad Ghavamzadeh",
neurips,https://proceedings.neurips.cc/paper/2018/file/4ff6fa96179cdc2838e8d8ce64cd10a7-Paper.pdf,Reversible Recurrent Neural Networks,"Matthew MacKay, Paul Vicol, Jimmy Ba, Roger B. Grosse",
neurips,https://proceedings.neurips.cc/paper/2018/file/4ffb0d2ba92f664c2281970110a2e071-Paper.pdf,Deep Poisson gamma dynamical systems,"Dandan Guo, Bo Chen, Hao Zhang, Mingyuan Zhou",
neurips,https://proceedings.neurips.cc/paper/2018/file/500e75a036dc2d7d2fec5da1b71d36cc-Paper.pdf,Regularization Learning Networks: Deep Learning for Tabular Datasets,"Ira Shavitt, Eran Segal",
neurips,https://proceedings.neurips.cc/paper/2018/file/50905d7b2216bfeccb5b41016357176b-Paper.pdf,Online Learning with an Unknown Fairness Metric,"Stephen Gillen, Christopher Jung, Michael Kearns, Aaron Roth",
neurips,https://proceedings.neurips.cc/paper/2018/file/51174add1c52758f33d414ceaf3fe6ba-Paper.pdf,Completing State Representations using Spectral Learning,"Nan Jiang, Alex Kulesza, Satinder Singh",
neurips,https://proceedings.neurips.cc/paper/2018/file/5129a5ddcd0dcd755232baa04c231698-Paper.pdf,From Stochastic Planning to Marginal MAP,"Hao(Jackson) Cui, Radu Marinescu, Roni Khardon",
neurips,https://proceedings.neurips.cc/paper/2018/file/51d92be1c60d1db1d2e5e7a07da55b26-Paper.pdf,Generalizing Graph Matching beyond Quadratic Assignment Model,"Tianshu Yu, Junchi Yan, Yilin Wang, Wei Liu, baoxin Li",
neurips,https://proceedings.neurips.cc/paper/2018/file/51de85ddd068f0bc787691d356176df9-Paper.pdf,On Learning Intrinsic Rewards for Policy Gradient Methods,"Zeyu Zheng, Junhyuk Oh, Satinder Singh",
neurips,https://proceedings.neurips.cc/paper/2018/file/522a9ae9a99880d39e5daec35375e999-Paper.pdf,Regularizing by the Variance of the Activations' Sample-Variances,"Etai Littwin, Lior Wolf",
neurips,https://proceedings.neurips.cc/paper/2018/file/52c5189391854c93e8a0e1326e56c14f-Paper.pdf,Single-Agent Policy Tree Search With Guarantees,"Laurent Orseau, Levi Lelis, Tor Lattimore, Theophane Weber",
neurips,https://proceedings.neurips.cc/paper/2018/file/5317b6799188715d5e00a638a4278901-Paper.pdf,Bias and Generalization in Deep Generative Models: An Empirical Study,"Shengjia Zhao, Hongyu Ren, Arianna Yuan, Jiaming Song, Noah Goodman, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2018/file/53edebc543333dfbf7c5933af792c9c4-Paper.pdf,Joint Autoregressive and Hierarchical Priors for Learned Image Compression,"David Minnen, Johannes Ballé, George D. Toderici",
neurips,https://proceedings.neurips.cc/paper/2018/file/53f0d7c537d99b3824f0f99d62ea2428-Paper.pdf,Link Prediction Based on Graph Neural Networks,"Muhan Zhang, Yixin Chen","Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a
heuristic'' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel
γ
γ
-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the
γ
γ
-decaying theory, we propose a new method to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems."
neurips,https://proceedings.neurips.cc/paper/2018/file/53fde96fcc4b4ce72d7739202324cd49-Paper.pdf,A flexible model for training action localization with varying levels of supervision,"Guilhem Chéron, Jean-Baptiste Alayrac, Ivan Laptev, Cordelia Schmid",
neurips,https://proceedings.neurips.cc/paper/2018/file/5401acfe633e6817b508b84d23686743-Paper.pdf,A probabilistic population code based on neural samples,"Sabyasachi Shivkumar, Richard Lange, Ankani Chattoraj, Ralf Haefner",
neurips,https://proceedings.neurips.cc/paper/2018/file/5421e013565f7f1afa0cfe8ad87a99ab-Paper.pdf,Generative Probabilistic Novelty Detection with Adversarial Autoencoders,"Stanislav Pidhorskyi, Ranya Almohsen, Gianfranco Doretto",
neurips,https://proceedings.neurips.cc/paper/2018/file/54c3d58c5efcf59ddeb7486b7061ea5a-Paper.pdf,Monte-Carlo Tree Search for Constrained POMDPs,"Jongmin Lee, Geon-hyeong Kim, Pascal Poupart, Kee-Eung Kim",
neurips,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf,Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,"Yuanzhi Li, Yingyu Liang",
neurips,https://proceedings.neurips.cc/paper/2018/file/550a141f12de6341fba65b0ad0433500-Paper.pdf,Informative Features for Model Comparison,"Wittawat Jitkrittum, Heishiro Kanagawa, Patsorn Sangkloy, James Hays, Bernhard Schölkopf, Arthur Gretton",
neurips,https://proceedings.neurips.cc/paper/2018/file/55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf,Discrimination-aware Channel Pruning for Deep Neural Networks,"Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, Jinhui Zhu",
neurips,https://proceedings.neurips.cc/paper/2018/file/55acf8539596d25624059980986aaa78-Paper.pdf,Reinforcement Learning of Theorem Proving,"Cezary Kaliszyk, Josef Urban, Henryk Michalewski, Miroslav Olšák",
neurips,https://proceedings.neurips.cc/paper/2018/file/56584778d5a8ab88d6393cc4cd11e090-Paper.pdf,On Fast Leverage Score Sampling and Optimal Learning,"Alessandro Rudi, Daniele Calandriello, Luigi Carratino, Lorenzo Rosasco",
neurips,https://proceedings.neurips.cc/paper/2018/file/565e8a413d0562de9ee4378402d2b481-Paper.pdf,Robustness of conditional GANs to noisy labels,"Kiran K. Thekumparampil, Ashish Khetan, Zinan Lin, Sewoong Oh",
neurips,https://proceedings.neurips.cc/paper/2018/file/566f0ea4f6c2e947f36795c8f58ba901-Paper.pdf,Removing Hidden Confounding by Experimental Grounding,"Nathan Kallus, Aahlad Manas Puli, Uri Shalit",
neurips,https://proceedings.neurips.cc/paper/2018/file/56a3107cad6611c8337ee36d178ca129-Paper.pdf,Legendre Decomposition for Tensors,"Mahito Sugiyama, Hiroyuki Nakahara, Koji Tsuda",
neurips,https://proceedings.neurips.cc/paper/2018/file/56bd37d3a2fda0f2f41925019c81011d-Paper.pdf,Bilevel learning of the Group Lasso structure,"Jordan Frecon, Saverio Salzo, Massimiliano Pontil",
neurips,https://proceedings.neurips.cc/paper/2018/file/56dc0997d871e9177069bb472574eb29-Paper.pdf,SING: Symbol-to-Instrument Neural Generator,"Alexandre Defossez, Neil Zeghidour, Nicolas Usunier, Leon Bottou, Francis Bach",
neurips,https://proceedings.neurips.cc/paper/2018/file/56e6a93212e4482d99c84a639d254b67-Paper.pdf,Forecasting Treatment Responses Over Time Using Recurrent Marginal Structural Networks,Bryan Lim,
neurips,https://proceedings.neurips.cc/paper/2018/file/57c0531e13f40b91b3b0f1a30b529a1d-Paper.pdf,Optimal Subsampling with Influence Functions,"Daniel Ting, Eric Brochu",
neurips,https://proceedings.neurips.cc/paper/2018/file/58238e9ae2dd305d79c2ebc8c1883422-Paper.pdf,LinkNet: Relational Embedding for Scene Graph,"Sanghyun Woo, Dahun Kim, Donghyeon Cho, In So Kweon",
neurips,https://proceedings.neurips.cc/paper/2018/file/584b98aac2dddf59ee2cf19ca4ccb75e-Paper.pdf,Meta-Learning MCMC Proposals,"Tongzhou Wang, YI WU, Dave Moore, Stuart J. Russell",
neurips,https://proceedings.neurips.cc/paper/2018/file/586f9b4035e5997f77635b13cc04984c-Paper.pdf,Bayesian Adversarial Learning,"Nanyang Ye, Zhanxing Zhu",
neurips,https://proceedings.neurips.cc/paper/2018/file/58a2fc6ed39fd083f55d4182bf88826d-Paper.pdf,Bayesian Pose Graph Optimization via Bingham Distributions and Tempered Geodesic MCMC,"Tolga Birdal, Umut Simsekli, Mustafa Onur Eken, Slobodan Ilic",
neurips,https://proceedings.neurips.cc/paper/2018/file/58ae749f25eded36f486bc85feb3f0ab-Paper.pdf,Quadratic Decomposable Submodular Function Minimization,"Pan Li, Niao He, Olgica Milenkovic",
neurips,https://proceedings.neurips.cc/paper/2018/file/59a3adea76fadcb6dd9e54c96fc155d1-Paper.pdf,An Improved Analysis of Alternating Minimization for Structured Multi-Response Regression,"Sheng Chen, Arindam Banerjee",
neurips,https://proceedings.neurips.cc/paper/2018/file/59ab3ba90ae4b4ab84fe69de7b8e3f5f-Paper.pdf,Uniform Convergence of Gradients for Non-Convex Learning and Optimization,"Dylan J. Foster, Ayush Sekhari, Karthik Sridharan","Moving to non-smooth models we show----in contrast to the smooth case---that even for a single ReLU it is not possible to obtain dimension-independent convergence rates for gradients in the worst case. On the positive side, it is still possible to obtain dimension-independent rates under a new type of distributional assumption."
neurips,https://proceedings.neurips.cc/paper/2018/file/59b90e1005a220e2ebc542eb9d950b1e-Paper.pdf,Posterior Concentration for Sparse Deep Learning,"Nicholas G. Polson, Veronika Ročková",
neurips,https://proceedings.neurips.cc/paper/2018/file/59e0b2658e9f2e77f8d4d83f8d07ca84-Paper.pdf,Sequence-to-Segment Networks for Segment Detection,"Zijun Wei, Boyu Wang, Minh Hoai Nguyen, Jianming Zhang, Zhe Lin, Xiaohui Shen, Radomir Mech, Dimitris Samaras","Detecting segments of interest from an input sequence is a challenging problem which often requires not only good knowledge of individual target segments, but also contextual understanding of the entire input sequence and the relationships between the target segments. To address this problem, we propose the Sequence-to-Segment Network (S
2
2
N), a novel end-to-end sequential encoder-decoder architecture. S
2
2
N first encodes the input into a sequence of hidden states that progressively capture both local and holistic information. It then employs a novel decoding architecture, called Segment Detection Unit (SDU), that integrates the decoder state and encoder hidden states to detect segments sequentially. During training, we formulate the assignment of predicted segments to ground truth as bipartite matching and use the Earth Mover's Distance to calculate the localization errors. We experiment with S
2
2
N on temporal action proposal generation and video summarization and show that S
2
2
N achieves state-of-the-art performance on both tasks."
neurips,https://proceedings.neurips.cc/paper/2018/file/5a378f8490c8d6af8647a753812f6e31-Paper.pdf,Multi-Agent Reinforcement Learning via Double Averaging Primal-Dual Optimization,"Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, Mingyi Hong",
neurips,https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf,Neural Tangent Kernel: Convergence and Generalization in Neural Networks,"Arthur Jacot, Franck Gabriel, Clement Hongler","We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping."
neurips,https://proceedings.neurips.cc/paper/2018/file/5a7b238ba0f6502e5d6be14424b20ded-Paper.pdf,Randomized Prior Functions for Deep Reinforcement Learning,"Ian Osband, John Aslanides, Albin Cassirer",
neurips,https://proceedings.neurips.cc/paper/2018/file/5a9d8bf5b7a4b35f3110dde8673bdda2-Paper.pdf,On the Convergence and Robustness of Training GANs with Regularized Optimal Transport,"Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, Jason D. Lee",
neurips,https://proceedings.neurips.cc/paper/2018/file/5abdf8b8520b71f3a528c7547ee92428-Paper.pdf,Uncertainty Sampling is Preconditioned Stochastic Gradient Descent on Zero-One Loss,"Stephen Mussmann, Percy S. Liang",
neurips,https://proceedings.neurips.cc/paper/2018/file/5b8e4fd39d9786228649a8a8bec4e008-Paper.pdf,Negotiable Reinforcement Learning for Pareto Optimal Sequential Decision-Making,"Nishant Desai, Andrew Critch, Stuart J. Russell",
neurips,https://proceedings.neurips.cc/paper/2018/file/5c936263f3428a40227908d5a3847c0b-Paper.pdf,Distributed Stochastic Optimization via Adaptive SGD,"Ashok Cutkosky, Róbert Busa-Fekete",
neurips,https://proceedings.neurips.cc/paper/2018/file/5cc3749a6e56ef6d656735dff9176074-Paper.pdf,Smoothed Analysis of Discrete Tensor Decomposition and Assemblies of Neurons,"Nima Anari, Constantinos Daskalakis, Wolfgang Maass, Christos Papadimitriou, Amin Saberi, Santosh Vempala","Assemblies are large sets of neurons representing specific memories or concepts. The size of the intersection of two assemblies has been shown in experiments to represent the extent to which these memories co-occur or these concepts are related; the phenomenon is called association of assemblies. This suggests that an animal's memory is a complex web of associations, and poses the problem of recovering this representation from cognitive data. Motivated by this problem, we study the following more general question: Can we reconstruct the Venn diagram of a family of sets, given the sizes of their l-wise intersections? We show that as long as the family of sets is randomly perturbed, it is enough for the number of measurements to be polynomially larger than the number of nonempty regions of the Venn diagram to fully reconstruct the diagram."
neurips,https://proceedings.neurips.cc/paper/2018/file/5cf68969fb67aa6082363a6d4e6468e2-Paper.pdf,Deep State Space Models for Time Series Forecasting,"Syama Sundar Rangapuram, Matthias W. Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, Tim Januschowski",
neurips,https://proceedings.neurips.cc/paper/2018/file/5d50d22735a7469266aab23fd8aeb536-Paper.pdf,Learning Temporal Point Processes via Reinforcement Learning,"Shuang Li, Shuai Xiao, Shixiang Zhu, Nan Du, Yao Xie, Le Song",
neurips,https://proceedings.neurips.cc/paper/2018/file/5dbc8390f17e019d300d5a162c3ce3bc-Paper.pdf,GLoMo: Unsupervised Learning of Transferable Relational Graphs,"Zhilin Yang, Jake Zhao, Bhuwan Dhingra, Kaiming He, William W. Cohen, Russ R. Salakhutdinov, Yann LeCun",
neurips,https://proceedings.neurips.cc/paper/2018/file/5e388103a391daabe3de1d76a6739ccd-Paper.pdf,Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding,"Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Josh Tenenbaum",
neurips,https://proceedings.neurips.cc/paper/2018/file/5e62d03aec0d17facfc5355dd90d441c-Paper.pdf,Deep Anomaly Detection Using Geometric Transformations,"Izhak Golan, Ran El-Yaniv",
neurips,https://proceedings.neurips.cc/paper/2018/file/5f0f5e5f33945135b874349cfbed4fb9-Paper.pdf,On Oracle-Efficient PAC RL with Rich Observations,"Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, Robert E. Schapire",
neurips,https://proceedings.neurips.cc/paper/2018/file/5f93f983524def3dca464469d2cf9f3e-Paper.pdf,Joint Sub-bands Learning with Clique Structures for Wavelet Domain Super-Resolution,"Zhisheng Zhong, Tiancheng Shen, Yibo Yang, Zhouchen Lin, Chao Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/5fc34ed307aac159a30d81181c99847e-Paper.pdf,Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation,"Liwei Wang, Lunjia Hu, Jiayuan Gu, Zhiqiang Hu, Yue Wu, Kun He, John Hopcroft",
neurips,https://proceedings.neurips.cc/paper/2018/file/5fd0245f6c9ddbdf3eff0f505975b6a7-Paper.pdf,Non-delusional Q-learning and value-iteration,"Tyler Lu, Dale Schuurmans, Craig Boutilier",
neurips,https://proceedings.neurips.cc/paper/2018/file/60106888f8977b71e1f15db7bc9a88d1-Paper.pdf,An intriguing failing of convolutional neural networks and the CoordConv solution,"Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, Jason Yosinski",
neurips,https://proceedings.neurips.cc/paper/2018/file/60243f9b1ac2dba11ff8131c8f4431e0-Paper.pdf,Adversarially Robust Optimization with Gaussian Processes,"Ilija Bogunovic, Jonathan Scarlett, Stefanie Jegelka, Volkan Cevher",
neurips,https://proceedings.neurips.cc/paper/2018/file/602d1305678a8d5fdb372271e980da6a-Paper.pdf,Learning Hierarchical Semantic Image Manipulation through Structured Representations,"Seunghoon Hong, Xinchen Yan, Thomas S. Huang, Honglak Lee",
neurips,https://proceedings.neurips.cc/paper/2018/file/61d009da208a34ae155420e55f97abc7-Paper.pdf,Neural Proximal Gradient Descent for Compressive Imaging,"Morteza Mardani, Qingyun Sun, David Donoho, Vardan Papyan, Hatef Monajemi, Shreyas Vasanawala, John Pauly",
neurips,https://proceedings.neurips.cc/paper/2018/file/61d77652c97ef636343742fc3dcf3ba9-Paper.pdf,Power-law efficient neural codes provide general link between perceptual bias and discriminability,"Michael Morais, Jonathan W. Pillow",
neurips,https://proceedings.neurips.cc/paper/2018/file/61f2585b0ebcf1f532c4d1ec9a7d51aa-Paper.pdf,Stochastic Nonparametric Event-Tensor Decomposition,"Shandian Zhe, Yishuai Du",
neurips,https://proceedings.neurips.cc/paper/2018/file/6211080fa89981f66b1a0c9d55c61d0f-Paper.pdf,A Smoother Way to Train Structured Prediction Models,"Venkata Krishna Pillutla, Vincent Roulet, Sham M. Kakade, Zaid Harchaoui",
neurips,https://proceedings.neurips.cc/paper/2018/file/62da8c91ce7b10846231921795d6059e-Paper.pdf,Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural Networks,"Xiaodong Cui, Wei Zhang, Zoltán Tüske, Michael Picheny",
neurips,https://proceedings.neurips.cc/paper/2018/file/63bfd6e8f26d1d3537f4c5038264ef36-Paper.pdf,On Coresets for Logistic Regression,"Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, David Woodruff","Coresets are one of the central methods to facilitate the analysis of large data. We continue a recent line of research applying the theory of coresets to logistic regression. First, we show the negative result that no strongly sublinear sized coresets exist for logistic regression. To deal with intractable worst-case instances we introduce a complexity measure
μ
(
X
)
μ
, which quantifies the hardness of compressing a data set for logistic regression.
μ
(
X
)
μ
has an intuitive statistical interpretation that may be of independent interest. For data sets with bounded
μ
(
X
)
μ
-complexity, we show that a novel sensitivity sampling scheme produces the first provably sublinear
(
1
±
\eps
)
(
-coreset. We illustrate the performance of our method by comparing to uniform sampling as well as to state of the art methods in the area. The experiments are conducted on real world benchmark data for logistic regression."
neurips,https://proceedings.neurips.cc/paper/2018/file/63c3ddcc7b23daa1e42dc41f9a44a873-Paper.pdf,Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures,"Sergey Bartunov, Adam Santoro, Blake Richards, Luke Marris, Geoffrey E. Hinton, Timothy Lillicrap",
neurips,https://proceedings.neurips.cc/paper/2018/file/64223ccf70bbb65a3a4aceac37e21016-Paper.pdf,3D-Aware Scene Manipulation via Inverse Graphics,"Shunyu Yao, Tzu Ming Hsu, Jun-Yan Zhu, Jiajun Wu, Antonio Torralba, Bill Freeman, Josh Tenenbaum",
neurips,https://proceedings.neurips.cc/paper/2018/file/645098b086d2f9e1e0e939c27f9f2d6f-Paper.pdf,Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning,"Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J. Mankowitz, Shie Mannor",
neurips,https://proceedings.neurips.cc/paper/2018/file/6459257ddab7b85bf4b57845e875e4d4-Paper.pdf,Connecting Optimization and Regularization Paths,"Arun Suggala, Adarsh Prasad, Pradeep K. Ravikumar",
neurips,https://proceedings.neurips.cc/paper/2018/file/6463c88460bd63bbe256e495c63aa40b-Paper.pdf,A Theory-Based Evaluation of Nearest Neighbor Models Put Into Practice,"Hendrik Fichtenberger, Dennis Rohde","In the
k
k
-nearest neighborhood model (
k
k
-NN), we are given a set of points
P
P
, and we shall answer queries
q
q
by returning the
k
k
nearest neighbors of
q
q
in
P
P
according to some metric. This concept is crucial in many areas of data analysis and data processing, e.g., computer vision, document retrieval and machine learning. Many
k
k
-NN algorithms have been published and implemented, but often the relation between parameters and accuracy of the computed
k
k
-NN is not explicit. We study property testing of
k
k
-NN graphs in theory and evaluate it empirically: given a point set
P
⊂
R
δ
P
and a directed graph
G
=
(
P
,
E
)
G
, is
G
G
a
k
k
-NN graph, i.e., every point
p
∈
P
p
has outgoing edges to its
k
k
nearest neighbors, or is it
ϵ
ϵ
-far from being a
k
k
-NN graph? Here,
ϵ
ϵ
-far means that one has to change more than an
ϵ
ϵ
-fraction of the edges in order to make
G
G
a
k
k
-NN graph. We develop a randomized algorithm with one-sided error that decides this question, i.e., a property tester for the
k
k
-NN property, with complexity
O
(
√
n
k
2
/
ϵ
2
)
O
measured in terms of the number of vertices and edges it inspects, and we prove a lower bound of
Ω
(
√
n
/
ϵ
k
)
Ω
. We evaluate our tester empirically on the
k
k
-NN models computed by various algorithms and show that it can be used to detect
k
k
-NN models with bad accuracy in significantly less time than the building time of the
k
k
-NN model."
neurips,https://proceedings.neurips.cc/paper/2018/file/647bba344396e7c8170902bcf2e15551-Paper.pdf,MetaReg: Towards Domain Generalization using Meta-Regularization,"Yogesh Balaji, Swami Sankaranarayanan, Rama Chellappa",
neurips,https://proceedings.neurips.cc/paper/2018/file/6490791e7abf6b29a381288cc23a8223-Paper.pdf,Mirrored Langevin Dynamics,"Ya-Ping Hsieh, Ali Kavis, Paul Rolland, Volkan Cevher",
neurips,https://proceedings.neurips.cc/paper/2018/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf,Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals,"Tom Dupré la Tour, Thomas Moreau, Mainak Jas, Alexandre Gramfort",
neurips,https://proceedings.neurips.cc/paper/2018/file/652cf38361a209088302ba2b8b7f51e0-Paper.pdf,Complex Gated Recurrent Neural Networks,"Moritz Wolter, Angela Yao",
neurips,https://proceedings.neurips.cc/paper/2018/file/653ac11ca60b3e021a8c609c7198acfc-Paper.pdf,Active Matting,"Xin Yang, Ke Xu, Shaozhe Chen, Shengfeng He, Baocai Yin Yin, Rynson Lau",
neurips,https://proceedings.neurips.cc/paper/2018/file/653c579e3f9ba5c03f2f2f8cf4512b39-Paper.pdf,Scalable End-to-End Autonomous Vehicle Testing via Rare-event Simulation,"Matthew O'Kelly, Aman Sinha, Hongseok Namkoong, Russ Tedrake, John C. Duchi",
neurips,https://proceedings.neurips.cc/paper/2018/file/65b0df23fd2d449ae1e4b2d27151d73b-Paper.pdf,Improving Explorability in Variational Inference with Annealed Variational Objectives,"Chin-Wei Huang, Shawn Tan, Alexandre Lacoste, Aaron C. Courville",
neurips,https://proceedings.neurips.cc/paper/2018/file/65b1e92c585fd4c2159d5f33b5030ff2-Paper.pdf,Learning Loop Invariants for Program Verification,"Xujie Si, Hanjun Dai, Mukund Raghothaman, Mayur Naik, Le Song",
neurips,https://proceedings.neurips.cc/paper/2018/file/65fc52ed8f88c81323a418ca94cec2ed-Paper.pdf,Faster Online Learning of Optimal Threshold for Consistent F-measure Optimization,"Xiaoxuan Zhang, Mingrui Liu, Xun Zhou, Tianbao Yang","In this paper, we consider online F-measure optimization (OFO). Unlike traditional performance metrics (e.g., classification error rate), F-measure is non-decomposable over training examples and is a non-convex function of model parameters, making it much more difficult to be optimized in an online fashion. Most existing results of OFO usually suffer from high memory/computational costs and/or lack statistical consistency guarantee for optimizing F-measure at the population level. To advance OFO, we propose an efficient online algorithm based on simultaneously learning a posterior probability of class and learning an optimal threshold by minimizing a stochastic strongly convex function with unknown strong convexity parameter. A key component of the proposed method is a novel stochastic algorithm with low memory and computational costs, which can enjoy a convergence rate of
˜
O
(
1
/
√
n
)
O
for learning the optimal threshold under a mild condition on the convergence of the posterior probability, where
n
n
is the number of processed examples. It is provably faster than its predecessor based on a heuristic for updating the threshold. The experiments verify the efficiency of the proposed algorithm in comparison with state-of-the-art OFO algorithms."
neurips,https://proceedings.neurips.cc/paper/2018/file/65fc9fb4897a89789352e211ca2d398f-Paper.pdf,Learning Others' Intentional Models in Multi-Agent Settings Using Interactive POMDPs,"Yanlin Han, Piotr Gmytrasiewicz",
neurips,https://proceedings.neurips.cc/paper/2018/file/66121d1f782d29b62a286909165517bc-Paper.pdf,Online Structure Learning for Feed-Forward and Recurrent Sum-Product Networks,"Agastya Kalra, Abdullah Rashwan, Wei-Shou Hsu, Pascal Poupart, Prashant Doshi, Georgios Trimponias",
neurips,https://proceedings.neurips.cc/paper/2018/file/6616758da438b02b8d360ad83a5b3d77-Paper.pdf,Balanced Policy Evaluation and Learning,Nathan Kallus,
neurips,https://proceedings.neurips.cc/paper/2018/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf,Visual Memory for Robust Path Following,"Ashish Kumar, Saurabh Gupta, David Fouhey, Sergey Levine, Jitendra Malik",
neurips,https://proceedings.neurips.cc/paper/2018/file/664dd858db942cad06f24ff25df56716-Paper.pdf,Representation Learning of Compositional Data,"Marta Avalos, Richard Nock, Cheng Soon Ong, Julien Rouar, Ke Sun",
neurips,https://proceedings.neurips.cc/paper/2018/file/6651526b6fb8f29a00507de6a49ce30f-Paper.pdf,How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective,"Lei Wu, Chao Ma, Weinan E",
neurips,https://proceedings.neurips.cc/paper/2018/file/66808e327dc79d135ba18e051673d906-Paper.pdf,TADAM: Task dependent adaptive metric for improved few-shot learning,"Boris Oreshkin, Pau Rodríguez López, Alexandre Lacoste",
neurips,https://proceedings.neurips.cc/paper/2018/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf,A Bayes-Sard Cubature Method,"Toni Karvonen, Chris J. Oates, Simo Sarkka",
neurips,https://proceedings.neurips.cc/paper/2018/file/6788076842014c83cedadbe6b0ba0314-Paper.pdf,Learning to Infer Graphics Programs from Hand-Drawn Images,"Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, Josh Tenenbaum",
neurips,https://proceedings.neurips.cc/paper/2018/file/67d16d00201083a2b118dd5128dd6f59-Paper.pdf,Neural Guided Constraint Logic Programming for Program Synthesis,"Lisa Zhang, Gregory Rosenblatt, Ethan Fetaya, Renjie Liao, William Byrd, Matthew Might, Raquel Urtasun, Richard Zemel",
neurips,https://proceedings.neurips.cc/paper/2018/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf,Overcoming Language Priors in Visual Question Answering with Adversarial Regularization,"Sainandan Ramakrishnan, Aishwarya Agrawal, Stefan Lee","In this work, we present a novel regularization scheme for VQA that reduces this effect. We introduce a question-only model that takes as input the question encoding from the VQA model and must leverage language biases in order to succeed. We then pose training as an adversarial game between the VQA model and this question-only adversary -- discouraging the VQA model from capturing language biases in its question encoding.Further, we leverage this question-only model to estimate the mutual information between the image and answer given the question, which we maximize explicitly to encourage visual grounding. Our approach is a model agnostic training procedure and simple to implement. We show empirically that it can improve performance significantly on a bias-sensitive split of the VQA dataset for multiple base models -- achieving state-of-the-art on this task. Further, on standard VQA tasks, our approach shows significantly less drop in accuracy compared to existing bias-reducing VQA models."
neurips,https://proceedings.neurips.cc/paper/2018/file/67e103b0761e60683e83c559be18d40c-Paper.pdf,New Insight into Hybrid Stochastic Gradient Descent: Beyond With-Replacement Sampling and Convexity,"Pan Zhou, Xiaotong Yuan, Jiashi Feng",
neurips,https://proceedings.neurips.cc/paper/2018/file/68148596109e38cf9367d27875e185be-Paper.pdf,Variational PDEs for Acceleration on Manifolds and Application to Diffeomorphisms,"Ganesh Sundaramoorthi, Anthony Yezzi","We consider the optimization of cost functionals on manifolds and derive a variational approach to accelerated methods on manifolds. We demonstrate the methodology on the infinite-dimensional manifold of diffeomorphisms, motivated by registration problems in computer vision. We build on the variational approach to accelerated optimization by Wibisono, Wilson and Jordan, which applies in finite dimensions, and generalize that approach to infinite dimensional manifolds. We derive the continuum evolution equations, which are partial differential equations (PDE), and relate them to simple mechanical principles. Our approach can also be viewed as a generalization of the
L
2
L
optimal mass transport problem. Our approach evolves an infinite number of particles endowed with mass, represented as a mass density. The density evolves with the optimization variable, and endows the particles with dynamics. This is different than current accelerated methods where only a single particle moves and hence the dynamics does not depend on the mass. We derive the theory, compute the PDEs for acceleration, and illustrate the behavior of this new accelerated optimization scheme."
neurips,https://proceedings.neurips.cc/paper/2018/file/6832a7b24bc06775d02b7406880b93fc-Paper.pdf,Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis,"Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, zhifeng Chen, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu",
neurips,https://proceedings.neurips.cc/paper/2018/file/68331ff0427b551b68e911eebe35233b-Paper.pdf,Learning to Solve SMT Formulas,"Mislav Balunovic, Pavol Bielik, Martin Vechev",
neurips,https://proceedings.neurips.cc/paper/2018/file/68abef8ee1ac9b664a90b0bbaff4f770-Paper.pdf,Learning to Repair Software Vulnerabilities with Generative Adversarial Networks,"Jacob Harer, Onur Ozdemir, Tomo Lazovich, Christopher Reale, Rebecca Russell, Louis Kim, peter chin",
neurips,https://proceedings.neurips.cc/paper/2018/file/68b1fbe7f16e4ae3024973f12f3cb313-Paper.pdf,Algorithmic Linearly Constrained Gaussian Processes,Markus Lange-Hegermann,
neurips,https://proceedings.neurips.cc/paper/2018/file/68d3743587f71fbaa5062152985aff40-Paper.pdf,RenderNet: A deep convolutional network for differentiable rendering from 3D shapes,"Thu H. Nguyen-Phuoc, Chuan Li, Stephen Balaban, Yongliang Yang",
neurips,https://proceedings.neurips.cc/paper/2018/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf,Universal Growth in Production Economies,"Simina Branzei, Ruta Mehta, Noam Nisan","We show that a simple decentralized dynamic, where players update their bids on the goods in the market proportionally to how useful the investments were, leads to growth of the economy in the long term (whenever growth is possible) but also creates unbounded inequality, i.e. very rich and very poor players emerge. We analyze several other phenomena, such as how the relation of a player with others influences its development and the Gini index of the system."
neurips,https://proceedings.neurips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf,Neural Ordinary Differential Equations,"Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David K. Duvenaud",
neurips,https://proceedings.neurips.cc/paper/2018/file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf,MetaAnchor: Learning to Detect Objects with Customized Anchors,"Tong Yang, Xiangyu Zhang, Zeming Li, Wenqiang Zhang, Jian Sun",
neurips,https://proceedings.neurips.cc/paper/2018/file/6a4cbdaedcbda0fa8ddc7ea32073c475-Paper.pdf,ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions,"Hongyang Gao, Zhengyang Wang, Shuiwang Ji",
neurips,https://proceedings.neurips.cc/paper/2018/file/6a4d5952d4c018a1c1af9fa590a10dda-Paper.pdf,On Controllable Sparse Alternatives to Softmax,"Anirban Laha, Saneem Ahmed Chemmengath, Priyanka Agrawal, Mitesh Khapra, Karthik Sankaranarayanan, Harish G. Ramaswamy",
neurips,https://proceedings.neurips.cc/paper/2018/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf,Gaussian Process Conditional Density Estimation,"Vincent Dutordoir, Hugh Salimbeni, James Hensman, Marc Deisenroth",
neurips,https://proceedings.neurips.cc/paper/2018/file/6a6610feab86a1f294dbbf5855c74af9-Paper.pdf,Deep Network for the Integrated 3D Sensing of Multiple People in Natural Images,"Andrei Zanfir, Elisabeta Marinoiu, Mihai Zanfir, Alin-Ionut Popa, Cristian Sminchisescu",
neurips,https://proceedings.neurips.cc/paper/2018/file/6a8018b3a00b69c008601b8becae392b-Paper.pdf,Learning Attentional Communication for Multi-Agent Cooperation,"Jiechuan Jiang, Zongqing Lu",
neurips,https://proceedings.neurips.cc/paper/2018/file/6a81681a7af700c6385d36577ebec359-Paper.pdf,Speaker-Follower Models for Vision-and-Language Navigation,"Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell",
neurips,https://proceedings.neurips.cc/paper/2018/file/6a9aeddfc689c1d0e3b9ccc3ab651bc5-Paper.pdf,Training DNNs with Hybrid Block Floating Point,"Mario Drumond, Tao LIN, Martin Jaggi, Babak Falsafi",
neurips,https://proceedings.neurips.cc/paper/2018/file/6aaba9a124857622930ca4e50f5afed2-Paper.pdf,Coupled Variational Bayes via Optimization Embedding,"Bo Dai, Hanjun Dai, Niao He, Weiyang Liu, Zhen Liu, Jianshu Chen, Lin Xiao, Le Song",
neurips,https://proceedings.neurips.cc/paper/2018/file/6ad4174eba19ecb5fed17411a34ff5e6-Paper.pdf,Multi-domain Causal Structure Learning in Linear Systems,"AmirEmad Ghassami, Negar Kiyavash, Biwei Huang, Kun Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/6aed000af86a084f9cb0264161e29dd3-Paper.pdf,Policy Optimization via Importance Sampling,"Alberto Maria Metelli, Matteo Papini, Francesco Faccio, Marcello Restelli",
neurips,https://proceedings.neurips.cc/paper/2018/file/6be93f7a96fed60c477d30ae1de032fd-Paper.pdf,Task-Driven Convolutional Recurrent Models of the Visual System,"Aran Nayebi, Daniel Bear, Jonas Kubilius, Kohitij Kar, Surya Ganguli, David Sussillo, James J. DiCarlo, Daniel L. Yamins",
neurips,https://proceedings.neurips.cc/paper/2018/file/6bf733bb7f81e866306e9b5f012419cb-Paper.pdf,Contrastive Learning from Pairwise Measurements,"Yi Chen, Zhuoran Yang, Yuchen Xie, Zhaoran Wang",
neurips,https://proceedings.neurips.cc/paper/2018/file/6c1e55ec7c43dc51a37472ddcbd756fb-Paper.pdf,Regret Bounds for Online Portfolio Selection with a Cardinality Constraint,"Shinji Ito, Daisuke Hatano, Hanna Sumita, Akihiro Yabe, Takuro Fukunaga, Naonori Kakimura, Ken-Ichi Kawarabayashi",
neurips,https://proceedings.neurips.cc/paper/2018/file/6cd9313ed34ef58bad3fdd504355e72c-Paper.pdf,Hunting for Discriminatory Proxies in Linear Regression Models,"Samuel Yeom, Anupam Datta, Matt Fredrikson",
neurips,https://proceedings.neurips.cc/paper/2018/file/6d0f846348a856321729a2f36734d1a7-Paper.pdf,Entropy and mutual information in models of deep neural networks,"Marylou Gabrié, Andre Manoel, Clément Luneau, jean barbier, Nicolas Macris, Florent Krzakala, Lenka Zdeborová",
neurips,https://proceedings.neurips.cc/paper/2018/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf,Paraphrasing Complex Network: Network Compression via Factor Transfer,"Jangho Kim, Seonguk Park, Nojun Kwak",
neurips,https://proceedings.neurips.cc/paper/2018/file/6e0917469214d8fbd8c517dcdc6b8dcf-Paper.pdf,A Simple Cache Model for Image Recognition,Emin Orhan,
neurips,https://proceedings.neurips.cc/paper/2018/file/6e4243f5511fd6ef0f03e9f386d54403-Paper.pdf,Learning Attractor Dynamics for Generative Memory,"Yan Wu, Gregory Wayne, Karol Gregor, Timothy Lillicrap",
neurips,https://proceedings.neurips.cc/paper/2018/file/6e923226e43cd6fac7cfe1e13ad000ac-Paper.pdf,Quadrature-based features for kernel approximation,"Marina Munkhoeva, Yermek Kapushev, Evgeny Burnaev, Ivan Oseledets",
neurips,https://proceedings.neurips.cc/paper/2018/file/6ea9ab1baa0efb9e19094440c317e21b-Paper.pdf,Efficient Algorithms for Non-convex Isotonic Regression through Submodular Optimization,Francis Bach,
neurips,https://proceedings.neurips.cc/paper/2018/file/6ecbdd6ec859d284dc13885a37ce8d81-Paper.pdf,Deep Neural Nets with Interpolating Function as Output Activation,"Bao Wang, Xiyang Luo, Zhen Li, Wei Zhu, Zuoqiang Shi, Stanley Osher",
neurips,https://proceedings.neurips.cc/paper/2018/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf,Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior,"Sid Reddy, Anca Dragan, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2018/file/6f3ef77ac0e3619e98159e9b6febf557-Paper.pdf,Image Inpainting via Generative Multi-column Convolutional Neural Networks,"Yi Wang, Xin Tao, Xiaojuan Qi, Xiaoyong Shen, Jiaya Jia",
neurips,https://proceedings.neurips.cc/paper/2018/file/6fbd841e2e4b2938351a4f9b68f12e6b-Paper.pdf,Clustering Redemption–Beyond the Impossibility of Kleinberg’s Axioms,"Vincent Cohen-Addad, Varun Kanade, Frederik Mallmann-Trenn",
neurips,https://proceedings.neurips.cc/paper/2018/file/7070f9088e456682f0f84f815ebda761-Paper.pdf,A Statistical Recurrent Model on the Manifold of Symmetric Positive Definite Matrices,"Rudrasis Chakraborty, Chun-Hao Yang, Xingjian Zhen, Monami Banerjee, Derek Archer, David Vaillancourt, Vikas Singh, Baba Vemuri",
neurips,https://proceedings.neurips.cc/paper/2018/file/70ece1e1e0931919438fcfc6bd5f199c-Paper.pdf,Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes,"Hassan Ashtiani, Shai Ben-David, Nicholas Harvey, Christopher Liaw, Abbas Mehrabian, Yaniv Plan","The upper bound is based on a novel technique for distribution learning based on a notion of sample compression. Any class of distributions that allows such a sample compression scheme can also be learned with few samples. Moreover, if a class of distributions has such a compression scheme, then so do the classes of products and mixtures of those distributions. The core of our main result is showing that the class of Gaussians in R^d has an efficient sample compression."
neurips,https://proceedings.neurips.cc/paper/2018/file/713fd63d76c8a57b16fc433fb4ae718a-Paper.pdf,Object-Oriented Dynamics Predictor,"Guangxiang Zhu, Zhiao Huang, Chongjie Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/716e1b8c6cd17b771da77391355749f3-Paper.pdf,Fast Rates of ERM and Stochastic Approximation: Adaptive to Error Bound Conditions,"Mingrui Liu, Xiaoxuan Zhang, Lijun Zhang, Rong Jin, Tianbao Yang","Error bound conditions (EBC) are properties that characterize the growth of an objective function when a point is moved away from the optimal set. They have recently received increasing attention in the field of optimization for developing optimization algorithms with fast convergence. However, the studies of EBC in statistical learning are hitherto still limited. The main contributions of this paper are two-fold. First, we develop fast and intermediate rates of empirical risk minimization (ERM) under EBC for risk minimization with Lipschitz continuous, and smooth convex random functions. Second, we establish fast and intermediate rates of an efficient stochastic approximation (SA) algorithm for risk minimization with Lipschitz continuous random functions, which requires only one pass of
n
n
samples and adapts to EBC. For both approaches, the convergence rates span a full spectrum between
˜
O
(
1
/
√
n
)
O
and
˜
O
(
1
/
n
)
O
depending on the power constant in EBC, and could be even faster than
O
(
1
/
n
)
O
in special cases for ERM. Moreover, these convergence rates are automatically adaptive without using any knowledge of EBC. Overall, this work not only strengthens the understanding of ERM for statistical learning but also brings new fast stochastic algorithms for solving a broad range of statistical learning problems."
neurips,https://proceedings.neurips.cc/paper/2018/file/717d8b3d60d9eea997b35b02b6a4e867-Paper.pdf,Adversarial Multiple Source Domain Adaptation,"Han Zhao, Shanghang Zhang, Guanhang Wu, José M. F. Moura, Joao P. Costeira, Geoffrey J. Gordon",
neurips,https://proceedings.neurips.cc/paper/2018/file/7180cffd6a8e829dacfc2a31b3f72ece-Paper.pdf,To Trust Or Not To Trust A Classifier,"Heinrich Jiang, Been Kim, Melody Guan, Maya Gupta",
neurips,https://proceedings.neurips.cc/paper/2018/file/71a58e8cb75904f24cde464161c3e766-Paper.pdf,Deep Reinforcement Learning of Marked Temporal Point Processes,"Utkarsh Upadhyay, Abir De, Manuel Gomez Rodriguez",
neurips,https://proceedings.neurips.cc/paper/2018/file/71e63ef5b7249cfc60852f0e0f5bf4c8-Paper.pdf,"Learning to Play With Intrinsically-Motivated, Self-Aware Agents","Nick Haber, Damian Mrowca, Stephanie Wang, Li F. Fei-Fei, Daniel L. Yamins",
neurips,https://proceedings.neurips.cc/paper/2018/file/7221e5c8ec6b08ef6d3f9ff3ce6eb1d1-Paper.pdf,Distributed Learning without Distress: Privacy-Preserving Empirical Risk Minimization,"Bargav Jayaraman, Lingxiao Wang, David Evans, Quanquan Gu",
neurips,https://proceedings.neurips.cc/paper/2018/file/729c68884bd359ade15d5f163166738a-Paper.pdf,Continuous-time Value Function Approximation in Reproducing Kernel Hilbert Spaces,"Motoya Ohnishi, Masahiro Yukawa, Mikael Johansson, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2018/file/72da7fd6d1302c0a159f6436d01e9eb0-Paper.pdf,Hybrid Knowledge Routed Modules for Large-scale Object Detection,"ChenHan Jiang, Hang Xu, Xiaodan Liang, Liang Lin",
neurips,https://proceedings.neurips.cc/paper/2018/file/72e6d3238361fe70f22fb0ac624a7072-Paper.pdf,Supervising Unsupervised Learning,"Vikas Garg, Adam T. Kalai",
neurips,https://proceedings.neurips.cc/paper/2018/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf,"Overlapping Clustering Models, and One (class) SVM to Bind Them All","Xueyu Mao, Purnamrita Sarkar, Deepayan Chakrabarti",
neurips,https://proceedings.neurips.cc/paper/2018/file/734e6bfcd358e25ac1db0a4241b95651-Paper.pdf,BRITS: Bidirectional Recurrent Imputation for Time Series,"Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, Yitan Li",
neurips,https://proceedings.neurips.cc/paper/2018/file/73a427badebe0e32caa2e1fc7530b7f3-Paper.pdf,Improving Online Algorithms via ML Predictions,"Manish Purohit, Zoya Svitkina, Ravi Kumar",
neurips,https://proceedings.neurips.cc/paper/2018/file/73e5080f0f3804cb9cf470a8ce895dac-Paper.pdf,Learning Latent Subspaces in Variational Autoencoders,"Jack Klys, Jake Snell, Richard Zemel",
neurips,https://proceedings.neurips.cc/paper/2018/file/73f104c9fba50050eea11d9d075247cc-Paper.pdf,VideoCapsuleNet: A Simplified Network for Action Detection,"Kevin Duarte, Yogesh Rawat, Mubarak Shah",
neurips,https://proceedings.neurips.cc/paper/2018/file/73fed7fd472e502d8908794430511f4d-Paper.pdf,Causal Inference via Kernel Deviance Measures,"Jovana Mitrovic, Dino Sejdinovic, Yee Whye Teh",
neurips,https://proceedings.neurips.cc/paper/2018/file/7417744a2bac776fabe5a09b21c707a2-Paper.pdf,"Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects","Adam Kosiorek, Hyunjik Kim, Yee Whye Teh, Ingmar Posner",
neurips,https://proceedings.neurips.cc/paper/2018/file/741a0099c9ac04c7bfc822caf7c7459f-Paper.pdf,Learning with SGD and Random Features,"Luigi Carratino, Alessandro Rudi, Lorenzo Rosasco",
neurips,https://proceedings.neurips.cc/paper/2018/file/743394beff4b1282ba735e5e3723ed74-Paper.pdf,Boolean Decision Rules via Column Generation,"Sanjeeb Dash, Oktay Gunluk, Dennis Wei",
neurips,https://proceedings.neurips.cc/paper/2018/file/74378afe5e8b20910cf1f939e57f0480-Paper.pdf,Neural Interaction Transparency (NIT): Disentangling Learned Interactions for Improved Interpretability,"Michael Tsang, Hanpeng Liu, Sanjay Purushotham, Pavankumar Murali, Yan Liu","Neural networks are known to model statistical interactions, but they entangle the interactions at intermediate hidden layers for shared representation learning. We propose a framework, Neural Interaction Transparency (NIT), that disentangles the shared learning across different interactions to obtain their intrinsic lower-order and interpretable structure. This is done through a novel regularizer that directly penalizes interaction order. We show that disentangling interactions reduces a feedforward neural network to a generalized additive model with interactions, which can lead to transparent models that perform comparably to the state-of-the-art models. NIT is also flexible and efficient; it can learn generalized additive models with maximum
K
K
-order interactions by training only
O
(
1
)
O
models."
neurips,https://proceedings.neurips.cc/paper/2018/file/74563ba21a90da13dacf2a73e3ddefa7-Paper.pdf,Boosting Black Box Variational Inference,"Francesco Locatello, Gideon Dresdner, Rajiv Khanna, Isabel Valera, Gunnar Raetsch",
neurips,https://proceedings.neurips.cc/paper/2018/file/74627b65e6e6a4c21e06809b8e02114a-Paper.pdf,Transfer of Deep Reactive Policies for MDP Planning,"Aniket (Nick) Bajpai, Sankalp Garg, Mausam","In this paper, we present the first domain-independent transfer algorithm for MDP planning domains expressed in an RDDL representation. Our architecture exploits the symbolic state configuration and transition function of the domain (available via RDDL) to learn a shared embedding space for states and state-action pairs for all problem instances of a domain. We then learn an RL agent in the embedding space, making a near zero-shot transfer possible, i.e., without much training on the new instance, and without using the domain simulator at all. Experiments on three different benchmark domains underscore the value of our transfer algorithm. Compared against planning from scratch, and a state-of-the-art RL transfer algorithm, our transfer solution has significantly superior learning curves."
neurips,https://proceedings.neurips.cc/paper/2018/file/747c1bcceb6109a4ef936bc70cfe67de-Paper.pdf,Variational Bayesian Monte Carlo,Luigi Acerbi,
neurips,https://proceedings.neurips.cc/paper/2018/file/748d6b6ed8e13f857ceaa6cfbdca14b8-Paper.pdf,Tangent: Automatic differentiation using source-code transformation for dynamically typed array programming,"Bart van Merrienboer, Dan Moldovan, Alexander Wiltschko",
neurips,https://proceedings.neurips.cc/paper/2018/file/74934548253bcab8490ebd74afed7031-Paper.pdf,Learning Task Specifications from Demonstrations,"Marcell Vazquez-Chanlatte, Susmit Jha, Ashish Tiwari, Mark K. Ho, Sanjit Seshia",
neurips,https://proceedings.neurips.cc/paper/2018/file/74f23f9e28cbc5ddaae8582f48642a59-Paper.pdf,Sparse PCA from Sparse Linear Regression,"Guy Bresler, Sung Min Park, Madalina Persu",
neurips,https://proceedings.neurips.cc/paper/2018/file/7535bbb91c8fde347ad861f293126633-Paper.pdf,GILBO: One Metric to Measure Them All,"Alexander A. Alemi, Ian Fischer",
neurips,https://proceedings.neurips.cc/paper/2018/file/758be1f9f7a7efac938ed8bd97c0e1cb-Paper.pdf,Maximizing Induced Cardinality Under a Determinantal Point Process,"Jennifer A. Gillenwater, Alex Kulesza, Sergei Vassilvitskii, Zelda E. Mariet",
neurips,https://proceedings.neurips.cc/paper/2018/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf,"FishNet: A Versatile Backbone for Image, Region, and Pixel Level Prediction","Shuyang Sun, Jiangmiao Pang, Jianping Shi, Shuai Yi, Wanli Ouyang",
neurips,https://proceedings.neurips.cc/paper/2018/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf,Simple random search of static linear policies is competitive for reinforcement learning,"Horia Mania, Aurelia Guy, Benjamin Recht",
neurips,https://proceedings.neurips.cc/paper/2018/file/770f8e448d07586afbf77bb59f698587-Paper.pdf,Automatic differentiation in ML: Where we are and where we should be going,"Bart van Merrienboer, Olivier Breuleux, Arnaud Bergeron, Pascal Lamblin",
neurips,https://proceedings.neurips.cc/paper/2018/file/7776e88b0c189539098176589250bcba-Paper.pdf,Improving Neural Program Synthesis with Inferred Execution Traces,"Eui Chul Shin, Illia Polosukhin, Dawn Song",
neurips,https://proceedings.neurips.cc/paper/2018/file/7790583c0d8d74e930a4441ad75ebc64-Paper.pdf,Discretely Relaxing Continuous Variables for tractable Variational Inference,"Trefor Evans, Prasanth Nair",
neurips,https://proceedings.neurips.cc/paper/2018/file/77ee3bc58ce560b86c2b59363281e914-Paper.pdf,The Limits of Post-Selection Generalization,"Jonathan Ullman, Adam Smith, Kobbi Nissim, Uri Stemmer, Thomas Steinke","In this work we show several limitations on the power of algorithms satisfying post hoc generalization. First, we show a tight lower bound on the error of any algorithm that satisfies post hoc generalization and answers adaptively chosen statistical queries, showing a strong barrier to progress in post selection data analysis. Second, we show that post hoc generalization is not closed under composition, despite many examples of such algorithms exhibiting strong composition properties."
neurips,https://proceedings.neurips.cc/paper/2018/file/781397bc0630d47ab531ea850bddcf63-Paper.pdf,"Revisiting
(
ϵ
,
γ
,
τ
)
(
-similarity learning for domain adaptation","Sofiane Dhouib, Ievgen Redko","Similarity learning is an active research area in machine learning that tackles the problem of finding a similarity function tailored to an observable data sample in order to achieve efficient classification. This learning scenario has been generally formalized by the means of a
(
ϵ
,
γ
,
τ
)
−
(
good similarity learning framework in the context of supervised classification and has been shown to have strong theoretical guarantees. In this paper, we propose to extend the theoretical analysis of similarity learning to the domain adaptation setting, a particular situation occurring when the similarity is learned and then deployed on samples following different probability distributions. We give a new definition of an
(
ϵ
,
γ
)
−
(
good similarity for domain adaptation and prove several results quantifying the performance of a similarity function on a target domain after it has been trained on a source domain. We particularly show that if the source distribution dominates the target one, then principally new domain adaptation learning bounds can be proved."
neurips,https://proceedings.neurips.cc/paper/2018/file/78631a4bb5303be54fa1cfdcb958c00a-Paper.pdf,Learning and Testing Causal Models with Interventions,"Jayadev Acharya, Arnab Bhattacharyya, Constantinos Daskalakis, Saravanan Kandasamy",
neurips,https://proceedings.neurips.cc/paper/2018/file/7876acb66640bad41f1e1371ef30c180-Paper.pdf,Evolved Policy Gradients,"Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly Stadie, Filip Wolski, OpenAI Jonathan Ho, Pieter Abbeel",
neurips,https://proceedings.neurips.cc/paper/2018/file/78b9cab19959e4af8ff46156ee460c74-Paper.pdf,Demystifying excessively volatile human learning: A Bayesian persistent prior and a neural approximation,"Chaitanya Ryali, Gautam Reddy, Angela J. Yu",
neurips,https://proceedings.neurips.cc/paper/2018/file/79a3308b13cd31f096d8a4a34f96b66b-Paper.pdf,Distributionally Robust Graphical Models,"Rizal Fathony, Ashkan Rezaei, Mohammad Ali Bashiri, Xinhua Zhang, Brian Ziebart",
neurips,https://proceedings.neurips.cc/paper/2018/file/79a49b3e3762632813f9e35f4ba53d6c-Paper.pdf,Natasha 2: Faster Non-Convex Optimization Than SGD,Zeyuan Allen-Zhu,"We design a stochastic algorithm to find
ε
ε
-approximate local minima of any smooth nonconvex function in rate
O
(
ε
−
3.25
)
O
, with only oracle access to stochastic gradients. The best result before this work was
O
(
ε
−
4
)
O
by stochastic gradient descent (SGD)."
neurips,https://proceedings.neurips.cc/paper/2018/file/7a2347d96752880e3d58d72e9813cc14-Paper.pdf,Iterative Value-Aware Model Learning,Amir-massoud Farahmand,
neurips,https://proceedings.neurips.cc/paper/2018/file/7a576629fef88f3e636afd33b09e8289-Paper.pdf,PCA of high dimensional random walks with comparison to neural network training,"Joseph Antognini, Jascha Sohl-Dickstein",
neurips,https://proceedings.neurips.cc/paper/2018/file/7aa685b3b1dc1d6780bf36f7340078c9-Paper.pdf,Learning Libraries of Subroutines for Neurally–Guided Bayesian Program Induction,"Kevin Ellis, Lucas Morales, Mathias Sablé-Meyer, Armando Solar-Lezama, Josh Tenenbaum",
neurips,https://proceedings.neurips.cc/paper/2018/file/7ae11af20803185120e83d3ce4fb4ed7-Paper.pdf,"Streaming Kernel PCA with
~
O
(
√
n
)
O
Random Features","Enayat Ullah, Poorya Mianjy, Teodor Vanislavov Marinov, Raman Arora","We study the statistical and computational aspects of kernel principal component analysis using random Fourier features and show that under mild assumptions,
O
(
√
n
log
n
)
O
features suffices to achieve
O
(
1
/
ϵ
2
)
O
sample complexity. Furthermore, we give a memory efficient streaming algorithm based on classical Oja's algorithm that achieves this rate"
neurips,https://proceedings.neurips.cc/paper/2018/file/7af6266cc52234b5aa339b16695f7fc4-Paper.pdf,Faster Neural Networks Straight from JPEG,"Lionel Gueguen, Alex Sergeev, Ben Kadlec, Rosanne Liu, Jason Yosinski",
neurips,https://proceedings.neurips.cc/paper/2018/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf,Bayesian Model Selection Approach to Boundary Detection with Non-Local Priors,"Fei Jiang, Guosheng Yin, Francesca Dominici",
neurips,https://proceedings.neurips.cc/paper/2018/file/7b66b4fd401a271a1c7224027ce111bc-Paper.pdf,Densely Connected Attention Propagation for Reading Comprehension,"Yi Tay, Anh Tuan Luu, Siu Cheung Hui, Jian Su",
neurips,https://proceedings.neurips.cc/paper/2018/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf,Query Complexity of Bayesian Private Learning,Kuang Xu,"We study the query complexity of Bayesian Private Learning: a learner wishes to locate a random target within an interval by submitting queries, in the presence of an adversary who observes all of her queries but not the responses. How many queries are necessary and sufficient in order for the learner to accurately estimate the target, while simultaneously concealing the target from the adversary? Our main result is a query complexity lower bound that is tight up to the first order. We show that if the learner wants to estimate the target within an error of
ϵ
ϵ
, while ensuring that no adversary estimator can achieve a constant additive error with probability greater than
1
/
L
1
, then the query complexity is on the order of
L
log
(
1
/
ϵ
)
L
as
ϵ
→
0
ϵ
. Our result demonstrates that increased privacy, as captured by
L
L
, comes at the expense of a \emph{multiplicative} increase in query complexity. The proof builds on Fano's inequality and properties of certain proportional-sampling estimators."
neurips,https://proceedings.neurips.cc/paper/2018/file/7bd28f15a49d5e5848d6ec70e584e625-Paper.pdf,NAIS-Net: Stable Deep Networks from Non-Autonomous Differential Equations,"Marco Ciccone, Marco Gallieri, Jonathan Masci, Christian Osendorfer, Faustino Gomez",
neurips,https://proceedings.neurips.cc/paper/2018/file/7c78335a8924215ea5c22fda1aac7b75-Paper.pdf,Sequential Test for the Lowest Mean: From Thompson to Murphy Sampling,"Emilie Kaufmann, Wouter M. Koolen, Aurélien Garivier",
neurips,https://proceedings.neurips.cc/paper/2018/file/7cf64379eb6f29a4d25c4b6a2df713e4-Paper.pdf,Content preserving text generation with attribute controls,"Lajanugen Logeswaran, Honglak Lee, Samy Bengio",
neurips,https://proceedings.neurips.cc/paper/2018/file/7dd0240cd412efde8bc165e864d3644f-Paper.pdf,Latent Gaussian Activity Propagation: Using Smoothness and Structure to Separate and Localize Sounds in Large Noisy Environments,"Daniel Johnson, Daniel Gorelik, Ross E. Mawhorter, Kyle Suver, Weiqing Gu, Steven Xing, Cody Gabriel, Peter Sankhagowit",
neurips,https://proceedings.neurips.cc/paper/2018/file/7de32147a4f1055bed9e4faf3485a84d-Paper.pdf,Differentially Private Testing of Identity and Closeness of Discrete Distributions,"Jayadev Acharya, Ziteng Sun, Huanyu Zhang","We study the fundamental problems of identity testing (goodness of fit), and closeness testing (two sample test) of distributions over
k
k
elements, under differential privacy. While the problems have a long history in statistics, finite sample bounds for these problems have only been established recently. In this work, we derive upper and lower bounds on the sample complexity of both the problems under
(
ε
,
δ
)
(
-differential privacy. We provide optimal sample complexity algorithms for identity testing problem for all parameter ranges, and the first results for closeness testing. Our closeness testing bounds are optimal in the sparse regime where the number of samples is at most
k
k
. Our upper bounds are obtained by privatizing non-private estimators for these problems. The non-private estimators are chosen to have small sensitivity. We propose a general framework to establish lower bounds on the sample complexity of statistical tasks under differential privacy. We show a bound on differentially private algorithms in terms of a coupling between the two hypothesis classes we aim to test. By constructing carefully chosen priors over the hypothesis classes, and using Le Cam's two point theorem we provide a general mechanism for proving lower bounds. We believe that the framework can be used to obtain strong lower bounds for other statistical tasks under privacy."
neurips,https://proceedings.neurips.cc/paper/2018/file/7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Paper.pdf,Non-Ergodic Alternating Proximal Augmented Lagrangian Algorithms with Optimal Rates,Quoc Tran Dinh,"We develop two new non-ergodic alternating proximal augmented Lagrangian algorithms (NEAPAL) to solve a class of nonsmooth constrained convex optimization problems. Our approach relies on a novel combination of the augmented Lagrangian framework, alternating/linearization scheme, Nesterov's acceleration techniques, and adaptive strategy for parameters. Our algorithms have several new features compared to existing methods. Firstly, they have a Nesterov's acceleration step on the primal variables compared to the dual one in several methods in the literature. Secondly, they achieve non-ergodic optimal convergence rates under standard assumptions, i.e. an
O
(
1
k
)
O
rate without any smoothness or strong convexity-type assumption, or an
O
(
1
k
2
)
O
rate under only semi-strong convexity, where
k
k
is the iteration counter. Thirdly, they preserve or have better per-iteration complexity compared to existing algorithms. Fourthly, they can be implemented in a parallel fashion. Finally, all the parameters are adaptively updated without heuristic tuning. We verify our algorithms on different numerical examples and compare them with some state-of-the-art methods."
neurips,https://proceedings.neurips.cc/paper/2018/file/7e448ed9dd44e6e22442dac8e21856ae-Paper.pdf,Multi-objective Maximization of Monotone Submodular Functions with Cardinality Constraint,Rajan Udwani,"We consider the problem of multi-objective maximization of monotone submodular functions subject to cardinality constraint, often formulated as
max
|
A
|
=
k
min
i
∈
{
1
,
…
,
m
}
f
i
(
A
)
max
. While it is widely known that greedy methods work well for a single objective, the problem becomes much harder with multiple objectives. In fact, Krause et al.\ (2008) showed that when the number of objectives
m
m
grows as the cardinality
k
k
i.e.,
m
=
Ω
(
k
)
m
, the problem is inapproximable (unless
P
=
N
P
P
). On the other hand, when
m
m
is constant Chekuri et al.\ (2010) showed a randomized
(
1
−
1
/
e
)
−
ϵ
(
approximation with runtime (number of queries to function oracle)
n
m
/
ϵ
3
n
. %In fact, the result of Chekuri et al.\ (2010) is for the far more general case of matroid constant. We focus on finding a fast and practical algorithm that has (asymptotic) approximation guarantees even when
m
m
is super constant. We first modify the algorithm of Chekuri et al.\ (2010) to achieve a
(
1
−
1
/
e
)
(
approximation for
m
=
o
(
k
log
3
k
)
m
. This demonstrates a steep transition from constant factor approximability to inapproximability around
m
=
Ω
(
k
)
m
. Then using Multiplicative-Weight-Updates (MWU), we find a much faster
~
O
(
n
/
δ
3
)
O
time asymptotic
(
1
−
1
/
e
)
2
−
δ
(
approximation. While the above results are all randomized, we also give a simple deterministic
(
1
−
1
/
e
)
−
ϵ
(
approximation with runtime
k
n
m
/
ϵ
4
k
. Finally, we run synthetic experiments using Kronecker graphs and find that our MWU inspired heuristic outperforms existing heuristics."
neurips,https://proceedings.neurips.cc/paper/2018/file/7e83722522e8aeb7512b7075311316b7-Paper.pdf,Fully Understanding The Hashing Trick,"Casper B. Freksen, Lior Kamma, Kasper Green Larsen","Feature hashing, also known as {\em the hashing trick}, introduced by Weinberger et al. (2009), is one of the key techniques used in scaling-up machine learning algorithms. Loosely speaking, feature hashing uses a random sparse projection matrix
A
:
R
n
→
R
m
A
(where
m
≪
n
m
) in order to reduce the dimension of the data from
n
n
to
m
m
while approximately preserving the Euclidean norm. Every column of
A
A
contains exactly one non-zero entry, equals to either
−
1
−
or
1
1
. Weinberger et al. showed tail bounds on
∥
A
x
∥
2
2
‖
. Specifically they showed that for every
ε
,
δ
ε
, if
∥
x
∥
∞
/
∥
x
∥
2
‖
is sufficiently small, and
m
m
is sufficiently large, then
Pr
[
|
∥
A
x
∥
2
2
−
∥
x
∥
2
2
|
<
ε
∥
x
∥
2
2
]
≥
1
−
δ
.
Pr[|‖Ax‖22−‖x‖22|<ε‖x‖22]≥1−δ.
These bounds were later extended by Dasgupta et al. (2010) and most recently refined by Dahlgaard et al. (2017), however, the true nature of the performance of this key technique, and specifically the correct tradeoff between the pivotal parameters
∥
x
∥
∞
/
∥
x
∥
2
,
m
,
ε
,
δ
‖
remained an open question. We settle this question by giving tight asymptotic bounds on the exact tradeoff between the central parameters, thus providing a complete understanding of the performance of feature hashing. We complement the asymptotic bound with empirical data, which shows that the constants ""hiding"" in the asymptotic notation are, in fact, very close to
1
1
, thus further illustrating the tightness of the presented bounds in practice."
neurips,https://proceedings.neurips.cc/paper/2018/file/7ec0dbeee45813422897e04ad8424a5e-Paper.pdf,Policy-Conditioned Uncertainty Sets for Robust Markov Decision Processes,"Andrea Tirinzoni, Marek Petrik, Xiangli Chen, Brian Ziebart",
neurips,https://proceedings.neurips.cc/paper/2018/file/7ec69dd44416c46745f6edd947b470cd-Paper.pdf,Visual Reinforcement Learning with Imagined Goals,"Ashvin V. Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2018/file/7edcfb2d8f6a659ef4cd1e6c9b6d7079-Paper.pdf,DropBlock: A regularization method for convolutional networks,"Golnaz Ghiasi, Tsung-Yi Lin, Quoc V. Le","Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks. On ImageNet classification, ResNet-50 architecture with DropBlock achieves
78.13
%
78.13
accuracy, which is more than
1.6
%
1.6
improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from
36.8
%
36.8
to
38.4
%
38.4
."
neurips,https://proceedings.neurips.cc/paper/2018/file/7f018eb7b301a66658931cb8a93fd6e8-Paper.pdf,Exact natural gradient in deep linear networks and its application to the nonlinear case,"Alberto Bernacchia, Mate Lengyel, Guillaume Hennequin",
neurips,https://proceedings.neurips.cc/paper/2018/file/7f16109f1619fd7a733daf5a84c708c1-Paper.pdf,RetGK: Graph Kernels based on Return Probabilities of Random Walks,"Zhen Zhang, Mianzhi Wang, Yijian Xiang, Yan Huang, Arye Nehorai",
neurips,https://proceedings.neurips.cc/paper/2018/file/7f5d04d189dfb634e6a85bb9d9adf21e-Paper.pdf,Revisiting Multi-Task Learning with ROCK: a Deep Residual Auxiliary Block for Visual Detection,"Taylor Mordan, Nicolas THOME, Gilles Henaff, Matthieu Cord",
neurips,https://proceedings.neurips.cc/paper/2018/file/7fb8ceb3bd59c7956b1df66729296a4c-Paper.pdf,Gradient Descent Meets Shift-and-Invert Preconditioning for Eigenvector Computation,Zhiqiang Xu,"Shift-and-invert preconditioning, as a classic acceleration technique for the leading eigenvector computation, has received much attention again recently, owing to fast least-squares solvers for efficiently approximating matrix inversions in power iterations. In this work, we adopt an inexact Riemannian gradient descent perspective to investigate this technique on the effect of the step-size scheme. The shift-and-inverted power method is included as a special case with adaptive step-sizes. Particularly, two other step-size settings, i.e., constant step-sizes and Barzilai-Borwein (BB) step-sizes, are examined theoretically and/or empirically. We present a novel convergence analysis for the constant step-size setting that achieves a rate at
~
O
(
√
λ
1
λ
1
−
λ
p
+
1
)
O
, where
λ
i
λ
represents the
i
i
-th largest eigenvalue of the given real symmetric matrix and
p
p
is the multiplicity of
λ
1
λ
. Our experimental studies show that the proposed algorithm can be significantly faster than the shift-and-inverted power method in practice."
neurips,https://proceedings.neurips.cc/paper/2018/file/7fea637fd6d02b8f0adf6f7dc36aed93-Paper.pdf,Inequity aversion improves cooperation in intertemporal social dilemmas,"Edward Hughes, Joel Z. Leibo, Matthew Phillips, Karl Tuyls, Edgar Dueñez-Guzman, Antonio García Castañeda, Iain Dunning, Tina Zhu, Kevin McKee, Raphael Koster, Heather Roff, Thore Graepel",
neurips,https://proceedings.neurips.cc/paper/2018/file/800de15c79c8d840f4e78d3af937d4d4-Paper.pdf,Towards Deep Conversational Recommendations,"Raymond Li, Samira Ebrahimi Kahou, Hannes Schulz, Vincent Michalski, Laurent Charlin, Chris Pal",
neurips,https://proceedings.neurips.cc/paper/2018/file/801fd8c2a4e79c1d24a40dc735c051ae-Paper.pdf,Deep Generative Models for Distribution-Preserving Lossy Compression,"Michael Tschannen, Eirikur Agustsson, Mario Lucic",
neurips,https://proceedings.neurips.cc/paper/2018/file/803a82dee7e3fbb3438a149508484250-Paper.pdf,"With Friends Like These, Who Needs Adversaries?","Saumya Jetley, Nicholas Lord, Philip Torr",
neurips,https://proceedings.neurips.cc/paper/2018/file/8051a3c40561002834e59d566b7430cf-Paper.pdf,Learning to Teach with Dynamic Loss Functions,"Lijun Wu, Fei Tian, Yingce Xia, Yang Fan, Tao Qin, Lai Jian-Huang, Tie-Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2018/file/80537a945c7aaa788ccfcdf1b99b5d8f-Paper.pdf,Learning Bounds for Greedy Approximation with Explicit Feature Maps from Multiple Kernels,"Shahin Shahrampour, Vahid Tarokh",
neurips,https://proceedings.neurips.cc/paper/2018/file/8073bd4ed0fe0c330290c58056a2cd5e-Paper.pdf,Distributed Multitask Reinforcement Learning with Quadratic Convergence,"Rasul Tutunov, Dongho Kim, Haitham Bou Ammar",
neurips,https://proceedings.neurips.cc/paper/2018/file/80b618ebcac7aa97a6dac2ba65cb7e36-Paper.pdf,The Global Anchor Method for Quantifying Linguistic Shifts and Domain Adaptation,"Zi Yin, Vin Sachidananda, Balaji Prabhakar",
neurips,https://proceedings.neurips.cc/paper/2018/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf,Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units,"Yixi Xu, Xiao Wang","This paper presents a general framework for norm-based capacity control for
L
p
,
q
L
weight normalized deep neural networks. We establish the upper bound on the Rademacher complexities of this family. With an
L
p
,
q
L
normalization where
q
≤
p
∗
q
and
1
/
p
+
1
/
p
∗
=
1
1
, we discuss properties of a width-independent capacity control, which only depends on the depth by a square root term. We further analyze the approximation properties of
L
p
,
q
L
weight normalized deep neural networks. In particular, for an
L
1
,
∞
L
weight normalized network, the approximation error can be controlled by the
L
1
L
norm of the output layer, and the corresponding generalization error only depends on the architecture by the square root of the depth."
neurips,https://proceedings.neurips.cc/paper/2018/file/81448138f5f163ccdba4acc69819f280-Paper.pdf,Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks,"Hang Gao, Zheng Shou, Alireza Zareian, Hanwang Zhang, Shih-Fu Chang",
neurips,https://proceedings.neurips.cc/paper/2018/file/814a9c18f5abff398787c9cfcbf3d80c-Paper.pdf,Bilevel Distance Metric Learning for Robust Image Recognition,"Jie Xu, Lei Luo, Cheng Deng, Heng Huang",
neurips,https://proceedings.neurips.cc/paper/2018/file/819e3d6c1381eac87c17617e5165f38c-Paper.pdf,Integrated accounts of behavioral and neuroimaging data using flexible recurrent neural network models,"Amir Dezfouli, Richard Morris, Fabio T. Ramos, Peter Dayan, Bernard Balleine",
neurips,https://proceedings.neurips.cc/paper/2018/file/8208974663db80265e9bfe7b222dcb18-Paper.pdf,Learning from Group Comparisons: Exploiting Higher Order Interactions,"Yao Li, Minhao Cheng, Kevin Fujii, Fushing Hsieh, Cho-Jui Hsieh",
neurips,https://proceedings.neurips.cc/paper/2018/file/82161242827b703e6acf9c726942a1e4-Paper.pdf,"Cooperative Holistic Scene Understanding: Unifying 3D Object, Layout, and Camera Pose Estimation","Siyuan Huang, Siyuan Qi, Yinxue Xiao, Yixin Zhu, Ying Nian Wu, Song-Chun Zhu",
neurips,https://proceedings.neurips.cc/paper/2018/file/82cadb0649a3af4968404c9f6031b233-Paper.pdf,A Bandit Approach to Sequential Experimental Design with False Discovery Control,"Kevin G. Jamieson, Lalit Jain","We propose a new adaptive sampling approach to multiple testing which aims to maximize statistical power while ensuring anytime false discovery control. We consider
n
n
distributions whose means are partitioned by whether they are below or equal to a baseline (nulls), versus above the baseline (true positives). In addition, each distribution can be sequentially and repeatedly sampled. Using techniques from multi-armed bandits, we provide an algorithm that takes as few samples as possible to exceed a target true positive proportion (i.e. proportion of true positives discovered) while giving anytime control of the false discovery proportion (nulls predicted as true positives). Our sample complexity results match known information theoretic lower bounds and through simulations we show a substantial performance improvement over uniform sampling and an adaptive elimination style algorithm. Given the simplicity of the approach, and its sample efficiency, the method has promise for wide adoption in the biological sciences, clinical testing for drug discovery, and maximization of click through in A/B/n testing problems."
neurips,https://proceedings.neurips.cc/paper/2018/file/82cec96096d4281b7c95cd7e74623496-Paper.pdf,HitNet: Hybrid Ternary Recurrent Neural Network,"Peiqi Wang, Xinfeng Xie, Lei Deng, Guoqi Li, Dongsheng Wang, Yuan Xie",
neurips,https://proceedings.neurips.cc/paper/2018/file/82f2b308c3b01637c607ce05f52a2fed-Paper.pdf,SLAYER: Spike Layer Error Reassignment in Time,"Sumit Bam Shrestha, Garrick Orchard",
neurips,https://proceedings.neurips.cc/paper/2018/file/831caa1b600f852b7844499430ecac17-Paper.pdf,A Convex Duality Framework for GANs,"Farzan Farnia, David Tse",
neurips,https://proceedings.neurips.cc/paper/2018/file/83cdcec08fbf90370fcf53bdd56604ff-Paper.pdf,Empirical Risk Minimization Under Fairness Constraints,"Michele Donini, Luca Oneto, Shai Ben-David, John S. Shawe-Taylor, Massimiliano Pontil",
neurips,https://proceedings.neurips.cc/paper/2018/file/842424a1d0595b76ec4fa03c46e8d755-Paper.pdf,End-to-End Differentiable Physics for Learning and Control,"Filipe de Avila Belbute-Peres, Kevin Smith, Kelsey Allen, Josh Tenenbaum, J. Zico Kolter",
neurips,https://proceedings.neurips.cc/paper/2018/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf,A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation,"Alexander H. Liu, Yen-Cheng Liu, Yu-Ying Yeh, Yu-Chiang Frank Wang",
neurips,https://proceedings.neurips.cc/paper/2018/file/84b20b1f5a0d103f5710bb67a043cd78-Paper.pdf,Horizon-Independent Minimax Linear Regression,"Alan Malek, Peter L. Bartlett",
neurips,https://proceedings.neurips.cc/paper/2018/file/84b64e537f08e81b8dea8cce972a28b2-Paper.pdf,Support Recovery for Orthogonal Matching Pursuit: Upper and Lower bounds,"Raghav Somani, Chirag Gupta, Prateek Jain, Praneeth Netrapalli",
neurips,https://proceedings.neurips.cc/paper/2018/file/84ddfb34126fc3a48ee38d7044e87276-Paper.pdf,Beauty-in-averageness and its contextual modulations: A Bayesian statistical account,"Chaitanya Ryali, Angela J. Yu",
neurips,https://proceedings.neurips.cc/paper/2018/file/84f0f20482cde7e5eacaf7364a643d33-Paper.pdf,The committee machine: Computational to statistical gaps in learning a two-layers neural network,"Benjamin Aubin, Antoine Maillard, jean barbier, Florent Krzakala, Nicolas Macris, Lenka Zdeborová",
neurips,https://proceedings.neurips.cc/paper/2018/file/851ddf5058cf22df63d3344ad89919cf-Paper.pdf,Adversarial vulnerability for any classifier,"Alhussein Fawzi, Hamza Fawzi, Omar Fawzi",
neurips,https://proceedings.neurips.cc/paper/2018/file/85422afb467e9456013a2a51d4dff702-Paper.pdf,A Deep Bayesian Policy Reuse Approach Against Non-Stationary Agents,"YAN ZHENG, Zhaopeng Meng, Jianye Hao, Zongzhang Zhang, Tianpei Yang, Changjie Fan",
neurips,https://proceedings.neurips.cc/paper/2018/file/8562ae5e286544710b2e7ebe9858833b-Paper.pdf,Adversarial Examples that Fool both Computer Vision and Time-Limited Humans,"Gamaleldin Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alexey Kurakin, Ian Goodfellow, Jascha Sohl-Dickstein",
neurips,https://proceedings.neurips.cc/paper/2018/file/8597a6cfa74defcbde3047c891d78f90-Paper.pdf,Trajectory Convolution for Action Recognition,"Yue Zhao, Yuanjun Xiong, Dahua Lin",
neurips,https://proceedings.neurips.cc/paper/2018/file/85f007f8c50dd25f5a45fca73cad64bd-Paper.pdf,Adversarial Attacks on Stochastic Bandits,"Kwang-Sung Jun, Lihong Li, Yuzhe Ma, Jerry Zhu","We study adversarial attacks that manipulate the reward signals to control the actions chosen by a stochastic multi-armed bandit algorithm. We propose the first attack against two popular bandit algorithms:
ϵ
ϵ
-greedy and UCB, \emph{without} knowledge of the mean rewards. The attacker is able to spend only logarithmic effort, multiplied by a problem-specific parameter that becomes smaller as the bandit problem gets easier to attack. The result means the attacker can easily hijack the behavior of the bandit algorithm to promote or obstruct certain actions, say, a particular medical treatment. As bandits are seeing increasingly wide use in practice, our study exposes a significant security threat."
neurips,https://proceedings.neurips.cc/paper/2018/file/85fc37b18c57097425b52fc7afbb6969-Paper.pdf,Evolution-Guided Policy Gradient in Reinforcement Learning,"Shauharda Khadka, Kagan Tumer",
neurips,https://proceedings.neurips.cc/paper/2018/file/8643c8e2107ba86c47371e037059c4b7-Paper.pdf,Policy Regret in Repeated Games,"Raman Arora, Michael Dinitz, Teodor Vanislavov Marinov, Mehryar Mohri",
neurips,https://proceedings.neurips.cc/paper/2018/file/86a1793f65aeef4aeef4b479fc9b2bca-Paper.pdf,Causal Inference with Noisy and Missing Covariates via Matrix Factorization,"Nathan Kallus, Xiaojie Mao, Madeleine Udell",
neurips,https://proceedings.neurips.cc/paper/2018/file/86b20716fbd5b253d27cec43127089bc-Paper.pdf,Bayesian Distributed Stochastic Gradient Descent,"Michael Teng, Frank Wood",
neurips,https://proceedings.neurips.cc/paper/2018/file/86ba98bcbd3466d253841907ba1fc725-Paper.pdf,Benefits of over-parameterization with EM,"Ji Xu, Daniel J. Hsu, Arian Maleki",
neurips,https://proceedings.neurips.cc/paper/2018/file/882735cbdfd9f810814d17892ae50023-Paper.pdf,How to tell when a clustering is (approximately) correct using convex relaxations,Marina Meila,
neurips,https://proceedings.neurips.cc/paper/2018/file/894b77f805bd94d292574c38c5d628d5-Paper.pdf,Faithful Inversion of Generative Models for Effective Amortized Inference,"Stefan Webb, Adam Golinski, Rob Zinkov, Siddharth N, Tom Rainforth, Yee Whye Teh, Frank Wood",
neurips,https://proceedings.neurips.cc/paper/2018/file/89885ff2c83a10305ee08bd507c1049c-Paper.pdf,TETRIS: TilE-matching the TRemendous Irregular Sparsity,"Yu Ji, Ling Liang, Lei Deng, Youyang Zhang, Youhui Zhang, Yuan Xie","In this work, we propose a novel method, TETRIS, to achieve both better hardware utilization and higher sparsity. Just like a tile-matching game, we cluster the irregularly distributed weights with small value into structured groups by reordering the input/output dimension and structurally prune them. Results show that it can achieve comparable sparsity with the irregular element-wise pruning and demonstrate negligible accuracy loss. The experiments also shows ideal speedup, which is proportional to the sparsity, on GPU platforms. Our proposed method provides a new solution toward algorithm and architecture co-optimization for accuracy-efficiency trade-off."
neurips,https://proceedings.neurips.cc/paper/2018/file/898aef0932f6aaecda27aba8e9903991-Paper.pdf,Stochastic Chebyshev Gradient Descent for Spectral Optimization,"Insu Han, Haim Avron, Jinwoo Shin",
neurips,https://proceedings.neurips.cc/paper/2018/file/8a1ee9f2b7abe6e88d1a479ab6a42c5e-Paper.pdf,Model-based targeted dimensionality reduction for neuronal population data,"Mikio Aoi, Jonathan W. Pillow",
neurips,https://proceedings.neurips.cc/paper/2018/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf,BourGAN: Generative Networks with Metric Embeddings,"Chang Xiao, Peilin Zhong, Changxi Zheng",
neurips,https://proceedings.neurips.cc/paper/2018/file/8a36dfc67ebfbbea9bd01cd8a4c8ad32-Paper.pdf,Online Robust Policy Learning in the Presence of Unknown Adversaries,"Aaron Havens, Zhanhong Jiang, Soumik Sarkar",
neurips,https://proceedings.neurips.cc/paper/2018/file/8a7129b8f3edd95b7d969dfc2c8e9d9d-Paper.pdf,Representer Point Selection for Explaining Deep Neural Networks,"Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, Pradeep K. Ravikumar",
neurips,https://proceedings.neurips.cc/paper/2018/file/8a94ecfa54dcb88a2fa993bfa6388f9e-Paper.pdf,Watch Your Step: Learning Node Embeddings via Graph Attention,"Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, Alexander A. Alemi",
neurips,https://proceedings.neurips.cc/paper/2018/file/8b16ebc056e613024c057be590b542eb-Paper.pdf,"ℓ
1
ℓ
-regression with Heavy-tailed Distributions","Lijun Zhang, Zhi-Hua Zhou","In this paper, we consider the problem of linear regression with heavy-tailed distributions. Different from previous studies that use the squared loss to measure the performance, we choose the absolute loss, which is capable of estimating the conditional median. To address the challenge that both the input and output could be heavy-tailed, we propose a truncated minimization problem, and demonstrate that it enjoys an
O
(
√
d
/
n
)
O
excess risk, where
d
d
is the dimensionality and
n
n
is the number of samples. Compared with traditional work on
ℓ
1
ℓ
-regression, the main advantage of our result is that we achieve a high-probability risk bound without exponential moment conditions on the input and output. Furthermore, if the input is bounded, we show that the classical empirical risk minimization is competent for
ℓ
1
ℓ
-regression even when the output is heavy-tailed."
neurips,https://proceedings.neurips.cc/paper/2018/file/8b4224068a41c5d37f5e2d54f3995089-Paper.pdf,Learning Confidence Sets using Support Vector Machines,"Wenbo Wang, Xingye Qiao",
neurips,https://proceedings.neurips.cc/paper/2018/file/8b5700012be65c9da25f49408d959ca0-Paper.pdf,Learning to Optimize Tensor Programs,"Tianqi Chen, Lianmin Zheng, Eddie Yan, Ziheng Jiang, Thierry Moreau, Luis Ceze, Carlos Guestrin, Arvind Krishnamurthy",
neurips,https://proceedings.neurips.cc/paper/2018/file/8b6a80c3cf2cbd5f967063618dc54f39-Paper.pdf,Hybrid-MST: A Hybrid Active Sampling Strategy for Pairwise Preference Aggregation,"JING LI, Rafal Mantiuk, Junle Wang, Suiyi Ling, Patrick Le Callet",
neurips,https://proceedings.neurips.cc/paper/2018/file/8b8388180314a337c9aa3c5aa8e2f37a-Paper.pdf,A no-regret generalization of hierarchical softmax to extreme multi-label classification,"Marek Wydmuch, Kalina Jasinska, Mikhail Kuznetsov, Róbert Busa-Fekete, Krzysztof Dembczynski","Extreme multi-label classification (XMLC) is a problem of tagging an instance with a small subset of relevant labels chosen from an extremely large pool of possible labels. Large label spaces can be efficiently handled by organizing labels as a tree, like in the hierarchical softmax (HSM) approach commonly used for multi-class problems. In this paper, we investigate probabilistic label trees (PLTs) that have been recently devised for tackling XMLC problems. We show that PLTs are a no-regret multi-label generalization of HSM when precision@
k
k
is used as a model evaluation metric. Critically, we prove that pick-one-label heuristic---a reduction technique from multi-label to multi-class that is routinely used along with HSM---is not consistent in general. We also show that our implementation of PLTs, referred to as extremeText (XT), obtains significantly better results than HSM with the pick-one-label heuristic and XML-CNN, a deep network specifically designed for XMLC problems. Moreover, XT is competitive to many state-of-the-art approaches in terms of statistical performance, model size and prediction time which makes it amenable to deploy in an online system."
neurips,https://proceedings.neurips.cc/paper/2018/file/8ba6c657b03fc7c8dd4dff8e45defcd2-Paper.pdf,The Sample Complexity of Semi-Supervised Learning with Nonparametric Mixture Models,"Chen Dan, Liu Leqi, Bryon Aragam, Pradeep K. Ravikumar, Eric P. Xing","We study the sample complexity of semi-supervised learning (SSL) and introduce new assumptions based on the mismatch between a mixture model learned from unlabeled data and the true mixture model induced by the (unknown) class conditional distributions. Under these assumptions, we establish an
Ω
(
K
log
K
)
Ω
labeled sample complexity bound without imposing parametric assumptions, where
K
K
is the number of classes. Our results suggest that even in nonparametric settings it is possible to learn a near-optimal classifier using only a few labeled samples. Unlike previous theoretical work which focuses on binary classification, we consider general multiclass classification (
K
>
2
K
), which requires solving a difficult permutation learning problem. This permutation defines a classifier whose classification error is controlled by the Wasserstein distance between mixing measures, and we provide finite-sample results characterizing the behaviour of the excess risk of this classifier. Finally, we describe three algorithms for computing these estimators based on a connection to bipartite graph matching, and perform experiments to illustrate the superiority of the MLE over the majority vote estimator."
neurips,https://proceedings.neurips.cc/paper/2018/file/8bd39eae38511daad6152e84545e504d-Paper.pdf,Estimating Learnability in the Sublinear Data Regime,"Weihao Kong, Gregory Valiant","We consider the problem of estimating how well a model class is capable of fitting a distribution of labeled data. We show that it is often possible to accurately estimate this
learnability'' even when given an amount of data that is too small to reliably learn any accurate model. Our first result applies to the setting where the data is drawn from a
d
d
-dimensional distribution with isotropic covariance, and the label of each datapoint is an arbitrary noisy function of the datapoint. In this setting, we show that with
O
(
√
d
)
O
samples, one can accurately estimate the fraction of the variance of the label that can be explained via the best linear function of the data. We extend these techniques to a binary classification, and show that the prediction error of the best linear classifier can be accurately estimated given
O
(
√
d
)
O
labeled samples. For comparison, in both the linear regression and binary classification settings, even if there is no noise in the labels, a sample size linear in the dimension,
d
d
, is required to \emph{learn} any function correlated with the underlying model. We further extend our estimation approach to the setting where the data distribution has an (unknown) arbitrary covariance matrix, allowing these techniques to be applied to settings where the model class consists of a linear function applied to a nonlinear embedding of the data. We demonstrate the practical viability of our approaches on synthetic and real data. This ability to estimate the explanatory value of a set of features (or dataset), even in the regime in which there is too little data to realize that explanatory value, may be relevant to the scientific and industrial settings for which data collection is expensive and there are many potentially relevant feature sets that could be collected."
neurips,https://proceedings.neurips.cc/paper/2018/file/8bdb5058376143fa358981954e7626b8-Paper.pdf,Multi-armed Bandits with Compensation,"Siwei Wang, Longbo Huang","We propose and study the known-compensation multi-arm bandit (KCMAB) problem, where a system controller offers a set of arms to many short-term players for
T
T
steps. In each step, one short-term player arrives to the system. Upon arrival, the player greedily selects an arm with the current best average reward and receives a stochastic reward associated with the arm. In order to incentivize players to explore other arms, the controller provides proper payment compensation to players. The objective of the controller is to maximize the total reward collected by players while minimizing the compensation. We first give a compensation lower bound
Θ
(
∑
i
Δ
i
log
T
K
L
i
)
Θ
, where
Δ
i
Δ
and
K
L
i
K
are the expected reward gap and Kullback-Leibler (KL) divergence between distributions of arm
i
i
and the best arm, respectively. We then analyze three algorithms to solve the KCMAB problem, and obtain their regrets and compensations. We show that the algorithms all achieve
O
(
log
T
)
O
regret and
O
(
log
T
)
O
compensation that match the theoretical lower bound. Finally, we use experiments to show the behaviors of those algorithms."
neurips,https://proceedings.neurips.cc/paper/2018/file/8bf1211fd4b7b94528899de0a43b9fb3-Paper.pdf,A Neural Compositional Paradigm for Image Captioning,"Bo Dai, Sanja Fidler, Dahua Lin",
neurips,https://proceedings.neurips.cc/paper/2018/file/8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf,Stochastic Composite Mirror Descent: Optimal Bounds with High Probabilities,"Yunwen Lei, Ke Tang",
neurips,https://proceedings.neurips.cc/paper/2018/file/8c9f32e03aeb2e3000825c8c875c4edd-Paper.pdf,Geometry-Aware Recurrent Neural Networks for Active Visual Recognition,"Ricson Cheng, Ziyan Wang, Katerina Fragkiadaki",
neurips,https://proceedings.neurips.cc/paper/2018/file/8cbe9ce23f42628c98f80fa0fac8b19a-Paper.pdf,Reward learning from human preferences and demonstrations in Atari,"Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, Dario Amodei",
neurips,https://proceedings.neurips.cc/paper/2018/file/8ce87bdda85cd44f14de9afb86491884-Paper.pdf,Rectangular Bounding Process,"Xuhui Fan, Bin Li, Scott SIsson",
neurips,https://proceedings.neurips.cc/paper/2018/file/8cea559c47e4fbdb73b23e0223d04e79-Paper.pdf,Constructing Unrestricted Adversarial Examples with Generative Models,"Yang Song, Rui Shu, Nate Kushman, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2018/file/8d3369c4c086f236fabf61d614a32818-Paper.pdf,Causal Discovery from Discrete Data using Hidden Compact Representation,"Ruichu Cai, Jie Qiao, Kun Zhang, Zhenjie Zhang, Zhifeng Hao",
neurips,https://proceedings.neurips.cc/paper/2018/file/8d34201a5b85900908db6cae92723617-Paper.pdf,Boosted Sparse and Low-Rank Tensor Regression,"Lifang He, Kun Chen, Wanwan Xu, Jiayu Zhou, Fei Wang",
neurips,https://proceedings.neurips.cc/paper/2018/file/8d3bba7425e7c98c50f52ca1b52d3735-Paper.pdf,Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search,"Zhuwen Li, Qifeng Chen, Vladlen Koltun",
neurips,https://proceedings.neurips.cc/paper/2018/file/8d7628dd7a710c8638dbd22d4421ee46-Paper.pdf,Chaining Mutual Information and Tightening Generalization Bounds,"Amir Asadi, Emmanuel Abbe, Sergio Verdu",
neurips,https://proceedings.neurips.cc/paper/2018/file/8d9766a69b764fefc12f56739424d136-Paper.pdf,Modeling Dynamic Missingness of Implicit Feedback for Recommendation,"Menghan Wang, Mingming Gong, Xiaolin Zheng, Kun Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/8e0384779e58ce2af40eb365b318cc32-Paper.pdf,Does mitigating ML's impact disparity require treatment disparity?,"Zachary Lipton, Julian McAuley, Alexandra Chouldechova",
neurips,https://proceedings.neurips.cc/paper/2018/file/8e19a39c36b8e5e3afd2a3b2692aea96-Paper.pdf,The Sparse Manifold Transform,"Yubei Chen, Dylan Paiton, Bruno Olshausen",
neurips,https://proceedings.neurips.cc/paper/2018/file/8e2c381d4dd04f1c55093f22c59c3a08-Paper.pdf,Probabilistic Model-Agnostic Meta-Learning,"Chelsea Finn, Kelvin Xu, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2018/file/8e489b4966fe8f703b5be647f1cbae63-Paper.pdf,Deep Neural Networks with Box Convolutions,"Egor Burkov, Victor Lempitsky",
neurips,https://proceedings.neurips.cc/paper/2018/file/8e621619d71d0ae5ef4e631ad586334f-Paper.pdf,Learning Compressed Transforms with Low Displacement Rank,"Anna Thomas, Albert Gu, Tri Dao, Atri Rudra, Christopher Ré",
neurips,https://proceedings.neurips.cc/paper/2018/file/8f121ce07d74717e0b1f21d122e04521-Paper.pdf,Deep Defense: Training DNNs with Improved Adversarial Robustness,"Ziang Yan, Yiwen Guo, Changshui Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf,Precision and Recall for Time Series,"Nesime Tatbul, Tae Jun Lee, Stan Zdonik, Mejbah Alam, Justin Gottschlich",
neurips,https://proceedings.neurips.cc/paper/2018/file/8f7d807e1f53eff5f9efbe5cb81090fb-Paper.pdf,Neighbourhood Consensus Networks,"Ignacio Rocco, Mircea Cimpoi, Relja Arandjelović, Akihiko Torii, Tomas Pajdla, Josef Sivic",
neurips,https://proceedings.neurips.cc/paper/2018/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf,PAC-learning in the presence of adversaries,"Daniel Cullina, Arjun Nitin Bhagoji, Prateek Mittal",
neurips,https://proceedings.neurips.cc/paper/2018/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf,Optimal Algorithms for Non-Smooth Distributed Optimization in Networks,"Kevin Scaman, Francis Bach, Sebastien Bubeck, Laurent Massoulié, Yin Tat Lee","In this work, we consider the distributed optimization of non-smooth convex functions using a network of computing units. We investigate this problem under two regularity assumptions: (1) the Lipschitz continuity of the global objective function, and (2) the Lipschitz continuity of local individual functions. Under the local regularity assumption, we provide the first optimal first-order decentralized algorithm called multi-step primal-dual (MSPD) and its corresponding optimal convergence rate. A notable aspect of this result is that, for non-smooth functions, while the dominant term of the error is in
O
(
1
/
√
t
)
O
, the structure of the communication network only impacts a second-order term in
O
(
1
/
t
)
O
, where
t
t
is time. In other words, the error due to limits in communication resources decreases at a fast rate even in the case of non-strongly-convex objective functions. Under the global regularity assumption, we provide a simple yet efficient algorithm called distributed randomized smoothing (DRS) based on a local smoothing of the objective function, and show that DRS is within a
d
1
/
4
d
multiplicative factor of the optimal convergence rate, where
d
d
is the underlying dimension."
neurips,https://proceedings.neurips.cc/paper/2018/file/900c563bfd2c48c16701acca83ad858a-Paper.pdf,Large-Scale Stochastic Sampling from the Probability Simplex,"Jack Baker, Paul Fearnhead, Emily Fox, Christopher Nemeth",
neurips,https://proceedings.neurips.cc/paper/2018/file/9023effe3c16b0477df9b93e26d57e2c-Paper.pdf,Transfer of Value Functions via Variational Methods,"Andrea Tirinzoni, Rafael Rodriguez Sanchez, Marcello Restelli",
neurips,https://proceedings.neurips.cc/paper/2018/file/90365351ccc7437a1309dc64e4db32a3-Paper.pdf,Adaptive Methods for Nonconvex Optimization,"Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, Sanjiv Kumar",
neurips,https://proceedings.neurips.cc/paper/2018/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf,How Does Batch Normalization Help Optimization?,"Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, Aleksander Madry",
neurips,https://proceedings.neurips.cc/paper/2018/file/907edb0aa6986220dbffb79a788596ee-Paper.pdf,Compact Generalized Non-local Network,"Kaiyu Yue, Ming Sun, Yuchen Yuan, Feng Zhou, Errui Ding, Fuxin Xu",
neurips,https://proceedings.neurips.cc/paper/2018/file/9087b0efc7c7acd1ef7e153678809c77-Paper.pdf,Stacked Semantics-Guided Attention Model for Fine-Grained Zero-Shot Learning,"yunlong yu, Zhong Ji, Yanwei Fu, Jichang Guo, Yanwei Pang, Zhongfei (Mark) Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/908c9a564a86426585b29f5335b619bc-Paper.pdf,MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models,"Boyuan Pan, Yazheng Yang, Hao Li, Zhou Zhao, Yueting Zhuang, Deng Cai, Xiaofei He",
neurips,https://proceedings.neurips.cc/paper/2018/file/90e1357833654983612fb05e3ec9148c-Paper.pdf,Multiplicative Weights Updates with Constant Step-Size in Graphical Constant-Sum Games,Yun Kuen Cheung,"Interestingly, the regret perspective can be useful for better understanding of the behaviours of MW updates. In a two-person zero-sum game, if it has a unique NE which is fully mixed, then we show, via regret, that for any sufficiently small eps, there exist at least two probability densities and a constant Z > 0, such that for any arbitrarily small z > 0, each of the two densities fluctuates above Z and below z infinitely often."
neurips,https://proceedings.neurips.cc/paper/2018/file/91c77393975889bd08f301c9e13a44b7-Paper.pdf,Efficient Online Portfolio with Logarithmic Regret,"Haipeng Luo, Chen-Yu Wei, Kai Zheng","We study the decades-old problem of online portfolio management and propose the first algorithm with logarithmic regret that is not based on Cover's Universal Portfolio algorithm and admits much faster implementation. Specifically Universal Portfolio enjoys optimal regret
O
(
N
ln
T
)
O
for
N
N
financial instruments over
T
T
rounds, but requires log-concave sampling and has a large polynomial running time. Our algorithm, on the other hand, ensures a slightly larger but still logarithmic regret of
O
(
N
2
(
ln
T
)
4
)
O
, and is based on the well-studied Online Mirror Descent framework with a novel regularizer that can be implemented via standard optimization methods in time
O
(
T
N
2.5
)
O
per round. The regret of all other existing works is either polynomial in
T
T
or has a potentially unbounded factor such as the inverse of the smallest price relative."
neurips,https://proceedings.neurips.cc/paper/2018/file/91d0dbfd38d950cb716c4dd26c5da08a-Paper.pdf,Banach Wasserstein GAN,"Jonas Adler, Sebastian Lunz","Wasserstein Generative Adversarial Networks (WGANs) can be used to generate realistic samples from complicated image distributions. The Wasserstein metric used in WGANs is based on a notion of distance between individual images, which induces a notion of distance between probability distributions of images. So far the community has considered
ℓ
2
ℓ
as the underlying distance. We generalize the theory of WGAN with gradient penalty to Banach spaces, allowing practitioners to select the features to emphasize in the generator. We further discuss the effect of some particular choices of underlying norms, focusing on Sobolev norms. Finally, we demonstrate a boost in performance for an appropriate choice of norm on CIFAR-10 and CelebA."
neurips,https://proceedings.neurips.cc/paper/2018/file/9246444d94f081e3549803b928260f56-Paper.pdf,SplineNets: Continuous Neural Decision Graphs,"Cem Keskin, Shahram Izadi",
neurips,https://proceedings.neurips.cc/paper/2018/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf,Post: Device Placement with Cross-Entropy Minimization and Proximal Policy Optimization,"Yuanxiang Gao, Li Chen, Baochun Li",
neurips,https://proceedings.neurips.cc/paper/2018/file/92c8c96e4c37100777c7190b76d28233-Paper.pdf,Implicit Reparameterization Gradients,"Mikhail Figurnov, Shakir Mohamed, Andriy Mnih",
neurips,https://proceedings.neurips.cc/paper/2018/file/92cc227532d17e56e07902b254dfad10-Paper.pdf,Visual Object Networks: Image Generation with Disentangled 3D Representations,"Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jiajun Wu, Antonio Torralba, Josh Tenenbaum, Bill Freeman",
neurips,https://proceedings.neurips.cc/paper/2018/file/933670f1ac8ba969f32989c312faba75-Paper.pdf,Neural Architecture Optimization,"Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, Tie-Yan Liu","Automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Existing methods, no matter based on reinforcement learning or evolutionary algorithms (EA), conduct architecture search in a discrete space, which is highly inefficient. In this paper, we propose a simple and efficient method to automatic neural architecture design based on continuous optimization. We call this new approach neural architecture optimization (NAO). There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. (2) A predictor takes the continuous representation of a network as input and predicts its accuracy. (3) A decoder maps a continuous representation of a network back to its architecture. The performance predictor and the encoder enable us to perform gradient based optimization in the continuous space to find the embedding of a new architecture with potentially better accuracy. Such a better embedding is then decoded to a network by the decoder. Experiments show that the architecture discovered by our method is very competitive for image classification task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods with a significantly reduction of computational resources. Specifically we obtain
2.11
%
2.11
test set error rate for CIFAR-10 image classification task and
56.0
56.0
test set perplexity of PTB language modeling task. The best discovered architectures on both tasks are successfully transferred to other tasks such as CIFAR-100 and WikiText-2. Furthermore, combined with the recent proposed weight sharing mechanism, we discover powerful architecture on CIFAR-10 (with error rate
3.53
%
3.53
) and on PTB (with test set perplexity
56.6
56.6
), with very limited computational resources (less than
10
10
GPU hours) for both tasks."
neurips,https://proceedings.neurips.cc/paper/2018/file/934b535800b1cba8f96a5d72f72f1611-Paper.pdf,MiME: Multilevel Medical Embedding of Electronic Health Records for Predictive Healthcare,"Edward Choi, Cao Xiao, Walter Stewart, Jimeng Sun",
neurips,https://proceedings.neurips.cc/paper/2018/file/93d65641ff3f1586614cf2c1ad240b6c-Paper.pdf,Learning Optimal Reserve Price against Non-myopic Bidders,"Jinyan Liu, Zhiyi Huang, Xiangning Wang",
neurips,https://proceedings.neurips.cc/paper/2018/file/943aa0fcda4ee2901a7de9321663b114-Paper.pdf,A Bayesian Approach to Generative Adversarial Imitation Learning,"Wonseok Jeon, Seokin Seo, Kee-Eung Kim",
neurips,https://proceedings.neurips.cc/paper/2018/file/94bb077f18daa6620efa5cf6e6f178d2-Paper.pdf,Credit Assignment For Collective Multiagent RL With Global Rewards,"Duc Thien Nguyen, Akshat Kumar, Hoong Chuin Lau",
neurips,https://proceedings.neurips.cc/paper/2018/file/94ef7214c4a90790186e255304f8fd1f-Paper.pdf,Knowledge Distillation by On-the-Fly Native Ensemble,"xu lan, Xiatian Zhu, Shaogang Gong",
neurips,https://proceedings.neurips.cc/paper/2018/file/955cb567b6e38f4c6b3f28cc857fc38c-Paper.pdf,Flexible and accurate inference and learning for deep generative models,"Eszter Vértes, Maneesh Sahani",
neurips,https://proceedings.neurips.cc/paper/2018/file/959a557f5f6beb411fd954f3f34b21c3-Paper.pdf,A loss framework for calibrated anomaly detection,,
neurips,https://proceedings.neurips.cc/paper/2018/file/959ab9a0695c467e7caf75431a872e5c-Paper.pdf,Persistence Fisher Kernel: A Riemannian Manifold Kernel for Persistence Diagrams,"Tam Le, Makoto Yamada",
neurips,https://proceedings.neurips.cc/paper/2018/file/95d309f0b035d97f69902e7972c2b2e6-Paper.pdf,Constructing Deep Neural Networks by Bayesian Network Structure Learning,"Raanan Y. Rohekar, Shami Nisimov, Yaniv Gurwicz, Guy Koren, Gal Novik",
neurips,https://proceedings.neurips.cc/paper/2018/file/967990de5b3eac7b87d49a13c6834978-Paper.pdf,"Training Deep Models Faster with Robust, Approximate Importance Sampling","Tyler B. Johnson, Carlos Guestrin",
neurips,https://proceedings.neurips.cc/paper/2018/file/967c2ae04b169f07e7fa8fdfd110551e-Paper.pdf,Learning Beam Search Policies via Imitation Learning,"Renato Negrinho, Matthew Gormley, Geoffrey J. Gordon",
neurips,https://proceedings.neurips.cc/paper/2018/file/96b9bff013acedfb1d140579e2fbeb63-Paper.pdf,Multivariate Time Series Imputation with Generative Adversarial Networks,"Yonghong Luo, Xiangrui Cai, Ying ZHANG, Jun Xu, Yuan xiaojie",
neurips,https://proceedings.neurips.cc/paper/2018/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf,An Efficient Pruning Algorithm for Robust Isotonic Regression,Cong Han Lim,
neurips,https://proceedings.neurips.cc/paper/2018/file/96ea64f3a1aa2fd00c72faacf0cb8ac9-Paper.pdf,Bilinear Attention Networks,"Jin-Hwa Kim, Jaehyun Jun, Byoung-Tak Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf,Unsupervised Video Object Segmentation for Deep Reinforcement Learning,"Vikash Goel, Jameson Weng, Pascal Poupart",
neurips,https://proceedings.neurips.cc/paper/2018/file/9719a00ed0c5709d80dfef33795dcef3-Paper.pdf,Constructing Fast Network through Deconstruction of Convolution,"Yunho Jeon, Junmo Kim",
neurips,https://proceedings.neurips.cc/paper/2018/file/972cda1e62b72640cb7ac702714a115f-Paper.pdf,Improving Simple Models with Confidence Profiles,"Amit Dhurandhar, Karthikeyan Shanmugam, Ronny Luss, Peder A. Olsen","In this paper, we propose a new method called ProfWeight for transferring information from a pre-trained deep neural network that has a high test accuracy to a simpler interpretable model or a very shallow network of low complexity and a priori low test accuracy. We are motivated by applications in interpretability and model deployment in severely memory constrained environments (like sensors). Our method uses linear probes to generate confidence scores through flattened intermediate representations. Our transfer method involves a theoretically justified weighting of samples during the training of the simple model using confidence scores of these intermediate layers. The value of our method is first demonstrated on CIFAR-10, where our weighting method significantly improves (3-4\%) networks with only a fraction of the number of Resnet blocks of a complex Resnet model. We further demonstrate operationally significant results on a real manufacturing problem, where we dramatically increase the test accuracy of a CART model (the domain standard) by roughly
13
%
13
."
neurips,https://proceedings.neurips.cc/paper/2018/file/976abf49974d4686f87192efa0513ae0-Paper.pdf,Turbo Learning for CaptionBot and DrawingBot,"Qiuyuan Huang, Pengchuan Zhang, Dapeng Wu, Lei Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/97af07a14cacba681feacf3012730892-Paper.pdf,Online Reciprocal Recommendation with Theoretical Performance Guarantees,"Fabio Vitale, Nikos Parotsidis, Claudio Gentile",
neurips,https://proceedings.neurips.cc/paper/2018/file/97e8527feaf77a97fc38f34216141515-Paper.pdf,Learning semantic similarity in a continuous space,Michel Deudon,
neurips,https://proceedings.neurips.cc/paper/2018/file/980ecd059122ce2e50136bda65c25e07-Paper.pdf,Representation Balancing MDPs for Off-policy Policy Evaluation,"Yao Liu, Omer Gottesman, Aniruddh Raghu, Matthieu Komorowski, Aldo A. Faisal, Finale Doshi-Velez, Emma Brunskill",
neurips,https://proceedings.neurips.cc/paper/2018/file/9872ed9fc22fc182d371c3e9ed316094-Paper.pdf,See and Think: Disentangling Semantic Scene Completion,"Shice Liu, YU HU, Yiming Zeng, Qiankun Tang, Beibei Jin, Yinhe Han, Xiaowei Li",
neurips,https://proceedings.neurips.cc/paper/2018/file/98b17f068d5d9b7668e19fb8ae470841-Paper.pdf,L4: Practical loss-based stepsize adaptation for deep learning,"Michal Rolinek, Georg Martius",
neurips,https://proceedings.neurips.cc/paper/2018/file/99064ba6631e279d4a74622df99657d6-Paper.pdf,Generalisation of structural knowledge in the hippocampal-entorhinal system,"James Whittington, Timothy Muller, Shirely Mark, Caswell Barry, Tim Behrens",
neurips,https://proceedings.neurips.cc/paper/2018/file/9908279ebbf1f9b250ba689db6a0222b-Paper.pdf,Pelee: A Real-Time Object Detection System on Mobile Devices,"Robert J. Wang, Xiang Li, Charles X. Ling",
neurips,https://proceedings.neurips.cc/paper/2018/file/99607461cdb9c26e2bd5f31b12dcf27a-Paper.pdf,Co-regularized Alignment for Unsupervised Domain Adaptation,"Abhishek Kumar, Prasanna Sattigeri, Kahini Wadhawan, Leonid Karlinsky, Rogerio Feris, Bill Freeman, Gregory Wornell",
neurips,https://proceedings.neurips.cc/paper/2018/file/996a7fa078cc36c46d02f9af3bef918b-Paper.pdf,How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD,Zeyuan Allen-Zhu,"Stochastic gradient descent (SGD) gives an optimal convergence rate when minimizing convex stochastic objectives
f
(
x
)
f
. However, in terms of making the gradients small, the original SGD does not give an optimal rate, even when
f
(
x
)
f
is convex. If
f
(
x
)
f
is convex, to find a point with gradient norm
ε
ε
, we design an algorithm SGD3 with a near-optimal rate
~
O
(
ε
−
2
)
O
, improving the best known rate
O
(
ε
−
8
/
3
)
O
. If
f
(
x
)
f
is nonconvex, to find its
ε
ε
-approximate local minimum, we design an algorithm SGD5 with rate
~
O
(
ε
−
3.5
)
O
, where previously SGD variants only achieve
~
O
(
ε
−
4
)
O
. This is no slower than the best known stochastic version of Newton's method in all parameter regimes."
neurips,https://proceedings.neurips.cc/paper/2018/file/99cad265a1768cc2dd013f0e740300ae-Paper.pdf,Entropy Rate Estimation for Markov Chains with Large State Space,"Yanjun Han, Jiantao Jiao, Chuan-Zheng Lee, Tsachy Weissman, Yihong Wu, Tiancheng Yu","Entropy estimation is one of the prototypical problems in distribution property testing. To consistently estimate the Shannon entropy of a distribution on
S
S
elements with independent samples, the optimal sample complexity scales sublinearly with
S
S
as
Θ
(
S
log
S
)
Θ
as shown by Valiant and Valiant \cite{Valiant--Valiant2011}. Extending the theory and algorithms for entropy estimation to dependent data, this paper considers the problem of estimating the entropy rate of a stationary reversible Markov chain with
S
S
states from a sample path of
n
n
observations. We show that
\begin{itemize}
\item Provided the Markov chain mixes not too slowly, \textit{i.e.}, the relaxation time is at most $O(\frac{S}{\ln^3 S})$, consistent estimation is achievable when $n \gg \frac{S^2}{\log S}$.
\item Provided the Markov chain has some slight dependency, \textit{i.e.}, the relaxation time is at least $1+\Omega(\frac{\ln^2 S}{\sqrt{S}})$, consistent estimation is impossible when $n \lesssim \frac{S^2}{\log S}$.
\end{itemize}
\begin{itemize} \item Provided the Markov chain mixes not too slowly, \textit{i.e.}, the relaxation time is at most $O(\frac{S}{\ln^3 S})$, consistent estimation is achievable when $n \gg \frac{S^2}{\log S}$. \item Provided the Markov chain has some slight dependency, \textit{i.e.}, the relaxation time is at least $1+\Omega(\frac{\ln^2 S}{\sqrt{S}})$, consistent estimation is impossible when $n \lesssim \frac{S^2}{\log S}$.\end{itemize}
Under both assumptions, the optimal estimation accuracy is shown to be
Θ
(
S
2
n
log
S
)
Θ
. In comparison, the empirical entropy rate requires at least
Ω
(
S
2
)
Ω
samples to be consistent, even when the Markov chain is memoryless. In addition to synthetic experiments, we also apply the estimators that achieve the optimal sample complexity to estimate the entropy rate of the English language in the Penn Treebank and the Google One Billion Words corpora, which provides a natural benchmark for language modeling and relates it directly to the widely used perplexity measure."
neurips,https://proceedings.neurips.cc/paper/2018/file/9a0ee0a9e7a42d2d69b8f86b3a0756b1-Paper.pdf,Data-dependent PAC-Bayes priors via differential privacy,"Gintare Karolina Dziugaite, Daniel M. Roy",
neurips,https://proceedings.neurips.cc/paper/2018/file/9a1335ef5ffebb0de9d089c4182e4868-Paper.pdf,"Unsupervised Depth Estimation, 3D Face Rotation and Replacement","Joel Ruben Antony Moniz, Christopher Beckham, Simon Rajotte, Sina Honari, Chris Pal",
neurips,https://proceedings.neurips.cc/paper/2018/file/9a96a2c73c0d477ff2a6da3bf538f4f4-Paper.pdf,Information Constraints on Auto-Encoding Variational Bayes,"Romain Lopez, Jeffrey Regier, Michael I. Jordan, Nir Yosef","Parameterizing the approximate posterior of a generative model with neural networks has become a common theme in recent machine learning research. While providing appealing flexibility, this approach makes it difficult to impose or assess structural constraints such as conditional independence. We propose a framework for learning representations that relies on Auto-Encoding Variational Bayes and whose search space is constrained via kernel-based measures of independence. In particular, our method employs the
d
d
-variable Hilbert-Schmidt Independence Criterion (dHSIC) to enforce independence between the latent representations and arbitrary nuisance factors. We show how to apply this method to a range of problems, including the problems of learning invariant representations and the learning of interpretable representations. We also present a full-fledged application to single-cell RNA sequencing (scRNA-seq). In this setting the biological signal in mixed in complex ways with sequencing errors and sampling effects. We show that our method out-performs the state-of-the-art in this domain."
neurips,https://proceedings.neurips.cc/paper/2018/file/9b04d152845ec0a378394003c96da594-Paper.pdf,On Misinformation Containment in Online Social Networks,"Amo Tong, Ding-Zhu Du, Weili Wu","The widespread online misinformation could cause public panic and serious economic damages. The misinformation containment problem aims at limiting the spread of misinformation in online social networks by launching competing campaigns. Motivated by realistic scenarios, we present the first analysis of the misinformation containment problem for the case when an arbitrary number of cascades are allowed. This paper makes four contributions. First, we provide a formal model for multi-cascade diffusion and introduce an important concept called as cascade priority. Second, we show that the misinformation containment problem cannot be approximated within a factor of
Ω
(
2
log
1
−
ϵ
n
4
)
Ω
in polynomial time unless
N
P
⊆
D
T
I
M
E
(
n
\polylog
n
)
N
. Third, we introduce several types of cascade priority that are frequently seen in real social networks. Finally, we design novel algorithms for solving the misinformation containment problem. The effectiveness of the proposed algorithm is supported by encouraging experimental results."
neurips,https://proceedings.neurips.cc/paper/2018/file/9b89bedda1fc8a2d88c448e361194f02-Paper.pdf,Autoconj: Recognizing and Exploiting Conjugacy Without a Domain-Specific Language,"Matthew D. Hoffman, Matthew J. Johnson, Dustin Tran",
neurips,https://proceedings.neurips.cc/paper/2018/file/9bd5ee6fe55aaeb673025dbcb8f939c1-Paper.pdf,Size-Noise Tradeoffs in Generative Networks,"Bolton Bailey, Matus J. Telgarsky","This paper investigates the ability of generative networks to convert their input noise distributions into other distributions. Firstly, we demonstrate a construction that allows ReLU networks to increase the dimensionality of their noise distribution by implementing a
space-filling'' function based on iterated tent maps. We show this construction is optimal by analyzing the number of affine pieces in functions computed by multivariate ReLU networks. Secondly, we provide efficient ways (using polylog
(
1
/
ϵ
)
(
nodes) for networks to pass between univariate uniform and normal distributions, using a Taylor series approximation and a binary search gadget for computing function inverses. Lastly, we indicate how high dimensional distributions can be efficiently transformed into low dimensional distributions."
neurips,https://proceedings.neurips.cc/paper/2018/file/9c19a2aa1d84e04b0bd4bc888792bd1e-Paper.pdf,Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex Optimization,"Pan Xu, Jinghui Chen, Difan Zou, Quanquan Gu","We present a unified framework to analyze the global convergence of Langevin dynamics based algorithms for nonconvex finite-sum optimization with
n
n
component functions. At the core of our analysis is a direct analysis of the ergodicity of the numerical approximations to Langevin dynamics, which leads to faster convergence rates. Specifically, we show that gradient Langevin dynamics (GLD) and stochastic gradient Langevin dynamics (SGLD) converge to the \textit{almost minimizer}\footnote{Following \citet{raginsky2017non}, an almost minimizer is defined to be a point which is within the ball of the global minimizer with radius
O
(
d
log
(
β
+
1
)
/
β
)
O
, where
d
d
is the problem dimension and
β
β
is the inverse temperature parameter.} within
~
O
(
n
d
/
(
λ
ϵ
)
)
O
\footnote{
~
O
(
⋅
)
O
notation hides polynomials of logarithmic terms and constants.} and
~
O
(
d
7
/
(
λ
5
ϵ
5
)
)
O
stochastic gradient evaluations respectively, where
d
d
is the problem dimension, and
λ
λ
is the spectral gap of the Markov chain generated by GLD. Both results improve upon the best known gradient complexity\footnote{Gradient complexity is defined as the total number of stochastic gradient evaluations of an algorithm, which is the number of stochastic gradients calculated per iteration times the total number of iterations.} results \citep{raginsky2017non}. Furthermore, for the first time we prove the global convergence guarantee for variance reduced stochastic gradient Langevin dynamics (VR-SGLD) to the almost minimizer within
~
O
(
√
n
d
5
/
(
λ
4
ϵ
5
/
2
)
)
O
stochastic gradient evaluations, which outperforms the gradient complexities of GLD and SGLD in a wide regime. Our theoretical analyses shed some light on using Langevin dynamics based algorithms for nonconvex optimization with provable guarantees."
neurips,https://proceedings.neurips.cc/paper/2018/file/9cb9ed4f35cf7c2f295cc2bc6f732a84-Paper.pdf,Online convex optimization for cumulative constraints,"Jianjun Yuan, Andrew Lamperski","We propose the algorithms for online convex optimization which lead to cumulative squared constraint violations of the form
T
∑
t
=
1
(
[
g
(
x
t
)
]
+
)
2
=
O
(
T
1
−
β
)
∑
, where
β
∈
(
0
,
1
)
β
. Previous literature has focused on long-term constraints of the form
T
∑
t
=
1
g
(
x
t
)
∑
. There, strictly feasible solutions can cancel out the effects of violated constraints. In contrast, the new form heavily penalizes large constraint violations and cancellation effects cannot occur. Furthermore, useful bounds on the single step constraint violation
[
g
(
x
t
)
]
+
[
are derived. For convex objectives, our regret bounds generalize existing bounds, and for strongly convex objectives we give improved regret bounds. In numerical experiments, we show that our algorithm closely follows the constraint boundary leading to low cumulative violation."
neurips,https://proceedings.neurips.cc/paper/2018/file/9d28de8ff9bb6a3fa41fddfdc28f3bc1-Paper.pdf,Semi-supervised Deep Kernel Learning: Regression with Unlabeled Data by Minimizing Predictive Variance,"Neal Jean, Sang Michael Xie, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2018/file/9d684c589d67031a627ad33d59db65e5-Paper.pdf,Stimulus domain transfer in recurrent models for large scale cortical population prediction on video,"Fabian Sinz, Alexander S. Ecker, Paul Fahey, Edgar Walker, Erick Cobos, Emmanouil Froudarakis, Dimitri Yatsenko, Zachary Pitkow, Jacob Reimer, Andreas Tolias",
neurips,https://proceedings.neurips.cc/paper/2018/file/9dcb88e0137649590b755372b040afad-Paper.pdf,Sigsoftmax: Reanalysis of the Softmax Bottleneck,"Sekitoshi Kanai, Yasuhiro Fujiwara, Yuki Yamanaka, Shuichi Adachi",
neurips,https://proceedings.neurips.cc/paper/2018/file/9e3cfc48eccf81a0d57663e129aef3cb-Paper.pdf,Parsimonious Quantile Regression of Financial Asset Tail Dynamics via Sequential Learning,"Xing Yan, Weizhong Zhang, Lin Ma, Wei Liu, Qi Wu",
neurips,https://proceedings.neurips.cc/paper/2018/file/9e740b84bb48a64dde25061566299467-Paper.pdf,Adaptive Learning with Unknown Information Flows,"Yonatan Gur, Ahmadreza Momeni",
neurips,https://proceedings.neurips.cc/paper/2018/file/9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf,DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors,"Arash Vahdat, Evgeny Andriyash, William Macready",
neurips,https://proceedings.neurips.cc/paper/2018/file/9fb4651c05b2ed70fba5afe0b039a550-Paper.pdf,Reinforcement Learning for Solving the Vehicle Routing Problem,"MohammadReza Nazari, Afshin Oroojlooy, Lawrence Snyder, Martin Takac",
neurips,https://proceedings.neurips.cc/paper/2018/file/a00e5eb0973d24649a4a920fc53d9564-Paper.pdf,GumBolt: Extending Gumbel trick to Boltzmann priors,"Amir H. Khoshaman, Mohammad Amin",
neurips,https://proceedings.neurips.cc/paper/2018/file/a012869311d64a44b5a0d567cd20de04-Paper.pdf,Adding One Neuron Can Eliminate All Bad Local Minima,"SHIYU LIANG, Ruoyu Sun, Jason D. Lee, R. Srikant",
neurips,https://proceedings.neurips.cc/paper/2018/file/a0160709701140704575d499c997b6ca-Paper.pdf,Norm matters: efficient and accurate normalization schemes in deep networks,"Elad Hoffer, Ron Banner, Itay Golan, Daniel Soudry","Over the past few years, Batch-Normalization has been commonly used in deep networks, allowing faster training and high performance for a wide variety of applications. However, the reasons behind its merits remained unanswered, with several shortcomings that hindered its use for certain tasks. In this work, we present a novel view on the purpose and function of normalization methods and weight-decay, as tools to decouple weights' norm from the underlying optimized objective. This property highlights the connection between practices such as normalization, weight decay and learning-rate adjustments. We suggest several alternatives to the widely used
L
2
L
batch-norm, using normalization in
L
1
L
and
L
∞
L
spaces that can substantially improve numerical stability in low-precision implementations as well as provide computational and memory benefits. We demonstrate that such methods enable the first batch-norm alternative to work for half-precision implementations. Finally, we suggest a modification to weight-normalization, which improves its performance on large-scale tasks."
neurips,https://proceedings.neurips.cc/paper/2018/file/a01610228fe998f515a72dd730294d87-Paper.pdf,Local Differential Privacy for Evolving Data,"Matthew Joseph, Aaron Roth, Jonathan Ullman, Bo Waggoner","In this paper, we introduce a new technique for local differential privacy that makes it possible to maintain up-to-date statistics over time, with privacy guarantees that degrade only in the number of changes in the underlying distribution rather than the number of collection periods. We use our technique for tracking a changing statistic in the setting where users are partitioned into an unknown collection of groups, and at every time period each user draws a single bit from a common (but changing) group-specific distribution. We also provide an application to frequency and heavy-hitter estimation."
neurips,https://proceedings.neurips.cc/paper/2018/file/a01a0380ca3c61428c26a231f0e49a09-Paper.pdf,Dialog-based Interactive Image Retrieval,"Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, Rogerio Feris",
neurips,https://proceedings.neurips.cc/paper/2018/file/a07c2f3b3b907aaf8436a26c6d77f0a2-Paper.pdf,Byzantine Stochastic Gradient Descent,"Dan Alistarh, Zeyuan Allen-Zhu, Jerry Li","This paper studies the problem of distributed stochastic optimization in an adversarial setting where, out of
m
m
machines which allegedly compute stochastic gradients every iteration, an
α
α
-fraction are Byzantine, and may behave adversarially. Our main result is a variant of stochastic gradient descent (SGD) which finds
ε
ε
-approximate minimizers of convex functions in
T
=
~
O
(
1
ε
2
m
+
α
2
ε
2
)
T
iterations. In contrast, traditional mini-batch SGD needs
T
=
O
(
1
ε
2
m
)
T
iterations, but cannot tolerate Byzantine failures. Further, we provide a lower bound showing that, up to logarithmic factors, our algorithm is information-theoretically optimal both in terms of sample complexity and time complexity."
neurips,https://proceedings.neurips.cc/paper/2018/file/a08e32d2f9a8b78894d964ec7fd4172e-Paper.pdf,Robust Hypothesis Testing Using Wasserstein Uncertainty Sets,"RUI GAO, Liyan Xie, Yao Xie, Huan Xu",
neurips,https://proceedings.neurips.cc/paper/2018/file/a0afdf1ac166b8652ffe9dee6eac779e-Paper.pdf,Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies,"Alessandro Achille, Tom Eccles, Loic Matthey, Chris Burgess, Nicholas Watters, Alexander Lerchner, Irina Higgins",
neurips,https://proceedings.neurips.cc/paper/2018/file/a0b45d1bb84fe1bedbb8449764c4d5d5-Paper.pdf,Computationally and statistically efficient learning of causal Bayes nets using path queries,"Kevin Bello, Jean Honorio",
neurips,https://proceedings.neurips.cc/paper/2018/file/a14185bf0c82b3369f86efb3cac5ad28-Paper.pdf,Predictive Approximate Bayesian Computation via Saddle Points,"Yingxiang Yang, Bo Dai, Negar Kiyavash, Niao He",
neurips,https://proceedings.neurips.cc/paper/2018/file/a19744e268754fb0148b017647355b7b-Paper.pdf,Co-teaching: Robust training of deep neural networks with extremely noisy labels,"Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2018/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf,On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport,"Lénaïc Chizat, Francis Bach",
neurips,https://proceedings.neurips.cc/paper/2018/file/a1d50185e7426cbb0acad1e6ca74b9aa-Paper.pdf,Smoothed analysis of the low-rank approach for smooth semidefinite programs,"Thomas Pumir, Samy Jelassi, Nicolas Boumal","We consider semidefinite programs (SDPs) of size
n
n
with equality constraints. In order to overcome scalability issues, Burer and Monteiro proposed a factorized approach based on optimizing over a matrix
Y
Y
of size
n
×
k
n
such that
X
=
Y
Y
∗
X
is the SDP variable. The advantages of such formulation are twofold: the dimension of the optimization variable is reduced, and positive semidefiniteness is naturally enforced. However, optimization in
Y
Y
is non-convex. In prior work, it has been shown that, when the constraints on the factorized variable regularly define a smooth manifold, provided
k
k
is large enough, for almost all cost matrices, all second-order stationary points (SOSPs) are optimal. Importantly, in practice, one can only compute points which approximately satisfy necessary optimality conditions, leading to the question: are such points also approximately optimal? To this end, and under similar assumptions, we use smoothed analysis to show that approximate SOSPs for a randomly perturbed objective function are approximate global optima, with
k
k
scaling like the square root of the number of constraints (up to log factors). We particularize our results to an SDP relaxation of phase retrieval."
neurips,https://proceedings.neurips.cc/paper/2018/file/a1d7311f2a312426d710e1c617fcbc8c-Paper.pdf,Differentially Private Contextual Linear Bandits,"Roshan Shariff, Or Sheffet","We first show that using the standard definition of differential privacy results in linear regret. So instead, we adopt the notion of joint differential privacy, where we assume that the action chosen on day t is only revealed to user t and thus needn't be kept private that day, only on following days. We give a general scheme converting the classic linear-UCB algorithm into a joint differentially private algorithm using the tree-based algorithm. We then apply either Gaussian noise or Wishart noise to achieve joint-differentially private algorithms and bound the resulting algorithms' regrets. In addition, we give the first lower bound on the additional regret any private algorithms for the MAB problem must incur."
neurips,https://proceedings.neurips.cc/paper/2018/file/a274315e1abede44d63005826249d1df-Paper.pdf,Learning to Reason with Third Order Tensor Products,"Imanol Schlag, Jürgen Schmidhuber",
neurips,https://proceedings.neurips.cc/paper/2018/file/a2802cade04644083dcde1c8c483ed9a-Paper.pdf,Diversity-Driven Exploration Strategy for Deep Reinforcement Learning,"Zhang-Wei Hong, Tzu-Yun Shann, Shih-Yang Su, Yi-Hsiang Chang, Tsu-Jui Fu, Chun-Yi Lee",
neurips,https://proceedings.neurips.cc/paper/2018/file/a292f1c5874b2be8395ffd75f313937f-Paper.pdf,On Neuronal Capacity,"Pierre Baldi, Roman Vershynin",
neurips,https://proceedings.neurips.cc/paper/2018/file/a2b8a85a29b2d64ad6f47275bf1360c6-Paper.pdf,GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking,"Patrick Chen, Si Si, Yang Li, Ciprian Chelba, Cho-Jui Hsieh","Model compression is essential for serving large deep neural nets on devices with limited resources or applications that require real-time responses. For advanced NLP problems, a neural language model usually consists of recurrent layers (e.g., using LSTM cells), an embedding matrix for representing input tokens, and a softmax layer for generating output tokens. For problems with a very large vocabulary size, the embedding and the softmax matrices can account for more than half of the model size. For instance, the bigLSTM model achieves state-of-the-art performance on the One-Billion-Word (OBW) dataset with around 800k vocabulary, and its word embedding and softmax matrices use more than 6GBytes space, and are responsible for over 90\% of the model parameters. In this paper, we propose GroupReduce, a novel compression method for neural language models, based on vocabulary-partition (block) based low-rank matrix approximation and the inherent frequency distribution of tokens (the power-law distribution of words). We start by grouping words into
c
c
blocks based on their frequency, and then refine the clustering iteratively by constructing weighted low-rank approximation for each block, where the weights are based the frequencies of the words in the block. The experimental results show our method can significantly outperform traditional compression methods such as low-rank approximation and pruning. On the OBW dataset, our method achieved 6.6x compression rate for the embedding and softmax matrices, and when combined with quantization, our method can achieve 26x compression rate without losing prediction accuracy."
neurips,https://proceedings.neurips.cc/paper/2018/file/a2d10d355cdebc879e4fc6ecc6f63dd7-Paper.pdf,Deep Structured Prediction with Nonlinear Output Transformations,"Colin Graber, Ofer Meshi, Alexander Schwing",
neurips,https://proceedings.neurips.cc/paper/2018/file/a36b598abb934e4528412e5a2127b931-Paper.pdf,Training Neural Networks Using Features Replay,"Zhouyuan Huo, Bin Gu, Heng Huang",
neurips,https://proceedings.neurips.cc/paper/2018/file/a381c2c35c9157f6b67fd07d5a200ae1-Paper.pdf,Mallows Models for Top-k Lists,"Flavio Chierichetti, Anirban Dasgupta, Shahrzad Haddadan, Ravi Kumar, Silvio Lattanzi",
neurips,https://proceedings.neurips.cc/paper/2018/file/a3eb043e7bf775de87763e9f8121c953-Paper.pdf,Information-based Adaptive Stimulus Selection to Optimize Communication Efficiency in Brain-Computer Interfaces,"Boyla Mainsah, Dmitry Kalika, Leslie Collins, Siyuan Liu, Chandra Throckmorton",
neurips,https://proceedings.neurips.cc/paper/2018/file/a3f390d88e4c41f2747bfa2f1b5f87db-Paper.pdf,"Doubly Robust Bayesian Inference for Non-Stationary Streaming Data with
β
β
-Divergences","Jeremias Knoblauch, Jack E. Jewson, Theodoros Damoulas","We present the very first robust Bayesian Online Changepoint Detection algorithm through General Bayesian Inference (GBI) with
β
β
-divergences. The resulting inference procedure is doubly robust for both the predictive and the changepoint (CP) posterior, with linear time and constant space complexity. We provide a construction for exponential models and demonstrate it on the Bayesian Linear Regression model. In so doing, we make two additional contributions: Firstly, we make GBI scalable using Structural Variational approximations that are exact as
β
→
0
β
. Secondly, we give a principled way of choosing the divergence parameter
β
β
by minimizing expected predictive loss on-line. Reducing False Discovery Rates of \CPs from up to 99\% to 0\% on real world data, this offers the state of the art."
neurips,https://proceedings.neurips.cc/paper/2018/file/a3fc981af450752046be179185ebc8b5-Paper.pdf,Clebsch–Gordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network,"Risi Kondor, Zhen Lin, Shubhendu Trivedi",
neurips,https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf,Visualizing the Loss Landscape of Neural Nets,"Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein",
neurips,https://proceedings.neurips.cc/paper/2018/file/a42a596fc71e17828440030074d15e74-Paper.pdf,Non-monotone Submodular Maximization in Exponentially Fewer Iterations,"Eric Balkanski, Adam Breuer, Yaron Singer",
neurips,https://proceedings.neurips.cc/paper/2018/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf,Representation Learning for Treatment Effect Estimation from Observational Data,"Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, Aidong Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/a57e8915461b83adefb011530b711704-Paper.pdf,Memory Replay GANs: Learning to Generate New Categories without Forgetting,"Chenshen Wu, Luis Herranz, Xialei Liu, yaxing wang, Joost van de Weijer, Bogdan Raducanu",
neurips,https://proceedings.neurips.cc/paper/2018/file/a5bfc9e07964f8dddeb95fc584cd965d-Paper.pdf,HOGWILD!-Gibbs can be PanAccurate,"Constantinos Daskalakis, Nishanth Dikkala, Siddhartha Jayanti","Asynchronous Gibbs sampling has been recently shown to be fast-mixing and an accurate method for estimating probabilities of events on a small number of variables of a graphical model satisfying Dobrushin's condition~\cite{DeSaOR16}. We investigate whether it can be used to accurately estimate expectations of functions of {\em all the variables} of the model. Under the same condition, we show that the synchronous (sequential) and asynchronous Gibbs samplers can be coupled so that the expected Hamming distance between their (multivariate) samples remains bounded by
O
(
τ
log
n
)
,
O
where
n
n
is the number of variables in the graphical model, and
τ
τ
is a measure of the asynchronicity. A similar bound holds for any constant power of the Hamming distance. Hence, the expectation of any function that is Lipschitz with respect to a power of the Hamming distance, can be estimated with a bias that grows logarithmically in
n
n
. Going beyond Lipschitz functions, we consider the bias arising from asynchronicity in estimating the expectation of polynomial functions of all variables in the model. Using recent concentration of measure results~\cite{DaskalakisDK17,GheissariLP17,GotzeSS18}, we show that the bias introduced by the asynchronicity is of smaller order than the standard deviation of the function value already present in the true model. We perform experiments on a multi-processor machine to empirically illustrate our theoretical findings."
neurips,https://proceedings.neurips.cc/paper/2018/file/a5e0ff62be0b08456fc7f1e88812af3d-Paper.pdf,DeepExposure: Learning to Expose Photos with Asynchronously Reinforced Adversarial Learning,"Runsheng Yu, Wenyu Liu, Yasen Zhang, Zhi Qu, Deli Zhao, Bo Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/a753a43564c29148df3150afb4475440-Paper.pdf,Data Amplification: A Unified and Competitive Approach to Property Estimation,"Yi Hao, Alon Orlitsky, Ananda Theertha Suresh, Yihong Wu","We illustrate the estimator's practical advantages by comparing it to existing estimators for a wide variety of properties and distributions. In most cases, its performance with n samples is even as good as that of the empirical estimator with n\log n samples, and for essentially all properties, its performance is comparable to that of the best existing estimator designed specifically for that property."
neurips,https://proceedings.neurips.cc/paper/2018/file/a7a3d70c6d17a73140918996d03c014f-Paper.pdf,Insights on representational similarity in neural networks with canonical correlation,"Ari Morcos, Maithra Raghu, Samy Bengio",
neurips,https://proceedings.neurips.cc/paper/2018/file/a7aeed74714116f3b292a982238f83d2-Paper.pdf,Efficient nonmyopic batch active search,"Shali Jiang, Gustavo Malkomes, Matthew Abbott, Benjamin Moseley, Roman Garnett",
neurips,https://proceedings.neurips.cc/paper/2018/file/a89b71bb5227c75d463dd82a03115738-Paper.pdf,Learning Safe Policies with Expert Guidance,"Jessie Huang, Fa Wu, Doina Precup, Yang Cai",
neurips,https://proceedings.neurips.cc/paper/2018/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf,Fast Similarity Search via Optimal Sparse Lifting,"Wenye Li, Jingwei Mao, Yin Zhang, Shuguang Cui",
neurips,https://proceedings.neurips.cc/paper/2018/file/a9813e9550fee3110373c21fa012eee7-Paper.pdf,Differentially Private Robust Low-Rank Approximation,"Raman Arora, Vladimir braverman, Jalaj Upadhyay","In this paper, we study the following robust low-rank matrix approximation problem: given a matrix
A
∈
\R
n
×
d
A
, find a rank-
k
k
matrix
B
B
, while satisfying differential privacy, such that
\norm
A
−
B
p
≤
α
O
P
T
k
(
A
)
+
τ
,
\norm
where
\norm
M
p
\norm
is the entry-wise
ℓ
p
ℓ
-norm and
O
P
T
k
(
A
)
:=
min
r
a
n
k
(
X
)
≤
k
\norm
A
−
X
p
O
. It is well known that low-rank approximation w.r.t. entrywise
ℓ
p
ℓ
-norm, for
p
∈
[
1
,
2
)
p
, yields robustness to gross outliers in the data. We propose an algorithm that guarantees
α
=
˜
O
(
k
2
)
,
τ
=
˜
O
(
k
2
(
n
+
k
d
)
/
ε
)
α
, runs in
˜
O
(
(
n
+
d
)
\poly
 k
)
O
time and uses
O
(
k
(
n
+
d
)
log
k
)
O
space. We study extensions to the streaming setting where entries of the matrix arrive in an arbitrary order and output is produced at the very end or continually. We also study the related problem of differentially private robust principal component analysis (PCA), wherein we return a rank-
k
k
projection matrix
Π
Π
such that
\norm
A
−
A
Π
p
≤
α
O
P
T
k
(
A
)
+
τ
.
\norm"
neurips,https://proceedings.neurips.cc/paper/2018/file/a981f2b708044d6fb4a71a1463242520-Paper.pdf,Evidential Deep Learning to Quantify Classification Uncertainty,"Murat Sensoy, Lance Kaplan, Melih Kandemir",
neurips,https://proceedings.neurips.cc/paper/2018/file/a9a1d5317a33ae8cef33961c34144f84-Paper.pdf,A Game-Theoretic Approach to Recommendation Systems with Strategic Content Providers,"Omer Ben-Porat, Moshe Tennenholtz",
neurips,https://proceedings.neurips.cc/paper/2018/file/a9a6653e48976138166de32772b1bf40-Paper.pdf,Frequency-Domain Dynamic Pruning for Convolutional Neural Networks,"Zhenhua Liu, Jizheng Xu, Xiulian Peng, Ruiqin Xiong",
neurips,https://proceedings.neurips.cc/paper/2018/file/aa0d2a804a3510442f2fd40f2100b054-Paper.pdf,Adaptive Path-Integral Autoencoders: Representation Learning and Planning for Dynamical Systems,"Jung-Su Ha, Young-Jin Park, Hyeok-Joo Chae, Soon-Seo Park, Han-Lim Choi",
neurips,https://proceedings.neurips.cc/paper/2018/file/aa8fdbb7d8159b3048daca36fe5c06d2-Paper.pdf,Testing for Families of Distributions via the Fourier Transform,"Clément L. Canonne, Ilias Diakonikolas, Alistair Stewart",
neurips,https://proceedings.neurips.cc/paper/2018/file/aa942ab2bfa6ebda4840e7360ce6e7ef-Paper.pdf,A Unified Framework for Extensive-Form Game Abstraction with Bounds,"Christian Kroer, Tuomas Sandholm","Abstraction has long been a key component in the practical solving of large-scale extensive-form games. Despite this, abstraction remains poorly understood. There have been some recent theoretical results but they have been confined to specific assumptions on abstraction structure and are specific to various disjoint types of abstraction, and specific solution concepts, for example, exact Nash equilibria or strategies with bounded immediate regret. In this paper we present a unified framework for analyzing abstractions that can express all types of abstractions and solution concepts used in prior papers with performance guarantees---while maintaining comparable bounds on abstraction quality. Moreover, our framework gives an exact decomposition of abstraction error in a much broader class of games, albeit only in an ex-post sense, as our results depend on the specific strategy chosen. Nonetheless, we use this ex-post decomposition along with slightly weaker assumptions than prior work to derive generalizations of prior bounds on abstraction quality. We also show, via counterexample, that such assumptions are necessary for some games. Finally, we prove the first bounds for how
ϵ
ϵ
-Nash equilibria computed in abstractions perform in the original game. This is important because often one cannot afford to compute an exact Nash equilibrium in the abstraction. All our results apply to general-sum n-player games."
neurips,https://proceedings.neurips.cc/paper/2018/file/aa97d584861474f4097cf13ccb5325da-Paper.pdf,Model-Agnostic Private Learning,"Raef Bassily, Om Thakkar, Abhradeep Guha Thakurta","We design differentially private learning algorithms that are agnostic to the learning model assuming access to limited amount of unlabeled public data. First, we give a new differentially private algorithm for answering a sequence of
m
m
online classification queries (given by a sequence of
m
m
unlabeled public feature vectors) based on a private training set. Our private algorithm follows the paradigm of subsample-and-aggregate, in which any generic non-private learner is trained on disjoint subsets of the private training set, then for each classification query, the votes of the resulting classifiers ensemble are aggregated in a differentially private fashion. Our private aggregation is based on a novel combination of distance-to-instability framework [Smith & Thakurta 2013] and the sparse-vector technique [Dwork et al. 2009, Hardt & Talwar 2010]. We show that our algorithm makes a conservative use of the privacy budget. In particular, if the underlying non-private learner yields classification error at most
α
∈
(
0
,
1
)
α
, then our construction answers more queries, by at least a factor of
1
/
α
1
in some cases, than what is implied by a straightforward application of the advanced composition theorem for differential privacy. Next, we apply the knowledge transfer technique to construct a private learner that outputs a classifier, which can be used to answer unlimited number of queries. In the PAC model, we analyze our construction and prove upper bounds on the sample complexity for both the realizable and the non-realizable cases. As in non-private sample complexity, our bounds are completely characterized by the VC dimension of the concept class."
neurips,https://proceedings.neurips.cc/paper/2018/file/aaaccd2766ec67aecbe26459bb828d81-Paper.pdf,Towards Text Generation with Adversarially Learned Neural Outlines,"Sandeep Subramanian, Sai Rajeswar Mudumba, Alessandro Sordoni, Adam Trischler, Aaron C. Courville, Chris Pal",
neurips,https://proceedings.neurips.cc/paper/2018/file/ab013ca67cf2d50796b0c11d1b8bc95d-Paper.pdf,"FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network","Aditya Kusupati, Manish Singh, Kush Bhatia, Ashish Kumar, Prateek Jain, Manik Varma",
neurips,https://proceedings.neurips.cc/paper/2018/file/ab7314887865c4265e896c6e209d1cd6-Paper.pdf,Bipartite Stochastic Block Models with Tiny Clusters,Stefan Neumann,"We study the problem of finding clusters in random bipartite graphs. We present a simple two-step algorithm which provably finds even tiny clusters of size
O
(
n
ϵ
)
O
, where
n
n
is the number of vertices in the graph and
ϵ
>
0
ϵ
. Previous algorithms were only able to identify clusters of size
Ω
(
√
n
)
Ω
. We evaluate the algorithm on synthetic and on real-world data; the experiments show that the algorithm can find extremely small clusters even in presence of high destructive noise."
neurips,https://proceedings.neurips.cc/paper/2018/file/ab88b15733f543179858600245108dd8-Paper.pdf,Conditional Adversarial Domain Adaptation,"Mingsheng Long, ZHANGJIE CAO, Jianmin Wang, Michael I. Jordan",
neurips,https://proceedings.neurips.cc/paper/2018/file/aba22f748b1a6dff75bda4fd1ee9fe07-Paper.pdf,Stochastic Expectation Maximization with Variance Reduction,"Jianfei Chen, Jun Zhu, Yee Whye Teh, Tong Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/abd1c782880cc59759f4112fda0b8f98-Paper.pdf,Bayesian Nonparametric Spectral Estimation,Felipe Tobar,
neurips,https://proceedings.neurips.cc/paper/2018/file/abdeb6f575ac5c6676b747bca8d09cc2-Paper.pdf,A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks,"Kimin Lee, Kibok Lee, Honglak Lee, Jinwoo Shin",
neurips,https://proceedings.neurips.cc/paper/2018/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf,Breaking the Span Assumption Yields Fast Finite-Sum Minimization,"Robert Hannah, Yanli Liu, Daniel O'Connor, Wotao Yin","In this paper, we show that SVRG and SARAH can be modified to be fundamentally faster than all of the other standard algorithms that minimize the sum of
n
n
smooth functions, such as SAGA, SAG, SDCA, and SDCA without duality. Most finite sum algorithms follow what we call the
span assumption'': Their updates are in the span of a sequence of component gradients chosen in a random IID fashion. In the big data regime, where the condition number
κ
=
O
(
n
)
κ
, the span assumption prevents algorithms from converging to an approximate solution of accuracy
ϵ
ϵ
in less than
n
ln
(
1
/
ϵ
)
n
iterations. SVRG and SARAH do not follow the span assumption since they are updated with a hybrid of full-gradient and component-gradient information. We show that because of this, they can be up to
Ω
(
1
+
(
ln
(
n
/
κ
)
)
+
)
Ω
times faster. In particular, to obtain an accuracy
ϵ
=
1
/
n
α
ϵ
for
κ
=
n
β
κ
and
α
,
β
∈
(
0
,
1
)
α
, modified SVRG requires
O
(
n
)
O
iterations, whereas algorithms that follow the span assumption require
O
(
n
ln
(
n
)
)
O
iterations. Moreover, we present lower bound results that show this speedup is optimal, and provide analysis to help explain why this speedup exists. With the understanding that the span assumption is a point of weakness of finite sum algorithms, future work may purposefully exploit this to yield faster algorithms in the big data regime."
neurips,https://proceedings.neurips.cc/paper/2018/file/ac27b77292582bc293a51055bfc994ee-Paper.pdf,Differential Privacy for Growing Databases,"Rachel Cummings, Sara Krehbiel, Kevin A. Lai, Uthaipon Tantipongpipat",
neurips,https://proceedings.neurips.cc/paper/2018/file/ac627ab1ccbdb62ec96e702f07f6425b-Paper.pdf,Learning Pipelines with Limited Data and Domain Knowledge: A Study in Parsing Physics Problems,"Mrinmaya Sachan, Kumar Avinava Dubey, Tom M. Mitchell, Dan Roth, Eric P. Xing",
neurips,https://proceedings.neurips.cc/paper/2018/file/acc21473c4525b922286130ffbfe00b5-Paper.pdf,Theoretical guarantees for EM under misspecified Gaussian mixture models,"Raaz Dwivedi, nhật Hồ, Koulik Khamaru, Martin J. Wainwright, Michael I. Jordan","Recent years have witnessed substantial progress in understanding the behavior of EM for mixture models that are correctly specified. Given that model misspecification is common in practice, it is important to understand EM in this more general setting. We provide non-asymptotic guarantees for population and sample-based EM for parameter estimation under a few specific univariate settings of misspecified Gaussian mixture models. Due to misspecification, the EM iterates no longer converge to the true model and instead converge to the projection of the true model over the set of models being searched over. We provide two classes of theoretical guarantees: first, we characterize the bias introduced due to the misspecification; and second, we prove that population EM converges at a geometric rate to the model projection under a suitable initialization condition. This geometric convergence rate for population EM imply a statistical complexity of order
1
/
√
n
1
when running EM with
n
n
samples. We validate our theoretical findings in different cases via several numerical examples."
neurips,https://proceedings.neurips.cc/paper/2018/file/ad47a008a2f806aa6eb1b53852cd8b37-Paper.pdf,Online Improper Learning with an Approximation Oracle,"Elad Hazan, Wei Hu, Yuanzhi Li, Zhiyuan Li",
neurips,https://proceedings.neurips.cc/paper/2018/file/ad554d8c3b06d6b97ee76a2448bd7913-Paper.pdf,Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise,"Dan Hendrycks, Mantas Mazeika, Duncan Wilson, Kevin Gimpel",
neurips,https://proceedings.neurips.cc/paper/2018/file/ad8e88c0f76fa4fc8e5474384142a00a-Paper.pdf,Multi-Task Zipping via Layer-wise Neuron Sharing,"Xiaoxi He, Zimu Zhou, Lothar Thiele",
neurips,https://proceedings.neurips.cc/paper/2018/file/ade55409d1224074754035a5a937d2e0-Paper.pdf,Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching,"Stepan Tulyakov, Anton Ivanov, François Fleuret","The Practical Deep Stereo (PDS) network that we propose addresses both issues: First, its architecture relies on novel bottleneck modules that drastically reduce the memory footprint in inference, and additional design choices allow to handle greater image size during training. This results in a model that leverages large image context to resolve matching ambiguities. Second, a novel sub-pixel cross-entropy loss combined with a MAP estimator make this network less sensitive to ambiguous matches, and applicable to any disparity range without re-training."
neurips,https://proceedings.neurips.cc/paper/2018/file/aee92f16efd522b9326c25cc3237ac15-Paper.pdf,Masking: A New Perspective of Noisy Supervision,"Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor Tsang, Ya Zhang, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2018/file/aeefb050911334869a7a5d9e4d0e1689-Paper.pdf,Learning to Multitask,"Yu Zhang, Ying Wei, Qiang Yang",
neurips,https://proceedings.neurips.cc/paper/2018/file/aef546f29283b6ccef3c61f58fb8e79b-Paper.pdf,"Thwarting Adversarial Examples: An
L
0
L
-Robust Sparse Fourier Transform","Mitali Bafna, Jack Murtagh, Nikhil Vyas","We give a new algorithm for approximating the Discrete Fourier transform of an approximately sparse signal that is robust to worst-case
L
0
L
corruptions, namely that some coordinates of the signal can be corrupt arbitrarily. Our techniques generalize to a wide range of linear transformations that are used in data analysis such as the Discrete Cosine and Sine transforms, the Hadamard transform, and their high-dimensional analogs. We use our algorithm to successfully defend against worst-case
L
0
L
adversaries in the setting of image classification. We give experimental results on the Jacobian-based Saliency Map Attack (JSMA) and the CW
L
0
L
attack on the MNIST and Fashion-MNIST datasets as well as the Adversarial Patch on the ImageNet dataset."
neurips,https://proceedings.neurips.cc/paper/2018/file/af1b5754061ebbd4412adfb34c8d3534-Paper.pdf,"Constant Regret, Generalized Mixability, and Mirror Descent","Zakaria Mhammedi, Robert C. Williamson","We consider the setting of prediction with expert advice; a learner makes predictions by aggregating those of a group of experts. Under this setting, and for the right choice of loss function and
mixing'' algorithm, it is possible for the learner to achieve a constant regret regardless of the number of prediction rounds. For example, a constant regret can be achieved for \emph{mixable} losses using the \emph{aggregating algorithm}. The \emph{Generalized Aggregating Algorithm} (GAA) is a name for a family of algorithms parameterized by convex functions on simplices (entropies), which reduce to the aggregating algorithm when using the \emph{Shannon entropy}
S
S
. For a given entropy
Φ
Φ
, losses for which a constant regret is possible using the \textsc{GAA} are called
Φ
Φ
-mixable. Which losses are
Φ
Φ
-mixable was previously left as an open question. We fully characterize
Φ
Φ
-mixability and answer other open questions posed by \cite{Reid2015}. We show that the Shannon entropy
S
S
is fundamental in nature when it comes to mixability; any
Φ
Φ
-mixable loss is necessarily
S
S
-mixable, and the lowest worst-case regret of the \textsc{GAA} is achieved using the Shannon entropy. Finally, by leveraging the connection between the \emph{mirror descent algorithm} and the update step of the GAA, we suggest a new \emph{adaptive} generalized aggregating algorithm and analyze its performance in terms of the regret bound."
neurips,https://proceedings.neurips.cc/paper/2018/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf,Dual Principal Component Pursuit: Improved Analysis and Efficient Algorithms,"Zhihui Zhu, Yifan Wang, Daniel Robinson, Daniel Naiman, René Vidal, Manolis Tsakiris",
neurips,https://proceedings.neurips.cc/paper/2018/file/afa299a4d1d8c52e75dd8a24c3ce534f-Paper.pdf,Generative modeling for protein structures,"Namrata Anand, Possu Huang",
neurips,https://proceedings.neurips.cc/paper/2018/file/afd4836712c5e77550897e25711e1d96-Paper.pdf,Found Graph Data and Planted Vertex Covers,"Austin R. Benson, Jon Kleinberg",
neurips,https://proceedings.neurips.cc/paper/2018/file/aff0a6a4521232970b2c1cf539ad0a19-Paper.pdf,Fast Estimation of Causal Interactions using Wold Processes,"Flavio Figueiredo, Guilherme Resende Borges, Pedro O.S. Vaz de Melo, Renato Assunção","We here focus on the task of learning Granger causality matrices for multivariate point processes. In order to accomplish this task, our work is the first to explore the use of Wold processes. By doing so, we are able to develop asymptotically fast MCMC learning algorithms. With
N
N
being the total number of events and
K
K
the number of processes, our learning algorithm has a
O
(
N
(
log
(
N
)
+
log
(
K
)
)
)
O
cost per iteration. This is much faster than the
O
(
N
3
K
2
)
O
or
O
(
K
3
)
O
for the state of the art. Our approach, called GrangerBusca, is validated on nine datasets. This is an advance in relation to most prior efforts which focus mostly on subsets of the Memetracker data. Regarding accuracy, GrangerBusca is three times more accurate (in Precision@10) than the state of the art for the commonly explored subsets Memetracker. Due to GrangerBusca's much lower training complexity, our approach is the only one able to train models for larger, full, sets of data."
neurips,https://proceedings.neurips.cc/paper/2018/file/aff1621254f7c1be92f64550478c56e6-Paper.pdf,Relating Leverage Scores and Density using Regularized Christoffel Functions,"Edouard Pauwels, Francis Bach, Jean-Philippe Vert",
neurips,https://proceedings.neurips.cc/paper/2018/file/b0169350cd35566c47ba83c6ec1d6f82-Paper.pdf,"Online Adaptive Methods, Universality and Acceleration","Kfir Y. Levy, Alp Yurtsever, Volkan Cevher",
neurips,https://proceedings.neurips.cc/paper/2018/file/b096577e264d1ebd6b41041f392eec23-Paper.pdf,Reparameterization Gradient for Non-differentiable Models,"Wonyeol Lee, Hangyeol Yu, Hongseok Yang",
neurips,https://proceedings.neurips.cc/paper/2018/file/b1301141feffabac455e1f90a7de2054-Paper.pdf,Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents,"Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth Stanley, Jeff Clune",
neurips,https://proceedings.neurips.cc/paper/2018/file/b132ecc1609bfcf302615847c1caa69a-Paper.pdf,Optimistic optimization of a Brownian,"Jean-Bastien Grill, Michal Valko, Remi Munos","We address the problem of optimizing a Brownian motion. We consider a (random) realization
W
W
of a Brownian motion with input space in
[
0
,
1
]
[
. Given
W
W
, our goal is to return an
ϵ
ϵ
-approximation of its maximum using the smallest possible number of function evaluations, the sample complexity of the algorithm. We provide an algorithm with sample complexity of order
log
2
(
1
/
ϵ
)
log
. This improves over previous results of Al-Mharmah and Calvin (1996) and Calvin et al. (2017) which provided only polynomial rates. Our algorithm is adaptive---each query depends on previous values---and is an instance of the optimism-in-the-face-of-uncertainty principle."
neurips,https://proceedings.neurips.cc/paper/2018/file/b137fdd1f79d56c7edf3365fea7520f2-Paper.pdf,Generalizing Tree Probability Estimation via Bayesian Networks,"Cheng Zhang, Frederick A Matsen IV",
neurips,https://proceedings.neurips.cc/paper/2018/file/b197ffdef2ddc3308584dce7afa3661b-Paper.pdf,Safe Active Learning for Time-Series Modeling with Gaussian Processes,"Christoph Zimmer, Mona Meister, Duy Nguyen-Tuong",
neurips,https://proceedings.neurips.cc/paper/2018/file/b19aa25ff58940d974234b48391b9549-Paper.pdf,"Computing Kantorovich-Wasserstein Distances on
d
d
-dimensional histograms using
(
d
+
1
)
(
-partite graphs","Gennaro Auricchio, Federico Bassetti, Stefano Gualandi, Marco Veneroni","This paper presents a novel method to compute the exact Kantorovich-Wasserstein distance between a pair of
d
d
-dimensional histograms having
n
n
bins each. We prove that this problem is equivalent to an uncapacitated minimum cost flow problem on a
(
d
+
1
)
(
-partite graph with
(
d
+
1
)
n
(
nodes and
d
n
d
+
1
d
d
arcs, whenever the cost is separable along the principal
d
d
-dimensional directions. We show numerically the benefits of our approach by computing the Kantorovich-Wasserstein distance of order 2 among two sets of instances: gray scale images and
d
d
-dimensional biomedical histograms. On these types of instances, our approach is competitive with state-of-the-art optimal transport algorithms."
neurips,https://proceedings.neurips.cc/paper/2018/file/b2ab001909a8a6f04b51920306046ce5-Paper.pdf,SimplE Embedding for Link Prediction in Knowledge Graphs,"Seyed Mehran Kazemi, David Poole",
neurips,https://proceedings.neurips.cc/paper/2018/file/b2dc449578a4744a1684d3b0ea933702-Paper.pdf,Bounded-Loss Private Prediction Markets,"Rafael Frongillo, Bo Waggoner",
neurips,https://proceedings.neurips.cc/paper/2018/file/b3848d61bbbc6207c6668a8a9e2730ed-Paper.pdf,Statistical mechanics of low-rank tensor decomposition,"Jonathan Kadmon, Surya Ganguli",
neurips,https://proceedings.neurips.cc/paper/2018/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf,A theory on the absence of spurious solutions for nonconvex and nonsmooth optimization,"Cedric Josz, Yi Ouyang, Richard Zhang, Javad Lavaei, Somayeh Sojoudi","We study the set of continuous functions that admit no spurious local optima (i.e. local minima that are not global minima) which we term global functions. They satisfy various powerful properties for analyzing nonconvex and nonsmooth optimization problems. For instance, they satisfy a theorem akin to the fundamental uniform limit theorem in the analysis regarding continuous functions. Global functions are also endowed with useful properties regarding the composition of functions and change of variables. Using these new results, we show that a class of non-differentiable nonconvex optimization problems arising in tensor decomposition applications are global functions. This is the first result concerning nonconvex methods for nonsmooth objective functions. Our result provides a theoretical guarantee for the widely-used
ℓ
1
ℓ
norm to avoid outliers in nonconvex optimization."
neurips,https://proceedings.neurips.cc/paper/2018/file/b3dd760eb02d2e669c604f6b2f1e803f-Paper.pdf,A Structured Prediction Approach for Label Ranking,"Anna Korba, Alexandre Garcia, Florence d'Alché-Buc",
neurips,https://proceedings.neurips.cc/paper/2018/file/b3e3e393c77e35a4a3f3cbd1e429b5dc-Paper.pdf,Geometrically Coupled Monte Carlo Sampling,"Mark Rowland, Krzysztof M. Choromanski, François Chalus, Aldo Pacchiano, Tamas Sarlos, Richard E. Turner, Adrian Weller",
neurips,https://proceedings.neurips.cc/paper/2018/file/b4288d9c0ec0a1841b3b3728321e7088-Paper.pdf,The Lingering of Gradients: How to Reuse Gradients Over Time,"Zeyuan Allen-Zhu, David Simchi-Levi, Xinshang Wang","Classically, the time complexity of a first-order method is estimated by its number of gradient computations. In this paper, we study a more refined complexity by taking into account the
lingering'' of gradients: once a gradient is computed at
x
k
x
, the additional time to compute gradients at
x
k
+
1
,
x
k
+
2
,
…
x
may be reduced. We show how this improves the running time of gradient descent and SVRG. For instance, if the ""additional time'' scales linearly with respect to the traveled distance, then the ""convergence rate'' of gradient descent can be improved from
1
/
T
1
to
exp
(
−
T
1
/
3
)
exp
. On the empirical side, we solve a hypothetical revenue management problem on the Yahoo! Front Page Today Module application with 4.6m users to
10
−
6
10
error (or
10
−
12
10
dual error) using 6 passes of the dataset."
neurips,https://proceedings.neurips.cc/paper/2018/file/b43a6403c17870707ca3c44984a2da22-Paper.pdf,Submodular Maximization via Gradient Ascent: The Case of Deep Submodular Functions,"Wenruo Bai, William Stafford Noble, Jeff A. Bilmes","We study the problem of maximizing deep submodular functions (DSFs) subject to a matroid constraint. DSFs are an expressive class of submodular functions that include, as strict subfamilies, the facility location, weighted coverage, and sums of concave composed with modular functions. We use a strategy similar to the continuous greedy approach, but we show that the multilinear extension of any DSF has a natural and computationally attainable concave relaxation that we can optimize using gradient ascent. Our results show a guarantee of
max
0
<
δ
<
1
(
1
−
ϵ
−
δ
−
e
−
δ
2
Ω
(
k
)
)
max
with a running time of
O
(
\nicefrac
n
2
ϵ
2
)
O
plus time for pipage rounding to recover a discrete solution, where
k
k
is the rank of the matroid constraint. This bound is often better than the standard
1
−
1
/
e
1
guarantee of the continuous greedy algorithm, but runs much faster. Our bound also holds even for fully curved (
c
=
1
c
) functions where the guarantee of
1
−
c
/
e
1
degenerates to
1
−
1
/
e
1
where
c
c
is the curvature of
f
f
. We perform computational experiments that support our theoretical results."
neurips,https://proceedings.neurips.cc/paper/2018/file/b440509a0106086a67bc2ea9df0a1dab-Paper.pdf,Sparsified SGD with Memory,"Sebastian U. Stich, Jean-Baptiste Cordonnier, Martin Jaggi","In this work we analyze Stochastic Gradient Descent (SGD) with k-sparsification or compression (for instance top-k or random-k) and show that this scheme converges at the same rate as vanilla SGD when equipped with error compensation (keeping track of accumulated errors in memory). That is, communication can be reduced by a factor of the dimension of the problem (sometimes even more) whilst still converging at the same rate. We present numerical experiments to illustrate the theoretical findings and the good scalability for distributed applications."
neurips,https://proceedings.neurips.cc/paper/2018/file/b4568df26077653eeadf29596708c94b-Paper.pdf,Convergence of Cubic Regularization for Nonconvex Optimization under KL Property,"Yi Zhou, Zhe Wang, Yingbin Liang",
neurips,https://proceedings.neurips.cc/paper/2018/file/b495ce63ede0f4efc9eec62cb947c162-Paper.pdf,Model Agnostic Supervised Local Explanations,"Gregory Plumb, Denali Molitor, Ameet S. Talwalkar",
neurips,https://proceedings.neurips.cc/paper/2018/file/b4a721cfb62f5d19ec61575114d8a2d1-Paper.pdf,Mental Sampling in Multimodal Representations,"Jianqiao Zhu, Adam Sanborn, Nick Chater",
neurips,https://proceedings.neurips.cc/paper/2018/file/b4d168b48157c623fbd095b4a565b5bb-Paper.pdf,Nonparametric learning from Bayesian models with randomized objective functions,"Simon Lyddon, Stephen Walker, Chris C. Holmes",
neurips,https://proceedings.neurips.cc/paper/2018/file/b534ba68236ba543ae44b22bd110a1d6-Paper.pdf,On the Dimensionality of Word Embedding,"Zi Yin, Yuanyuan Shen",
neurips,https://proceedings.neurips.cc/paper/2018/file/b58f7d184743106a8a66028b7a28937c-Paper.pdf,Large Scale computation of Means and Clusters for Persistence Diagrams using Optimal Transport,"Theo Lacombe, Marco Cuturi, Steve OUDOT",
neurips,https://proceedings.neurips.cc/paper/2018/file/b59a51a3c0bf9c5228fde841714f523a-Paper.pdf,Probabilistic Matrix Factorization for Automated Machine Learning,"Nicolo Fusi, Rishit Sheth, Melih Elibol",
neurips,https://proceedings.neurips.cc/paper/2018/file/b5a1d925221b37e2e399f7b319038ba0-Paper.pdf,REFUEL: Exploring Sparse Features in Deep Reinforcement Learning for Fast Disease Diagnosis,"Yu-Shao Peng, Kai-Fu Tang, Hsuan-Tien Lin, Edward Chang",
neurips,https://proceedings.neurips.cc/paper/2018/file/b607aa5b2fd58dd860bfb55619389982-Paper.pdf,Cluster Variational Approximations for Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data,"Dominik Linzner, Heinz Koeppl",
neurips,https://proceedings.neurips.cc/paper/2018/file/b60c5ab647a27045b462934977ccad9a-Paper.pdf,Norm-Ranging LSH for Maximum Inner Product Search,"Xiao Yan, Jinfeng Li, Xinyan Dai, Hongzhi Chen, James Cheng",
neurips,https://proceedings.neurips.cc/paper/2018/file/b613e70fd9f59310cf0a8d33de3f2800-Paper.pdf,Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions,"Boris Muzellec, Marco Cuturi",
neurips,https://proceedings.neurips.cc/paper/2018/file/b6617980ce90f637e68c3ebe8b9be745-Paper.pdf,Dirichlet-based Gaussian Processes for Large-scale Calibrated Classification,"Dimitrios Milios, Raffaello Camoriano, Pietro Michiardi, Lorenzo Rosasco, Maurizio Filippone",
neurips,https://proceedings.neurips.cc/paper/2018/file/b691334ccf10d4ab144d672f7783c8a3-Paper.pdf,Latent Alignment and Variational Attention,"Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, Alexander Rush",
neurips,https://proceedings.neurips.cc/paper/2018/file/b6a1085a27ab7bff7550f8a3bd017df8-Paper.pdf,The Pessimistic Limits and Possibilities of Margin-based Losses in Semi-supervised Learning,"Jesse Krijthe, Marco Loog",
neurips,https://proceedings.neurips.cc/paper/2018/file/b6cda17abb967ed28ec9610137aa45f7-Paper.pdf,Porcupine Neural Networks: Approximating Neural Network Landscapes,"Soheil Feizi, Hamid Javadi, Jesse Zhang, David Tse",
neurips,https://proceedings.neurips.cc/paper/2018/file/b6d67a24906e8a8541291882f81d31ca-Paper.pdf,On the Local Hessian in Back-propagation,"Huishuai Zhang, Wei Chen, Tie-Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2018/file/b865367fc4c0845c0682bd466e6ebf4c-Paper.pdf,Infinite-Horizon Gaussian Processes,"Arno Solin, James Hensman, Richard E. Turner",
neurips,https://proceedings.neurips.cc/paper/2018/file/b8a03c5c15fcfa8dae0b03351eb1742f-Paper.pdf,Constrained Graph Variational Autoencoders for Molecule Design,"Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, Alexander Gaunt",
neurips,https://proceedings.neurips.cc/paper/2018/file/b8cfbf77a3d250a4523ba67a65a7d031-Paper.pdf,Hardware Conditioned Policies for Multi-Robot Transfer Learning,"Tao Chen, Adithyavairavan Murali, Abhinav Gupta",
neurips,https://proceedings.neurips.cc/paper/2018/file/b91f4f4d36fa98a94ac5584af95594a0-Paper.pdf,Learning without the Phase: Regularized PhaseMax Achieves Optimal Sample Complexity,"Fariborz Salehi, Ehsan Abbasi, Babak Hassibi","The problem of estimating an unknown signal,
x
0
∈
R
n
x
, from a vector
y
∈
R
m
y
consisting of
m
m
magnitude-only measurements of the form
y
i
=
|
a
i
x
0
|
y
, where
a
i
a
's are the rows of a known measurement matrix
A
A
is a classical problem known as phase retrieval. This problem arises when measuring the phase is costly or altogether infeasible. In many applications in machine learning, signal processing, statistics, etc., the underlying signal has certain structure (sparse, low-rank, finite alphabet, etc.), opening of up the possibility of recovering
x
0
x
from a number of measurements smaller than the ambient dimension, i.e., $m"
neurips,https://proceedings.neurips.cc/paper/2018/file/b9228e0962a78b84f3d5d92f4faa000b-Paper.pdf,Learning Disentangled Joint Continuous and Discrete Representations,Emilien Dupont,
neurips,https://proceedings.neurips.cc/paper/2018/file/b994697479c5716eda77e8e9713e5f0f-Paper.pdf,Attacks Meet Interpretability: Attribute-steered Detection of Adversarial Samples,"Guanhong Tao, Shiqing Ma, Yingqi Liu, Xiangyu Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/b9a25e422ba96f7572089a00b838c3f8-Paper.pdf,Learning To Learn Around A Common Mean,"Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, Massimiliano Pontil",
neurips,https://proceedings.neurips.cc/paper/2018/file/b9f94c77652c9a76fc8a442748cd54bd-Paper.pdf,Recurrent Relational Networks,"Rasmus Palm, Ulrich Paquet, Ole Winther",
neurips,https://proceedings.neurips.cc/paper/2018/file/ba3e9b6a519cfddc560b5d53210df1bd-Paper.pdf,Experimental Design for Cost-Aware Learning of Causal Graphs,"Erik Lindgren, Murat Kocaoglu, Alexandros G. Dimakis, Sriram Vishwanath",
neurips,https://proceedings.neurips.cc/paper/2018/file/ba4002d88b8860b6a684ade8357aba56-Paper.pdf,Reinforcement Learning with Multiple Experts: A Bayesian Model Combination Approach,"Michael Gimelfarb, Scott Sanner, Chi-Guhn Lee",
neurips,https://proceedings.neurips.cc/paper/2018/file/ba6d843eb4251a4526ce65d1807a9309-Paper.pdf,Differentiable MPC for End-to-end Planning and Control,"Brandon Amos, Ivan Jimenez, Jacob Sacks, Byron Boots, J. Zico Kolter",
neurips,https://proceedings.neurips.cc/paper/2018/file/ba9a56ce0a9bfa26e8ed9e10b2cc8f46-Paper.pdf,Zeroth-Order Stochastic Variance Reduction for Nonconvex Optimization,"Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu Chang, Lisa Amini","As application demands for zeroth-order (gradient-free) optimization accelerate, the need for variance reduced and faster converging approaches is also intensifying. This paper addresses these challenges by presenting: a) a comprehensive theoretical analysis of variance reduced zeroth-order (ZO) optimization, b) a novel variance reduced ZO algorithm, called ZO-SVRG, and c) an experimental evaluation of our approach in the context of two compelling applications, black-box chemical material classification and generation of adversarial examples from black-box deep neural network models. Our theoretical analysis uncovers an essential difficulty in the analysis of ZO-SVRG: the unbiased assumption on gradient estimates no longer holds. We prove that compared to its first-order counterpart, ZO-SVRG with a two-point random gradient estimator could suffer an additional error of order
O
(
1
/
b
)
O
, where
b
b
is the mini-batch size. To mitigate this error, we propose two accelerated versions of ZO-SVRG utilizing variance reduced gradient estimators, which achieve the best rate known for ZO stochastic optimization (in terms of iterations). Our extensive experimental results show that our approaches outperform other state-of-the-art ZO algorithms, and strike a balance between the convergence rate and the function query complexity."
neurips,https://proceedings.neurips.cc/paper/2018/file/bb03e43ffe34eeb242a2ee4a4f125e56-Paper.pdf,Near-Optimal Time and Sample Complexities for Solving Markov Decision Processes with a Generative Model,"Aaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, Yinyu Ye","In this paper we consider the problem of computing an
ϵ
ϵ
-optimal policy of a discounted Markov Decision Process (DMDP) provided we can only access its transition function through a generative sampling model that given any state-action pair samples from the transition function in
O
(
1
)
O
time. Given such a DMDP with states
\states
\states
, actions
\actions
\actions
, discount factor
γ
∈
(
0
,
1
)
γ
, and rewards in range
[
0
,
1
]
[
we provide an algorithm which computes an
ϵ
ϵ
-optimal policy with probability
1
−
δ
1
where {\it both} the run time spent and number of sample taken is upper bounded by
O
[
|
\cS
|
|
\cA
|
(
1
−
γ
)
3
ϵ
2
log
(
|
\cS
|
|
\cA
|
(
1
−
γ
)
δ
ϵ
)
log
(
1
(
1
−
γ
)
ϵ
)
]
 .
O[|\cS||\cA|(1−γ)3ϵ2log⁡(|\cS||\cA|(1−γ)δϵ)log⁡(1(1−γ
For fixed values of
ϵ
ϵ
, this improves upon the previous best known bounds by a factor of
(
1
−
γ
)
−
1
(
and matches the sample complexity lower bounds proved in \cite{azar2013minimax} up to logarithmic factors. We also extend our method to computing
ϵ
ϵ
-optimal policies for finite-horizon MDP with a generative model and provide a nearly matching sample complexity lower bound."
neurips,https://proceedings.neurips.cc/paper/2018/file/bbb001ba009ed11717eaec9305b2feb6-Paper.pdf,Algebraic tests of general Gaussian latent tree models,"Dennis Leung, Mathias Drton",
neurips,https://proceedings.neurips.cc/paper/2018/file/bd1354624fbae3b2149878941c60df99-Paper.pdf,Binary Classification from Positive-Confidence Data,"Takashi Ishida, Gang Niu, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2018/file/bdb3c278f45e6734c35733d24299d3f4-Paper.pdf,Transfer Learning with Neural AutoML,"Catherine Wong, Neil Houlsby, Yifeng Lu, Andrea Gesmundo",
neurips,https://proceedings.neurips.cc/paper/2018/file/be3087e74e9100d4bc4c6268cdbe8456-Paper.pdf,"Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs","Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P. Vetrov, Andrew G. Wilson",
neurips,https://proceedings.neurips.cc/paper/2018/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf,Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated Decision Making,"Hoda Heidari, Claudio Ferrari, Krishna Gummadi, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2018/file/be53d253d6bc3258a8160556dda3e9b2-Paper.pdf,A Unified View of Piecewise Linear Neural Network Verification,"Rudy R. Bunel, Ilker Turkaslan, Philip Torr, Pushmeet Kohli, Pawan K. Mudigonda",
neurips,https://proceedings.neurips.cc/paper/2018/file/bea6cfd50b4f5e3c735a972cf0eb8450-Paper.pdf,Lifted Weighted Mini-Bucket,"Nicholas Gallo, Alexander T. Ihler",
neurips,https://proceedings.neurips.cc/paper/2018/file/bf424cb7b0dea050a42b9739eb261a3a-Paper.pdf,Can We Gain More from Orthogonality Regularizations in Training Deep Networks?,"Nitin Bansal, Xiaohan Chen, Zhangyang Wang",
neurips,https://proceedings.neurips.cc/paper/2018/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf,Understanding the Role of Adaptivity in Machine Teaching: The Case of Version Space Learners,"Yuxin Chen, Adish Singla, Oisin Mac Aodha, Pietro Perona, Yisong Yue",
neurips,https://proceedings.neurips.cc/paper/2018/file/bf764716fe1a58cb07f8a377ec25c16d-Paper.pdf,Wavelet regression and additive models for irregularly spaced data,"Asad Haris, Ali Shojaie, Noah Simon",
neurips,https://proceedings.neurips.cc/paper/2018/file/c042f4db68f23406c6cecf84a7ebb0fe-Paper.pdf,Self-Erasing Network for Integral Object Attention,"Qibin Hou, PengTao Jiang, Yunchao Wei, Ming-Ming Cheng",
neurips,https://proceedings.neurips.cc/paper/2018/file/c0560792e4a3c79e62f76cbf9fb277dd-Paper.pdf,Training deep learning based denoisers without ground truth data,"Shakarim Soltanayev, Se Young Chun",
neurips,https://proceedings.neurips.cc/paper/2018/file/c0a271bc0ecb776a094786474322cb82-Paper.pdf,Structural Causal Bandits: Where to Intervene?,"Sanghack Lee, Elias Bareinboim",
neurips,https://proceedings.neurips.cc/paper/2018/file/c157297d1a1ff043255bfb18530caaa2-Paper.pdf,Scalar Posterior Sampling with Applications,"Georgios Theocharous, Zheng Wen, Yasin Abbasi Yadkori, Nikos Vlassis",
neurips,https://proceedings.neurips.cc/paper/2018/file/c17028c9b6e0c5deaad29665d582284a-Paper.pdf,Ex ante coordination and collusion in zero-sum multi-player extensive-form games,"Gabriele Farina, Andrea Celli, Nicola Gatti, Tuomas Sandholm",
neurips,https://proceedings.neurips.cc/paper/2018/file/c1a3d34711ab5d85335331ca0e57f067-Paper.pdf,Online Learning of Quantum States,"Scott Aaronson, Xinyi Chen, Elad Hazan, Satyen Kale, Ashwin Nayak","Suppose we have many copies of an unknown n-qubit state
ρ
ρ
. We measure some copies of
ρ
ρ
using a known two-outcome measurement E_1, then other copies using a measurement E_2, and so on. At each stage t, we generate a current hypothesis
ω
t
ω
about the state
ρ
ρ
, using the outcomes of the previous measurements. We show that it is possible to do this in a way that guarantees that
|
\trace
(
E
i
ω
t
)
−
\trace
(
E
i
ρ
)
|
|
, the error in our prediction for the next measurement, is at least
e
p
s
e
at most
O
(
n
/
e
p
s
2
)
O
\ times. Even in the non-realizable setting---where there could be arbitrary noise in the measurement outcomes---we show how to output hypothesis states that incur at most
O
(
√
T
n
)
O
excess loss over the best possible state on the first
T
T
measurements. These results generalize a 2007 theorem by Aaronson on the PAC-learnability of quantum states, to the online and regret-minimization settings. We give three different ways to prove our results---using convex optimization, quantum postselection, and sequential fat-shattering dimension---which have different advantages in terms of parameters and portability."
neurips,https://proceedings.neurips.cc/paper/2018/file/c1fea270c48e8079d8ddf7d06d26ab52-Paper.pdf,Realistic Evaluation of Deep Semi-Supervised Learning Algorithms,"Avital Oliver, Augustus Odena, Colin A. Raffel, Ekin Dogus Cubuk, Ian Goodfellow",
neurips,https://proceedings.neurips.cc/paper/2018/file/c203d8a151612acf12457e4d67635a95-Paper.pdf,Long short-term memory and Learning-to-learn in networks of spiking neurons,"Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, Wolfgang Maass","A second major mismatch between RSNNs in the brain and models is that the latter only show a small fraction of the dynamics of neurons and synapses in the brain. We include neurons in our RSNN model that reproduce one prominent dynamical process of biological neurons that takes place at the behaviourally relevant time scale of seconds: neuronal adaptation. We denote these networks as LSNNs because of their Long short-term memory. The inclusion of adapting neurons drastically increases the computing and learning capability of RSNNs if they are trained and configured by deep learning (BPTT combined with a rewiring algorithm that optimizes the network architecture). In fact, the computational performance of these RSNNs approaches for the first time that of LSTM networks. In addition RSNNs with adapting neurons can acquire abstract knowledge from prior learning in a Learning-to-Learn (L2L) scheme, and transfer that knowledge in order to learn new but related tasks from very few examples. We demonstrate this for supervised learning and reinforcement learning."
neurips,https://proceedings.neurips.cc/paper/2018/file/c21002f464c5fc5bee3b98ced83963b8-Paper.pdf,Revisiting Decomposable Submodular Function Minimization with Incidence Relations,"Pan Li, Olgica Milenkovic",
neurips,https://proceedings.neurips.cc/paper/2018/file/c2626d850c80ea07e7511bbae4c76f4b-Paper.pdf,DifNet: Semantic Segmentation by Diffusion Networks,"Peng Jiang, Fanglin Gu, Yunhai Wang, Changhe Tu, Baoquan Chen",
neurips,https://proceedings.neurips.cc/paper/2018/file/c26820b8a4c1b3c2aa868d6d57e14a79-Paper.pdf,Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering,"Medhini Narasimhan, Svetlana Lazebnik, Alexander Schwing",
neurips,https://proceedings.neurips.cc/paper/2018/file/c2964caac096f26db222cb325aa267cb-Paper.pdf,Distributed Multi-Player Bandits - a Game of Thrones Approach,"Ilai Bistritz, Amir Leshem",
neurips,https://proceedings.neurips.cc/paper/2018/file/c2f32522a84d5e6357e6abac087f1b0b-Paper.pdf,Scaling Gaussian Process Regression with Derivatives,"David Eriksson, Kun Dong, Eric Lee, David Bindel, Andrew G. Wilson","Gaussian processes (GPs) with derivatives are useful in many applications, including Bayesian optimization, implicit surface reconstruction, and terrain reconstruction. Fitting a GP to function values and derivatives at
n
n
points in
d
d
dimensions requires linear solves and log determinants with an
n
(
d
+
1
)
×
n
(
d
+
1
)
n
positive definite matrix-- leading to prohibitive
O
(
n
3
d
3
)
O
computations for standard direct methods. We propose iterative solvers using fast
O
(
n
d
)
O
matrix-vector multiplications (MVMs), together with pivoted Cholesky preconditioning that cuts the iterations to convergence by several orders of magnitude, allowing for fast kernel learning and prediction. Our approaches, together with dimensionality reduction, allows us to scale Bayesian optimization with derivatives to high-dimensional problems and large evaluation budgets."
neurips,https://proceedings.neurips.cc/paper/2018/file/c32d9bf27a3da7ec8163957080c8628e-Paper.pdf,Deep Attentive Tracking via Reciprocative Learning,"Shi Pu, Yibing Song, Chao Ma, Honggang Zhang, Ming-Hsuan Yang",
neurips,https://proceedings.neurips.cc/paper/2018/file/c344336196d5ec19bd54fd14befdde87-Paper.pdf,Trading robust representations for sample complexity through self-supervised visual experience,"Andrea Tacchetti, Stephen Voinea, Georgios Evangelopoulos",
neurips,https://proceedings.neurips.cc/paper/2018/file/c3992e9a68c5ae12bd18488bc579b30d-Paper.pdf,Global Geometry of Multichannel Sparse Blind Deconvolution on the Sphere,"Yanjun Li, Yoram Bresler","Multichannel blind deconvolution is the problem of recovering an unknown signal
f
f
and multiple unknown channels
x
i
x
from convolutional measurements
y
i
=
x
i
⊛
f
y
(
i
=
1
,
2
,
…
,
N
i
). We consider the case where the
x
i
x
's are sparse, and convolution with
f
f
is invertible. Our nonconvex optimization formulation solves for a filter
h
h
on the unit sphere that produces sparse output
y
i
⊛
h
y
. Under some technical assumptions, we show that all local minima of the objective function correspond to the inverse filter of
f
f
up to an inherent sign and shift ambiguity, and all saddle points have strictly negative curvatures. This geometric structure allows successful recovery of
f
f
and
x
i
x
using a simple manifold gradient descent algorithm with random initialization. Our theoretical findings are complemented by numerical experiments, which demonstrate superior performance of the proposed approach over the previous methods."
neurips,https://proceedings.neurips.cc/paper/2018/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf,Sample Efficient Stochastic Gradient Iterative Hard Thresholding Method for Stochastic Sparse Linear Regression with Limited Attribute Observation,"Tomoya Murata, Taiji Suzuki","We develop new stochastic gradient methods for efficiently solving sparse linear regression in a partial attribute observation setting, where learners are only allowed to observe a fixed number of actively chosen attributes per example at training and prediction times. It is shown that the methods achieve essentially a sample complexity of
O
(
1
/
ε
)
O
to attain an error of
ε
ε
under a variant of restricted eigenvalue condition, and the rate has better dependency on the problem dimension than existing methods. Particularly, if the smallest magnitude of the non-zero components of the optimal solution is not too small, the rate of our proposed {\it Hybrid} algorithm can be boosted to near the minimax optimal sample complexity of {\it full information} algorithms. The core ideas are (i) efficient construction of an unbiased gradient estimator by the iterative usage of the hard thresholding operator for configuring an exploration algorithm; and (ii) an adaptive combination of the exploration and an exploitation algorithms for quickly identifying the support of the optimum and efficiently searching the optimal parameter in its support. Experimental results are presented to validate our theoretical findings and the superiority of our proposed methods."
neurips,https://proceedings.neurips.cc/paper/2018/file/c4127b9194fe8562c64dc0f5bf2c93bc-Paper.pdf,Blockwise Parallel Decoding for Deep Autoregressive Models,"Mitchell Stern, Noam Shazeer, Jakob Uszkoreit",
neurips,https://proceedings.neurips.cc/paper/2018/file/c45008212f7bdf6eab6050c2a564435a-Paper.pdf,Sublinear Time Low-Rank Approximation of Distance Matrices,"Ainesh Bakshi, David Woodruff","Let
\PP
=
{
p
1
,
p
2
,
…
p
n
}
\PP
and
\QQ
=
{
q
1
,
q
2
…
q
m
}
\QQ
be two point sets in an arbitrary metric space. Let
\AA
\AA
represent the
m
×
n
m
pairwise distance matrix with
\AA
i
,
j
=
d
(
p
i
,
q
j
)
\AA
. Such distance matrices are commonly computed in software packages and have applications to learning image manifolds, handwriting recognition, and multi-dimensional unfolding, among other things. In an attempt to reduce their description size, we study low rank approximation of such matrices. Our main result is to show that for any underlying distance metric
d
d
, it is possible to achieve an additive error low rank approximation in sublinear time. We note that it is provably impossible to achieve such a guarantee in sublinear time for arbitrary matrices
\AA
\AA
, and our proof exploits special properties of distance matrices. We develop a recursive algorithm based on additive projection-cost preserving sampling. We then show that in general, relative error approximation in sublinear time is impossible for distance matrices, even if one allows for bicriteria solutions. Additionally, we show that if
\PP
=
\QQ
\PP
and
d
d
is the squared Euclidean distance, which is not a metric but rather the square of a metric, then a relative error bicriteria solution can be found in sublinear time. Finally, we empirically compare our algorithm with the SVD and input sparsity time algorithms. Our algorithm is several hundred times faster than the SVD, and about
8
8
-
20
20
times faster than input sparsity methods on real-world and and synthetic datasets of size
10
8
10
. Accuracy-wise, our algorithm is only slightly worse than that of the SVD (optimal) and input-sparsity time algorithms."
neurips,https://proceedings.neurips.cc/paper/2018/file/c4616f5a24a66668f11ca4fa80525dc4-Paper.pdf,Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization,"Bruno Korbar, Du Tran, Lorenzo Torresani",
neurips,https://proceedings.neurips.cc/paper/2018/file/c5866e93cab1776890fe343c9e7063fb-Paper.pdf,"Submodular Field Grammars: Representation, Inference, and Application to Image Parsing","Abram L. Friesen, Pedro M. Domingos","Natural scenes contain many layers of part-subpart structure, and distributions over them are thus naturally represented by stochastic image grammars, with one production per decomposition of a part. Unfortunately, in contrast to language grammars, where the number of possible split points for a production
A
→
B
C
A
is linear in the length of
A
A
, in an image there are an exponential number of ways to split a region into subregions. This makes parsing intractable and requires image grammars to be severely restricted in practice, for example by allowing only rectangular regions. In this paper, we address this problem by associating with each production a submodular Markov random field whose labels are the subparts and whose labeling segments the current object into these subparts. We call the result a submodular field grammar (SFG). Finding the MAP split of a region into subregions is now tractable, and by exploiting this we develop an efficient approximate algorithm for MAP parsing of images with SFGs. Empirically, we present promising improvements in accuracy when using SFGs for scene understanding, and show exponential improvements in inference time compared to traditional methods, while returning comparable minima."
neurips,https://proceedings.neurips.cc/paper/2018/file/c5ab0bc60ac7929182aadd08703f1ec6-Paper.pdf,FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification,"Yixiao Ge, Zhuowan Li, Haiyu Zhao, Guojun Yin, Shuai Yi, Xiaogang Wang, hongsheng Li",
neurips,https://proceedings.neurips.cc/paper/2018/file/c5ab6cebaca97f7171139e4d414ff5a6-Paper.pdf,Estimators for Multivariate Information Measures in General Probability Spaces,"Arman Rahimzamani, Himanshu Asnani, Pramod Viswanath, Sreeram Kannan","Information theoretic quantities play an important role in various settings in machine learning, including causality testing, structure inference in graphical models, time-series problems, feature selection as well as in providing privacy guarantees. A key quantity of interest is the mutual information and generalizations thereof, including conditional mutual information, multivariate mutual information, total correlation and directed information. While the aforementioned information quantities are well defined in arbitrary probability spaces, existing estimators employ a
Σ
H
Σ
method, which can only work in purely discrete space or purely continuous case since entropy (or differential entropy) is well defined only in that regime. In this paper, we define a general graph divergence measure (
G
D
M
G
), generalizing the aforementioned information measures and we construct a novel estimator via a coupling trick that directly estimates these multivariate information measures using the Radon-Nikodym derivative. These estimators are proven to be consistent in a general setting which includes several cases where the existing estimators fail, thus providing the only known estimators for the following settings: (1) the data has some discrete and some continuous valued components (2) some (or all) of the components themselves are discrete-continuous \textit{mixtures} (3) the data is real-valued but does not have a joint density on the entire space, rather is supported on a low-dimensional manifold. We show that our proposed estimators significantly outperform known estimators on synthetic and real datasets."
neurips,https://proceedings.neurips.cc/paper/2018/file/c5c1cb0bebd56ae38817b251ad72bedb-Paper.pdf,Graphical Generative Adversarial Networks,"Chongxuan LI, Max Welling, Jun Zhu, Bo Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/c5f5c23be1b71adb51ea9dc8e9d444a8-Paper.pdf,"Deep Homogeneous Mixture Models: Representation, Separation, and Approximation","Priyank Jaini, Pascal Poupart, Yaoliang Yu","At their core, many unsupervised learning models provide a compact representation of homogeneous density mixtures, but their similarities and differences are not always clearly understood. In this work, we formally establish the relationships among latent tree graphical models (including special cases such as hidden Markov models and tensorial mixture models), hierarchical tensor formats and sum-product networks. Based on this connection, we then give a unified treatment of exponential separation in \emph{exact} representation size between deep mixture architectures and shallow ones. In contrast, for \emph{approximate} representation, we show that the conditional gradient algorithm can approximate any homogeneous mixture within
ϵ
ϵ
accuracy by combining
O
(
1
/
ϵ
2
)
O
shallow'' architectures, where the hidden constant may decrease (exponentially) with respect to the depth. Our experiments on both synthetic and real datasets confirm the benefits of depth in density estimation."
neurips,https://proceedings.neurips.cc/paper/2018/file/c5ff2543b53f4cc0ad3819a36752467b-Paper.pdf,Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives,"Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shanmugam, Payel Das",
neurips,https://proceedings.neurips.cc/paper/2018/file/c60d870eaad6a3946ab3e8734466e532-Paper.pdf,Community Exploration: From Offline Optimization to Online Learning,"Xiaowei Chen, Weiran Huang, Wei Chen, John C. S. Lui",
neurips,https://proceedings.neurips.cc/paper/2018/file/c6969ae30d99f73951cb976b88a457af-Paper.pdf,Context-aware Synthesis and Placement of Object Instances,"Donghoon Lee, Sifei Liu, Jinwei Gu, Ming-Yu Liu, Ming-Hsuan Yang, Jan Kautz",
neurips,https://proceedings.neurips.cc/paper/2018/file/c6ede20e6f597abf4b3f6bb30cee16c7-Paper.pdf,Beyond Log-concavity: Provable Guarantees for Sampling Multi-modal Distributions using Simulated Tempering Langevin Monte Carlo,"Holden Lee, Andrej Risteski, Rong Ge","Classical results (going back to Bakry and Emery) on sampling focus on log-concave distributions, and show a natural Markov chain called Langevin diffusion mix in polynomial time. However, all log-concave distributions are uni-modal, while in practice it is very common for the distribution of interest to have multiple modes. In this case, Langevin diffusion suffers from torpid mixing."
neurips,https://proceedings.neurips.cc/paper/2018/file/c6f798b844366ccd65d99bc7f31e0e02-Paper.pdf,M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search,"Yelong Shen, Jianshu Chen, Po-Sen Huang, Yuqing Guo, Jianfeng Gao",
neurips,https://proceedings.neurips.cc/paper/2018/file/c7c46d4baf816bfb07c7f3bf96d88544-Paper.pdf,Unsupervised Image-to-Image Translation Using Domain-Specific Variational Information Bound,"Hadi Kazemi, Sobhan Soleymani, Fariborz Taherkhani, Seyed Iranmanesh, Nasser Nasrabadi",
neurips,https://proceedings.neurips.cc/paper/2018/file/c7d0e7e2922845f3e1185d246d01365d-Paper.pdf,Group Equivariant Capsule Networks,"Jan Eric Lenssen, Matthias Fey, Pascal Libuschewski",
neurips,https://proceedings.neurips.cc/paper/2018/file/c8dfece5cc68249206e4690fc4737a8d-Paper.pdf,Fairness Through Computationally-Bounded Awareness,"Michael Kim, Omer Reingold, Guy Rothblum",
neurips,https://proceedings.neurips.cc/paper/2018/file/c8ed21db4f678f3b13b9d5ee16489088-Paper.pdf,Geometry Based Data Generation,"Ofir Lindenbaum, Jay Stanley, Guy Wolf, Smita Krishnaswamy",
neurips,https://proceedings.neurips.cc/paper/2018/file/c90070e1f03e982448983975a0f52d57-Paper.pdf,Searching for Efficient Multi-Scale Architectures for Dense Image Prediction,"Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam, Jon Shlens",
neurips,https://proceedings.neurips.cc/paper/2018/file/c911241d00294e8bb714eee2e83fa475-Paper.pdf,Adversarial Scene Editing: Automatic Object Removal from Weak Supervision,"Rakshith R. Shetty, Mario Fritz, Bernt Schiele",
neurips,https://proceedings.neurips.cc/paper/2018/file/c9319967c038f9b923068dabdf60cfe3-Paper.pdf,Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition,"Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2018/file/c9f95a0a5af052bffce5c89917335f67-Paper.pdf,MULAN: A Blind and Off-Grid Method for Multichannel Echo Retrieval,"Helena Peic Tukuljac, Antoine Deleforge, Remi Gribonval",
neurips,https://proceedings.neurips.cc/paper/2018/file/caa202034f268232c26fac9435f54e15-Paper.pdf,Diminishing Returns Shape Constraints for Interpretability and Regularization,"Maya Gupta, Dara Bahri, Andrew Cotter, Kevin Canini",
neurips,https://proceedings.neurips.cc/paper/2018/file/cac8e13055d2e4f62b6322254203b293-Paper.pdf,Breaking the Activation Function Bottleneck through Adaptive Parameterization,"Sebastian Flennerhag, Hujun Yin, John Keane, Mark Elliot",
neurips,https://proceedings.neurips.cc/paper/2018/file/cb463625fc9dde2d82207e15bde1b674-Paper.pdf,Sketching Method for Large Scale Combinatorial Inference,"Wei Sun, Junwei Lu, Han Liu",
neurips,https://proceedings.neurips.cc/paper/2018/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Paper.pdf,Symbolic Graph Reasoning Meets Convolutions,"Xiaodan Liang, Zhiting Hu, Hao Zhang, Liang Lin, Eric P. Xing",
neurips,https://proceedings.neurips.cc/paper/2018/file/cc06a6150b92e17dd3076a0f0f9d2af4-Paper.pdf,Topkapi: Parallel and Fast Sketches for Finding Top-K Frequent Elements,"Ankush Mandal, He Jiang, Anshumali Shrivastava, Vivek Sarkar",
neurips,https://proceedings.neurips.cc/paper/2018/file/cc4af25fa9d2d5c953496579b75f6f6c-Paper.pdf,The Price of Fair PCA: One Extra dimension,"Samira Samadi, Uthaipon Tantipongpipat, Jamie H. Morgenstern, Mohit Singh, Santosh Vempala",
neurips,https://proceedings.neurips.cc/paper/2018/file/cc638784cf213986ec75983a4aa08cdb-Paper.pdf,Orthogonally Decoupled Variational Gaussian Processes,"Hugh Salimbeni, Ching-An Cheng, Byron Boots, Marc Deisenroth",
neurips,https://proceedings.neurips.cc/paper/2018/file/cc70903297fe1e25537ae50aea186306-Paper.pdf,Algorithmic Assurance: An Active Approach to Algorithmic Testing using Bayesian Optimisation,"Shivapratap Gopakumar, Sunil Gupta, Santu Rana, Vu Nguyen, Svetha Venkatesh",
neurips,https://proceedings.neurips.cc/paper/2018/file/ccb1d45fb76f7c5a0bf619f979c6cf36-Paper.pdf,Domain-Invariant Projection Learning for Zero-Shot Recognition,"An Zhao, Mingyu Ding, Jiechao Guan, Zhiwu Lu, Tao Xiang, Ji-Rong Wen",
neurips,https://proceedings.neurips.cc/paper/2018/file/ccc0aa1b81bf81e16c676ddb977c5881-Paper.pdf,High Dimensional Linear Regression using Lattice Basis Reduction,"Ilias Zadik, David Gamarnik",
neurips,https://proceedings.neurips.cc/paper/2018/file/cd17d3ce3b64f227987cd92cd701cc58-Paper.pdf,A Retrieve-and-Edit Framework for Predicting Structured Outputs,"Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, Percy S. Liang",
neurips,https://proceedings.neurips.cc/paper/2018/file/cdcb2f5c7b071143529ef7f2705dfbc4-Paper.pdf,Efficient inference for time-varying behavior during learning,"Nicholas A. Roy, Ji Hyun Bak, Athena Akrami, Carlos Brody, Jonathan W. Pillow",
neurips,https://proceedings.neurips.cc/paper/2018/file/cdf1035c34ec380218a8cc9a43d438f9-Paper.pdf,Re-evaluating evaluation,"David Balduzzi, Karl Tuyls, Julien Perolat, Thore Graepel",
neurips,https://proceedings.neurips.cc/paper/2018/file/cdf28f8b7d14ab02d12a2329d71e4079-Paper.pdf,Learning Abstract Options,"Matthew Riemer, Miao Liu, Gerald Tesauro",
neurips,https://proceedings.neurips.cc/paper/2018/file/cdfa4c42f465a5a66871587c69fcfa34-Paper.pdf,Optimal Algorithms for Continuous Non-monotone Submodular and DR-Submodular Maximization,"Rad Niazadeh, Tim Roughgarden, Joshua Wang","Our first algorithm is a single-pass algorithm that uses novel ideas such as reducing the guaranteed approximation problem to analyzing a zero-sum game for each coordinate, and incorporates the geometry of this zero-sum game to fix the value at this coordinate. Our second algorithm is a faster single-pass algorithm that exploits coordinate-wise concavity to identify a monotone equilibrium condition sufficient for getting the required approximation guarantee, and hunts for the equilibrium point using binary search. We further run experiments to verify the performance of our proposed algorithms in related machine learning applications."
neurips,https://proceedings.neurips.cc/paper/2018/file/ce5140df15d046a66883807d18d0264b-Paper.pdf,Sequential Context Encoding for Duplicate Removal,"Lu Qi, Shu Liu, Jianping Shi, Jiaya Jia",
neurips,https://proceedings.neurips.cc/paper/2018/file/ce6c92303f38d297e263c7180f03d402-Paper.pdf,PG-TS: Improved Thompson Sampling for Logistic Contextual Bandits,"Bianca Dumitrascu, Karen Feng, Barbara Engelhardt",
neurips,https://proceedings.neurips.cc/paper/2018/file/cebd648f9146a6345d604ab093b02c73-Paper.pdf,Variance-Reduced Stochastic Gradient Descent on Streaming Data,"Ellango Jothimurugesan, Ashraf Tahmasbi, Phillip Gibbons, Srikanta Tirthapura",
neurips,https://proceedings.neurips.cc/paper/2018/file/cee631121c2ec9232f3a2f028ad5c89b-Paper.pdf,Reinforced Continual Learning,"Ju Xu, Zhanxing Zhu",
neurips,https://proceedings.neurips.cc/paper/2018/file/cf05968255451bdefe3c5bc64d550517-Paper.pdf,GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation in Distributed CNN Training,"Mingchao Yu, Zhifeng Lin, Krishna Narra, Songze Li, Youjie Li, Nam Sung Kim, Alexander Schwing, Murali Annavaram, Salman Avestimehr",
neurips,https://proceedings.neurips.cc/paper/2018/file/cf8c9be2a4508a24ae92c9d3d379131d-Paper.pdf,Theoretical Linear Convergence of Unfolded ISTA and Its Practical Weights and Thresholds,"Xiaohan Chen, Jialin Liu, Zhangyang Wang, Wotao Yin",
neurips,https://proceedings.neurips.cc/paper/2018/file/cfa0860e83a4c3a763a7e62d825349f7-Paper.pdf,Optimization for Approximate Submodularity,"Yaron Singer, Avinatan Hassidim",
neurips,https://proceedings.neurips.cc/paper/2018/file/d04863f100d59b3eb688a11f95b0ae60-Paper.pdf,Efficient Neural Network Robustness Certification with General Activation Functions,"Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, Luca Daniel",
neurips,https://proceedings.neurips.cc/paper/2018/file/d0921d442ee91b896ad95059d13df618-Paper.pdf,Neural Edit Operations for Biological Sequences,"Satoshi Koide, Keisuke Kawano, Takuro Kutsuna",
neurips,https://proceedings.neurips.cc/paper/2018/file/d09bf41544a3365a46c9077ebb5e35c3-Paper.pdf,Adapted Deep Embeddings: A Synthesis of Methods for k-Shot Inductive Transfer Learning,"Tyler Scott, Karl Ridgeway, Michael C. Mozer",
neurips,https://proceedings.neurips.cc/paper/2018/file/d0f5722f11a0cc839fa2ca6ea49d8585-Paper.pdf,The Importance of Sampling inMeta-Reinforcement Learning,"Bradly Stadie, Ge Yang, Rein Houthooft, Peter Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, Ilya Sutskever","We interpret meta-reinforcement learning as the problem of learning how to quickly find a good sampling distribution in a new environment. This interpretation leads to the development of two new meta-reinforcement learning algorithms: E-MAML and E-
RL
2
RL
. Results are presented on a new environment we call `Krazy World': a difficult high-dimensional gridworld which is designed to highlight the importance of correctly differentiating through sampling distributions in meta-reinforcement learning. Further results are presented on a set of maze environments. We show E-MAML and E-
RL
2
RL
deliver better performance than baseline algorithms on both tasks."
neurips,https://proceedings.neurips.cc/paper/2018/file/d0fb963ff976f9c37fc81fe03c21ea7b-Paper.pdf,KONG: Kernels for ordered-neighborhood graphs,"Moez Draief, Konstantin Kutzkov, Kevin Scaman, Milan Vojnovic",
neurips,https://proceedings.neurips.cc/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf,Glow: Generative Flow with Invertible 1x1 Convolutions,"Durk P. Kingma, Prafulla Dhariwal",
neurips,https://proceedings.neurips.cc/paper/2018/file/d198bd736a97e7cecfdf8f4f2027ef80-Paper.pdf,Efficient Projection onto the Perfect Phylogeny Model,"Bei Jia, Surjyendu Ray, Sam Safavi, José Bento",
neurips,https://proceedings.neurips.cc/paper/2018/file/d1f255a373a3cef72e03aa9d980c7eca-Paper.pdf,"Do Less, Get More: Streaming Submodular Maximization with Subsampling","Moran Feldman, Amin Karbasi, Ehsan Kazemi","In this paper, we develop the first one-pass streaming algorithm for submodular maximization that does not evaluate the entire stream even once. By carefully subsampling each element of the data stream, our algorithm enjoys the tightest approximation guarantees in various settings while having the smallest memory footprint and requiring the lowest number of function evaluations. More specifically, for a monotone submodular function and a
p
p
-matchoid constraint, our randomized algorithm achieves a
4
p
4
approximation ratio (in expectation) with
O
(
k
)
O
memory and
O
(
k
m
/
p
)
O
queries per element (
k
k
is the size of the largest feasible solution and
m
m
is the number of matroids used to define the constraint). For the non-monotone case, our approximation ratio increases only slightly to
4
p
+
2
−
o
(
1
)
4
. To the best or our knowledge, our algorithm is the first that combines the benefits of streaming and subsampling in a novel way in order to truly scale submodular maximization to massive machine learning problems. To showcase its practicality, we empirically evaluated the performance of our algorithm on a video summarization application and observed that it outperforms the state-of-the-art algorithm by up to fifty-fold while maintaining practically the same utility. We also evaluated the scalability of our algorithm on a large dataset of Uber pick up locations."
neurips,https://proceedings.neurips.cc/paper/2018/file/d1ff1ec86b62cd5f3903ff19c3a326b2-Paper.pdf,Temporal alignment and latent Gaussian process factor inference in population spike trains,"Lea Duncker, Maneesh Sahani",
neurips,https://proceedings.neurips.cc/paper/2018/file/d25414405eb37dae1c14b18d6a2cac34-Paper.pdf,Dimensionality Reduction for Stationary Time Series via Stochastic Nonconvex Optimization,"Minshuo Chen, Lin Yang, Mengdi Wang, Tuo Zhao",
neurips,https://proceedings.neurips.cc/paper/2018/file/d296c101daa88a51f6ca8cfc1ac79b50-Paper.pdf,"Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling","Yunzhe Tao, Qi Sun, Qiang Du, Wei Liu",
neurips,https://proceedings.neurips.cc/paper/2018/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf,Partially-Supervised Image Captioning,"Peter Anderson, Stephen Gould, Mark Johnson",
neurips,https://proceedings.neurips.cc/paper/2018/file/d3157f2f0212a80a5d042c127522a2d5-Paper.pdf,SLANG: Fast Structured Covariance Approximations for Bayesian Deep Learning with Natural Gradient,"Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark Schmidt, Mohammad Emtiyaz Khan",
neurips,https://proceedings.neurips.cc/paper/2018/file/d34ab169b70c9dcd35e62896010cd9ff-Paper.pdf,On Learning Markov Chains,"Yi Hao, Alon Orlitsky, Venkatadheeraj Pichapati","For the first measure, we determine the min-max prediction risk to within a linear factor in the alphabet size, showing it is \Omega(k\log\log n/n) and O(k^2\log\log n/n). For the second, if the transition probabilities can be arbitrarily small, then only trivial uniform risk upper bounds can be derived. We therefore consider transition probabilities that are bounded away from zero, and resolve the problem for essentially all sufficiently smooth f-divergences, including KL-, L_2-, Chi-squared, Hellinger, and Alpha-divergences."
neurips,https://proceedings.neurips.cc/paper/2018/file/d3b1fb02964aa64e257f9f26a31f72cf-Paper.pdf,Is Q-Learning Provably Efficient?,"Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, Michael I. Jordan","Model-free reinforcement learning (RL) algorithms directly parameterize and update value functions or policies, bypassing the modeling of the environment. They are typically simpler, more flexible to use, and thus more prevalent in modern deep RL than model-based approaches. However, empirical work has suggested that they require large numbers of samples to learn. The theoretical question of whether not model-free algorithms are in fact \emph{sample efficient} is one of the most fundamental questions in RL. The problem is unsolved even in the basic scenario with finitely many states and actions. We prove that, in an episodic MDP setting, Q-learning with UCB exploration achieves regret
\tlO
(
√
H
3
S
A
T
)
\tlO
where
S
S
and
A
A
are the numbers of states and actions,
H
H
is the number of steps per episode, and
T
T
is the total number of steps. Our regret matches the optimal regret up to a single
√
H
H
factor. Thus we establish the sample efficiency of a classical model-free approach. Moreover, to the best of our knowledge, this is the first model-free analysis to establish
√
T
T
regret \emph{without} requiring access to a
simulator.''"
neurips,https://proceedings.neurips.cc/paper/2018/file/d403137434343677b98efc88cbd5493d-Paper.pdf,Preference Based Adaptation for Learning Objectives,"Yao-Xiang Ding, Zhi-Hua Zhou",
neurips,https://proceedings.neurips.cc/paper/2018/file/d43ab110ab2489d6b9b2caa394bf920f-Paper.pdf,Learning Gaussian Processes by Minimizing PAC-Bayesian Generalization Bounds,"David Reeb, Andreas Doerr, Sebastian Gerwinn, Barbara Rakitsch",
neurips,https://proceedings.neurips.cc/paper/2018/file/d465f14a648b3d0a1faa6f447e526c60-Paper.pdf,Learning Invariances using the Marginal Likelihood,"Mark van der Wilk, Matthias Bauer, ST John, James Hensman",
neurips,https://proceedings.neurips.cc/paper/2018/file/d4b2aeb2453bdadaa45cbe9882ffefcf-Paper.pdf,NEON2: Finding Local Minima via First-Order Oracles,"Zeyuan Allen-Zhu, Yuanzhi Li","As applications, our reduction turns Natasha2 into a first-order method without hurting its theoretical performance. It also converts SGD, GD, SCSG, and SVRG into algorithms finding approximate local minima, outperforming some best known results."
neurips,https://proceedings.neurips.cc/paper/2018/file/d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf,Genetic-Gated Networks for Deep Reinforcement Learning,"Simyung Chang, John Yang, Jaeseok Choi, Nojun Kwak",
neurips,https://proceedings.neurips.cc/paper/2018/file/d54e99a6c03704e95e6965532dec148b-Paper.pdf,Lipschitz regularity of deep neural networks: analysis and efficient estimation,"Aladin Virmaux, Kevin Scaman",
neurips,https://proceedings.neurips.cc/paper/2018/file/d554f7bb7be44a7267068a7df88ddd20-Paper.pdf,Accelerated Stochastic Matrix Inversion: General Theory and Speeding up BFGS Rules for Faster Second-Order Optimization,"Robert Gower, Filip Hanzely, Peter Richtarik, Sebastian U. Stich",
neurips,https://proceedings.neurips.cc/paper/2018/file/d5b3d8dadd770c460b1cde910a711987-Paper.pdf,Blind Deconvolutional Phase Retrieval via Convex Programming,"Ali Ahmed, Alireza Aghasi, Paul Hand","We consider the task of recovering two real or complex
m
m
-vectors from phaseless Fourier measurements of their circular convolution. Our method is a novel convex relaxation that is based on a lifted matrix recovery formulation that allows a nontrivial convex relaxation of the bilinear measurements from convolution. We prove that if the two signals belong to known random subspaces of dimensions
k
k
and
n
n
, then they can be recovered up to the inherent scaling ambiguity with
m
>>
(
k
+
n
)
log
2
m
m
phaseless measurements. Our method provides the first theoretical recovery guarantee for this problem by a computationally efficient algorithm and does not require a solution estimate to be computed for initialization. Our proof is based Rademacher complexity estimates. Additionally, we provide an ADMM implementation of the method and provide numerical experiments that verify the theory."
neurips,https://proceedings.neurips.cc/paper/2018/file/d60678e8f2ba9c540798ebbde31177e8-Paper.pdf,Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation,"Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2018/file/d6288499d0083cc34e60a077b7c4b3e1-Paper.pdf,Spectral Filtering for General Linear Dynamical Systems,"Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, Yi Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/d63fbf8c3173730f82b150c5ef38b8ff-Paper.pdf,Dialog-to-Action: Conversational Question Answering Over a Large-Scale Knowledge Base,"Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, Jian Yin",
neurips,https://proceedings.neurips.cc/paper/2018/file/d645920e395fedad7bbbed0eca3fe2e0-Paper.pdf,Text-Adaptive Generative Adversarial Networks: Manipulating Images with Natural Language,"Seonghyeon Nam, Yunji Kim, Seon Joo Kim",
neurips,https://proceedings.neurips.cc/paper/2018/file/d693d554e0ede0d75f7d2873b015f228-Paper.pdf,Exploration in Structured Reinforcement Learning,"Jungseul Ok, Alexandre Proutiere, Damianos Tranos",
neurips,https://proceedings.neurips.cc/paper/2018/file/d759175de8ea5b1d9a2660e45554894f-Paper.pdf,Tree-to-tree Neural Networks for Program Translation,"Xinyun Chen, Chang Liu, Dawn Song",
neurips,https://proceedings.neurips.cc/paper/2018/file/d79aac075930c83c2f1e369a511148fe-Paper.pdf,Virtual Class Enhanced Discriminative Embedding Learning,"Binghui Chen, Weihong Deng, Haifeng Shen",
neurips,https://proceedings.neurips.cc/paper/2018/file/d7a84628c025d30f7b2c52c958767e76-Paper.pdf,Neural Networks Trained to Solve Differential Equations Learn General Representations,"Martin Magill, Faisal Qureshi, Hendrick de Haan",
neurips,https://proceedings.neurips.cc/paper/2018/file/d7e77c835af3d2a803c1cf28d60575bc-Paper.pdf,Deep Generative Models with Learnable Knowledge Constraints,"Zhiting Hu, Zichao Yang, Russ R. Salakhutdinov, LIANHUI Qin, Xiaodan Liang, Haoye Dong, Eric P. Xing",
neurips,https://proceedings.neurips.cc/paper/2018/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf,How to Start Training: The Effect of Initialization and Architecture,"Boris Hanin, David Rolnick",
neurips,https://proceedings.neurips.cc/paper/2018/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf,Video-to-Video Synthesis,"Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, Bryan Catanzaro",
neurips,https://proceedings.neurips.cc/paper/2018/file/d88518acbcc3d08d1f18da62f9bb26ec-Paper.pdf,Near-Optimal Policies for Dynamic Multinomial Logit Assortment Selection Models,"Yining Wang, Xi Chen, Yuan Zhou",
neurips,https://proceedings.neurips.cc/paper/2018/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf,Occam's razor is insufficient to infer the preferences of irrational agents,"Stuart Armstrong, Sören Mindermann",
neurips,https://proceedings.neurips.cc/paper/2018/file/d8c9d05ec6e86d5bbad7a2f88a1701d0-Paper.pdf,Bandit Learning with Implicit Feedback,"Yi Qi, Qingyun Wu, Hongning Wang, Jie Tang, Maosong Sun",
neurips,https://proceedings.neurips.cc/paper/2018/file/d903e9608cfbf08910611e4346a0ba44-Paper.pdf,Adversarial Regularizers in Inverse Problems,"Sebastian Lunz, Ozan Öktem, Carola-Bibiane Schönlieb",
neurips,https://proceedings.neurips.cc/paper/2018/file/d94fd74dcde1aa553be72c1006578b23-Paper.pdf,The emergence of multiple retinal cell types through efficient coding of natural movies,"Samuel Ocko, Jack Lindsey, Surya Ganguli, Stephane Deny",
neurips,https://proceedings.neurips.cc/paper/2018/file/d9fbed9da256e344c1fa46bb46c34c5f-Paper.pdf,Multiple Instance Learning for Efficient Sequential Data Classification on Resource-constrained Devices,"Don Dennis, Chirag Pabbaraju, Harsha Vardhan Simhadri, Prateek Jain",
neurips,https://proceedings.neurips.cc/paper/2018/file/d9ff90f4000eacd3a6c9cb27f78994cf-Paper.pdf,Dropping Symmetry for Fast Symmetric Nonnegative Matrix Factorization,"Zhihui Zhu, Xiao Li, Kai Liu, Qiuwei Li",
neurips,https://proceedings.neurips.cc/paper/2018/file/da4902cb0bc38210839714ebdcf0efc3-Paper.pdf,On the Local Minima of the Empirical Risk,"Chi Jin, Lydia T. Liu, Rong Ge, Michael I. Jordan","Population risk is always of primary interest in machine learning; however, learning algorithms only have access to the empirical risk. Even for applications with nonconvex non-smooth losses (such as modern deep networks), the population risk is generally significantly more well behaved from an optimization point of view than the empirical risk. In particular, sampling can create many spurious local minima. We consider a general framework which aims to optimize a smooth nonconvex function
F
F
(population risk) given only access to an approximation
f
f
(empirical risk) that is pointwise close to
F
F
(i.e.,
\norm
F
−
f
∞
≤
ν
\norm
). Our objective is to find the
ϵ
ϵ
-approximate local minima of the underlying function
F
F
while avoiding the shallow local minima---arising because of the tolerance
ν
ν
---which exist only in
f
f
. We propose a simple algorithm based on stochastic gradient descent (SGD) on a smoothed version of
f
f
that is guaranteed to achieve our goal as long as
ν
≤
O
(
ϵ
1.5
/
d
)
ν
. We also provide an almost matching lower bound showing that our algorithm achieves optimal error tolerance
ν
ν
among all algorithms making a polynomial number of queries of
f
f
. As a concrete example, we show that our results can be directly used to give sample complexities for learning a ReLU unit."
neurips,https://proceedings.neurips.cc/paper/2018/file/dabd8d2ce74e782c65a973ef76fd540b-Paper.pdf,GIANT: Globally Improved Approximate Newton Method for Distributed Optimization,"Shusen Wang, Fred Roosta, Peng Xu, Michael W. Mahoney",
neurips,https://proceedings.neurips.cc/paper/2018/file/db1915052d15f7815c8b88e879465a1e-Paper.pdf,Stochastic Cubic Regularization for Fast Nonconvex Optimization,"Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, Michael I. Jordan","This paper proposes a stochastic variant of a classic algorithm---the cubic-regularized Newton method [Nesterov and Polyak]. The proposed algorithm efficiently escapes saddle points and finds approximate local minima for general smooth, nonconvex functions in only
~
O
(
ϵ
−
3.5
)
O
stochastic gradient and stochastic Hessian-vector product evaluations. The latter can be computed as efficiently as stochastic gradients. This improves upon the
~
O
(
ϵ
−
4
)
O
rate of stochastic gradient descent. Our rate matches the best-known result for finding local minima without requiring any delicate acceleration or variance-reduction techniques."
neurips,https://proceedings.neurips.cc/paper/2018/file/db29450c3f5e97f97846693611f98c15-Paper.pdf,Derivative Estimation in Random Design,"Yu Liu, Kris De Brabanter",
neurips,https://proceedings.neurips.cc/paper/2018/file/dba132f6ab6a3e3d17a8d59e82105f4c-Paper.pdf,Approximating Real-Time Recurrent Learning with Random Kronecker Factors,"Asier Mujika, Florian Meier, Angelika Steger",
neurips,https://proceedings.neurips.cc/paper/2018/file/dbab2adc8f9d078009ee3fa810bea142-Paper.pdf,Hyperbolic Neural Networks,"Octavian Ganea, Gary Becigneul, Thomas Hofmann",
neurips,https://proceedings.neurips.cc/paper/2018/file/dbb240d23ce3d732b67bcfbae5956b18-Paper.pdf,Mean Field for the Stochastic Blockmodel: Optimization Landscape and Convergence Issues,"Soumendu Sundar Mukherjee, Purnamrita Sarkar, Y. X. Rachel Wang, Bowei Yan",
neurips,https://proceedings.neurips.cc/paper/2018/file/dbbf603ff0e99629dda5d75b6f75f966-Paper.pdf,Fast Greedy MAP Inference for Determinantal Point Process to Improve Recommendation Diversity,"Laming Chen, Guoxin Zhang, Eric Zhou",
neurips,https://proceedings.neurips.cc/paper/2018/file/dc363817786ff182b7bc59565d864523-Paper.pdf,Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks,"Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Andrea Vedaldi",
neurips,https://proceedings.neurips.cc/paper/2018/file/dc4c44f624d600aa568390f1f1104aa0-Paper.pdf,Active Learning for Non-Parametric Regression Using Purely Random Trees,"Jack Goetz, Ambuj Tewari, Paul Zimmerman",
neurips,https://proceedings.neurips.cc/paper/2018/file/dc5d637ed5e62c36ecb73b654b05ba2a-Paper.pdf,DeepProbLog: Neural Probabilistic Logic Programming,"Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, Luc De Raedt",
neurips,https://proceedings.neurips.cc/paper/2018/file/dc6a70712a252123c40d2adba6a11d84-Paper.pdf,Image-to-image translation for cross-domain disentanglement,"Abel Gonzalez-Garcia, Joost van de Weijer, Yoshua Bengio",
neurips,https://proceedings.neurips.cc/paper/2018/file/dd28e50635038e9cf3a648c2dd17ad0a-Paper.pdf,Expanding Holographic Embeddings for Knowledge Completion,"Yexiang Xue, Yang Yuan, Zhitian Xu, Ashish Sabharwal",
neurips,https://proceedings.neurips.cc/paper/2018/file/dda04f9d634145a9c68d5dfe53b21272-Paper.pdf,Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation,"Qiang Liu, Lihong Li, Ziyang Tang, Dengyong Zhou",
neurips,https://proceedings.neurips.cc/paper/2018/file/de03beffeed9da5f3639a621bcab5dd4-Paper.pdf,TopRank: A practical algorithm for online stochastic ranking,"Tor Lattimore, Branislav Kveton, Shuai Li, Csaba Szepesvari",
neurips,https://proceedings.neurips.cc/paper/2018/file/de7f47e09c8e05e6021ababdf6bc58e7-Paper.pdf,rho-POMDPs have Lipschitz-Continuous epsilon-Optimal Value Functions,"Mathieu Fehr, Olivier Buffet, Vincent Thomas, Jilles Dibangoye",
neurips,https://proceedings.neurips.cc/paper/2018/file/dea9ddb25cbf2352cf4dec30222a02a5-Paper.pdf,Minimax Estimation of Neural Net Distance,"Kaiyi Ji, Yingbin Liang",
neurips,https://proceedings.neurips.cc/paper/2018/file/dead35fa1512ad67301d09326177c42f-Paper.pdf,Using Large Ensembles of Control Variates for Variational Inference,"Tomas Geffner, Justin Domke",
neurips,https://proceedings.neurips.cc/paper/2018/file/deb54ffb41e085fd7f69a75b6359c989-Paper.pdf,Deep Generative Markov State Models,"Hao Wu, Andreas Mardt, Luca Pasquali, Frank Noe",
neurips,https://proceedings.neurips.cc/paper/2018/file/df0aab058ce179e4f7ab135ed4e641a9-Paper.pdf,Zero-Shot Transfer with Deictic Object-Oriented Representation in Reinforcement Learning,"Ofir Marom, Benjamin Rosman",
neurips,https://proceedings.neurips.cc/paper/2018/file/dfa92d8f817e5b08fcaafb50d03763cf-Paper.pdf,Practical Methods for Graph Two-Sample Testing,"Debarghya Ghoshdastidar, Ulrike von Luxburg","In this paper, we consider the problem of two-sample testing of large graphs. We demonstrate the practical merits and limitations of existing theoretical tests and their bootstrapped variants. We also propose two new tests based on asymptotic distributions. We show that these tests are computationally less expensive and, in some cases, more reliable than the existing methods."
neurips,https://proceedings.neurips.cc/paper/2018/file/e02af5824e1eb6ad58d6bc03ac9e827f-Paper.pdf,Point process latent variable models of larval zebrafish behavior,"Anuj Sharma, Robert Johnson, Florian Engert, Scott Linderman",
neurips,https://proceedings.neurips.cc/paper/2018/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf,Learning to Navigate in Cities Without a Map,"Piotr Mirowski, Matt Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, koray kavukcuoglu, Andrew Zisserman, Raia Hadsell",
neurips,https://proceedings.neurips.cc/paper/2018/file/e069ea4c9c233d36ff9c7f329bc08ff1-Paper.pdf,Fast greedy algorithms for dictionary selection with generalized sparsity constraints,"Kaito Fujii, Tasuku Soma",
neurips,https://proceedings.neurips.cc/paper/2018/file/e06f967fb0d355592be4e7674fa31d26-Paper.pdf,Acceleration through Optimistic No-Regret Dynamics,"Jun-Kun Wang, Jacob D. Abernethy","We consider the problem of minimizing a smooth convex function by reducing the optimization to computing the Nash equilibrium of a particular zero-sum convex-concave game. Zero-sum games can be solved using online learning dynamics, where a classical technique involves simulating two no-regret algorithms that play against each other and, after
T
T
rounds, the average iterate is guaranteed to solve the original optimization problem with error decaying as
O
(
log
T
/
T
)
O
. In this paper we show that the technique can be enhanced to a rate of
O
(
1
/
T
2
)
O
by extending recent work \cite{RS13,SALS15} that leverages \textit{optimistic learning} to speed up equilibrium computation. The resulting optimization algorithm derived from this analysis coincides \textit{exactly} with the well-known \NA \cite{N83a} method, and indeed the same story allows us to recover several variants of the Nesterov's algorithm via small tweaks. We are also able to establish the accelerated linear rate for a function which is both strongly-convex and smooth. This methodology unifies a number of different iterative optimization methods: we show that the \HB algorithm is precisely the non-optimistic variant of \NA, and recent prior work already established a similar perspective on \FW \cite{AW17,ALLW18}."
neurips,https://proceedings.neurips.cc/paper/2018/file/e07413354875be01a996dc560274708e-Paper.pdf,Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation,"Yuan Li, Xiaodan Liang, Zhiting Hu, Eric P. Xing",
neurips,https://proceedings.neurips.cc/paper/2018/file/e07bceab69529b0f0b43625953fbf2a0-Paper.pdf,Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo,"Oren Mangoubi, Nisheeth Vishnoi","Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from high-dimensional distributions in Statistics and Machine learning. HMC is known to run very efficiently in practice and its popular second-order
leapfrog"" implementation has long been conjectured to run in
d
1
/
4
d
gradient evaluations. Here we show that this conjecture is true when sampling from strongly log-concave target distributions that satisfy a weak third-order regularity property associated with the input data. Our regularity condition is weaker than the Lipschitz Hessian property and allows us to show faster convergence bounds for a much larger class of distributions than would be possible with the usual Lipschitz Hessian constant alone. Important distributions that satisfy our regularity condition include posterior distributions used in Bayesian logistic regression for which the data satisfies an
incoherence"" property. Our result compares favorably with the best available bounds for the class of strongly log-concave distributions, which grow like
d
1
/
2
d
gradient evaluations with the dimension. Moreover, our simulations on synthetic data suggest that, when our regularity condition is satisfied, leapfrog HMC performs better than its competitors -- both in terms of accuracy and in terms of the number of gradient evaluations it requires."
neurips,https://proceedings.neurips.cc/paper/2018/file/e0ae4561193dbf6e4cf7e8f4006948e3-Paper.pdf,Invertibility of Convolutional Generative Networks from Partial Measurements,"Fangchang Ma, Ulas Ayaz, Sertac Karaman",
neurips,https://proceedings.neurips.cc/paper/2018/file/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Paper.pdf,Towards Robust Detection of Adversarial Examples,"Tianyu Pang, Chao Du, Yinpeng Dong, Jun Zhu",
neurips,https://proceedings.neurips.cc/paper/2018/file/e1021d43911ca2c1845910d84f40aeae-Paper.pdf,Bayesian Model-Agnostic Meta-Learning,"Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, Sungjin Ahn",
neurips,https://proceedings.neurips.cc/paper/2018/file/e1228be46de6a0234ac22ded31417bc7-Paper.pdf,Learning Signed Determinantal Point Processes through the Principal Minor Assignment Problem,Victor-Emmanuel Brunel,"Symmetric determinantal point processes (DPP) are a class of probabilistic models that encode the random selection of items that have a repulsive behavior. They have attracted a lot of attention in machine learning, where returning diverse sets of items is sought for. Sampling and learning these symmetric DPP's is pretty well understood. In this work, we consider a new class of DPP's, which we call signed DPP's, where we break the symmetry and allow attractive behaviors. We set the ground for learning signed DPP's through a method of moments, by solving the so called principal assignment problem for a class of matrices
K
K
that satisfy
K
i
,
j
=
±
K
j
,
i
K
,
i
≠
j
i
, in polynomial time."
neurips,https://proceedings.neurips.cc/paper/2018/file/e1314fc026da60d837353d20aefaf054-Paper.pdf,Direct Estimation of Differences in Causal Graphs,"Yuhao Wang, Chandler Squires, Anastasiya Belyaeva, Caroline Uhler",
neurips,https://proceedings.neurips.cc/paper/2018/file/e165421110ba03099a1c0393373c5b43-Paper.pdf,A^2-Nets: Double Attention Networks,"Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, Jiashi Feng",
neurips,https://proceedings.neurips.cc/paper/2018/file/e16e74a63567ecb44ade5c87002bb1d9-Paper.pdf,Sparse Attentive Backtracking: Temporal Credit Assignment Through Reminding,"Nan Rosemary Ke, Anirudh Goyal ALIAS PARTH GOYAL, Olexa Bilaniuk, Jonathan Binas, Michael C. Mozer, Chris Pal, Yoshua Bengio",
neurips,https://proceedings.neurips.cc/paper/2018/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf,Ridge Regression and Provable Deterministic Ridge Leverage Score Sampling,Shannon McCurdy,"Ridge leverage scores provide a balance between low-rank approximation and regularization, and are ubiquitous in randomized linear algebra and machine learning. Deterministic algorithms are also of interest in the moderately big data regime, because deterministic algorithms provide interpretability to the practitioner by having no failure probability and always returning the same results. We provide provable guarantees for deterministic column sampling using ridge leverage scores. The matrix sketch returned by our algorithm is a column subset of the original matrix, yielding additional interpretability. Like the randomized counterparts, the deterministic algorithm provides
(
1
+
ϵ
)
(
error column subset selection,
(
1
+
ϵ
)
(
error projection-cost preservation, and an additive-multiplicative spectral bound. We also show that under the assumption of power-law decay of ridge leverage scores, this deterministic algorithm is provably as accurate as randomized algorithms. Lastly, ridge regression is frequently used to regularize ill-posed linear least-squares problems. While ridge regression provides shrinkage for the regression coefficients, many of the coefficients remain small but non-zero. Performing ridge regression with the matrix sketch returned by our algorithm and a particular regularization parameter forces coefficients to zero and has a provable
(
1
+
ϵ
)
(
bound on the statistical risk. As such, it is an interesting alternative to elastic net regularization."
neurips,https://proceedings.neurips.cc/paper/2018/file/e1dc4bf1f94e87fdfeb2d91ae3dc10ef-Paper.pdf,Dynamic Network Model from Partial Observations,"Elahe Ghalebi, Baharan Mirzasoleiman, Radu Grosu, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2018/file/e22312179bf43e61576081a2f250f845-Paper.pdf,Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate,"Mikhail Belkin, Daniel J. Hsu, Partha Mitra","Many modern machine learning models are trained to achieve zero or near-zero training error in order to obtain near-optimal (but non-zero) test error. This phenomenon of strong generalization performance for
overfitted'' / interpolated classifiers appears to be ubiquitous in high-dimensional data, having been observed in deep networks, kernel machines, boosting and random forests. Their performance is consistently robust even when the data contain large amounts of label noise. Very little theory is available to explain these observations. The vast majority of theoretical analyses of generalization allows for interpolation only when there is little or no label noise. This paper takes a step toward a theoretical foundation for interpolated classifiers by analyzing local interpolating schemes, including geometric simplicial interpolation algorithm and singularly weighted
k
k
-nearest neighbor schemes. Consistency or near-consistency is proved for these schemes in classification and regression problems. Moreover, the nearest neighbor schemes exhibit optimal rates under some standard statistical assumptions. Finally, this paper suggests a way to explain the phenomenon of adversarial examples, which are seemingly ubiquitous in modern machine learning, and also discusses some connections to kernel machines and random forests in the interpolated regime."
neurips,https://proceedings.neurips.cc/paper/2018/file/e22dd5dabde45eda5a1a67772c8e25dd-Paper.pdf,Actor-Critic Policy Optimization in Partially Observable Multiagent Environments,"Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien Perolat, Karl Tuyls, Remi Munos, Michael Bowling",
neurips,https://proceedings.neurips.cc/paper/2018/file/e2ad76f2326fbc6b56a45a56c59fafdb-Paper.pdf,End-to-end Symmetry Preserving Inter-atomic Potential Energy Model for Finite and Extended Systems,"Linfeng Zhang, Jiequn Han, Han Wang, Wissam Saidi, Roberto Car, Weinan E",
neurips,https://proceedings.neurips.cc/paper/2018/file/e2eabaf96372e20a9e3d4b5f83723a61-Paper.pdf,Relational recurrent neural networks,"Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap",
neurips,https://proceedings.neurips.cc/paper/2018/file/e32c51ad39723ee92b285b362c916ca7-Paper.pdf,Improved Expressivity Through Dendritic Neural Networks,"Xundong Wu, Xiangwen Liu, Wei Li, Qing Wu",
neurips,https://proceedings.neurips.cc/paper/2018/file/e347c51419ffb23ca3fd5050202f9c3d-Paper.pdf,DAGs with NO TEARS: Continuous Optimization for Structure Learning,"Xun Zheng, Bryon Aragam, Pradeep K. Ravikumar, Eric P. Xing",
neurips,https://proceedings.neurips.cc/paper/2018/file/e369853df766fa44e1ed0ff613f563bd-Paper.pdf,Kalman Normalization: Normalizing Internal Representations Across Network Layers,"Guangrun Wang, jiefeng peng, Ping Luo, Xinjiang Wang, Liang Lin",
neurips,https://proceedings.neurips.cc/paper/2018/file/e44fea3bec53bcea3b7513ccef5857ac-Paper.pdf,Connectionist Temporal Classification with Maximum Entropy Regularization,"Hu Liu, Sheng Jin, Changshui Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Paper.pdf,Are GANs Created Equal? A Large-Scale Study,"Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, Olivier Bousquet",
neurips,https://proceedings.neurips.cc/paper/2018/file/e4a93f0332b2519177ed55741ea4e5e7-Paper.pdf,Recurrent Transformer Networks for Semantic Correspondence,"Seungryong Kim, Stephen Lin, SANG RYUL JEON, Dongbo Min, Kwanghoon Sohn",
neurips,https://proceedings.neurips.cc/paper/2018/file/e4bb4c5173c2ce17fd8fcd40041c068f-Paper.pdf,Generative Neural Machine Translation,"Harshil Shah, David Barber",
neurips,https://proceedings.neurips.cc/paper/2018/file/e555ebe0ce426f7f9b2bef0706315e0c-Paper.pdf,FRAGE: Frequency-Agnostic Word Representation,"Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2018/file/e57c6b956a6521b28495f2886ca0977a-Paper.pdf,Variational Memory Encoder-Decoder,"Hung Le, Truyen Tran, Thin Nguyen, Svetha Venkatesh",
neurips,https://proceedings.neurips.cc/paper/2018/file/e5841df2166dd424a57127423d276bbe-Paper.pdf,Joint Active Feature Acquisition and Classification with Variable-Size Set Encoding,"Hajin Shim, Sung Ju Hwang, Eunho Yang",
neurips,https://proceedings.neurips.cc/paper/2018/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf,Data-Efficient Hierarchical Reinforcement Learning,"Ofir Nachum, Shixiang (Shane) Gu, Honglak Lee, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2018/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf,Verifiable Reinforcement Learning via Policy Extraction,"Osbert Bastani, Yewen Pu, Armando Solar-Lezama",
neurips,https://proceedings.neurips.cc/paper/2018/file/e721a54a8cf18c8543d44782d9ef681f-Paper.pdf,Stochastic Spectral and Conjugate Descent Methods,"Dmitry Kovalev, Peter Richtarik, Eduard Gorbunov, Elnur Gasanov",
neurips,https://proceedings.neurips.cc/paper/2018/file/e727fa59ddefcefb5d39501167623132-Paper.pdf,A Simple Proximal Stochastic Gradient Method for Nonsmooth Nonconvex Optimization,"Zhize Li, Jian Li",
neurips,https://proceedings.neurips.cc/paper/2018/file/e77dbaf6759253c7c6d0efc5690369c7-Paper.pdf,Hierarchical Graph Representation Learning with Differentiable Pooling,"Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2018/file/e7a425c6ece20cbc9056f98699b53c6f-Paper.pdf,Robust Detection of Adversarial Attacks by Modeling the Intrinsic Properties of Deep Neural Networks,"Zhihao Zheng, Pengyu Hong",
neurips,https://proceedings.neurips.cc/paper/2018/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf,Removing the Feature Correlation Effect of Multiplicative Noise,"Zijun Zhang, Yining Zhang, Zongpeng Li",
neurips,https://proceedings.neurips.cc/paper/2018/file/e7c573c14a09b84f6b7782ce3965f335-Paper.pdf,The Effect of Network Width on the Performance of Large-batch Training,"Lingjiao Chen, Hongyi Wang, Jinman Zhao, Dimitris Papailiopoulos, Paraschos Koutris","In this work, we take a first step towards analyzing how the structure (width and depth) of a neural network affects the performance of large-batch training. We present new theoretical results which suggest that--for a fixed number of parameters--wider networks are more amenable to fast large-batch training compared to deeper ones. We provide extensive experiments on residual and fully-connected neural networks which suggest that wider networks can be trained using larger batches without incurring a convergence slow-down, unlike their deeper variants."
neurips,https://proceedings.neurips.cc/paper/2018/file/e7e69cdf28f8ce6b69b4e1853ee21bab-Paper.pdf,Efficient Loss-Based Decoding on Graphs for Extreme Classification,"Itay Evron, Edward Moroshko, Koby Crammer",
neurips,https://proceedings.neurips.cc/paper/2018/file/e82c4b19b8151ddc25d4d93baf7b908f-Paper.pdf,Scalable methods for 8-bit training of neural networks,"Ron Banner, Itay Hubara, Elad Hoffer, Daniel Soudry",
neurips,https://proceedings.neurips.cc/paper/2018/file/e836d813fd184325132fca8edcdfb40e-Paper.pdf,Solving Large Sequential Games with the Excessive Gap Technique,"Christian Kroer, Gabriele Farina, Tuomas Sandholm",
neurips,https://proceedings.neurips.cc/paper/2018/file/e8fd4a8a5bab2b3785d794ab51fef55c-Paper.pdf,Step Size Matters in Deep Learning,"Kamil Nar, Shankar Sastry",
neurips,https://proceedings.neurips.cc/paper/2018/file/e9257036daf20f062a498aab563d7712-Paper.pdf,A Reduction for Efficient LDA Topic Reconstruction,"Matteo Almanza, Flavio Chierichetti, Alessandro Panconesi, Andrea Vattani","We present a novel approach for LDA (Latent Dirichlet Allocation) topic reconstruction. The main technical idea is to show that the distribution over the documents generated by LDA can be transformed into a distribution for a much simpler generative model in which documents are generated from {\em the same set of topics} but have a much simpler structure: documents are single topic and topics are chosen uniformly at random. Furthermore, this reduction is approximation preserving, in the sense that approximate distributions-- the only ones we can hope to compute in practice-- are mapped into approximate distribution in the simplified world. This opens up the possibility of efficiently reconstructing LDA topics in a roundabout way. Compute an approximate document distribution from the given corpus, transform it into an approximate distribution for the single-topic world, and run a reconstruction algorithm in the uniform, single topic world-- a much simpler task than direct LDA reconstruction. Indeed, we show the viability of the approach by giving very simple algorithms for a generalization of two notable cases that have been studied in the literature,
p
p
-separability and Gibbs sampling for matrix-like topics."
neurips,https://proceedings.neurips.cc/paper/2018/file/e9510081ac30ffa83f10b68cde1cac07-Paper.pdf,Convex Elicitation of Continuous Properties,"Jessica Finocchiaro, Rafael Frongillo",
neurips,https://proceedings.neurips.cc/paper/2018/file/e9fd7c2c6623306db59b6aef5c0d5cac-Paper.pdf,The Nearest Neighbor Information Estimator is Adaptively Near Minimax Rate-Optimal,"Jiantao Jiao, Weihao Gao, Yanjun Han","We analyze the Kozachenko–Leonenko (KL) fixed k-nearest neighbor estimator for the differential entropy. We obtain the first uniform upper bound on its performance for any fixed k over H\""{o}lder balls on a torus without assuming any conditions on how close the density could be from zero. Accompanying a recent minimax lower bound over the H\""{o}lder ball, we show that the KL estimator for any fixed k is achieving the minimax rates up to logarithmic factors without cognizance of the smoothness parameter s of the H\""{o}lder ball for
s
∈
(
0
,
2
]
s
and arbitrary dimension d, rendering it the first estimator that provably satisfies this property."
neurips,https://proceedings.neurips.cc/paper/2018/file/ea159dc9788ffac311592613b7f71fbb-Paper.pdf,Bandit Learning with Positive Externalities,"Virag Shah, Jose Blanchet, Ramesh Johari",
neurips,https://proceedings.neurips.cc/paper/2018/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf,Minimax Statistical Learning with Wasserstein distances,"Jaeho Lee, Maxim Raginsky",
neurips,https://proceedings.neurips.cc/paper/2018/file/eaae5e04a259d09af85c108fe4d7dd0c-Paper.pdf,Dirichlet belief networks for topic structure learning,"He Zhao, Lan Du, Wray Buntine, Mingyuan Zhou",
neurips,https://proceedings.neurips.cc/paper/2018/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf,Efficient Stochastic Gradient Hard Thresholding,"Pan Zhou, Xiaotong Yuan, Jiashi Feng",
neurips,https://proceedings.neurips.cc/paper/2018/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf,Learning long-range spatial dependencies with horizontal gated recurrent units,"Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, Thomas Serre",
neurips,https://proceedings.neurips.cc/paper/2018/file/ed519dacc89b2bead3f453b0b05a4a8b-Paper.pdf,Tight Bounds for Collaborative PAC Learning via Multiplicative Weights,"Jiecao Chen, Qin Zhang, Yuan Zhou","We study the collaborative PAC learning problem recently proposed in Blum et al.~\cite{BHPQ17}, in which we have
k
k
players and they want to learn a target function collaboratively, such that the learned function approximates the target function well on all players' distributions simultaneously. The quality of the collaborative learning algorithm is measured by the ratio between the sample complexity of the algorithm and that of the learning algorithm for a single distribution (called the overhead). We obtain a collaborative learning algorithm with overhead
O
(
ln
k
)
O
, improving the one with overhead
O
(
ln
2
k
)
O
in \cite{BHPQ17}. We also show that an
Ω
(
ln
k
)
Ω
overhead is inevitable when
k
k
is polynomial bounded by the VC dimension of the hypothesis class. Finally, our experimental study has demonstrated the superiority of our algorithm compared with the one in Blum et al.~\cite{BHPQ17} on real-world datasets."
neurips,https://proceedings.neurips.cc/paper/2018/file/eda80a3d5b344bc40f3bc04f65b7a357-Paper.pdf,(Probably) Concave Graph Matching,"Haggai Maron, Yaron Lipman",
neurips,https://proceedings.neurips.cc/paper/2018/file/edc27f139c3b4e4bb29d1cdbc45663f9-Paper.pdf,HOUDINI: Lifelong Learning as Program Synthesis,"Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, Swarat Chaudhuri",
neurips,https://proceedings.neurips.cc/paper/2018/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf,Video Prediction via Selective Sampling,"Jingwei Xu, Bingbing Ni, Xiaokang Yang",
neurips,https://proceedings.neurips.cc/paper/2018/file/ee14c41e92ec5c97b54cf9b74e25bd99-Paper.pdf,Manifold-tiling Localized Receptive Fields are Optimal in Similarity-preserving Neural Networks,"Anirvan Sengupta, Cengiz Pehlevan, Mariano Tepper, Alexander Genkin, Dmitri Chklovskii",
neurips,https://proceedings.neurips.cc/paper/2018/file/eecca5b6365d9607ee5a9d336962c534-Paper.pdf,Snap ML: A Hierarchical Framework for Machine Learning,"Celestine Dünner, Thomas Parnell, Dimitrios Sarigiannis, Nikolas Ioannou, Andreea Anghel, Gummadi Ravi, Madhusudanan Kandasamy, Haralampos Pozidis",
neurips,https://proceedings.neurips.cc/paper/2018/file/ef50c335cca9f340bde656363ebd02fd-Paper.pdf,Embedding Logical Queries on Knowledge Graphs,"Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2018/file/efb76cff97aaf057654ef2f38cd77d73-Paper.pdf,Parsimonious Bayesian deep networks,Mingyuan Zhou,
neurips,https://proceedings.neurips.cc/paper/2018/file/f02208a057804ee16ac72ff4d3cec53b-Paper.pdf,Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion,"Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, Honglak Lee",
neurips,https://proceedings.neurips.cc/paper/2018/file/f0adc8838f4bdedde4ec2cfad0515589-Paper.pdf,Learning Versatile Filters for Efficient Convolutional Neural Networks,"Yunhe Wang, Chang Xu, Chunjing XU, Chao Xu, Dacheng Tao",
neurips,https://proceedings.neurips.cc/paper/2018/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Paper.pdf,Neural Nearest Neighbors Networks,"Tobias Plötz, Stefan Roth",
neurips,https://proceedings.neurips.cc/paper/2018/file/f18a6d1cde4b205199de8729a6637b42-Paper.pdf,Learning latent variable structured prediction models with Gaussian perturbations,"Kevin Bello, Jean Honorio",
neurips,https://proceedings.neurips.cc/paper/2018/file/f19ec2b84181033bf4753a5a51d5d608-Paper.pdf,Differentially Private Change-Point Detection,"Rachel Cummings, Sara Krehbiel, Yajun Mei, Rui Tuo, Wanrong Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/f1ababf130ee6a25f12da7478af8f1ac-Paper.pdf,Proximal Graphical Event Models,"Debarun Bhattacharjya, Dharmashankar Subramanian, Tian Gao",
neurips,https://proceedings.neurips.cc/paper/2018/file/f21e255f89e0f258accbe4e984eef486-Paper.pdf,Low-rank Interaction with Sparse Additive Effects Model for Large Data Frames,"Geneviève Robin, Hoi-To Wai, Julie Josse, Olga Klopp, Eric Moulines",
neurips,https://proceedings.neurips.cc/paper/2018/file/f2925f97bc13ad2852a7a551802feea0-Paper.pdf,Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels,"Zhilu Zhang, Mert Sabuncu",
neurips,https://proceedings.neurips.cc/paper/2018/file/f2e43fa3400d826df4195a9ac70dca62-Paper.pdf,Inference Aided Reinforcement Learning for Incentive Mechanism Design in Crowdsourcing,"Zehong Hu, Yitao Liang, Jie Zhang, Zhao Li, Yang Liu",
neurips,https://proceedings.neurips.cc/paper/2018/file/f2f446980d8e971ef3da97af089481c3-Paper.pdf,Fast and Effective Robustness Certification,"Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, Martin Vechev","We present a new method and system, called DeepZ, for certifying neural network robustness based on abstract interpretation. Compared to state-of-the-art automated verifiers for neural networks, DeepZ: (i) handles ReLU, Tanh and Sigmoid activation functions, (ii) supports feedforward and convolutional architectures, (iii) is significantly more scalable and precise, and (iv) and is sound with respect to floating point arithmetic. These benefits are due to carefully designed approximations tailored to the setting of neural networks. As an example, DeepZ achieves a verification accuracy of 97% on a large network with 88,500 hidden units under
L
∞
L
attack with
ϵ
=
0.1
ϵ
with an average runtime of 133 seconds."
neurips,https://proceedings.neurips.cc/paper/2018/file/f31b20466ae89669f9741e047487eb37-Paper.pdf,Online Structured Laplace Approximations for Overcoming Catastrophic Forgetting,"Hippolyt Ritter, Aleksandar Botev, David Barber",
neurips,https://proceedings.neurips.cc/paper/2018/file/f337d999d9ad116a7b4f3d409fcc6480-Paper.pdf,Optimization over Continuous and Multi-dimensional Decisions with Observational Data,"Dimitris Bertsimas, Christopher McCord",
neurips,https://proceedings.neurips.cc/paper/2018/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf,Neural Architecture Search with Bayesian Optimisation and Optimal Transport,"Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, Eric P. Xing",
neurips,https://proceedings.neurips.cc/paper/2018/file/f3e52c300b822a8123e7ace55fe15c08-Paper.pdf,An Information-Theoretic Analysis for Thompson Sampling with Many Actions,"Shi Dong, Benjamin Van Roy",
neurips,https://proceedings.neurips.cc/paper/2018/file/f410588e48dc83f2822a880a68f78923-Paper.pdf,"BML: A High-performance, Low-cost Gradient Synchronization Algorithm for DML Training","Songtao Wang, Dan Li, Yang Cheng, Jinkun Geng, Yanshu Wang, Shuai Wang, Shu-Tao Xia, Jianping Wu",
neurips,https://proceedings.neurips.cc/paper/2018/file/f4334c131c781e2a6f0a5e34814c8147-Paper.pdf,Proximal SCOPE for Distributed Sparse Learning,"Shenyi Zhao, Gong-Duo Zhang, Ming-Wei Li, Wu-Jun Li",
neurips,https://proceedings.neurips.cc/paper/2018/file/f442d33fa06832082290ad8544a8da27-Paper.pdf,BinGAN: Learning Compact Binary Descriptors with a Regularized GAN,"Maciej Zieba, Piotr Semberecki, Tarek El-Gaaly, Tomasz Trzcinski",
neurips,https://proceedings.neurips.cc/paper/2018/file/f471223d1a1614b58a7dc45c9d01df19-Paper.pdf,Incorporating Context into Language Encoding Models for fMRI,"Shailee Jain, Alexander Huth",
neurips,https://proceedings.neurips.cc/paper/2018/file/f4a4da9aa7eadfd23c7bdb7cf57b3112-Paper.pdf,Communication Efficient Parallel Algorithms for Optimization on Manifolds,"Bayan Saparbayeva, Michael Zhang, Lizhen Lin",
neurips,https://proceedings.neurips.cc/paper/2018/file/f4e369c0a468d3aeeda0593ba90b5e55-Paper.pdf,Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing,"Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc V. Le, Ni Lao",
neurips,https://proceedings.neurips.cc/paper/2018/file/f516dfb84b9051ed85b89cdc3a8ab7f5-Paper.pdf,Context-dependent upper-confidence bounds for directed exploration,"Raksha Kumaraswamy, Matthew Schlegel, Adam White, Martha White",
neurips,https://proceedings.neurips.cc/paper/2018/file/f52854cc99ae1c1966b0a21d0127975b-Paper.pdf,Adaptive Negative Curvature Descent with Applications in Non-convex Optimization,"Mingrui Liu, Zhe Li, Xiaoyu Wang, Jinfeng Yi, Tianbao Yang","Negative curvature descent (NCD) method has been utilized to design deterministic or stochastic algorithms for non-convex optimization aiming at finding second-order stationary points or local minima. In existing studies, NCD needs to approximate the smallest eigen-value of the Hessian matrix with a sufficient precision (e.g.,
ϵ
2
≪
1
ϵ
) in order to achieve a sufficiently accurate second-order stationary solution (i.e.,
λ
min
(
∇
2
f
(
\x
)
)
≥
−
ϵ
2
)
λ
. One issue with this approach is that the target precision
ϵ
2
ϵ
is usually set to be very small in order to find a high quality solution, which increases the complexity for computing a negative curvature. To address this issue, we propose an adaptive NCD to allow for an adaptive error dependent on the current gradient's magnitude in approximating the smallest eigen-value of the Hessian, and to encourage competition between a noisy NCD step and gradient descent step. We consider the applications of the proposed adaptive NCD for both deterministic and stochastic non-convex optimization, and demonstrate that it can help reduce the the overall complexity in computing the negative curvatures during the course of optimization without sacrificing the iteration complexity."
neurips,https://proceedings.neurips.cc/paper/2018/file/f5496252609c43eb8a3d147ab9b9c006-Paper.pdf,LF-Net: Learning Local Features from Images,"Yuki Ono, Eduard Trulls, Pascal Fua, Kwang Moo Yi",
neurips,https://proceedings.neurips.cc/paper/2018/file/f55cadb97eaff2ba1980e001b0bd9842-Paper.pdf,Why so gloomy? A Bayesian explanation of human pessimism bias in the multi-armed bandit task,"Dalin Guo, Angela J. Yu",
neurips,https://proceedings.neurips.cc/paper/2018/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf,CapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule Subspaces,"Liheng Zhang, Marzieh Edraki, Guo-Jun Qi","In this paper, we formalize the idea behind capsule nets of using a capsule vector rather than a neuron activation to predict the label of samples. To this end, we propose to learn a group of capsule subspaces onto which an input feature vector is projected. Then the lengths of resultant capsules are used to score the probability of belonging to different classes. We train such a Capsule Projection Network (CapProNet) by learning an orthogonal projection matrix for each capsule subspace, and show that each capsule subspace is updated until it contains input feature vectors corresponding to the associated class. With low dimensionality of capsule subspace as well as an iterative method to estimate the matrix inverse, only a small negligible computing overhead is incurred to train the network. Experiment results on image datasets show the presented network can greatly improve the performance of state-of-the-art Resnet backbones by
10
−
20
%
10
with almost the same computing cost."
neurips,https://proceedings.neurips.cc/paper/2018/file/f5e536083a438cec5b64a4954abc17f1-Paper.pdf,Robust Subspace Approximation in a Stream,"Roie Levin, Anish Prasad Sevekari, David Woodruff",
neurips,https://proceedings.neurips.cc/paper/2018/file/f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf,PointCNN: Convolution On X-Transformed Points,"Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, Baoquan Chen",
neurips,https://proceedings.neurips.cc/paper/2018/file/f610a13de080fb8df6cf972fc01ad93f-Paper.pdf,Learning convex bounds for linear quadratic control policy synthesis,"Jack Umenberger, Thomas B. Schön",
neurips,https://proceedings.neurips.cc/paper/2018/file/f6185f0ef02dcaec414a3171cd01c697-Paper.pdf,Manifold Structured Prediction,"Alessandro Rudi, Carlo Ciliberto, GianMaria Marconi, Lorenzo Rosasco",
neurips,https://proceedings.neurips.cc/paper/2018/file/f6d9e459b9fbf6dd26c4f7d621adec1d-Paper.pdf,Efficient Gradient Computation for Structured Output Learning with Rational and Tropical Losses,"Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, Dmitry Storcheus, Scott Yang","Many structured prediction problems admit a natural loss function for evaluation such as the edit-distance or
n
n
-gram loss. However, existing learning algorithms are typically designed to optimize alternative objectives such as the cross-entropy. This is because a na\""{i}ve implementation of the natural loss functions often results in intractable gradient computations. In this paper, we design efficient gradient computation algorithms for two broad families of structured prediction loss functions: rational and tropical losses. These families include as special cases the
n
n
-gram loss, the edit-distance loss, and many other loss functions commonly used in natural language processing and computational biology tasks that are based on sequence similarity measures. Our algorithms make use of weighted automata and graph operations over appropriate semirings to design efficient solutions. They facilitate efficient gradient computation and hence enable one to train learning models such as neural networks with complex structured losses."
neurips,https://proceedings.neurips.cc/paper/2018/file/f6e794a75c5d51de081dbefa224304f9-Paper.pdf,Mean-field theory of graph neural networks in graph partitioning,"Tatsuro Kawamoto, Masashi Tsubaki, Tomoyuki Obuchi",
neurips,https://proceedings.neurips.cc/paper/2018/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf,Adversarially Robust Generalization Requires More Data,"Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, Aleksander Madry",
neurips,https://proceedings.neurips.cc/paper/2018/file/f7696a9b362ac5a51c3dc8f098b73923-Paper.pdf,Assessing Generative Models via Precision and Recall,"Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, Sylvain Gelly",
neurips,https://proceedings.neurips.cc/paper/2018/file/f77ecc17109b1b806350eb7e7bbfd861-Paper.pdf,Improved Network Robustness with Adversary Critic,"Alexander Matyasko, Lap-Pui Chau",
neurips,https://proceedings.neurips.cc/paper/2018/file/f7bdb0e100275600f9e183e25d81822d-Paper.pdf,Fast deep reinforcement learning using online adjustments from the past,"Steven Hansen, Alexander Pritzel, Pablo Sprechmann, Andre Barreto, Charles Blundell",
neurips,https://proceedings.neurips.cc/paper/2018/file/f7f580e11d00a75814d2ded41fe8e8fe-Paper.pdf,A Practical Algorithm for Distributed Clustering and Outlier Detection,"Jiecao Chen, Erfan Sadeqi Azer, Qin Zhang",
neurips,https://proceedings.neurips.cc/paper/2018/file/f8da71e562ff44a2bc7edf3578c593da-Paper.pdf,How Much Restricted Isometry is Needed In Nonconvex Matrix Recovery?,"Richard Zhang, Cedric Josz, Somayeh Sojoudi, Javad Lavaei","When the linear measurements of an instance of low-rank matrix recovery satisfy a restricted isometry property (RIP) --- i.e. they are approximately norm-preserving --- the problem is known to contain no spurious local minima, so exact recovery is guaranteed. In this paper, we show that moderate RIP is not enough to eliminate spurious local minima, so existing results can only hold for near-perfect RIP. In fact, counterexamples are ubiquitous: every
x
x
is the spurious local minimum of a rank-1 instance of matrix recovery that satisfies RIP. One specific counterexample has RIP constant
δ
=
1
/
2
δ
, but causes randomly initialized stochastic gradient descent (SGD) to fail 12\% of the time. SGD is frequently able to avoid and escape spurious local minima, but this empirical result shows that it can occasionally be defeated by their existence. Hence, while exact recovery guarantees will likely require a proof of no spurious local minima, arguments based solely on norm preservation will only be applicable to a narrow set of nearly-isotropic instances."
neurips,https://proceedings.neurips.cc/paper/2018/file/f8e59f4b2fe7c5705bf878bbd494ccdf-Paper.pdf,Limited Memory Kelley's Method Converges for Composite Convex and Submodular Objectives,"Song Zhou, Swati Gupta, Madeleine Udell",
neurips,https://proceedings.neurips.cc/paper/2018/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf,Towards Understanding Acceleration Tradeoff between Momentum and Asynchrony in Nonconvex Stochastic Optimization,"Tianyi Liu, Shiyang Li, Jianping Shi, Enlu Zhou, Tuo Zhao",
neurips,https://proceedings.neurips.cc/paper/2018/file/fa1e9c965314ccd7810fb5ea838303e5-Paper.pdf,Metric on Nonlinear Dynamical Systems with Perron-Frobenius Operators,"Isao Ishikawa, Keisuke Fujii, Masahiro Ikeda, Yuka Hashimoto, Yoshinobu Kawahara",
neurips,https://proceedings.neurips.cc/paper/2018/file/fa2431bf9d65058fe34e9713e32d60e6-Paper.pdf,A Mathematical Model For Optimal Decisions In A Representative Democracy,"Malik Magdon-Ismail, Lirong Xia",
neurips,https://proceedings.neurips.cc/paper/2018/file/fb3f76858cb38e5b7fd113e0bc1c0721-Paper.pdf,Loss Functions for Multiset Prediction,"Sean Welleck, Zixin Yao, Yu Gai, Jialin Mao, Zheng Zhang, Kyunghyun Cho",
neurips,https://proceedings.neurips.cc/paper/2018/file/fc2c7c47b918d0c2d792a719dfb602ef-Paper.pdf,SEGA: Variance Reduction via Gradient Sketching,"Filip Hanzely, Konstantin Mishchenko, Peter Richtarik",
neurips,https://proceedings.neurips.cc/paper/2018/file/fc325d4b598aaede18b53dca4ecfcb9c-Paper.pdf,Sharp Bounds for Generalized Uniformity Testing,"Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart",
neurips,https://proceedings.neurips.cc/paper/2018/file/fc49306d97602c8ed1be1dfbf0835ead-Paper.pdf,Non-Local Recurrent Network for Image Restoration,"Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, Thomas S. Huang",
neurips,https://proceedings.neurips.cc/paper/2018/file/fc79250f8c5b804390e8da280b4cf06e-Paper.pdf,Practical exact algorithm for trembling-hand equilibrium refinements in games,"Gabriele Farina, Nicola Gatti, Tuomas Sandholm",
neurips,https://proceedings.neurips.cc/paper/2018/file/fcf1d8d2f36c0cde8eca4b86a8fe1df8-Paper.pdf,Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for Bayesian learning,"Rui Luo, Jianhong Wang, Yaodong Yang, Jun WANG, Zhanxing Zhu",
neurips,https://proceedings.neurips.cc/paper/2018/file/fd9dd764a6f1d73f4340d570804eacc4-Paper.pdf,Flexible neural representation for physics prediction,"Damian Mrowca, Chengxu Zhuang, Elias Wang, Nick Haber, Li F. Fei-Fei, Josh Tenenbaum, Daniel L. Yamins",
neurips,https://proceedings.neurips.cc/paper/2018/file/fdaa09fc5ed18d3226b3a1a00f1bc48c-Paper.pdf,A Stein variational Newton method,"Gianluca Detommaso, Tiangang Cui, Youssef Marzouk, Alessio Spantini, Robert Scheichl",
neurips,https://proceedings.neurips.cc/paper/2018/file/fdf1bc5669e8ff5ba45d02fded729feb-Paper.pdf,Dual Swap Disentangling,"Zunlei Feng, Xinchao Wang, Chenglong Ke, An-Xiang Zeng, Dacheng Tao, Mingli Song",
neurips,https://proceedings.neurips.cc/paper/2018/file/fe131d7f5a6b38b23cc967316c13dae2-Paper.pdf,Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced,"Simon S. Du, Wei Hu, Jason D. Lee","We study the implicit regularization imposed by gradient descent for learning multi-layer homogeneous functions including feed-forward fully connected and convolutional deep neural networks with linear, ReLU or Leaky ReLU activation. We rigorously prove that gradient flow (i.e. gradient descent with infinitesimal step size) effectively enforces the differences between squared norms across different layers to remain invariant without any explicit regularization. This result implies that if the weights are initially small, gradient flow automatically balances the magnitudes of all layers. Using a discretization argument, we analyze gradient descent with positive step size for the non-convex low-rank asymmetric matrix factorization problem without any regularization. Inspired by our findings for gradient flow, we prove that gradient descent with step sizes
η
t
=
O
(
t
−
(
1
/
2
+
δ
)
)
(
0
<
δ
≤
1
/
2
)
η
automatically balances two low-rank factors and converges to a bounded global optimum. Furthermore, for rank-1 asymmetric matrix factorization we give a finer analysis showing gradient descent with constant step size converges to the global minimum at a globally linear rate. We believe that the idea of examining the invariance imposed by first order algorithms in learning homogeneous models could serve as a fundamental building block for studying optimization for learning deep models."
neurips,https://proceedings.neurips.cc/paper/2018/file/fea9c11c4ad9a395a636ed944a28b51a-Paper.pdf,Third-order Smoothness Helps: Faster Stochastic Optimization Algorithms for Finding Local Minima,"Yaodong Yu, Pan Xu, Quanquan Gu","We propose stochastic optimization algorithms that can find local minima faster than existing algorithms for nonconvex optimization problems, by exploiting the third-order smoothness to escape non-degenerate saddle points more efficiently. More specifically, the proposed algorithm only needs
~
O
(
ϵ
−
10
/
3
)
O
stochastic gradient evaluations to converge to an approximate local minimum
x
x
, which satisfies
∥
∇
f
(
x
)
∥
2
≤
ϵ
‖
and
λ
min
(
∇
2
f
(
x
)
)
≥
−
√
ϵ
λ
in unconstrained stochastic optimization, where
~
O
(
⋅
)
O
hides logarithm polynomial terms and constants. This improves upon the
~
O
(
ϵ
−
7
/
2
)
O
gradient complexity achieved by the state-of-the-art stochastic local minima finding algorithms by a factor of
~
O
(
ϵ
−
1
/
6
)
O
. Experiments on two nonconvex optimization problems demonstrate the effectiveness of our algorithm and corroborate our theory."
neurips,https://proceedings.neurips.cc/paper/2018/file/febefe1cc5c87748ea02036dbe9e3d67-Paper.pdf,Robot Learning in Homes: Improving Generalization and Reducing Dataset Bias,"Abhinav Gupta, Adithyavairavan Murali, Dhiraj Prakashchand Gandhi, Lerrel Pinto",
neurips,https://proceedings.neurips.cc/paper/2018/file/feecee9f1643651799ede2740927317a-Paper.pdf,LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning,"Tianyi Chen, Georgios Giannakis, Tao Sun, Wotao Yin",
neurips,https://proceedings.neurips.cc/paper/2018/file/ff1418e8cc993fe8abcfe3ce2003e5c5-Paper.pdf,Equality of Opportunity in Classification: A Causal Approach,"Junzhe Zhang, Elias Bareinboim",
neurips,https://proceedings.neurips.cc/paper/2018/file/fface8385abbf94b4593a0ed53a0c70f-Paper.pdf,Modern Neural Networks Generalize on Small Data Sets,"Matthew Olson, Abraham Wyner, Richard Berk",
