conference,url,title,authors,abstract
neurips,https://proceedings.neurips.cc/paper/2020/file/0004d0b59e19461ff126e3a08a814c33-Paper.pdf,A graph similarity for deep learning,Seongmin Ok,
neurips,https://proceedings.neurips.cc/paper/2020/file/00482b9bed15a272730fcb590ffebddd-Paper.pdf,An Unsupervised Information-Theoretic Perceptual Quality Metric,"Sangnie Bhardwaj, Ian Fischer, Johannes Ballé, Troy Chinen",
neurips,https://proceedings.neurips.cc/paper/2020/file/0060ef47b12160b9198302ebdb144dcf-Paper.pdf,Self-Supervised MultiModal Versatile Networks,"Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelović, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, Andrew Zisserman",
neurips,https://proceedings.neurips.cc/paper/2020/file/007ff380ee5ac49ffc34442f5c2a2b86-Paper.pdf,"Benchmarking Deep Inverse Models over time, and the Neural-Adjoint method","Simiao Ren, Willie Padilla, Jordan Malof",
neurips,https://proceedings.neurips.cc/paper/2020/file/0084ae4bc24c0795d1e6a4f58444d39b-Paper.pdf,Off-Policy Evaluation and Learning for External Validity under a Covariate Shift,"Masatoshi Uehara, Masahiro Kato, Shota Yasui",
neurips,https://proceedings.neurips.cc/paper/2020/file/00a03ec6533ca7f5c644d198d815329c-Paper.pdf,Neural Methods for Point-wise Dependency Estimation,"Yao-Hung Hubert Tsai, Han Zhao, Makoto Yamada, Louis-Philippe Morency, Russ R. Salakhutdinov",
neurips,https://proceedings.neurips.cc/paper/2020/file/00ac8ed3b4327bdd4ebbebcb2ba10a00-Paper.pdf,Fast and Flexible Temporal Point Processes with Triangular Maps,"Oleksandr Shchur, Nicholas Gao, Marin Biloš, Stephan Günnemann",
neurips,https://proceedings.neurips.cc/paper/2020/file/00e26af6ac3b1c1c49d7c3d79c60d000-Paper.pdf,Backpropagating Linearly Improves Transferability of Adversarial Examples,"Yiwen Guo, Qizhang Li, Hao Chen",
neurips,https://proceedings.neurips.cc/paper/2020/file/012a91467f210472fab4e11359bbfef6-Paper.pdf,PyGlove: Symbolic Programming for Automated Machine Learning,"Daiyi Peng, Xuanyi Dong, Esteban Real, Mingxing Tan, Yifeng Lu, Gabriel Bender, Hanxiao Liu, Adam Kraft, Chen Liang, Quoc Le","In this paper, we introduce a new way of programming AutoML based on symbolic programming. Under this paradigm, ML programs are mutable, thus can be manipulated easily by another program. As a result, AutoML can be reformulated as an automated process of symbolic manipulation. With this formulation, we decouple the triangle of the search algorithm, the search space and the child program. This decoupling makes it easy to change the search space and search algorithm (without and with weight sharing), as well as to add search capabilities to existing code and implement complex search flows. We then introduce PyGlove, a new Python library that implements this paradigm. Through case studies on ImageNet and NAS-Bench-101, we show that with PyGlove users can easily convert a static program into a search space, quickly iterate on the search spaces and search algorithms, and craft complex search flows to achieve better results."
neurips,https://proceedings.neurips.cc/paper/2020/file/012d9fe15b2493f21902cd55603382ec-Paper.pdf,Fourier Sparse Leverage Scores and Approximate Kernel Learning,"Tamas Erdelyi, Cameron Musco, Christopher Musco","We prove new explicit upper bounds on the leverage scores of Fourier sparse functions under both the Gaussian and Laplace measures. In particular, we study s-sparse functions of the form
f
(
x
)
=
∑
s
j
=
1
a
j
e
i
λ
j
x
f
for coefficients
a
j
∈
C
a
and frequencies
λ
j
∈
R
λ
. Bounding Fourier sparse leverage scores under various measures is of pure mathematical interest in approximation theory, and our work extends existing results for the uniform measure [Erd17,CP19a]. Practically, our bounds are motivated by two important applications in machine learning: 1. Kernel Approximation. They yield a new random Fourier features algorithm for approximating Gaussian and Cauchy (rational quadratic) kernel matrices. For low-dimensional data, our method uses a near optimal number of features, and its runtime is polynomial in the *statistical dimension* of the approximated kernel matrix. It is the first
oblivious sketching method'' with this property for any kernel besides the polynomial kernel, resolving an open question of [AKM+17,AKK+20b]. 2. Active Learning. They can be used as non-uniform sampling distributions for robust active learning when data follows a Gaussian or Laplace distribution. Using the framework of [AKM+19], we provide essentially optimal results for bandlimited and multiband interpolation, and Gaussian process regression. These results generalize existing work that only applies to uniformly distributed data."
neurips,https://proceedings.neurips.cc/paper/2020/file/0163cceb20f5ca7b313419c068abd9dc-Paper.pdf,Improved Algorithms for Online Submodular Maximization via First-order Regret Bounds,"Nicholas Harvey, Christopher Liaw, Tasuku Soma",
neurips,https://proceedings.neurips.cc/paper/2020/file/0169cf885f882efd795951253db5cdfb-Paper.pdf,Synbols: Probing Learning Algorithms with Synthetic Datasets,"Alexandre Lacoste, Pau Rodríguez López, Frederic Branchaud-Charron, Parmida Atighehchian, Massimo Caccia, Issam Hadj Laradji, Alexandre Drouin, Matthew Craddock, Laurent Charlin, David Vázquez",
neurips,https://proceedings.neurips.cc/paper/2020/file/0172d289da48c48de8c5ebf3de9f7ee1-Paper.pdf,Adversarially Robust Streaming Algorithms via Differential Privacy,"Avinatan Hasidim, Haim Kaplan, Yishay Mansour, Yossi Matias, Uri Stemmer",
neurips,https://proceedings.neurips.cc/paper/2020/file/019fa4fdf1c04cf73ba25aa2223769cd-Paper.pdf,Trading Personalization for Accuracy: Data Debugging in Collaborative Filtering,"Long Chen, Yuan Yao, Feng Xu, Miao Xu, Hanghang Tong",
neurips,https://proceedings.neurips.cc/paper/2020/file/01a0683665f38d8e5e567b3b15ca98bf-Paper.pdf,Cascaded Text Generation with Markov Transformers,"Yuntian Deng, Alexander Rush",
neurips,https://proceedings.neurips.cc/paper/2020/file/01c9d2c5b3ff5cbba349ec39a570b5e3-Paper.pdf,Improving Local Identifiability in Probabilistic Box Embeddings,"Shib Dasgupta, Michael Boratko, Dongxu Zhang, Luke Vilnis, Xiang Li, Andrew McCallum",
neurips,https://proceedings.neurips.cc/paper/2020/file/01e00f2f4bfcbb7505cb641066f2859b-Paper.pdf,Permute-and-Flip: A new mechanism for differentially private selection,"Ryan McKenna, Daniel R. Sheldon",
neurips,https://proceedings.neurips.cc/paper/2020/file/021bbc7ee20b71134d53e20206bd6feb-Paper.pdf,Deep reconstruction of strange attractors from time series,William Gilpin,
neurips,https://proceedings.neurips.cc/paper/2020/file/021f6dd88a11ca489936ae770e4634ad-Paper.pdf,Reciprocal Adversarial Learning via Characteristic Functions,"Shengxi Li, Zeyang Yu, Min Xiang, Danilo Mandic",
neurips,https://proceedings.neurips.cc/paper/2020/file/022e0ee5162c13d9a7bb3bd00fb032ce-Paper.pdf,Statistical Guarantees of Distributed Nearest Neighbor Classification,"Jiexin Duan, Xingye Qiao, Guang Cheng",
neurips,https://proceedings.neurips.cc/paper/2020/file/023d0a5671efd29e80b4deef8262e297-Paper.pdf,Stein Self-Repulsive Dynamics: Benefits From Past Samples,"Mao Ye, Tongzheng Ren, Qiang Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/024d2d699e6c1a82c9ba986386f4d824-Paper.pdf,The Statistical Complexity of Early-Stopped Mirror Descent,"Tomas Vaskevicius, Varun Kanade, Patrick Rebeschini",
neurips,https://proceedings.neurips.cc/paper/2020/file/02a3c7fb3f489288ae6942498498db20-Paper.pdf,Algorithmic recourse under imperfect causal knowledge: a probabilistic approach,"Amir-Hossein Karimi, Julius von Kügelgen, Bernhard Schölkopf, Isabel Valera",
neurips,https://proceedings.neurips.cc/paper/2020/file/02e74f10e0327ad868d138f2b4fdd6f0-Paper.pdf,Quantitative Propagation of Chaos for SGD in Wide Neural Networks,"Valentin De Bortoli, Alain Durmus, Xavier Fontaine, Umut Simsekli","In this paper, we investigate the limiting behavior of a continuous-time counterpart of the Stochastic Gradient Descent (SGD) algorithm applied to two-layer overparameterized neural networks, as the number or neurons (i.e., the size of the hidden layer)
N
→
\plusinfty
N
. Following a probabilistic approach, we show `propagation of chaos' for the particle system defined by this continuous-time dynamics under different scenarios, indicating that the statistical interaction between the particles asymptotically vanishes. In particular, we establish quantitative convergence with respect to
N
N
of any particle to a solution of a mean-field McKean-Vlasov equation in the metric space endowed with the Wasserstein distance. In comparison to previous works on the subject, we consider settings in which the sequence of stepsizes in SGD can potentially depend on the number of neurons and the iterations. We then identify two regimes under which different mean-field limits are obtained, one of them corresponding to an implicitly regularized version of the minimization problem at hand. We perform various experiments on real datasets to validate our theoretical results, assessing the existence of these two regimes on classification problems and illustrating our convergence results."
neurips,https://proceedings.neurips.cc/paper/2020/file/02ed812220b0705fabb868ddbf17ea20-Paper.pdf,A Causal View on Robustness of Neural Networks,"Cheng Zhang, Kun Zhang, Yingzhen Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/02f657d55eaf1c4840ce8d66fcdaf90c-Paper.pdf,Minimax Classification with 0-1 Loss and Performance Guarantees,"Santiago Mazuelas, Andrea Zanoni, Aritz Pérez",
neurips,https://proceedings.neurips.cc/paper/2020/file/03255088ed63354a54e0e5ed957e9008-Paper.pdf,How to Learn a Useful Critic? Model-based Action-Gradient-Estimator Policy Optimization,"Pierluca D'Oro, Wojciech Jaśkowski",
neurips,https://proceedings.neurips.cc/paper/2020/file/03287fcce194dbd958c2ec5b33705912-Paper.pdf,Coresets for Regressions with Panel Data,"Lingxiao Huang, K Sudhir, Nisheeth Vishnoi","A panel dataset contains features or observations for multiple individuals over multiple time periods and regression problems with panel data are common in statistics and applied ML. When dealing with massive datasets, coresets have emerged as a valuable tool from a computational, storage and privacy perspective, as one needs to work with and share much smaller datasets. However, results on coresets for regression problems thus far have only been available for cross-sectional data (
N
N
individuals each observed for a single time unit) or longitudinal data (a single individual observed for
T
>
1
T
time units), but there are no results for panel data (
N
>
1
N
,
T
>
1
T
). This paper introduces the problem of coresets to panel data settings; we first define coresets for several variants of regression problems with panel data and then present efficient algorithms to construct coresets of size that are independent of
N
N
and
T
T
, and only polynomially depend on
1
/
ε
1
(where
ε
ε
is the error parameter) and the number of regression parameters. Our approach is based on the Feldman-Langberg framework in which a key step is to upper bound the “total sensitivity” that is roughly the sum of maximum influences of all individual-time pairs taken over all possible choices of regression parameters. Empirically, we assess our approach with a synthetic and a real-world datasets; the coreset sizes constructed using our approach are much smaller than the full dataset and coresets indeed accelerate the running time of computing the regression objective."
neurips,https://proceedings.neurips.cc/paper/2020/file/0332d694daab22e0e0eaf7a5e88433f9-Paper.pdf,Learning Composable Energy Surrogates for PDE Order Reduction,"Alex Beatson, Jordan Ash, Geoffrey Roeder, Tianju Xue, Ryan P. Adams",
neurips,https://proceedings.neurips.cc/paper/2020/file/033cc385728c51d97360020ed57776f0-Paper.pdf,Efficient Contextual Bandits with Continuous Actions,"Maryam Majzoubi, Chicheng Zhang, Rajan Chari, Akshay Krishnamurthy, John Langford, Aleksandrs Slivkins",
neurips,https://proceedings.neurips.cc/paper/2020/file/03593ce517feac573fdaafa6dcedef61-Paper.pdf,Achieving Equalized Odds by Resampling Sensitive Attributes,"Yaniv Romano, Stephen Bates, Emmanuel Candes",
neurips,https://proceedings.neurips.cc/paper/2020/file/03793ef7d06ffd63d34ade9d091f1ced-Paper.pdf,Multi-Robot Collision Avoidance under Uncertainty with Probabilistic Safety Barrier Certificates,"Wenhao Luo, Wen Sun, Ashish Kapoor",
neurips,https://proceedings.neurips.cc/paper/2020/file/03fa2f7502f5f6b9169e67d17cbf51bb-Paper.pdf,Hard Shape-Constrained Kernel Machines,"Pierre-Cyril Aubin-Frankowski, Zoltan Szabo",
neurips,https://proceedings.neurips.cc/paper/2020/file/0415740eaa4d9decbc8da001d3fd805f-Paper.pdf,A Closer Look at the Training Strategy for Modern Meta-Learning,"JIAXIN CHEN, Xiao-Ming Wu, Yanke Li, Qimai LI, Li-Ming Zhan, Fu-lai Chung","The support/query (S/Q) episodic training strategy has been widely used in modern meta-learning algorithms and is believed to improve their generalization ability to test environments. This paper conducts a theoretical investigation of this training strategy on generalization. From a stability perspective, we analyze the generalization error bound of generic meta-learning algorithms trained with such strategy. We show that the S/Q episodic training strategy naturally leads to a counterintuitive generalization bound of
O
(
1
/
√
n
)
O
, which only depends on the task number
n
n
but independent of the inner-task sample size
m
m
. Under the common assumption $m<"
neurips,https://proceedings.neurips.cc/paper/2020/file/045117b0e0a11a242b9765e79cbf113f-Paper.pdf,On the Value of Out-of-Distribution Testing: An Example of Goodhart's Law,"Damien Teney, Ehsan Abbasnejad, Kushal Kafle, Robik Shrestha, Christopher Kanan, Anton van den Hengel",
neurips,https://proceedings.neurips.cc/paper/2020/file/04ecb1fa28506ccb6f72b12c0245ddbc-Paper.pdf,Generalised Bayesian Filtering via Sequential Monte Carlo,"Ayman Boustati, Omer Deniz Akyildiz, Theodoros Damoulas, Adam Johansen","We introduce a framework for inference in general state-space hidden Markov models (HMMs) under likelihood misspecification. In particular, we leverage the loss-theoretic perspective of Generalized Bayesian Inference (GBI) to define generalised filtering recursions in HMMs, that can tackle the problem of inference under model misspecification. In doing so, we arrive at principled procedures for robust inference against observation contamination by utilising the
β
β
-divergence. Operationalising the proposed framework is made possible via sequential Monte Carlo methods (SMC), where the standard particle methods, and their associated convergence results, are readily adapted to the new setting. We demonstrate our approach to object tracking and Gaussian process regression problems, and observe improved performance over standard filtering algorithms."
neurips,https://proceedings.neurips.cc/paper/2020/file/05128e44e27c36bdba71221bfccf735d-Paper.pdf,Deterministic Approximation for Submodular Maximization over a Matroid in Nearly Linear Time,"Kai Han, zongmai Cao, Shuang Cui, Benwei Wu","We study the problem of maximizing a non-monotone, non-negative submodular function subject to a matroid constraint. The prior best-known deterministic approximation ratio for this problem is
1
4
−
ϵ
1
under
O
(
(
n
4
/
ϵ
)
log
n
)
O
time complexity. We show that this deterministic ratio can be improved to
1
4
1
under
O
(
n
r
)
O
time complexity, and then present a more practical algorithm dubbed TwinGreedyFast which achieves
1
4
−
ϵ
1
deterministic ratio in nearly-linear running time of
O
(
n
ϵ
log
r
ϵ
)
O
. Our approach is based on a novel algorithmic framework of simultaneously constructing two candidate solution sets through greedy search, which enables us to get improved performance bounds by fully exploiting the properties of independence systems. As a byproduct of this framework, we also show that TwinGreedyFast achieves
1
2
p
+
2
−
ϵ
1
deterministic ratio under a
p
p
-set system constraint with the same time complexity. To showcase the practicality of our approach, we empirically evaluated the performance of TwinGreedyFast on two network applications, and observed that it outperforms the state-of-the-art deterministic and randomized algorithms with efficient implementations for our problem."
neurips,https://proceedings.neurips.cc/paper/2020/file/051928341be67dcba03f0e04104d9047-Paper.pdf,Flows for simultaneous manifold learning and density estimation,"Johann Brehmer, Kyle Cranmer",
neurips,https://proceedings.neurips.cc/paper/2020/file/0561bc7ecba98e39ca7994f93311ba23-Paper.pdf,Simultaneous Preference and Metric Learning from Paired Comparisons,"Austin Xu, Mark Davenport",
neurips,https://proceedings.neurips.cc/paper/2020/file/05a624166c8eb8273b8464e8d9cb5bd9-Paper.pdf,Efficient Variational Inference for Sparse Deep Learning with Theoretical Guarantee,"Jincheng Bai, Qifan Song, Guang Cheng",
neurips,https://proceedings.neurips.cc/paper/2020/file/05e2a0647e260c355dd2b2175edb45b8-Paper.pdf,Learning Manifold Implicitly via Explicit Heat-Kernel Learning,"Yufan Zhou, Changyou Chen, Jinhui Xu",
neurips,https://proceedings.neurips.cc/paper/2020/file/05ee45de8d877c3949760a94fa691533-Paper.pdf,Deep Relational Topic Modeling via Graph Poisson Gamma Belief Network,"Chaojie Wang, Hao Zhang, Bo Chen, Dongsheng Wang, Zhengjue Wang, Mingyuan Zhou",
neurips,https://proceedings.neurips.cc/paper/2020/file/05f971b5ec196b8c65b75d2ef8267331-Paper.pdf,One-bit Supervision for Image Classification,"Hengtong Hu, Lingxi Xie, Zewei Du, Richang Hong, Qi Tian",
neurips,https://proceedings.neurips.cc/paper/2020/file/0607f4c705595b911a4f3e7a127b44e0-Paper.pdf,What is being transferred in transfer learning?,"Behnam Neyshabur, Hanie Sedghi, Chiyuan Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/061412e4a03c02f9902576ec55ebbe77-Paper.pdf,Submodular Maximization Through Barrier Functions,"Ashwinkumar Badanidiyuru, Amin Karbasi, Ehsan Kazemi, Jan Vondrak","In this paper, we introduce a novel technique for constrained submodular maximization, inspired by barrier functions in continuous optimization. This connection not only improves the running time for constrained submodular maximization but also provides the state of the art guarantee. More precisely, for maximizing a monotone submodular function subject to the combination of a
k
k
-matchoid and
ℓ
ℓ
-knapsack constraints (for
ℓ
≤
k
ℓ
), we propose a potential function that can be approximately minimized. Once we minimize the potential function up to an
ϵ
ϵ
error, it is guaranteed that we have found a feasible set with a
2
(
k
+
1
+
ϵ
)
2
-approximation factor which can indeed be further improved to
(
k
+
1
+
ϵ
)
(
by an enumeration technique. We extensively evaluate the performance of our proposed algorithm over several real-world applications, including a movie recommendation system, summarization tasks for YouTube videos, Twitter feeds and Yelp business locations, and a set cover problem."
neurips,https://proceedings.neurips.cc/paper/2020/file/0660895c22f8a14eb039bfb9beb0778f-Paper.pdf,Neural Networks with Recurrent Generative Feedback,"Yujia Huang, James Gornet, Sihui Dai, Zhiding Yu, Tan Nguyen, Doris Tsao, Anima Anandkumar",
neurips,https://proceedings.neurips.cc/paper/2020/file/0663a4ddceacb40b095eda264a85f15c-Paper.pdf,Learning to Extrapolate Knowledge: Transductive Few-shot Out-of-Graph Link Prediction,"Jinheon Baek, Dong Bok Lee, Sung Ju Hwang",
neurips,https://proceedings.neurips.cc/paper/2020/file/066ca7bf90807fcd8e4f1eaef4e4e8f7-Paper.pdf,Exploiting weakly supervised visual patterns to learn from partial annotations,"Kaustav Kundu, Joseph Tighe",
neurips,https://proceedings.neurips.cc/paper/2020/file/066f182b787111ed4cb65ed437f0855b-Paper.pdf,Improving Inference for Neural Image Compression,"Yibo Yang, Robert Bamler, Stephan Mandt",
neurips,https://proceedings.neurips.cc/paper/2020/file/0678ca2eae02d542cc931e81b74de122-Paper.pdf,Neuron Merging: Compensating for Pruned Neurons,"Woojeong Kim, Suhyun Kim, Mincheol Park, Geunseok Jeon",
neurips,https://proceedings.neurips.cc/paper/2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf,FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence,"Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A. Raffel, Ekin Dogus Cubuk, Alexey Kurakin, Chun-Liang Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/06a9d51e04213572ef0720dd27a84792-Paper.pdf,Reinforcement Learning with Combinatorial Actions: An Application to Vehicle Routing,"Arthur Delarue, Ross Anderson, Christian Tjandraatmadja",
neurips,https://proceedings.neurips.cc/paper/2020/file/06d5ae105ea1bea4d800bc96491876e9-Paper.pdf,Towards Playing Full MOBA Games with Deep Reinforcement Learning,"Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, Yinyuting Yin, Bei Shi, Liang Wang, Tengfei Shi, Qiang Fu, Wei Yang, Lanxiao Huang, Wei Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/070dbb6024b5ef93784428afc71f2146-Paper.pdf,Rankmax: An Adaptive Projection Alternative to the Softmax Function,"Weiwei Kong, Walid Krichene, Nicolas Mayoraz, Steffen Rendle, Li Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/07168af6cb0ef9f78dae15739dd73255-Paper.pdf,Online Agnostic Boosting via Regret Minimization,"Nataly Brukhim, Xinyi Chen, Elad Hazan, Shay Moran",
neurips,https://proceedings.neurips.cc/paper/2020/file/07211688a0869d995947a8fb11b215d6-Paper.pdf,Causal Intervention for Weakly-Supervised Semantic Segmentation,"Dong Zhang, Hanwang Zhang, Jinhui Tang, Xian-Sheng Hua, Qianru Sun",
neurips,https://proceedings.neurips.cc/paper/2020/file/07217414eb3fbe24d4e5b6cafb91ca18-Paper.pdf,Belief Propagation Neural Networks,"Jonathan Kuck, Shuvam Chakraborty, Hao Tang, Rachel Luo, Jiaming Song, Ashish Sabharwal, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2020/file/0740bb92e583cd2b88ec7c59f985cb41-Paper.pdf,Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality,"Yi Zhang, Orestis Plevrakis, Simon S. Du, Xingguo Li, Zhao Song, Sanjeev Arora","Adversarial training is a popular method to give neural nets robustness against adversarial perturbations. In practice adversarial training leads to low robust training loss. However, a rigorous explanation for why this happens under natural conditions is still missing. Recently a convergence theory of standard (non-adversarial) supervised training was developed by various groups for {\em very overparametrized} nets. It is unclear how to extend these results to adversarial training because of the min-max objective. Recently, a first step towards this direction was made by Gao et al. using tools from online learning, but they require the width of the net to be \emph{exponential} in input dimension
d
d
, and with an unnatural activation function. Our work proves convergence to low robust training loss for \emph{polynomial} width instead of exponential, under natural assumptions and with ReLU activations. A key element of our proof is showing that ReLU networks near initialization can approximate the step function, which may be of independent interest."
neurips,https://proceedings.neurips.cc/paper/2020/file/074177d3eb6371e32c16c55a3b8f706b-Paper.pdf,Post-training Iterative Hierarchical Data Augmentation for Deep Networks,"Adil Khan, Khadija Fraz",
neurips,https://proceedings.neurips.cc/paper/2020/file/075b051ec3d22dac7b33f788da631fd4-Paper.pdf,Debugging Tests for Model Explanations,"Julius Adebayo, Michael Muelly, Ilaria Liccardi, Been Kim",
neurips,https://proceedings.neurips.cc/paper/2020/file/07cb5f86508f146774a2fac4373a8e50-Paper.pdf,Robust compressed sensing using generative models,"Ajil Jalal, Liu Liu, Alexandros G. Dimakis, Constantine Caramanis","We consider estimating a high dimensional signal in
\R
n
\R
using a sublinear number of linear measurements. In analogy to classical compressed sensing, here we assume a generative model as a prior, that is, we assume the signal is represented by a deep generative model
G
:
\R
k
→
\R
n
G
. Classical recovery approaches such as empirical risk minimization (ERM) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm enjoys the same sample complexity guarantees as ERM under sub-Gaussian assumptions. Our experiments validate both aspects of our claims: other algorithms are indeed fragile and fail under heavy tailed and/or corrupted data, while our approach exhibits the predicted robustness."
neurips,https://proceedings.neurips.cc/paper/2020/file/07fc15c9d169ee48573edd749d25945d-Paper.pdf,Fairness without Demographics through Adversarially Reweighted Learning,"Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, Ed Chi",
neurips,https://proceedings.neurips.cc/paper/2020/file/08058bf500242562c0d031ff830ad094-Paper.pdf,Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model,"Alex X. Lee, Anusha Nagabandi, Pieter Abbeel, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2020/file/08425b881bcde94a383cd258cea331be-Paper.pdf,Ridge Rider: Finding Diverse Solutions by Following Eigenvectors of the Hessian,"Jack Parker-Holder, Luke Metz, Cinjon Resnick, Hengyuan Hu, Adam Lerer, Alistair Letcher, Alexander Peysakhovich, Aldo Pacchiano, Jakob Foerster",
neurips,https://proceedings.neurips.cc/paper/2020/file/0887f1a5b9970ad13f46b8c1485f7900-Paper.pdf,The route to chaos in routing games: When is price of anarchy too optimistic?,"Thiparat Chotibut, Fryderyk Falniowski, Michał Misiurewicz, Georgios Piliouras","Routing games are amongst the most studied classes of games in game theory. Their most well-known property is that learning dynamics typically converge to equilibria implying approximately optimal performance (low Price of Anarchy). We perform a stress test for these classic results by studying the ubiquitous learning dynamics, Multiplicative Weights Update (MWU), in different classes of congestion games, uncovering intricate non-equilibrium phenomena. We study MWU using the actual game costs without applying cost normalization to
[
0
,
1
]
[
. Although this non-standard assumption leads to large regret, it captures realistic agents' behaviors. Namely, as the total demand increases, agents respond more aggressively to unbearably large costs. We start with the illustrative case of non-atomic routing games with two paths of linear cost, and show that every system has a carrying capacity, above which it becomes unstable. If the equilibrium flow is a symmetric
50
−
50
%
50
split, the system exhibits one period-doubling bifurcation. Although the Price of Anarchy is equal to one, in the large population limit the time-average social cost for all but a zero measure set of initial conditions converges to its worst possible value. For asymmetric equilibrium flows, increasing the demand eventually forces the system into Li-Yorke chaos with positive topological entropy and periodic orbits of all possible periods. Remarkably, in all non-equilibrating regimes, the time-average flows on the paths converge {\it exactly} to the equilibrium flows, a property akin to no-regret learning in zero-sum games. We extend our results to games with arbitrarily many strategies, polynomial cost functions, non-atomic as well as atomic routing games, and heterogenous users."
neurips,https://proceedings.neurips.cc/paper/2020/file/08e5d8066881eab185d0de9db3b36c7f-Paper.pdf,Online Algorithm for Unsupervised Sequential Selection with Contextual Information,"Arun Verma, Manjesh Kumar Hanawal, Csaba Szepesvari, Venkatesh Saligrama",
neurips,https://proceedings.neurips.cc/paper/2020/file/08f38e0434442128fab5ead6217ca759-Paper.pdf,Adapting Neural Architectures Between Domains,"Yanxi Li, Zhaohui Yang, Yunhe Wang, Chang Xu",
neurips,https://proceedings.neurips.cc/paper/2020/file/08fa43588c2571ade19bc0fa5936e028-Paper.pdf,What went wrong and when? Instance-wise feature importance for time-series black-box models,"Sana Tonekaboni, Shalmali Joshi, Kieran Campbell, David K. Duvenaud, Anna Goldenberg",
neurips,https://proceedings.neurips.cc/paper/2020/file/08fb104b0f2f838f3ce2d2b3741a12c2-Paper.pdf,Towards Better Generalization of Adaptive Gradient Methods,"Yingxue Zhou, Belhal Karimi, Jinxing Yu, Zhiqiang Xu, Ping Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/0912d0f15f1394268c66639e39b26215-Paper.pdf,Learning Guidance Rewards with Trajectory-space Smoothing,"Tanmay Gangwani, Yuan Zhou, Jian Peng",
neurips,https://proceedings.neurips.cc/paper/2020/file/093b60fd0557804c8ba0cbf1453da22f-Paper.pdf,Variance Reduction via Accelerated Dual Averaging for Finite-Sum Optimization,"Chaobing Song, Yong Jiang, Yi Ma","In this paper, we introduce a simplified and unified method for finite-sum convex optimization, named \emph{Variance Reduction via Accelerated Dual Averaging (VRADA)}. In the general convex and smooth setting, VRADA can attain an
O
(
1
n
)
O
-accurate solution in
O
(
n
log
log
n
)
O
number of stochastic gradient evaluations, where
n
n
is the number of samples; meanwhile, VRADA matches the lower bound of this setting up to a
log
log
n
log
factor. In the strongly convex and smooth setting, VRADA matches the lower bound in the regime
n
≤
Θ
(
κ
)
n
, while it improves the rate in the regime
n
≫
κ
n
to
O
(
n
+
n
log
(
1
/
ϵ
)
log
(
n
/
κ
)
)
O
, where
κ
κ
is the condition number. Besides improving the best known complexity results, VRADA has more unified and simplified algorithmic implementation and convergence analysis for both the general convex and strongly convex settings. Through experiments on real datasets, we show the good performance of VRADA over existing methods for large-scale machine learning problems."
neurips,https://proceedings.neurips.cc/paper/2020/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf,Tree! I am no Tree! I am a low dimensional Hyperbolic Embedding,"Rishi Sonthalia, Anna Gilbert","Given data, finding a faithful low-dimensional hyperbolic embedding of the data is a key method by which we can extract hierarchical information or learn representative geometric features of the data. In this paper, we explore a new method for learning hyperbolic representations by taking a metric-first approach. Rather than determining the low-dimensional hyperbolic embedding directly, we learn a tree structure on the data. This tree structure can then be used directly to extract hierarchical information, embedded into a hyperbolic manifold using Sarkar's construction \cite{sarkar}, or used as a tree approximation of the original metric. To this end, we present a novel fast algorithm \textsc{TreeRep} such that, given a
δ
δ
-hyperbolic metric (for any
δ
≥
0
δ
), the algorithm learns a tree structure that approximates the original metric. In the case when
δ
=
0
δ
, we show analytically that \textsc{TreeRep} exactly recovers the original tree structure. We show empirically that \textsc{TreeRep} is not only many orders of magnitude faster than previously known algorithms, but also produces metrics with lower average distortion and higher mean average precision than most previous algorithms for learning hyperbolic embeddings, extracting hierarchical information, and approximating metrics via tree metrics."
neurips,https://proceedings.neurips.cc/paper/2020/file/0987b8b338d6c90bbedd8631bc499221-Paper.pdf,Deep Structural Causal Models for Tractable Counterfactual Inference,"Nick Pawlowski, Daniel Coelho de Castro, Ben Glocker",
neurips,https://proceedings.neurips.cc/paper/2020/file/098d86c982354a96556bd861823ebfbd-Paper.pdf,Convolutional Generation of Textured 3D Meshes,"Dario Pavllo, Graham Spinks, Thomas Hofmann, Marie-Francine Moens, Aurelien Lucchi",
neurips,https://proceedings.neurips.cc/paper/2020/file/099fe6b0b444c23836c4a5d07346082b-Paper.pdf,A Statistical Framework for Low-bitwidth Training of Deep Neural Networks,"Jianfei Chen, Yu Gai, Zhewei Yao, Michael W. Mahoney, Joseph E. Gonzalez",
neurips,https://proceedings.neurips.cc/paper/2020/file/09ccf3183d9e90e5ae1f425d5f9b2c00-Paper.pdf,Better Set Representations For Relational Reasoning,"Qian Huang, Horace He, Abhay Singh, Yan Zhang, Ser Nam Lim, Austin R. Benson",
neurips,https://proceedings.neurips.cc/paper/2020/file/0a2298a72858d90d5c4b4fee954b6896-Paper.pdf,AutoSync: Learning to Synchronize for Data-Parallel Distributed Deep Learning,"Hao Zhang, Yuan Li, Zhijie Deng, Xiaodan Liang, Lawrence Carin, Eric Xing",
neurips,https://proceedings.neurips.cc/paper/2020/file/0a3b6f64f0523984e51323fe53b8c504-Paper.pdf,A Combinatorial Perspective on Transfer Learning,"Jianan Wang, Eren Sezener, David Budden, Marcus Hutter, Joel Veness",
neurips,https://proceedings.neurips.cc/paper/2020/file/0a4dc6dae338c9cb08947c07581f77a2-Paper.pdf,Hardness of Learning Neural Networks with Natural Weights,"Amit Daniely, Gal Vardi","Neural networks are nowadays highly successful despite strong hardness results. The existing hardness results focus on the network architecture, and assume that the network's weights are arbitrary. A natural approach to settle the discrepancy is to assume that the network's weights are
well-behaved"" and posses some generic properties that may allow efficient learning. This approach is supported by the intuition that the weights in real-world networks are not arbitrary, but exhibit some ''random-like"" properties with respect to some ''natural"" distributions. We prove negative results in this regard, and show that for depth-
2
2
networks, and many
natural"" weights distributions such as the normal and the uniform distribution, most networks are hard to learn. Namely, there is no efficient learning algorithm that is provably successful for most weights, and every input distribution. It implies that there is no generic property that holds with high probability in such random networks and allows efficient learning."
neurips,https://proceedings.neurips.cc/paper/2020/file/0a5052334511e344f15ae0bfafd47a67-Paper.pdf,Higher-Order Spectral Clustering of Directed Graphs,"Steinar Laenen, He Sun",
neurips,https://proceedings.neurips.cc/paper/2020/file/0a656cc19f3f5b41530182a9e03982a4-Paper.pdf,Primal-Dual Mesh Convolutional Neural Networks,"Francesco Milano, Antonio Loquercio, Antoni Rosinol, Davide Scaramuzza, Luca Carlone",
neurips,https://proceedings.neurips.cc/paper/2020/file/0a716fe8c7745e51a3185fc8be6ca23a-Paper.pdf,The Advantage of Conditional Meta-Learning for Biased Regularization and Fine Tuning,"Giulia Denevi, Massimiliano Pontil, Carlo Ciliberto",
neurips,https://proceedings.neurips.cc/paper/2020/file/0a73de68f10e15626eb98701ecf03adb-Paper.pdf,Watch out! Motion is Blurring the Vision of Your Deep Neural Networks,"Qing Guo, Felix Juefei-Xu, Xiaofei Xie, Lei Ma, Jian Wang, Bing Yu, Wei Feng, Yang Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/0a93091da5efb0d9d5649e7f6b2ad9d7-Paper.pdf,Sinkhorn Barycenter via Functional Gradient Descent,"Zebang Shen, Zhenfu Wang, Alejandro Ribeiro, Hamed Hassani","In this paper, we consider the problem of computing the barycenter of a set of probability distributions under the Sinkhorn divergence. This problem has recently found applications across various domains, including graphics, learning, and vision, as it provides a meaningful mechanism to aggregate knowledge. Unlike previous approaches which directly operate in the space of probability measures, we recast the Sinkhorn barycenter problem as an instance of unconstrained functional optimization and develop a novel functional gradient descent method named \texttt{Sinkhorn Descent} (\texttt{SD}). We prove that \texttt{SD} converges to a stationary point at a sublinear rate, and under reasonable assumptions, we further show that it asymptotically finds a global minimizer of the Sinkhorn barycenter problem. Moreover, by providing a mean-field analysis, we show that \texttt{SD} preserves the {weak convergence} of empirical measures. Importantly, the computational complexity of \texttt{SD} scales linearly in the dimension
d
d
and we demonstrate its scalability by solving a
100
100
-dimensional Sinkhorn barycenter problem."
neurips,https://proceedings.neurips.cc/paper/2020/file/0afe095e81a6ac76ff3f69975cb3e7ae-Paper.pdf,Coresets for Near-Convex Functions,"Murad Tukan, Alaa Maalouf, Dan Feldman","Coreset is usually a small weighted subset of
n
n
input points in
R
d
R
, that provably approximates their loss function for a given set of queries (models, classifiers, etc.). Coresets become increasingly common in machine learning since existing heuristics or inefficient algorithms may be improved by running them possibly many times on the small coreset that can be maintained for streaming distributed data. Coresets can be obtained by sensitivity (importance) sampling, where its size is proportional to the total sum of sensitivities. Unfortunately, computing the sensitivity of each point is problem dependent and may be harder to compute than the original optimization problem at hand. We suggest a generic framework for computing sensitivities (and thus coresets) for wide family of loss functions which we call near-convex functions. This is by suggesting the
f
f
-SVD factorization that generalizes the SVD factorization of matrices to functions. Example applications include coresets that are either new or significantly improves previous results, such as SVM, Logistic regression, M-estimators, and
ℓ
z
ℓ
-regression. Experimental results and open source are also provided."
neurips,https://proceedings.neurips.cc/paper/2020/file/0b1ec366924b26fc98fa7b71a9c249cf-Paper.pdf,Bayesian Deep Ensembles via the Neural Tangent Kernel,"Bobby He, Balaji Lakshminarayanan, Yee Whye Teh",
neurips,https://proceedings.neurips.cc/paper/2020/file/0b5e29aa1acf8bdc5d8935d7036fa4f5-Paper.pdf,Improved Schemes for Episodic Memory-based Lifelong Learning,"Yunhui Guo, Mingrui Liu, Tianbao Yang, Tajana Rosing",
neurips,https://proceedings.neurips.cc/paper/2020/file/0b6ace9e8971cf36f1782aa982a708db-Paper.pdf,Adaptive Sampling for Stochastic Risk-Averse Learning,"Sebastian Curi, Kfir Y. Levy, Stefanie Jegelka, Andreas Krause","In high-stakes machine learning applications, it is crucial to not only perform well {\em on average}, but also when restricted to {\em difficult} examples. To address this, we consider the problem of training models in a risk-averse manner. We propose an adaptive sampling algorithm for stochastically optimizing the {\em Conditional Value-at-Risk (CVaR)} of a loss distribution, which measures its performance on the
α
α
fraction of most difficult examples. We use a distributionally robust formulation of the CVaR to phrase the problem as a zero-sum game between two players, and solve it efficiently using regret minimization. Our approach relies on sampling from structured Determinantal Point Processes (DPPs), which enables scaling it to large data sets. Finally, we empirically demonstrate its effectiveness on large-scale convex and non-convex learning tasks."
neurips,https://proceedings.neurips.cc/paper/2020/file/0b8aff0438617c055eb55f0ba5d226fa-Paper.pdf,Deep Wiener Deconvolution: Wiener Meets Deep Learning for Image Deblurring,"Jiangxin Dong, Stefan Roth, Bernt Schiele",
neurips,https://proceedings.neurips.cc/paper/2020/file/0b96d81f0494fde5428c7aea243c9157-Paper.pdf,Discovering Reinforcement Learning Algorithms,"Junhyuk Oh, Matteo Hessel, Wojciech M. Czarnecki, Zhongwen Xu, Hado P. van Hasselt, Satinder Singh, David Silver",
neurips,https://proceedings.neurips.cc/paper/2020/file/0baf163c24ed14b515aaf57a9de5501c-Paper.pdf,Taming Discrete Integration via the Boon of Dimensionality,"Jeffrey Dudek, Dror Fried, Kuldeep S Meel","Building on the promising approach proposed by Chakraborty et al, our work overcomes the key weakness of their approach: a restriction to dyadic weights. We augment our proposed reduction, called DeWeight, with a state of the art efficient approximate model counter and perform detailed empirical analysis over benchmarks arising from neural network verification domains, an emerging application area of critical importance. DeWeight, to the best of our knowledge, is the first technique to compute estimates with provable guarantees for this class of benchmarks."
neurips,https://proceedings.neurips.cc/paper/2020/file/0c0a7566915f4f24853fc4192689aa7e-Paper.pdf,Blind Video Temporal Consistency via Deep Video Prior,"Chenyang Lei, Yazhou Xing, Qifeng Chen",
neurips,https://proceedings.neurips.cc/paper/2020/file/0c7119e3a6a2209da6a5b90e5b5b75bd-Paper.pdf,Simplify and Robustify Negative Sampling for Implicit Collaborative Filtering,"Jingtao Ding, Yuhan Quan, Quanming Yao, Yong Li, Depeng Jin",
neurips,https://proceedings.neurips.cc/paper/2020/file/0c72cb7ee1512f800abe27823a792d03-Paper.pdf,Model Selection for Production System via Automated Online Experiments,"Zhenwen Dai, Praveen Chandar, Ghazal Fazelnia, Benjamin Carterette, Mounia Lalmas",
neurips,https://proceedings.neurips.cc/paper/2020/file/0cb5ebb1b34ec343dfe135db691e4a85-Paper.pdf,On the Almost Sure Convergence of Stochastic Gradient Descent in Non-Convex Problems,"Panayotis Mertikopoulos, Nadav Hallak, Ali Kavis, Volkan Cevher","In this paper, we analyze the trajectories of stochastic gradient descent (SGD) with the aim of understanding their convergence properties in non-convex problems. We first show that the sequence of iterates generated by SGD remains bounded and converges with probability
1
1
under a very broad range of step-size schedules. Subsequently, we prove that the algorithm's rate of convergence to local minimizers with a positive-definite Hessian is
O
(
1
/
n
p
)
O
if the method is run with a
Θ
(
1
/
n
p
)
Θ
step-size. This provides an important guideline for tuning the algorithm's step-size as it suggests that a cool-down phase with a vanishing step-size could lead to significant performance gains; we demonstrate this heuristic using ResNet architectures on CIFAR. Finally, going beyond existing positive probability guarantees, we show that SGD avoids strict saddle points/manifolds with probability
1
1
for the entire spectrum of step-size policies considered."
neurips,https://proceedings.neurips.cc/paper/2020/file/0cbc5671ae26f67871cb914d81ef8fc1-Paper.pdf,Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond,"Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, Cho-Jui Hsieh",
neurips,https://proceedings.neurips.cc/paper/2020/file/0cc24cb7c26586310cc95c8cb1a81cbc-Paper.pdf,Adaptation Properties Allow Identification of Optimized Neural Codes,"Luke Rast, Jan Drugowitsch",
neurips,https://proceedings.neurips.cc/paper/2020/file/0cc6928e741d75e7a92396317522069e-Paper.pdf,Global Convergence and Variance Reduction for a Class of Nonconvex-Nonconcave Minimax Problems,"Junchi Yang, Negar Kiyavash, Niao He",
neurips,https://proceedings.neurips.cc/paper/2020/file/0cc6ee01c82fc49c28706e0918f57e2d-Paper.pdf,Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity,"Kaiqing Zhang, Sham Kakade, Tamer Basar, Lin Yang","Model-based reinforcement learning (RL), which finds an optimal policy using an empirical model, has long been recognized as one of the cornerstones of RL. It is especially suitable for multi-agent RL (MARL), as it naturally decouples the learning and the planning phases, and avoids the non-stationarity problem when all agents are improving their policies simultaneously using samples. Though intuitive and widely-used, the sample complexity of model-based MARL algorithms has been investigated relatively much less often. In this paper, we aim to address the fundamental open question about the sample complexity of model-based MARL. We study arguably the most basic MARL setting: two-player discounted zero-sum Markov games, given only access to a generative model of state transition. We show that model-based MARL achieves a sample complexity of
~
\cO
(
|
\cS
|
|
\cA
|
|
\cB
|
(
1
−
γ
)
−
3
ϵ
−
2
)
\cO
for finding the Nash equilibrium (NE) \emph{value} up to some
ϵ
ϵ
error, and the
ϵ
ϵ
-NE \emph{policies}, where
γ
γ
is the discount factor, and
\cS
,
\cA
,
\cB
\cS
denote the state space, and the action spaces for the two agents. We also show that this method is near-minimax optimal with a tight dependence on
1
−
γ
1
and
|
\cS
|
|
by providing a lower bound of
Ω
(
|
\cS
|
(
|
\cA
|
+
|
\cB
|
)
(
1
−
γ
)
−
3
ϵ
−
2
)
Ω
. Our results justify the efficiency of this simple model-based approach in the multi-agent RL setting."
neurips,https://proceedings.neurips.cc/paper/2020/file/0d2b2061826a5df3221116a5085a6052-Paper.pdf,Conservative Q-Learning for Offline Reinforcement Learning,"Aviral Kumar, Aurick Zhou, George Tucker, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2020/file/0d352b4d3a317e3eae221199fdb49651-Paper.pdf,Online Influence Maximization under Linear Threshold Model,"Shuai Li, Fang Kong, Kejie Tang, Qizhi Li, Wei Chen","Online influence maximization (OIM) is a popular problem in social networks to learn influence propagation model parameters and maximize the influence spread at the same time. Most previous studies focus on the independent cascade (IC) model under the edge-level feedback. In this paper, we address OIM in the linear threshold (LT) model. Because node activations in the LT model are due to the aggregated effect of all active neighbors, it is more natural to model OIM with the nodel-level feedback. And this brings new challenge in online learning since we only observe aggregated effect from groups of nodes and the groups are also random. Based on the linear structure in node activations, we incorporate ideas from linear bandits and design an algorithm
\ltlinucb
\ltlinucb
that is consistent with the observed feedback. By proving group observation modulated (GOM) bounded smoothness property, a novel result of the influence difference in terms of the random observations, we provide a regret of order
~
O
(
p
o
l
y
(
m
)
√
T
)
O
, where
m
m
is the number of edges and
T
T
is the number of rounds. This is the first theoretical result in such order for OIM under the LT model. In the end, we also provide an algorithm
\oimetc
\oimetc
with regret bound
O
(
p
o
l
y
(
m
)
 T
2
/
3
)
O
, which is model-independent, simple and has less requirement on online feedback and offline computation."
neurips,https://proceedings.neurips.cc/paper/2020/file/0d5501edb21a59a43435efa67f200828-Paper.pdf,Ensembling geophysical models with Bayesian Neural Networks,"Ushnish Sengupta, Matt Amos, Scott Hosking, Carl Edward Rasmussen, Matthew Juniper, Paul Young",
neurips,https://proceedings.neurips.cc/paper/2020/file/0d5bd023a3ee11c7abca5b42a93c4866-Paper.pdf,Delving into the Cyclic Mechanism in Semi-supervised Video Object Segmentation,"Yuxi Li, Ning Xu, Jinlong Peng, John See, Weiyao Lin",
neurips,https://proceedings.neurips.cc/paper/2020/file/0d770c496aa3da6d2c3f2bd19e7b9d6b-Paper.pdf,Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability,"Christopher Frye, Colin Rowat, Ilya Feige",
neurips,https://proceedings.neurips.cc/paper/2020/file/0d82627e10660af39ea7eb69c3568955-Paper.pdf,Understanding Deep Architecture with Reasoning Layer,"Xinshi Chen, Yufei Zhang, Christoph Reisinger, Le Song",
neurips,https://proceedings.neurips.cc/paper/2020/file/0d85eb24e2add96ff1a7021f83c1abc9-Paper.pdf,Planning in Markov Decision Processes with Gap-Dependent Sample Complexity,"Anders Jonsson, Emilie Kaufmann, Pierre Menard, Omar Darwiche Domingues, Edouard Leurent, Michal Valko",
neurips,https://proceedings.neurips.cc/paper/2020/file/0dc23b6a0e4abc39904388dd3ffadcd1-Paper.pdf,Provably Good Batch Off-Policy Reinforcement Learning Without Great Exploration,"Yao Liu, Adith Swaminathan, Alekh Agarwal, Emma Brunskill",
neurips,https://proceedings.neurips.cc/paper/2020/file/0dd1bc593a91620daecf7723d2235624-Paper.pdf,Detection as Regression: Certified Object Detection with Median Smoothing,"Ping-yeh Chiang, Michael Curry, Ahmed Abdelkader, Aounon Kumar, John Dickerson, Tom Goldstein","Despite the vulnerability of object detectors to adversarial attacks, very few defenses are known to date. While adversarial training can improve the empirical robustness of image classifiers, a direct extension to object detection is very expensive. This work is motivated by recent progress on certified classification by randomized smoothing. We start by presenting a reduction from object detection to a regression problem. Then, to enable certified regression, where standard mean smoothing fails, we propose median smoothing, which is of independent interest. We obtain the first model-agnostic, training-free, and certified defense for object detection against
ℓ
2
ℓ
-bounded attacks."
neurips,https://proceedings.neurips.cc/paper/2020/file/0e1bacf07b14673fcdb553da51b999a5-Paper.pdf,Contextual Reserve Price Optimization in Auctions via Mixed Integer Programming,"Joey Huchette, Haihao Lu, Hossein Esfandiari, Vahab Mirrokni",
neurips,https://proceedings.neurips.cc/paper/2020/file/0e1ebad68af7f0ae4830b7ac92bc3c6f-Paper.pdf,ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks,"Shuxuan Guo, Jose M. Alvarez, Mathieu Salzmann",
neurips,https://proceedings.neurips.cc/paper/2020/file/0e230b1a582d76526b7ad7fc62ae937d-Paper.pdf,FleXOR: Trainable Fractional Quantization,"Dongsoo Lee, Se Jung Kwon, Byeongwook Kim, Yongkweon Jeon, Baeseong Park, Jeongin Yun","Quantization based on the binary codes is gaining attention because each quantized bit can be directly utilized for computations without dequantization using look-up tables. Previous attempts, however, only allow for integer numbers of quantization bits, which ends up restricting the search space for compression ratio and accuracy. In this paper, we propose an encryption algorithm/architecture to compress quantized weights so as to achieve fractional numbers of bits per weight. Decryption during inference is implemented by digital XOR-gate networks added into the neural network model while XOR gates are described by utilizing
tanh
(
x
)
tanh
for backward propagation to enable gradient calculations. We perform experiments using MNIST, CIFAR-10, and ImageNet to show that inserting XOR gates learns quantization/encrypted bit decisions through training and obtains high accuracy even for fractional sub 1-bit weights. As a result, our proposed method yields smaller size and higher model accuracy compared to binary neural networks."
neurips,https://proceedings.neurips.cc/paper/2020/file/0e4ceef65add6cf21c0f3f9da53b71c0-Paper.pdf,The Implications of Local Correlation on Learning Some Deep Functions,"Eran Malach, Shai Shalev-Shwartz",
neurips,https://proceedings.neurips.cc/paper/2020/file/0e900ad84f63618452210ab8baae0218-Paper.pdf,Learning to search efficiently for causally near-optimal treatments,"Samuel Håkansson, Viktor Lindblom, Omer Gottesman, Fredrik D. Johansson",
neurips,https://proceedings.neurips.cc/paper/2020/file/0ea6f098a59fcf2462afc50d130ff034-Paper.pdf,A Game Theoretic Analysis of Additive Adversarial Attacks and Defenses,"Ambar Pal, Rene Vidal",
neurips,https://proceedings.neurips.cc/paper/2020/file/0eac690d7059a8de4b48e90f14510391-Paper.pdf,Posterior Network: Uncertainty Estimation without OOD Samples via Density-Based Pseudo-Counts,"Bertrand Charpentier, Daniel Zügner, Stephan Günnemann","In this work we propose the Posterior Network (PostNet), which uses Normalizing Flows to predict an individual closed-form posterior distribution over predicted probabilites for any input sample. The posterior distributions learned by PostNet accurately reflect uncertainty for in- and out-of-distribution data -- without requiring access to OOD data at training time. PostNet achieves state-of-the art results in OOD detection and in uncertainty calibration under dataset shifts."
neurips,https://proceedings.neurips.cc/paper/2020/file/0ec96be397dd6d3cf2fecb4a2d627c1c-Paper.pdf,Recurrent Quantum Neural Networks,Johannes Bausch,
neurips,https://proceedings.neurips.cc/paper/2020/file/0ed9422357395a0d4879191c66f4faa2-Paper.pdf,No-Regret Learning and Mixed Nash Equilibria: They Do Not Mix,"Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, Thanasis Lianeas, Panayotis Mertikopoulos, Georgios Piliouras",
neurips,https://proceedings.neurips.cc/paper/2020/file/0f0e13216262f4a201bec128044dd30f-Paper.pdf,A Unifying View of Optimism in Episodic Reinforcement Learning,"Gergely Neu, Ciara Pike-Burke",
neurips,https://proceedings.neurips.cc/paper/2020/file/0f34132b15dd02f282a11ea1e322a96d-Paper.pdf,Continuous Submodular Maximization: Beyond DR-Submodularity,"Moran Feldman, Amin Karbasi","In this paper, we propose the first continuous optimization algorithms that achieve a constant factor approximation guarantee for the problem of monotone continuous submodular maximization subject to a linear constraint. We first prove that a simple variant of the vanilla coordinate ascent, called \COORDINATE-ASCENT+, achieves a
(
e
−
1
2
e
−
1
−
\eps
)
(
-approximation guarantee while performing
O
(
n
/
ϵ
)
O
iterations, where the computational complexity of each iteration is roughly
O
(
n
/
√
ϵ
+
n
log
n
)
O
(here,
n
n
denotes the dimension of the optimization problem). We then propose \COORDINATE-ASCENT++, that achieves the tight
(
1
−
1
/
e
−
\eps
)
(
-approximation guarantee while performing the same number of iterations, but at a higher computational complexity of roughly
O
(
n
3
/
\eps
2.5
+
n
3
log
n
/
\eps
2
)
O
per iteration. However, the computation of each round of \COORDINATE-ASCENT++ can be easily parallelized so that the computational cost per machine scales as
O
(
n
/
√
ϵ
+
n
log
n
)
O
."
neurips,https://proceedings.neurips.cc/paper/2020/file/0f34314d2dd0c1b9311cb8f40eb4f255-Paper.pdf,An Asymptotically Optimal Primal-Dual Incremental Algorithm for Contextual Linear Bandits,"Andrea Tirinzoni, Matteo Pirotta, Marcello Restelli, Alessandro Lazaric",
neurips,https://proceedings.neurips.cc/paper/2020/file/0ff8033cf9437c213ee13937b1c4c455-Paper.pdf,Assessing SATNet's Ability to Solve the Symbol Grounding Problem,"Oscar Chang, Lampros Flokas, Hod Lipson, Michael Spranger",
neurips,https://proceedings.neurips.cc/paper/2020/file/0ffaca95e3e5242ba1097ad8a9a6e95d-Paper.pdf,A Bayesian Nonparametrics View into Deep Representations,"Michał Jamroż, Marcin Kurdziel, Mateusz Opala","We investigate neural network representations from a probabilistic perspective. Specifically, we leverage Bayesian nonparametrics to construct models of neural activations in Convolutional Neural Networks (CNNs) and latent representations in Variational Autoencoders (VAEs). This allows us to formulate a tractable complexity measure for distributions of neural activations and to explore global structure of latent spaces learned by VAEs. We use this machinery to uncover how memorization and two common forms of regularization, i.e. dropout and input augmentation, influence representational complexity in CNNs. We demonstrate that networks that can exploit patterns in data learn vastly less complex representations than networks forced to memorize. We also show marked differences between effects of input augmentation and dropout, with the latter strongly depending on network width. Next, we investigate latent representations learned by standard
β
β
-VAEs and Maximum Mean Discrepancy (MMD)
β
β
-VAEs. We show that aggregated posterior in standard VAEs quickly collapses to the diagonal prior when regularization strength increases. MMD-VAEs, on the other hand, learn more complex posterior distributions, even with strong regularization. While this gives a richer sample space, MMD-VAEs do not exhibit independence of latent dimensions. Finally, we leverage our probabilistic models as an effective sampling strategy for latent codes, improving quality of samples in VAEs with rich posteriors."
neurips,https://proceedings.neurips.cc/paper/2020/file/1006ff12c465532f8c574aeaa4461b16-Paper.pdf,On the Similarity between the Laplace and Neural Tangent Kernels,"Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, Basri Ronen","Recent theoretical work has shown that massively overparameterized neural networks are equivalent to kernel regressors that use Neural Tangent Kernels (NTKs). Experiments show that these kernel methods perform similarly to real neural networks. Here we show that NTK for fully connected networks with ReLU activation is closely related to the standard Laplace kernel. We show theoretically that for normalized data on the hypersphere both kernels have the same eigenfunctions and their eigenvalues decay polynomially at the same rate, implying that their Reproducing Kernel Hilbert Spaces (RKHS) include the same sets of functions. This means that both kernels give rise to classes of functions with the same smoothness properties. The two kernels differ for data off the hypersphere, but experiments indicate that when data is properly normalized these differences are not significant. Finally, we provide experiments on real data comparing NTK and the Laplace kernel, along with a larger class of
γ
γ
-exponential kernels. We show that these perform almost identically. Our results suggest that much insight about neural networks can be obtained from analysis of the well-known Laplace kernel, which has a simple closed form."
neurips,https://proceedings.neurips.cc/paper/2020/file/1010cedf85f6a7e24b087e63235dc12e-Paper.pdf,A causal view of compositional zero-shot recognition,"Yuval Atzmon, Felix Kreuk, Uri Shalit, Gal Chechik","Here we describe an approach for compositional generalization that builds on causal ideas. First, we describe compositional zero-shot learning from a causal perspective, and propose to view zero-shot inference as finding ""which intervention caused the image?"". Second, we present a causal-inspired embedding model that learns disentangled representations of elementary components of visual objects from correlated (confounded) training data. We evaluate this approach on two datasets for predicting new combinations of attribute-object pairs: A well-controlled synthesized images dataset and a real world dataset which consists of fine-grained types of shoes. We show improvements compared to strong baselines."
neurips,https://proceedings.neurips.cc/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf,HiPPO: Recurrent Memory with Optimal Polynomial Projections,"Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, Christopher Ré",
neurips,https://proceedings.neurips.cc/paper/2020/file/103303dd56a731e377d01f6a37badae3-Paper.pdf,Auto Learning Attention,"Benteng Ma, Jing Zhang, Yong Xia, Dacheng Tao",
neurips,https://proceedings.neurips.cc/paper/2020/file/1068bceb19323fe72b2b344ccf85c254-Paper.pdf,CASTLE: Regularization via Auxiliary Causal Graph Discovery,"Trent Kyono, Yao Zhang, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2020/file/1091660f3dff84fd648efe31391c5524-Paper.pdf,Long-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect,"Kaihua Tang, Jianqiang Huang, Hanwang Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/10c72a9d42dd07a028ee910f7854da5d-Paper.pdf,Explainable Voting,"Dominik Peters, Ariel D. Procaccia, Alexandros Psomas, Zixin Zhou","The design of voting rules is traditionally guided by desirable axioms. Recent work shows that, surprisingly, the axiomatic approach can also support the generation of explanations for voting outcomes. However, no bounds on the size of these explanations is given; for all we know, they may be unbearably tedious. We prove, however, that outcomes of the important Borda rule can be explained using
O
(
m
2
)
O
steps, where
m
m
is the number of alternatives. Our main technical result is a general lower bound that, in particular, implies that the foregoing bound is asymptotically tight. We discuss the significance of our results for AI and machine learning, including their potential to bolster an emerging paradigm of automated decision making called virtual democracy."
neurips,https://proceedings.neurips.cc/paper/2020/file/10eb6500bd1e4a3704818012a1593cc3-Paper.pdf,Deep Archimedean Copulas,"Chun Kai Ling, Fei Fang, J. Zico Kolter",
neurips,https://proceedings.neurips.cc/paper/2020/file/10fb6cfa4c990d2bad5ddef4f70e8ba2-Paper.pdf,Re-Examining Linear Embeddings for High-Dimensional Bayesian Optimization,"Ben Letham, Roberto Calandra, Akshara Rai, Eytan Bakshy",
neurips,https://proceedings.neurips.cc/paper/2020/file/1102a326d5f7c9e04fc3c89d0ede88c9-Paper.pdf,UnModNet: Learning to Unwrap a Modulo Image for High Dynamic Range Imaging,"Chu Zhou, Hang Zhao, Jin Han, Chang Xu, Chao Xu, Tiejun Huang, Boxin Shi",
neurips,https://proceedings.neurips.cc/paper/2020/file/11348e03e23b137d55d94464250a67a2-Paper.pdf,Thunder: a Fast Coordinate Selection Solver for Sparse Learning,"Shaogang Ren, Weijie Zhao, Ping Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/1160453108d3e537255e9f7b931f4e90-Paper.pdf,Neural Networks Fail to Learn Periodic Functions and How to Fix It,"Liu Ziyin, Tilman Hartwig, Masahito Ueda","Previous literature offers limited clues on how to learn a periodic function using modern neural networks. We start with a study of the extrapolation properties of neural networks; we prove and demonstrate experimentally that the standard activations functions, such as ReLU, tanh, sigmoid, along with their variants, all fail to learn to extrapolate simple periodic functions. We hypothesize that this is due to their lack of a
periodic"" inductive bias. As a fix of this problem, we propose a new activation, namely,
x
+
sin
2
(
x
)
x
, which achieves the desired periodic inductive bias to learn a periodic function while maintaining a favorable optimization property of the
\relu
\relu
-based activations. Experimentally, we apply the proposed method to temperature and financial data prediction."
neurips,https://proceedings.neurips.cc/paper/2020/file/118bd558033a1016fcc82560c65cca5f-Paper.pdf,Distribution Matching for Crowd Counting,"Boyu Wang, Huidong Liu, Dimitris Samaras, Minh Hoai Nguyen",
neurips,https://proceedings.neurips.cc/paper/2020/file/11953163dd7fb12669b41a48f78a29b6-Paper.pdf,Correspondence learning via linearly-invariant embedding,"Riccardo Marin, Marie-Julie Rakotosaona, Simone Melzi, Maks Ovsjanikov",
neurips,https://proceedings.neurips.cc/paper/2020/file/11958dfee29b6709f48a9ba0387a2431-Paper.pdf,Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement Learning,"Cong Zhang, Wen Song, Zhiguang Cao, Jie Zhang, Puay Siew Tan, Xu Chi",
neurips,https://proceedings.neurips.cc/paper/2020/file/11f38f8ecd71867b42433548d1078e38-Paper.pdf,On Adaptive Attacks to Adversarial Example Defenses,"Florian Tramer, Nicholas Carlini, Wieland Brendel, Aleksander Madry","While prior evaluation papers focused mainly on the end result---showing that a defense was ineffective---this paper focuses on laying out the methodology and the approach necessary to perform an adaptive attack. Some of our attack strategies are generalizable, but no single strategy would have been sufficient for all defenses. This underlines our key message that adaptive attacks cannot be automated and always require careful and appropriate tuning to a given defense. We hope that these analyses will serve as guidance on how to properly perform adaptive attacks against defenses to adversarial examples, and thus will allow the community to make further progress in building more robust models."
neurips,https://proceedings.neurips.cc/paper/2020/file/122e27d57ae8ecb37f3f1da67abb33cb-Paper.pdf,Sinkhorn Natural Gradient for Generative Models,"Zebang Shen, Zhenfu Wang, Alejandro Ribeiro, Hamed Hassani",
neurips,https://proceedings.neurips.cc/paper/2020/file/123650dd0560587918b3d771cf0c0171-Paper.pdf,Online Sinkhorn: Optimal Transport distances from sample streams,"Arthur Mensch, Gabriel Peyré",
neurips,https://proceedings.neurips.cc/paper/2020/file/123b7f02433572a0a560e620311a469c-Paper.pdf,Ultrahyperbolic Representation Learning,"Marc Law, Jos Stam",
neurips,https://proceedings.neurips.cc/paper/2020/file/12780ea688a71dabc284b064add459a4-Paper.pdf,Locally-Adaptive Nonparametric Online Learning,"Ilja Kuzborskij, Nicolò Cesa-Bianchi",
neurips,https://proceedings.neurips.cc/paper/2020/file/12b1e42dc0746f22cf361267de07073f-Paper.pdf,Compositional Generalization via Neural-Symbolic Stack Machines,"Xinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, Denny Zhou",
neurips,https://proceedings.neurips.cc/paper/2020/file/12bcd658ef0a540cabc36cdf2b1046fd-Paper.pdf,Graphon Neural Networks and the Transferability of Graph Neural Networks,"Luana Ruiz, Luiz Chamon, Alejandro Ribeiro",
neurips,https://proceedings.neurips.cc/paper/2020/file/12d16adf4a9355513f9d574b76087a08-Paper.pdf,Unreasonable Effectiveness of Greedy Algorithms in Multi-Armed Bandit with Many Arms,"Mohsen Bayati, Nima Hamidi, Ramesh Johari, Khashayar Khosravi","We study the structure of regret-minimizing policies in the {\em many-armed} Bayesian multi-armed bandit problem: in particular, with
k
k
the number of arms and
T
T
the time horizon, we consider the case where
k
≥
√
T
k
. We first show that {\em subsampling} is a critical step for designing optimal policies. In particular, the standard UCB algorithm leads to sub-optimal regret bounds in the many-armed regime. However, a subsampled UCB (SS-UCB), which samples
Θ
(
√
T
)
Θ
arms and executes UCB only on that subset, is rate-optimal. Despite theoretically optimal regret, even SS-UCB performs poorly due to excessive exploration of suboptimal arms. In particular, in numerical experiments SS-UCB performs worse than a simple greedy algorithm (and its subsampled version) that pulls the current empirical best arm at every time period. We show that these insights hold even in a contextual setting, using real-world data. These empirical results suggest a novel form of {\em free exploration} in the many-armed regime that benefits greedy algorithms. We theoretically study this new source of free exploration and find that it is deeply connected to the distribution of a certain tail event for the prior distribution of arm rewards. This is a fundamentally distinct phenomenon from free exploration as discussed in the recent literature on contextual bandits, where free exploration arises due to variation in contexts. We use this insight to prove that the subsampled greedy algorithm is rate-optimal for Bernoulli bandits when
k
>
√
T
k
, and achieves sublinear regret with more general distributions. This is a case where theoretical rate optimality does not tell the whole story: when complemented by the empirical observations of our paper, the power of greedy algorithms becomes quite evident. Taken together, from a practical standpoint, our results suggest that in applications it may be preferable to use a variant of the greedy algorithm in the many-armed regime."
neurips,https://proceedings.neurips.cc/paper/2020/file/12ffb0968f2f56e51a59a6beb37b2859-Paper.pdf,Gamma-Models: Generative Temporal Difference Learning for Infinite-Horizon Prediction,"Michael Janner, Igor Mordatch, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2020/file/1325cdae3b6f0f91a1b629307bf2d498-Paper.pdf,Deep Transformers with Latent Depth,"Xian Li, Asa Cooper Stickland, Yuqing Tang, Xiang Kong",
neurips,https://proceedings.neurips.cc/paper/2020/file/1349b36b01e0e804a6c2909a6d0ec72a-Paper.pdf,Neural Mesh Flow: 3D Manifold Mesh Generation via Diffeomorphic Flows,"Kunal Gupta, Manmohan Chandraker",
neurips,https://proceedings.neurips.cc/paper/2020/file/1359aa933b48b754a2f54adb688bfa77-Paper.pdf,Statistical control for spatio-temporal MEG/EEG source imaging with desparsified mutli-task Lasso,"Jerome-Alexis Chevalier, Joseph Salmon, Alexandre Gramfort, Bertrand Thirion",
neurips,https://proceedings.neurips.cc/paper/2020/file/1373b284bc381890049e92d324f56de0-Paper.pdf,A Scalable MIP-based Method for Learning Optimal Multivariate Decision Trees,"Haoran Zhu, Pavankumar Murali, Dzung Phan, Lam Nguyen, Jayant Kalagnanam",
neurips,https://proceedings.neurips.cc/paper/2020/file/1385974ed5904a438616ff7bdb3f7439-Paper.pdf,Efficient Exact Verification of Binarized Neural Networks,"Kai Jia, Martin Rinard",
neurips,https://proceedings.neurips.cc/paper/2020/file/13b919438259814cd5be8cb45877d577-Paper.pdf,Ultra-Low Precision 4-bit Training of Deep Neural Networks,"Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi (Viji) Srinivasan, Kailash Gopalakrishnan",
neurips,https://proceedings.neurips.cc/paper/2020/file/13d4635deccc230c944e4ff6e03404b5-Paper.pdf,Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS,"Han Shi, Renjie Pi, Hang Xu, Zhenguo Li, James Kwok, Tong Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/13e36f06c66134ad65f532e90d898545-Paper.pdf,On Numerosity of Deep Neural Networks,"Xi Zhang, Xiaolin Wu",
neurips,https://proceedings.neurips.cc/paper/2020/file/13ec9935e17e00bed6ec8f06230e33a9-Paper.pdf,Outlier Robust Mean Estimation with Subgaussian Rates via Stability,"Ilias Diakonikolas, Daniel M. Kane, Ankit Pensia",
neurips,https://proceedings.neurips.cc/paper/2020/file/13f320e7b5ead1024ac95c3b208610db-Paper.pdf,Self-Supervised Relationship Probing,"Jiuxiang Gu, Jason Kuen, Shafiq Joty, Jianfei Cai, Vlad Morariu, Handong Zhao, Tong Sun",
neurips,https://proceedings.neurips.cc/paper/2020/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf,Information Theoretic Counterfactual Learning from Missing-Not-At-Random Feedback,"Zifeng Wang, Xi Chen, Rui Wen, Shao-Lun Huang, Ercan Kuruoglu, Yefeng Zheng",
neurips,https://proceedings.neurips.cc/paper/2020/file/13fe9d84310e77f13a6d184dbf1232f3-Paper.pdf,Prophet Attention: Predicting Attention with Future Attention,"Fenglin Liu, Xuancheng Ren, Xian Wu, Shen Ge, Wei Fan, Yuexian Zou, Xu Sun",
neurips,https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf,Language Models are Few-Shot Learners,"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",
neurips,https://proceedings.neurips.cc/paper/2020/file/146f7dd4c91bc9d80cf4458ad6d6cd1b-Paper.pdf,Margins are Insufficient for Explaining Gradient Boosting,"Allan Grønlund, Lior Kamma, Kasper Green Larsen","Boosting is one of the most successful ideas in machine learning, achieving great practical performance with little fine-tuning. The success of boosted classifiers is most often attributed to improvements in margins. The focus on margin explanations was pioneered in the seminal work by Schaphire et al. (1998) and has culminated in the
k
k
'th margin generalization bound by Gao and Zhou (2013), which was recently proved to be near-tight for some data distributions (Gr\o nlund et al. 2019). In this work, we first demonstrate that the
k
k
'th margin bound is inadequate in explaining the performance of state-of-the-art gradient boosters. We then explain the short comings of the
k
k
'th margin bound and prove a stronger and more refined margin-based generalization bound that indeed succeeds in explaining the performance of modern gradient boosters. Finally, we improve upon the recent generalization lower bound by Gr\o nlund et al. (2019)."
neurips,https://proceedings.neurips.cc/paper/2020/file/1487987e862c44b91a0296cf3866387e-Paper.pdf,Fourier-transform-based attribution priors improve the interpretability and stability of deep learning models for genomics,"Alex Tseng, Avanti Shrikumar, Anshul Kundaje",
neurips,https://proceedings.neurips.cc/paper/2020/file/149ef6419512be56a93169cd5e6fa8fd-Paper.pdf,MomentumRNN: Integrating Momentum into Recurrent Neural Networks,"Tan Nguyen, Richard Baraniuk, Andrea Bertozzi, Stanley Osher, Bao Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/14da15db887a4b50efe5c1bc66537089-Paper.pdf,Marginal Utility for Planning in Continuous or Large Discrete Action Spaces,"Zaheen Ahmad, Levi Lelis, Michael Bowling",
neurips,https://proceedings.neurips.cc/paper/2020/file/14faf969228fc18fcd4fcf59437b0c97-Paper.pdf,Projected Stein Variational Gradient Descent,"Peng Chen, Omar Ghattas",
neurips,https://proceedings.neurips.cc/paper/2020/file/151d21647527d1079781ba6ae6571ffd-Paper.pdf,Minimax Lower Bounds for Transfer Learning with Linear and One-hidden Layer Neural Networks,"Mohammadreza Mousavi Kalan, Zalan Fabian, Salman Avestimehr, Mahdi Soltanolkotabi",
neurips,https://proceedings.neurips.cc/paper/2020/file/15231a7ce4ba789d13b722cc5c955834-Paper.pdf,SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks,"Fabian Fuchs, Daniel Worrall, Volker Fischer, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2020/file/1534b76d325a8f591b52d302e7181331-Paper.pdf,On the equivalence of molecular graph convolution and molecular wave function with poor basis set,"Masashi Tsubaki, Teruyasu Mizoguchi",
neurips,https://proceedings.neurips.cc/paper/2020/file/155fa09596c7e18e50b58eb7e0c6ccb4-Paper.pdf,The Power of Predictions in Online Control,"Chenkai Yu, Guanya Shi, Soon-Jo Chung, Yisong Yue, Adam Wierman","We study the impact of predictions in online Linear Quadratic Regulator control with both stochastic and adversarial disturbances in the dynamics. In both settings, we characterize the optimal policy and derive tight bounds on the minimum cost and dynamic regret. Perhaps surprisingly, our analysis shows that the conventional greedy MPC approach is a near-optimal policy in both stochastic and adversarial settings. Specifically, for length-
T
T
problems, MPC requires only
O
(
log
T
)
O
predictions to reach
O
(
1
)
O
dynamic regret, which matches (up to lower-order terms) our lower bound on the required prediction horizon for constant regret."
neurips,https://proceedings.neurips.cc/paper/2020/file/15825aee15eb335cc13f9b559f166ee8-Paper.pdf,Learning Affordance Landscapes for Interaction Exploration in 3D Environments,"Tushar Nagarajan, Kristen Grauman",
neurips,https://proceedings.neurips.cc/paper/2020/file/15ae3b9d6286f1b2a489ea4f3f4abaed-Paper.pdf,Cooperative Multi-player Bandit Optimization,"Ilai Bistritz, Nicholas Bambos",
neurips,https://proceedings.neurips.cc/paper/2020/file/15bb63b28926cd083b15e3b97567bbea-Paper.pdf,Tight First- and Second-Order Regret Bounds for Adversarial Linear Bandits,"Shinji Ito, Shuichi Hirahara, Tasuku Soma, Yuichi Yoshida",
neurips,https://proceedings.neurips.cc/paper/2020/file/16002f7a455a94aa4e91cc34ebdb9f2d-Paper.pdf,Just Pick a Sign: Optimizing Deep Multitask Models with Gradient Sign Dropout,"Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, Dragomir Anguelov",
neurips,https://proceedings.neurips.cc/paper/2020/file/165a59f7cf3b5c4396ba65953d679f17-Paper.pdf,A Loss Function for Generative Neural Networks Based on Watson’s Perceptual Model,"Steffen Czolbe, Oswin Krause, Ingemar Cox, Christian Igel",
neurips,https://proceedings.neurips.cc/paper/2020/file/16837163fee34175358a47e0b51485ff-Paper.pdf,Dynamic Fusion of Eye Movement Data and Verbal Narrations in Knowledge-rich Domains,"Ervine Zheng, Qi Yu, Rui Li, Pengcheng Shi, Anne Haake",
neurips,https://proceedings.neurips.cc/paper/2020/file/168efc366c449fab9c2843e9b54e2a18-Paper.pdf,Scalable Multi-Agent Reinforcement Learning for Networked Systems with Average Reward,"Guannan Qu, Yiheng Lin, Adam Wierman, Na Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/169806bb68ccbf5e6f96ddc60c40a044-Paper.pdf,Optimizing Neural Networks via Koopman Operator Theory,"Akshunna S. Dogra, William Redman",
neurips,https://proceedings.neurips.cc/paper/2020/file/16f8e136ee5693823268874e58795216-Paper.pdf,SVGD as a kernelized Wasserstein gradient flow of the chi-squared divergence,"Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, Philippe Rigollet",
neurips,https://proceedings.neurips.cc/paper/2020/file/170f6aa36530c364b77ddf83a84e7351-Paper.pdf,Adversarial Robustness of Supervised Sparse Coding,"Jeremias Sulam, Ramchandran Muthukumar, Raman Arora","Several recent results provide theoretical insights into the phenomena of adversarial examples. Existing results, however, are often limited due to a gap between the simplicity of the models studied and the complexity of those deployed in practice. In this work, we strike a better balance by considering a model that involves learning a representation while at the same time giving a precise generalization bound and a robustness certificate. We focus on the hypothesis class obtained by combining a sparsity-promoting encoder coupled with a linear classifier, and show an interesting interplay between the expressivity and stability of the (supervised) representation map and a notion of margin in the feature space. We bound the robust risk (to
ℓ
2
ℓ
-bounded perturbations) of hypotheses parameterized by dictionaries that achieve a mild encoder gap on training data. Furthermore, we provide a robustness certificate for end-to-end classification. We demonstrate the applicability of our analysis by computing certified accuracy on real data, and compare with other alternatives for certified robustness."
neurips,https://proceedings.neurips.cc/paper/2020/file/171ae1bbb81475eb96287dd78565b38b-Paper.pdf,Differentiable Meta-Learning of Bandit Policies,"Craig Boutilier, Chih-wei Hsu, Branislav Kveton, Martin Mladenov, Csaba Szepesvari, Manzil Zaheer",
neurips,https://proceedings.neurips.cc/paper/2020/file/17256f049f1e3fede17c7a313f7657f4-Paper.pdf,Biologically Inspired Mechanisms for Adversarial Robustness,"Manish Reddy Vuyyuru, Andrzej Banburski, Nishka Pant, Tomaso Poggio",
neurips,https://proceedings.neurips.cc/paper/2020/file/17257e81a344982579af1ae6415a7b8c-Paper.pdf,Statistical-Query Lower Bounds via Functional Gradients,"Surbhi Goel, Aravind Gollakota, Adam Klivans","We give the first statistical-query lower bounds for agnostically learning any non-polynomial activation with respect to Gaussian marginals (e.g., ReLU, sigmoid, sign). For the specific problem of ReLU regression (equivalently, agnostically learning a ReLU), we show that any statistical-query algorithm with tolerance
n
−
(
1
/
ϵ
)
b
n
must use at least
2
n
c
ϵ
2
queries for some constants
b
,
c
>
0
b
, where
n
n
is the dimension and
ϵ
ϵ
is the accuracy parameter. Our results rule out {\em general} (as opposed to correlational) SQ learning algorithms, which is unusual for real-valued learning problems. Our techniques involve a gradient boosting procedure for
amplifying'' recent lower bounds due to Diakonikolas et al.\ (COLT 2020) and Goel et al.\ (ICML 2020) on the SQ dimension of functions computed by two-layer neural networks. The crucial new ingredient is the use of a nonstandard convex functional during the boosting procedure. This also yields a best-possible reduction between two commonly studied models of learning: agnostic learning and probabilistic concepts."
neurips,https://proceedings.neurips.cc/paper/2020/file/172ef5a94b4dd0aa120c6878fc29f70c-Paper.pdf,Near-Optimal Reinforcement Learning with Self-Play,"Yu Bai, Chi Jin, Tiancheng Yu",
neurips,https://proceedings.neurips.cc/paper/2020/file/1730f69e6f66d5f0c741799e82351f81-Paper.pdf,Network Diffusions via Neural Mean-Field Dynamics,"Shushan He, Hongyuan Zha, Xiaojing Ye",
neurips,https://proceedings.neurips.cc/paper/2020/file/1731592aca5fb4d789c4119c65c10b4b-Paper.pdf,Self-Distillation as Instance-Specific Label Smoothing,"Zhilu Zhang, Mert Sabuncu",
neurips,https://proceedings.neurips.cc/paper/2020/file/174f8f613332b27e9e8a5138adb7e920-Paper.pdf,Towards Problem-dependent Optimal Learning Rates,"Yunbei Xu, Assaf Zeevi",
neurips,https://proceedings.neurips.cc/paper/2020/file/1763ea5a7e72dd7ee64073c2dda7a7a8-Paper.pdf,Cross-lingual Retrieval for Iterative Self-Supervised Training,"Chau Tran, Yuqing Tang, Xian Li, Jiatao Gu",
neurips,https://proceedings.neurips.cc/paper/2020/file/1764183ef03fc7324eb58c3842bd9a57-Paper.pdf,Rethinking pooling in graph neural networks,"Diego Mesquita, Amauri Souza, Samuel Kaski",
neurips,https://proceedings.neurips.cc/paper/2020/file/176bf6219855a6eb1f3a30903e34b6fb-Paper.pdf,Pointer Graph Networks,"Petar Veličković, Lars Buesing, Matthew Overlan, Razvan Pascanu, Oriol Vinyals, Charles Blundell",
neurips,https://proceedings.neurips.cc/paper/2020/file/17b3c7061788dbe82de5abe9f6fe22b3-Paper.pdf,Gradient Regularized V-Learning for Dynamic Treatment Regimes,"Yao Zhang, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2020/file/17f98ddf040204eda0af36a108cbdea4-Paper.pdf,Faster Wasserstein Distance Estimation with the Sinkhorn Divergence,"Lénaïc Chizat, Pierre Roussillon, Flavien Léger, François-Xavier Vialard, Gabriel Peyré","The squared Wasserstein distance is a natural quantity to compare probability distributions in a non-parametric setting. This quantity is usually estimated with the plug-in estimator, defined via a discrete optimal transport problem which can be solved to
ϵ
ϵ
-accuracy by adding an entropic regularization of order
ϵ
ϵ
and using for instance Sinkhorn's algorithm. In this work, we propose instead to estimate it with the Sinkhorn divergence, which is also built on entropic regularization but includes debiasing terms. We show that, for smooth densities, this estimator has a comparable sample complexity but allows higher regularization levels, of order
ϵ
1
/
2
ϵ
, which leads to improved computational complexity bounds and a strong speedup in practice. Our theoretical analysis covers the case of both randomly sampled densities and deterministic discretizations on uniform grids. We also propose and analyze an estimator based on Richardson extrapolation of the Sinkhorn divergence which enjoys improved statistical and computational efficiency guarantees, under a condition on the regularity of the approximation error, which is in particular satisfied for Gaussian densities. We finally demonstrate the efficiency of the proposed estimators with numerical experiments."
neurips,https://proceedings.neurips.cc/paper/2020/file/18064d61b6f93dab8681a460779b8429-Paper.pdf,Forethought and Hindsight in Credit Assignment,"Veronica Chelu, Doina Precup, Hado P. van Hasselt",
neurips,https://proceedings.neurips.cc/paper/2020/file/1819020b02e926785cf3be594d957696-Paper.pdf,Robust Recursive Partitioning for Heterogeneous Treatment Effects with Uncertainty Quantification,"Hyun-Suk Lee, Yao Zhang, William Zame, Cong Shen, Jang-Won Lee, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2020/file/186b690e29892f137b4c34cfa40a3a4d-Paper.pdf,Rescuing neural spike train models from bad MLE,"Diego Arribas, Yuan Zhao, Il Memming Park",
neurips,https://proceedings.neurips.cc/paper/2020/file/187acf7982f3c169b3075132380986e4-Paper.pdf,Lower Bounds and Optimal Algorithms for Personalized Federated Learning,"Filip Hanzely, Slavomír Hanzely, Samuel Horváth, Peter Richtarik",
neurips,https://proceedings.neurips.cc/paper/2020/file/1896a3bf730516dd643ba67b4c447d36-Paper.pdf,Black-Box Certification with Randomized Smoothing: A Functional Optimization Based Framework,"Dinghuai Zhang, Mao Ye, Chengyue Gong, Zhanxing Zhu, Qiang Liu","Randomized classifiers have been shown to provide a promising approach for achieving certified robustness against adversarial attacks in deep learning. However, most existing methods only leverage Gaussian smoothing noise and only work for
ℓ
2
ℓ
perturbation. We propose a general framework of adversarial certification with non-Gaussian noise and for more general types of attacks, from a unified \functional optimization perspective. Our new framework allows us to identify a key trade-off between accuracy and robustness via designing smoothing distributions, helping to design new families of non-Gaussian smoothing distributions that work more efficiently for different
ℓ
p
ℓ
settings, including
ℓ
1
ℓ
,
ℓ
2
ℓ
and
ℓ
∞
ℓ
attacks. Our proposed methods achieve better certification results than previous works and provide a new perspective on randomized smoothing certification."
neurips,https://proceedings.neurips.cc/paper/2020/file/18a010d2a9813e91907ce88cd9143fdf-Paper.pdf,Deep Imitation Learning for Bimanual Robotic Manipulation,"Fan Xie, Alexander Chowdhury, M. Clara De Paolis Kaluza, Linfeng Zhao, Lawson Wong, Rose Yu",
neurips,https://proceedings.neurips.cc/paper/2020/file/18a411989b47ed75a60ac69d9da05aa5-Paper.pdf,Stationary Activations for Uncertainty Calibration in Deep Learning,"Lassi Meronen, Christabella Irwanto, Arno Solin",
neurips,https://proceedings.neurips.cc/paper/2020/file/18df51b97ccd68128e994804f3eccc87-Paper.pdf,Ensemble Distillation for Robust Model Fusion in Federated Learning,"Tao Lin, Lingjing Kong, Sebastian U. Stich, Martin Jaggi","In this work we investigate more powerful and more flexible aggregation schemes for FL. Specifically, we propose ensemble distillation for model fusion, i.e. training the central classifier through unlabeled data on the outputs of the models from the clients. This knowledge distillation technique mitigates privacy risk and cost to the same extent as the baseline FL algorithms, but allows flexible aggregation over heterogeneous client models that can differ e.g. in size, numerical precision or structure. We show in extensive empirical experiments on various CV/NLP datasets (CIFAR-10/100, ImageNet, AG News, SST2) and settings (heterogeneous models/data) that the server model can be trained much faster, requiring fewer communication rounds than any existing FL technique so far."
neurips,https://proceedings.neurips.cc/paper/2020/file/18fc72d8b8aba03a4d84f66efabce82e-Paper.pdf,Falcon: Fast Spectral Inference on Encrypted Data,"Qian Lou, Wen-jie Lu, Cheng Hong, Lei Jiang","Homomorphic Encryption (HE) based secure Neural Networks(NNs) inference is one of the most promising security solutions to emerging Machine Learning as a Service (MLaaS). In the HE-based MLaaS setting, a client encrypts the sensitive data, and uploads the encrypted data to the server that directly processes the encrypted data without decryption, and returns the encrypted result to the client. The clients' data privacy is preserved since only the client has the private key. Existing HE-enabled Neural Networks (HENNs), however, suffer from heavy computational overheads. The state-of-the-art HENNs adopt ciphertext packing techniques to reduce homomorphic multiplications by packing multiple messages into one single ciphertext. Nevertheless, rotations are required in these HENNs to implement the sum of the elements within the same ciphertext. We observed that HENNs have to pay significant computing overhead on rotations, and each of rotations is
∼
10
×
∼
more expensive than homomorphic multiplications between ciphertext and plaintext. So the massive rotations have become a primary obstacle of efficient HENNs. In this paper, we propose a fast, frequency-domain deep neural network called Falcon, for fast inferences on encrypted data. Falcon includes a fast Homomorphic Discrete Fourier Transform (HDFT) using block-circulant matrices to homomorphically support spectral operations. We also propose several efficient methods to reduce inference latency, including Homomorphic Spectral Convolution and Homomorphic Spectral Fully Connected operations by combing the batched HE and block-circulant matrices. Our experimental results show Falcon achieves the state-of-the-art inference accuracy and reduces the inference latency by
45.45
%
∼
85.34
%
45.45
over prior HENNs on MNIST and CIFAR-10."
neurips,https://proceedings.neurips.cc/paper/2020/file/191595dc11b4d6e54f01504e3aa92f96-Paper.pdf,On Power Laws in Deep Ensembles,"Ekaterina Lobacheva, Nadezhda Chirkova, Maxim Kodryan, Dmitry P. Vetrov",
neurips,https://proceedings.neurips.cc/paper/2020/file/192fc044e74dffea144f9ac5dc9f3395-Paper.pdf,Practical Quasi-Newton Methods for Training Deep Neural Networks,"Donald Goldfarb, Yi Ren, Achraf Bahamou",
neurips,https://proceedings.neurips.cc/paper/2020/file/193002e668758ea9762904da1a22337c-Paper.pdf,Approximation Based Variance Reduction for Reparameterization Gradients,"Tomas Geffner, Justin Domke",
neurips,https://proceedings.neurips.cc/paper/2020/file/1943102704f8f8f3302c2b730728e023-Paper.pdf,Inference Stage Optimization for Cross-scenario 3D Human Pose Estimation,"Jianfeng Zhang, Xuecheng Nie, Jiashi Feng",
neurips,https://proceedings.neurips.cc/paper/2020/file/1959eb9d5a0f7ebc58ebde81d5df400d-Paper.pdf,Consistent feature selection for analytic deep neural networks,"Vu C. Dinh, Lam S. Ho","In this work, we investigate the problem of feature selection for analytic deep networks. We prove that for a wide class of networks, including deep feed-forward neural networks, convolutional neural networks and a major sub-class of residual neural networks, the Adaptive Group Lasso selection procedure with Group Lasso as the base estimator is selection-consistent. The work provides further evidence that Group Lasso might be inefficient for feature selection with neural networks and advocates the use of Adaptive Group Lasso over the popular Group Lasso."
neurips,https://proceedings.neurips.cc/paper/2020/file/1963bd5135521d623f6c29e6b1174975-Paper.pdf,Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification,"Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, Gao Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/196f5641aa9dc87067da4ff90fd81e7b-Paper.pdf,Information Maximization for Few-Shot Learning,"Malik Boudiaf, Imtiaz Ziko, Jérôme Rony, Jose Dolz, Pablo Piantanida, Ismail Ben Ayed",
neurips,https://proceedings.neurips.cc/paper/2020/file/19aa6c6fb4ba9fcf39e893ff1fd5b5bd-Paper.pdf,Inverse Reinforcement Learning from a Gradient-based Learner,"Giorgia Ramponi, Gianluca Drappo, Marcello Restelli",
neurips,https://proceedings.neurips.cc/paper/2020/file/19eca5979ccbb752778e6c5f090dc9b6-Paper.pdf,Bayesian Multi-type Mean Field Multi-agent Imitation Learning,"Fan Yang, Alina Vereshchaka, Changyou Chen, Wen Dong",
neurips,https://proceedings.neurips.cc/paper/2020/file/1a669e81c8093745261889539694be7f-Paper.pdf,Bayesian Robust Optimization for Imitation Learning,"Daniel Brown, Scott Niekum, Marek Petrik",
neurips,https://proceedings.neurips.cc/paper/2020/file/1a77befc3b608d6ed363567685f70e1e-Paper.pdf,Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance,"Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, Yaron Lipman",
neurips,https://proceedings.neurips.cc/paper/2020/file/1aa3d9c6ce672447e1e5d0f1b5207e85-Paper.pdf,Riemannian Continuous Normalizing Flows,"Emile Mathieu, Maximilian Nickel",
neurips,https://proceedings.neurips.cc/paper/2020/file/1abb1e1ea5f481b589da52303b091cbb-Paper.pdf,Attention-Gated Brain Propagation: How the brain can implement reward-based error backpropagation,"Isabella Pozzi, Sander Bohte, Pieter Roelfsema",
neurips,https://proceedings.neurips.cc/paper/2020/file/1ac978c8020be6d7212aa71d4f040fc3-Paper.pdf,Asymptotic Guarantees for Generative Modeling Based on the Smooth Wasserstein Distance,"Ziv Goldfeld, Kristjan Greenewald, Kengo Kato","Minimum distance estimation (MDE) gained recent attention as a formulation of (implicit) generative modeling. It considers minimizing, over model parameters, a statistical distance between the empirical data distribution and the model. This formulation lends itself well to theoretical analysis, but typical results are hindered by the curse of dimensionality. To overcome this and devise a scalable finite-sample statistical MDE theory, we adopt the framework of smooth 1-Wasserstein distance (SWD)
W
(
σ
)
1
W
. The SWD was recently shown to preserve the metric and topological structure of classic Wasserstein distances, while enjoying dimension-free empirical convergence rates. In this work, we conduct a thorough statistical study of the minimum smooth Wasserstein estimators (MSWEs), first proving the estimator's measurability and asymptotic consistency. We then characterize the limit distribution of the optimal model parameters and their associated minimal SWD. These results imply an
O
(
n
−
1
/
2
)
O
generalization bound for generative modeling based on MSWE, which holds in arbitrary dimension. Our main technical tool is a novel high-dimensional limit distribution result for empirical
W
(
σ
)
1
W
. The characterization of a nondegenerate limit stands in sharp contrast with the classic empirical 1-Wasserstein distance, for which a similar result is known only in the one-dimensional case. The validity of our theory is supported by empirical results, posing the SWD as a potent tool for learning and inference in high dimensions."
neurips,https://proceedings.neurips.cc/paper/2020/file/1ae6464c6b5d51b363d7d96f97132c75-Paper.pdf,Online Robust Regression via SGD on the l1 loss,"Scott Pesme, Nicolas Flammarion","We consider the robust linear regression problem in the online setting where we have access to the data in a streaming manner, one data point after the other. More specifically, for a true parameter
θ
∗
θ
, we consider the corrupted Gaussian linear model $y = + \varepsilon + b
w
h
e
r
e
t
h
e
a
d
v
e
r
s
a
r
i
a
l
n
o
i
s
e
w
b
c
a
n
t
a
k
e
a
n
y
v
a
l
u
e
w
i
t
h
p
r
o
b
a
b
i
l
i
t
y
c
\eta
a
n
d
e
q
u
a
l
s
z
e
r
o
o
t
h
e
r
w
i
s
e
.
W
e
c
o
n
s
i
d
e
r
t
h
i
s
a
d
v
e
r
s
a
r
y
t
o
b
e
o
b
l
i
v
i
o
u
s
(
i
.
e
.
,
a
b
i
n
d
e
p
e
n
d
e
n
t
o
f
t
h
e
d
a
t
a
)
s
i
n
c
e
t
h
i
s
i
s
t
h
e
o
n
l
y
c
o
n
t
a
m
i
n
a
t
i
o
n
m
o
d
e
l
u
n
d
e
r
w
h
i
c
h
c
o
n
s
i
s
t
e
n
c
y
i
s
p
o
s
s
i
b
l
e
.
C
u
r
r
e
n
t
a
l
g
o
r
i
t
h
m
s
r
e
l
y
o
n
h
a
v
i
n
g
t
h
e
w
h
o
l
e
d
a
t
a
a
t
h
a
n
d
i
n
o
r
d
e
r
t
o
i
d
e
n
t
i
f
y
a
n
d
r
e
m
o
v
e
t
h
e
o
u
t
l
i
e
r
s
.
I
n
c
o
n
t
r
a
s
t
,
w
e
s
h
o
w
i
n
t
h
i
s
w
o
r
k
t
h
a
t
s
t
o
c
h
a
s
t
i
c
g
r
a
d
i
e
n
t
d
e
s
c
e
n
t
o
n
t
h
e
l
1
l
o
s
s
c
o
n
v
e
r
g
e
s
t
o
t
h
e
t
r
u
e
p
a
r
a
m
e
t
e
r
v
e
c
t
o
r
a
t
a
i
\tilde{O}( 1 / (1 - \eta)^2 n )$ rate which is independent of the values of the contaminated measurements. Our proof relies on the elegant smoothing of the l1 loss by the Gaussian data and a classical non-asymptotic analysis of Polyak-Ruppert averaged SGD. In addition, we provide experimental evidence of the efficiency of this simple and highly scalable algorithm."
neurips,https://proceedings.neurips.cc/paper/2020/file/1b0251ccb8bd5f9ccf444e4bda7713e3-Paper.pdf,PRANK: motion Prediction based on RANKing,"Yuriy Biktairov, Maxim Stebelev, Irina Rudenko, Oleh Shliazhko, Boris Yangel",
neurips,https://proceedings.neurips.cc/paper/2020/file/1b113258af3968aaf3969ca67e744ff8-Paper.pdf,Fighting Copycat Agents in Behavioral Cloning from Observation Histories,"Chuan Wen, Jierui Lin, Trevor Darrell, Dinesh Jayaraman, Yang Gao",
neurips,https://proceedings.neurips.cc/paper/2020/file/1b33d16fc562464579b7199ca3114982-Paper.pdf,Tight Nonparametric Convergence Rates for Stochastic Gradient Descent under the Noiseless Linear Model,"Raphaël Berthier, Francis Bach, Pierre Gaillard","In the context of statistical supervised learning, the noiseless linear model assumes that there exists a deterministic linear relation
Y
=
⟨
θ
∗
,
Φ
(
U
)
⟩
Y
between the random output
Y
Y
and the random feature vector
Φ
(
U
)
Φ
, a potentially non-linear transformation of the inputs~
U
U
. We analyze the convergence of single-pass, fixed step-size stochastic gradient descent on the least-square risk under this model. The convergence of the iterates to the optimum
θ
∗
θ
and the decay of the generalization error follow polynomial convergence rates with exponents that both depend on the regularities of the optimum
θ
∗
θ
and of the feature vectors
Φ
(
U
)
Φ
. We interpret our result in the reproducing kernel Hilbert space framework. As a special case, we analyze an online algorithm for estimating a real function on the unit hypercube from the noiseless observation of its value at randomly sampled points; the convergence depends on the Sobolev smoothness of the function and of a chosen kernel. Finally, we apply our analysis beyond the supervised learning setting to obtain convergence rates for the averaging process (a.k.a. gossip algorithm) on a graph depending on its spectral dimension."
neurips,https://proceedings.neurips.cc/paper/2020/file/1b69ebedb522700034547abc5652ffac-Paper.pdf,Structured Prediction for Conditional Meta-Learning,"Ruohan Wang, Yiannis Demiris, Carlo Ciliberto",
neurips,https://proceedings.neurips.cc/paper/2020/file/1b742ae215adf18b75449c6e272fd92d-Paper.pdf,Optimal Lottery Tickets via Subset Sum: Logarithmic Over-Parameterization is Sufficient,"Ankit Pensia, Shashank Rajput, Alliot Nagle, Harit Vishwakarma, Dimitris Papailiopoulos","The strong lottery ticket hypothesis (LTH) postulates that one can approximate any target neural network by only pruning the weights of a sufficiently over-parameterized random network. A recent work by Malach et al. [MYSS20] establishes the first theoretical analysis for the strong LTH: one can provably approximate a neural network of width
d
d
and depth
l
l
, by pruning a random one that is a factor
O
(
d
4
l
2
)
O
wider and twice as deep. This polynomial over-parameterization requirement is at odds with recent experimental research that achieves good approximation with networks that are a small factor wider than the target. In this work, we close the gap and offer an exponential improvement to the over-parameterization requirement for the existence of lottery tickets. We show that any target network of width
d
d
and depth
l
l
can be approximated by pruning a random network that is a factor
O
(
l
o
g
(
d
l
)
)
O
wider and twice as deep. Our analysis heavily relies on connecting pruning random ReLU networks to random instances of the Subset Sum problem. We then show that this logarithmic over-parameterization is essentially optimal for constant depth networks. Finally, we verify several of our theoretical insights with experiments."
neurips,https://proceedings.neurips.cc/paper/2020/file/1b84c4cee2b8b3d823b30e2d604b1878-Paper.pdf,The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes,"Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, Davide Testuggine",
neurips,https://proceedings.neurips.cc/paper/2020/file/1b9a80606d74d3da6db2f1274557e644-Paper.pdf,Stochasticity of Deterministic Gradient Descent: Large Learning Rate for Multiscale Objective Function,"Lingkai Kong, Molei Tao",
neurips,https://proceedings.neurips.cc/paper/2020/file/1ba922ac006a8e5f2b123684c2f4d65f-Paper.pdf,Identifying Learning Rules From Neural Network Observables,"Aran Nayebi, Sanjana Srivastava, Surya Ganguli, Daniel L. Yamins",
neurips,https://proceedings.neurips.cc/paper/2020/file/1bd413de70f32142f4a33a94134c5690-Paper.pdf,Optimal Approximation - Smoothness Tradeoffs for Soft-Max Functions,"Alessandro Epasto, Mohammad Mahdian, Vahab Mirrokni, Emmanouil Zampetakis",
neurips,https://proceedings.neurips.cc/paper/2020/file/1bd69c7df3112fb9a584fbd9edfc6c90-Paper.pdf,Weakly-Supervised Reinforcement Learning for Controllable Behavior,"Lisa Lee, Ben Eysenbach, Russ R. Salakhutdinov, Shixiang (Shane) Gu, Chelsea Finn",
neurips,https://proceedings.neurips.cc/paper/2020/file/1bda4c789c38754f639a376716c5859f-Paper.pdf,Improving Policy-Constrained Kidney Exchange via Pre-Screening,"Duncan McElfresh, Michael Curry, Tuomas Sandholm, John Dickerson","In barter exchanges, participants swap goods with one another without exchanging money; these exchanges are often facilitated by a central clearinghouse, with the goal of maximizing the aggregate quality (or number) of swaps. Barter exchanges are subject to many forms of uncertainty--in participant preferences, the feasibility and quality of various swaps, and so on. Our work is motivated by kidney exchange, a real-world barter market in which patients in need of a kidney transplant swap their willing living donors, in order to find a better match. Modern exchanges include 2- and 3-way swaps, making the kidney exchange clearing problem NP-hard. Planned transplants often \emph{fail} for a variety of reasons--if the donor organ is rejected by the recipient's medical team, or if the donor and recipient are found to be medically incompatible. Due to 2- and 3-way swaps, failed transplants can
cascade'' through an exchange; one US-based exchange estimated that about
85
%
85
of planned transplants failed in 2019. Many optimization-based approaches have been designed to avoid these failures; however most exchanges cannot implement these methods, due to legal and policy constraints. Instead, we consider a setting where exchanges can \emph{query} the preferences of certain donors and recipients--asking whether they would accept a particular transplant. We characterize this as a two-stage decision problem, in which the exchange program (a) queries a small number of transplants before committing to a matching, and (b) constructs a matching according to fixed policy. We show that selecting these edges is a challenging combinatorial problem, which is non-monotonic and non-submodular, in addition to being NP-hard. We propose both a greedy heuristic and a Monte Carlo tree search, which outperforms previous approaches, using experiments on both synthetic data and real kidney exchange data from the United Network for Organ Sharing."
neurips,https://proceedings.neurips.cc/paper/2020/file/1c104b9c0accfca52ef21728eaf01453-Paper.pdf,Learning abstract structure for drawing by efficient motor program induction,"Lucas Tian, Kevin Ellis, Marta Kryven, Josh Tenenbaum",
neurips,https://proceedings.neurips.cc/paper/2020/file/1c336b8080f82bcc2cd2499b4c57261d-Paper.pdf,Why Do Deep Residual Networks Generalize Better than Deep Feedforward Networks? --- A Neural Tangent Kernel Perspective,"Kaixuan Huang, Yuqing Wang, Molei Tao, Tuo Zhao",
neurips,https://proceedings.neurips.cc/paper/2020/file/1c383cd30b7c298ab50293adfecb7b18-Paper.pdf,Dual Instrumental Variable Regression,"Krikamol Muandet, Arash Mehrjou, Si Kai Lee, Anant Raj",
neurips,https://proceedings.neurips.cc/paper/2020/file/1cb524b5a3f3f82be4a7d954063c07e2-Paper.pdf,Stochastic Gradient Descent in Correlated Settings: A Study on Gaussian Processes,"Hao Chen, Lili Zheng, Raed AL Kontar, Garvesh Raskutti","Stochastic gradient descent (SGD) and its variants have established themselves as the go-to algorithms for large-scale machine learning problems with independent samples due to their generalization performance and intrinsic computational advantage. However, the fact that the stochastic gradient is a biased estimator of the full gradient with correlated samples has led to the lack of theoretical understanding of how SGD behaves under correlated settings and hindered its use in such cases. In this paper, we focus on the Gaussian process (GP) and take a step forward towards breaking the barrier by proving minibatch SGD converges to a critical point of the full loss function, and recovers model hyperparameters with rate
O
(
1
K
)
O
up to a statistical error term depending on the minibatch size. Numerical studies on both simulated and real datasets demonstrate that minibatch SGD has better generalization over state-of-the-art GP methods while reducing the computational burden and opening a new, previously unexplored, data size regime for GPs."
neurips,https://proceedings.neurips.cc/paper/2020/file/1cc8a8ea51cd0adddf5dab504a285915-Paper.pdf,Interventional Few-Shot Learning,"Zhongqi Yue, Hanwang Zhang, Qianru Sun, Xian-Sheng Hua",
neurips,https://proceedings.neurips.cc/paper/2020/file/1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf,Minimax Value Interval for Off-Policy Evaluation and Policy Optimization,"Nan Jiang, Jiawei Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/1cdf14d1e3699d61d237cf76ce1c2dca-Paper.pdf,Biased Stochastic First-Order Methods for Conditional Stochastic Optimization and Applications in Meta Learning,"Yifan Hu, Siqi Zhang, Xin Chen, Niao He",
neurips,https://proceedings.neurips.cc/paper/2020/file/1cf44d7975e6c86cffa70cae95b5fbb2-Paper.pdf,ShiftAddNet: A Hardware-Inspired Deep Network,"Haoran You, Xiaohan Chen, Yongan Zhang, Chaojian Li, Sicheng Li, Zihao Liu, Zhangyang Wang, Yingyan Lin",
neurips,https://proceedings.neurips.cc/paper/2020/file/1cfa81af29c6f2d8cacb44921722e753-Paper.pdf,Network-to-Network Translation with Conditional Invertible Neural Networks,"Robin Rombach, Patrick Esser, Bjorn Ommer",
neurips,https://proceedings.neurips.cc/paper/2020/file/1d8d70dddf147d2d92a634817f01b239-Paper.pdf,Intra-Processing Methods for Debiasing Neural Networks,"Yash Savani, Colin White, Naveen Sundar Govindarajulu","In this work, we initiate the study of a new paradigm in debiasing research, intra-processing, which sits between in-processing and post-processing methods. Intra-processing methods are designed specifically to debias large models which have been trained on a generic dataset, and fine-tuned on a more specific task. We show how to repurpose existing in-processing methods for this use-case, and we also propose three baseline algorithms: random perturbation, layerwise optimization, and adversarial debiasing. We evaluate these methods across three popular datasets from the AIF360 toolkit, as well as on the CelebA faces dataset. Our code is available at https://github.com/abacusai/intraprocessing_debiasing."
neurips,https://proceedings.neurips.cc/paper/2020/file/1da546f25222c1ee710cf7e2f7a3ff0c-Paper.pdf,Finding Second-Order Stationary Points Efficiently in Smooth Nonconvex Linearly Constrained Optimization Problems,"Songtao Lu, Meisam Razaviyayn, Bo Yang, Kejun Huang, Mingyi Hong","This paper proposes two efficient algorithms for computing approximate second-order stationary points (SOSPs) of problems with generic smooth non-convex objective functions and generic linear constraints. While finding (approximate) SOSPs for the class of smooth non-convex linearly constrained problems is computationally intractable, we show that generic problem instances in this class can be solved efficiently. Specifically, for a generic problem instance, we show that certain strict complementarity (SC) condition holds for all Karush-Kuhn-Tucker (KKT) solutions. Based on this condition, we design an algorithm named Successive Negative-curvature grAdient Projection (SNAP), which performs either conventional gradient projection or some negative curvature-based projection steps to find SOSPs. SNAP is a second-order algorithm that requires
˜
O
(
max
{
1
/
ϵ
2
G
,
1
/
ϵ
3
H
}
)
O
iterations to compute an
(
ϵ
G
,
ϵ
H
)
(
-SOSP, where
˜
O
O
hides the iteration complexity for eigenvalue-decomposition. Building on SNAP, we propose a first-order algorithm, named SNAP
+
+
, that requires
O
(
1
/
ϵ
2.5
)
O
iterations to compute
(
ϵ
,
√
ϵ
)
(
-SOSP. The per-iteration computational complexities of our algorithms are polynomial in the number of constraints and problem dimension. To the best of our knowledge, this is the first time that first-order algorithms with polynomial per-iteration complexity and global sublinear rate are designed to find SOSPs of the important class of non-convex problems with linear constraints (almost surely)."
neurips,https://proceedings.neurips.cc/paper/2020/file/1dc3a89d0d440ba31729b0ba74b93a33-Paper.pdf,Model-based Policy Optimization with Unsupervised Model Adaptation,"Jian Shen, Han Zhao, Weinan Zhang, Yong Yu",
neurips,https://proceedings.neurips.cc/paper/2020/file/1de7d2b90d554be9f0db1c338e80197d-Paper.pdf,Implicit Regularization and Convergence for Weight Normalization,"Xiaoxia Wu, Edgar Dobriban, Tongzheng Ren, Shanshan Wu, Zhiyuan Li, Suriya Gunasekar, Rachel Ward, Qiang Liu","Normalization methods such as batch, weight, instance, and layer normalization are commonly used in modern machine learning. Here, we study the weight normalization (WN) method \cite{salimans2016weight} and a variant called reparametrized projected gradient descent (rPGD) for overparametrized least squares regression and some more general loss functions. WN and rPGD reparametrize the weights with a scale
g
g
and a unit vector such that the objective function becomes \emph{non-convex}. We show that this non-convex formulation has beneficial regularization effects compared to gradient descent on the original objective. These methods adaptively regularize the weights and \emph{converge linearly} close to the minimum
ℓ
2
ℓ
norm solution even for initializations far from zero. For certain two-phase variants, they can converge to the min norm solution. This is different from the behavior of gradient descent, which only converges to the min norm solution when started at zero, and thus more sensitive to initialization."
neurips,https://proceedings.neurips.cc/paper/2020/file/1def1713ebf17722cbe300cfc1c88558-Paper.pdf,Geometric All-way Boolean Tensor Decomposition,"Changlin Wan, Wennan Chang, Tong Zhao, Sha Cao, Chi Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/1e04b969bf040acd252e1faafb51f829-Paper.pdf,Modular Meta-Learning with Shrinkage,"Yutian Chen, Abram L. Friesen, Feryal Behbahani, Arnaud Doucet, David Budden, Matthew Hoffman, Nando de Freitas",
neurips,https://proceedings.neurips.cc/paper/2020/file/1e0b802d5c0e1e8434a771ba7ff2c301-Paper.pdf,A/B Testing in Dense Large-Scale Networks: Design and Inference,"Preetam Nandy, Kinjal Basu, Shaunak Chatterjee, Ye Tu",
neurips,https://proceedings.neurips.cc/paper/2020/file/1e14bfe2714193e7af5abc64ecbd6b46-Paper.pdf,What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation,"Vitaly Feldman, Chiyuan Zhang",In this work we design experiments to test the key ideas in this theory. The experiments require estimation of the influence of each training example on the accuracy at each test example as well as memorization values of training examples. Estimating these quantities directly is computationally prohibitive but we show that closely-related subsampled influence and memorization values can be estimated much more efficiently. Our experiments demonstrate the significant benefits of memorization for generalization on several standard benchmarks. They also provide quantitative and visually compelling evidence for the theory put forth in Feldman (2019).
neurips,https://proceedings.neurips.cc/paper/2020/file/1e591403ff232de0f0f139ac51d99295-Paper.pdf,Partially View-aligned Clustering,"Zhenyu Huang, Peng Hu, Joey Tianyi Zhou, Jiancheng Lv, Xi Peng","In this paper, we study one challenging issue in multi-view data clustering. To be specific, for two data matrices
X
(
1
)
X
and
X
(
2
)
X
corresponding to two views, we do not assume that
X
(
1
)
X
and
X
(
2
)
X
are fully aligned in row-wise. Instead, we assume that only a small portion of the matrices has established the correspondence in advance. Such a partially view-aligned problem (PVP) could lead to the intensive labor of capturing or establishing the aligned multi-view data, which has less been touched so far to the best of our knowledge. To solve this practical and challenging problem, we propose a novel multi-view clustering method termed partially view-aligned clustering (PVC). To be specific, PVC proposes to use a differentiable surrogate of the non-differentiable Hungarian algorithm and recasts it as a pluggable module. As a result, the category-level correspondence of the unaligned data could be established in a latent space learned by a neural network, while learning a common space across different views using the
aligned'' data. Extensive experimental results show promising results of our method in clustering partially view-aligned data."
neurips,https://proceedings.neurips.cc/paper/2020/file/1e6e25d952a0d639b676ee20d0519ee2-Paper.pdf,Partial Optimal Tranport with applications on Positive-Unlabeled Learning,"Laetitia Chapel, Mokhtar Z. Alaya, Gilles Gasso",
neurips,https://proceedings.neurips.cc/paper/2020/file/1e7875cf32d306989d80c14308f3a099-Paper.pdf,Toward the Fundamental Limits of Imitation Learning,"Nived Rajaraman, Lin Yang, Jiantao Jiao, Kannan Ramchandran","Imitation learning (IL) aims to mimic the behavior of an expert policy in a sequential decision-making problem given only demonstrations. In this paper, we focus on understanding the minimax statistical limits of IL in episodic Markov Decision Processes (MDPs). We first consider the setting where the learner is provided a dataset of
N
N
expert trajectories ahead of time, and cannot interact with the MDP. Here, we show that the policy which mimics the expert whenever possible is in expectation
≲
|
S
|
H
2
log
(
N
)
N
≲
suboptimal compared to the value of the expert, even when the expert plays a stochastic policy. Here
S
S
is the state space and
H
H
is the length of the episode. Furthermore, we establish a suboptimality lower bound of
≳
|
S
|
H
2
/
N
≳
which applies even if the expert is constrained to be deterministic, or if the learner is allowed to actively query the expert at visited states while interacting with the MDP for
N
N
episodes. To our knowledge, this is the first algorithm with suboptimality having no dependence on the number of actions, under no additional assumptions. We then propose a novel algorithm based on minimum-distance functionals in the setting where the transition model is given and the expert is deterministic. The algorithm is suboptimal by
≲
|
S
|
H
3
/
2
/
N
≲
, matching our lower bound up to a
√
H
H
factor, and breaks the
O
(
H
2
)
O
error compounding barrier of IL."
neurips,https://proceedings.neurips.cc/paper/2020/file/1e9491470749d5b0e361ce4f0b24d037-Paper.pdf,Logarithmic Pruning is All You Need,"Laurent Orseau, Marcus Hutter, Omar Rivasplata",
neurips,https://proceedings.neurips.cc/paper/2020/file/1ea97de85eb634d580161c603422437f-Paper.pdf,Hold me tight! Influence of discriminative features on deep network boundaries,"Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi, Pascal Frossard",
neurips,https://proceedings.neurips.cc/paper/2020/file/1ee942c6b182d0f041a2312947385b23-Paper.pdf,Learning from Mixtures of Private and Public Populations,"Raef Bassily, Shay Moran, Anupama Nandi","We initiate the study of a new model of supervised learning under privacy constraints. Imagine a medical study where a dataset is sampled from a population of both healthy and unhealthy individuals. Suppose healthy individuals have no privacy concerns (in such case, we call their data
public'') while the unhealthy individuals desire stringent privacy protection for their data. In this example, the population (data distribution) is a mixture of private (unhealthy) and public (healthy) sub-populations that could be very different. Inspired by the above example, we consider a model in which the population
\cD
\cD
is a mixture of two possibly distinct sub-populations: a private sub-population
\Dprv
\Dprv
of private and sensitive data, and a public sub-population
\Dpub
\Dpub
of data with no privacy concerns. Each example drawn from
\cD
\cD
is assumed to contain a privacy-status bit that indicates whether the example is private or public. The goal is to design a learning algorithm that satisfies differential privacy only with respect to the private examples. Prior works in this context assumed a homogeneous population where private and public data arise from the same distribution, and in particular designed solutions which exploit this assumption. We demonstrate how to circumvent this assumption by considering, as a case study, the problem of learning linear classifiers in
R
d
R
. We show that in the case where the privacy status is correlated with the target label (as in the above example), linear classifiers in
R
d
R
can be learned, in the agnostic as well as the realizable setting, with sample complexity which is comparable to that of the classical (non-private) PAC-learning. It is known that this task is impossible if all the data is considered private."
neurips,https://proceedings.neurips.cc/paper/2020/file/1ef91c212e30e14bf125e9374262401f-Paper.pdf,Adversarial Weight Perturbation Helps Robust Generalization,"Dongxian Wu, Shu-Tao Xia, Yisen Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/1f10c3650a3aa5912dccc5789fd515e8-Paper.pdf,Stateful Posted Pricing with Vanishing Regret via Dynamic Deterministic Markov Decision Processes,"Yuval Emek, Ron Lavi, Rad Niazadeh, Yangguang Shi",
neurips,https://proceedings.neurips.cc/paper/2020/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf,Adversarial Self-Supervised Contrastive Learning,"Minseon Kim, Jihoon Tack, Sung Ju Hwang",
neurips,https://proceedings.neurips.cc/paper/2020/file/1f47cef5e38c952f94c5d61726027439-Paper.pdf,Normalizing Kalman Filters for Multivariate Time Series Analysis,"Emmanuel de Bézenac, Syama Sundar Rangapuram, Konstantinos Benidis, Michael Bohlke-Schneider, Richard Kurle, Lorenzo Stella, Hilaf Hasson, Patrick Gallinari, Tim Januschowski",
neurips,https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf,Learning to summarize with human feedback,"Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F. Christiano",
neurips,https://proceedings.neurips.cc/paper/2020/file/1f8d87e1161af68b81bace188a1ec624-Paper.pdf,Fourier Spectrum Discrepancies in Deep Network Generated Images,"Tarik Dzanic, Karan Shah, Freddie Witherden",
neurips,https://proceedings.neurips.cc/paper/2020/file/1fc214004c9481e4c8073e85323bfd4b-Paper.pdf,"Lamina-specific neuronal properties promote robust, stable signal propagation in feedforward networks","Dongqi Han, Erik De Schutter, Sungho Hong",
neurips,https://proceedings.neurips.cc/paper/2020/file/1fc30b9d4319760b04fab735fbfed9a9-Paper.pdf,Learning Dynamic Belief Graphs to Generalize on Text-Based Games,"Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, Will Hamilton",
neurips,https://proceedings.neurips.cc/paper/2020/file/1fd09c5f59a8ff35d499c0ee25a1d47e-Paper.pdf,Triple descent and the two kinds of overfitting: where & why do they appear?,"Stéphane d'Ascoli, Levent Sagun, Giulio Biroli",
neurips,https://proceedings.neurips.cc/paper/2020/file/1fd6c4e41e2c6a6b092eb13ee72bce95-Paper.pdf,Multimodal Graph Networks for Compositional Generalization in Visual Question Answering,"Raeid Saqur, Karthik Narasimhan",
neurips,https://proceedings.neurips.cc/paper/2020/file/1fdc0ee9d95c71d73df82ac8f0721459-Paper.pdf,Learning Graph Structure With A Finite-State Automaton Layer,"Daniel Johnson, Hugo Larochelle, Daniel Tarlow",
neurips,https://proceedings.neurips.cc/paper/2020/file/2000f6325dfc4fc3201fc45ed01c7a5d-Paper.pdf,A Universal Approximation Theorem of Deep Neural Networks for Expressing Probability Distributions,"Yulong Lu, Jianfeng Lu","This paper studies the universal approximation property of deep neural networks for representing probability distributions. Given a target distribution
π
π
and a source distribution
p
z
p
both defined on
R
d
R
, we prove under some assumptions that there exists a deep neural network
g
:
R
d
>
R
g
with ReLU activation such that the push-forward measure
(
∇
g
)
#
p
z
(
of
p
z
p
under the map
∇
g
∇
is arbitrarily close to the target measure
π
π
. The closeness are measured by three classes of integral probability metrics between probability distributions:
1
1
-Wasserstein distance, maximum mean distance (MMD) and kernelized Stein discrepancy (KSD). We prove upper bounds for the size (width and depth) of the deep neural network in terms of the dimension
d
d
and the approximation error
ε
ε
with respect to the three discrepancies. In particular, the size of neural network can grow exponentially in
d
d
when
1
1
-Wasserstein distance is used as the discrepancy, whereas for both MMD and KSD the size of neural network only depends on
d
d
at most polynomially. Our proof relies on convergence estimates of empirical measures under aforementioned discrepancies and semi-discrete optimal transport."
neurips,https://proceedings.neurips.cc/paper/2020/file/20125fd9b2d43e340a35fb0278da235d-Paper.pdf,Unsupervised object-centric video generation and decomposition in 3D,"Paul Henderson, Christoph H. Lampert",
neurips,https://proceedings.neurips.cc/paper/2020/file/201d7288b4c18a679e48b31c72c30ded-Paper.pdf,Domain Generalization for Medical Imaging Classification with Linear-Dependency Regularization,"Haoliang Li, Yufei Wang, Renjie Wan, Shiqi Wang, Tie-Qiang Li, Alex Kot",
neurips,https://proceedings.neurips.cc/paper/2020/file/20479c788fb27378c2c99eadcf207e7f-Paper.pdf,Multi-label classification: do Hamming loss and subset accuracy really conflict with each other?,"Guoqiang Wu, Jun Zhu","Various evaluation measures have been developed for multi-label classification, including Hamming Loss (HL), Subset Accuracy (SA) and Ranking Loss (RL). However, there is a gap between empirical results and the existing theories: 1) an algorithm often empirically performs well on some measure(s) while poorly on others, while a formal theoretical analysis is lacking; and 2) in small label space cases, the algorithms optimizing HL often have comparable or even better performance on the SA measure than those optimizing SA directly, while existing theoretical results show that SA and HL are conflicting measures. This paper provides an attempt to fill up this gap by analyzing the learning guarantees of the corresponding learning algorithms on both SA and HL measures. We show that when a learning algorithm optimizes HL with its surrogate loss, it enjoys an error bound for the HL measure independent of
c
c
(the number of labels), while the bound for the SA measure depends on at most
O
(
c
)
O
. On the other hand, when directly optimizing SA with its surrogate loss, it has learning guarantees that depend on
O
(
√
c
)
O
for both HL and SA measures. This explains the observation that when the label space is not large, optimizing HL with its surrogate loss can have promising performance for SA. We further show that our techniques are applicable to analyze the learning guarantees of algorithms on other measures, such as RL. Finally, the theoretical analyses are supported by experimental results."
neurips,https://proceedings.neurips.cc/paper/2020/file/2051bd70fc110a2208bdbd4a743e7f79-Paper.pdf,A Novel Automated Curriculum Strategy to Solve Hard Sokoban Planning Instances,"Dieqiao Feng, Carla P. Gomes, Bart Selman",
neurips,https://proceedings.neurips.cc/paper/2020/file/205e73579f21c2ed134dbd6ce7e4a1ea-Paper.pdf,Causal analysis of Covid-19 Spread in Germany,"Atalanti Mastakouri, Bernhard Schölkopf",
neurips,https://proceedings.neurips.cc/paper/2020/file/20b02dc95171540bc52912baf3aa709d-Paper.pdf,Locally private non-asymptotic testing of discrete distributions is faster using interactive mechanisms,"Thomas Berrett, Cristina Butucea",
neurips,https://proceedings.neurips.cc/paper/2020/file/20b5e1cf8694af7a3c1ba4a87f073021-Paper.pdf,Adaptive Gradient Quantization for Data-Parallel SGD,"Fartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, Daniel M. Roy, Ali Ramezani-Kebrya",
neurips,https://proceedings.neurips.cc/paper/2020/file/20c86a628232a67e7bd46f76fba7ce12-Paper.pdf,Finite Continuum-Armed Bandits,Solenne Gaucher,"We consider a situation where an agent has
T
T
ressources to be allocated to a larger number
N
N
of actions. Each action can be completed at most once and results in a stochastic reward with unknown mean. The goal of the agent is to maximize her cumulative reward. Non trivial strategies are possible when side information on the actions is available, for example in the form of covariates. Focusing on a nonparametric setting, where the mean reward is an unknown function of a one-dimensional covariate, we propose an optimal strategy for this problem. Under natural assumptions on the reward function, we prove that the optimal regret scales as
O
(
T
1
/
3
)
O
up to poly-logarithmic factors when the budget
T
T
is proportional to the number of actions
N
N
. When
T
T
becomes small compared to
N
N
, a smooth transition occurs. When the ratio
T
/
N
T
decreases from a constant to
N
−
1
/
3
N
, the regret increases progressively up to the
O
(
T
1
/
2
)
O
rate encountered in continuum-armed bandits."
neurips,https://proceedings.neurips.cc/paper/2020/file/20d749bc05f47d2bd3026ce457dcfd8e-Paper.pdf,Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies,"Itai Gat, Idan Schwartz, Alexander Schwing, Tamir Hazan",
neurips,https://proceedings.neurips.cc/paper/2020/file/2109737282d2c2de4fc5534be26c9bb6-Paper.pdf,Compact task representations as a normative model for higher-order brain activity,"Severin Berger, Christian K. Machens",
neurips,https://proceedings.neurips.cc/paper/2020/file/211b39255232ab59ce78f2e28cd0292b-Paper.pdf,Robust-Adaptive Control of Linear Systems: beyond Quadratic Costs,"Edouard Leurent, Odalric-Ambrym Maillard, Denis Efimov",
neurips,https://proceedings.neurips.cc/paper/2020/file/212ab20dbdf4191cbcdcf015511783f4-Paper.pdf,Co-exposure Maximization in Online Social Networks,"Sijing Tu, Cigdem Aslay, Aristides Gionis","We show that the problem of maximizing co-exposure is NP-hard and its objective function is neither submodular nor supermodular. However, by exploiting a connection to a submodular function that acts as a lower bound to the objective, we are able to devise a greedy algorithm with provable approximation guarantee. We further provide a scalable instantiation of our approximation algorithm by introducing a novel extension to the notion of random reverse-reachable sets for efficiently estimating the expected co-exposure. We experimentally demonstrate the quality of our proposal on real-world social networks."
neurips,https://proceedings.neurips.cc/paper/2020/file/21327ba33b3689e713cdff1641128004-Paper.pdf,UCLID-Net: Single View Reconstruction in Object Space,"Benoit Guillard, Edoardo Remelli, Pascal Fua","We demonstrate both on ShapeNet synthetic images, which are often used for benchmarking purposes, and on real-world images that our approach outperforms state-of-the-art ones. Furthermore, the single-view pipeline naturally extends to multi-view reconstruction, which we also show."
neurips,https://proceedings.neurips.cc/paper/2020/file/216f44e2d28d4e175a194492bde9148f-Paper.pdf,Reinforcement Learning for Control with Multiple Frequencies,"Jongmin Lee, Byung-Jun Lee, Kee-Eung Kim","Many real-world sequential decision problems involve multiple action variables whose control frequencies are different, such that actions take their effects at different periods. While these problems can be formulated with the notion of multiple action persistences in factored-action MDP (FA-MDP), it is non-trivial to solve them efficiently since an action-persistent policy constructed from a stationary policy can be arbitrarily suboptimal, rendering solution methods for the standard FA-MDPs hardly applicable. In this paper, we formalize the problem of multiple control frequencies in RL and provide its efficient solution method. Our proposed method, Action-Persistent Policy Iteration (AP-PI), provides a theoretical guarantee on the convergence to an optimal solution while incurring only a factor of
|
A
|
|
increase in time complexity during policy improvement step, compared to the standard policy iteration for FA-MDPs. Extending this result, we present Action-Persistent Actor-Critic (AP-AC), a scalable RL algorithm for high-dimensional control tasks. In the experiments, we demonstrate that AP-AC significantly outperforms the baselines on several continuous control tasks and a traffic control simulation, which highlights the effectiveness of our method that directly optimizes the periodic non-stationary policy for tasks with multiple control frequencies."
neurips,https://proceedings.neurips.cc/paper/2020/file/2172fde49301047270b2897085e4319d-Paper.pdf,Complex Dynamics in Simple Neural Networks: Understanding Gradient Flow in Phase Retrieval,"Stefano Sarao Mannelli, Giulio Biroli, Chiara Cammarota, Florent Krzakala, Pierfrancesco Urbani, Lenka Zdeborová",
neurips,https://proceedings.neurips.cc/paper/2020/file/217eedd1ba8c592db97d0dbe54c7adfc-Paper.pdf,Neural Message Passing for Multi-Relational Ordered and Recursive Hypergraphs,Naganand Yadati,
neurips,https://proceedings.neurips.cc/paper/2020/file/219e052492f4008818b8adb6366c7ed6-Paper.pdf,A Unified View of Label Shift Estimation,"Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, Zachary Lipton","Under label shift, the label distribution
p
(
y
)
p
might change but the class-conditional distributions
p
(
x
|
y
)
p
do not. There are two dominant approaches for estimating the label marginal. BBSE, a moment-matching approach based on confusion matrices, is provably consistent and provides interpretable error bounds. However, a maximum likelihood estimation approach, which we call MLLS, dominates empirically. In this paper, we present a unified view of the two methods and the first theoretical characterization of MLLS. Our contributions include (i) consistency conditions for MLLS, which include calibration of the classifier and a confusion matrix invertibility condition that BBSE also requires; (ii) a unified framework, casting BBSE as roughly equivalent to MLLS for a particular choice of calibration method; and (iii) a decomposition of MLLS's finite-sample error into terms reflecting miscalibration and estimation error. Our analysis attributes BBSE's statistical inefficiency to a loss of information due to coarse calibration. Experiments on synthetic data, MNIST, and CIFAR10 support our findings."
neurips,https://proceedings.neurips.cc/paper/2020/file/21d144c75af2c3a1cb90441bbb7d8b40-Paper.pdf,Optimal Private Median Estimation under Minimal Distributional Assumptions,"Christos Tzamos, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Ilias Zadik",
neurips,https://proceedings.neurips.cc/paper/2020/file/222afbe0d68c61de60374b96f1d86715-Paper.pdf,Breaking the Communication-Privacy-Accuracy Trilemma,"Wei-Ning Chen, Peter Kairouz, Ayfer Ozgur","In particular, we consider the problems of mean estimation and frequency estimation under epsilon-local differential privacy and b-bit communication constraints. For mean estimation, we propose a scheme based on Kashin’s representation and random sampling, with order-optimal estimation error under both constraints. For frequency estimation, we present a mechanism that leverages the recursive structure of Walsh-Hadamard matrices and achieves order-optimal estimation error for all privacy levels and communication budgets. As a by-product, we also construct a distribution estimation mechanism that is rate-optimal for all privacy regimes and communication constraints, extending recent work that is limited to b = 1 and epsilon = O(1). Our results demonstrate that intelligent encoding under joint privacy and communication constraints can yield a performance that matches the optimal accuracy achievable under either constraint alone."
neurips,https://proceedings.neurips.cc/paper/2020/file/227f6afd3b7f89b96c4bb91f95d50f6d-Paper.pdf,Audeo: Audio Generation for a Silent Performance Video,"Kun Su, Xiulong Liu, Eli Shlizerman",
neurips,https://proceedings.neurips.cc/paper/2020/file/228669109aa3ab1b4ec06b7722efb105-Paper.pdf,Ode to an ODE,"Krzysztof M. Choromanski, Jared Quincy Davis, Valerii Likhosherstov, Xingyou Song, Jean-Jacques Slotine, Jacob Varley, Honglak Lee, Adrian Weller, Vikas Sindhwani",
neurips,https://proceedings.neurips.cc/paper/2020/file/2288f691b58edecadcc9a8691762b4fd-Paper.pdf,Self-Distillation Amplifies Regularization in Hilbert Space,"Hossein Mobahi, Mehrdad Farajtabar, Peter Bartlett",
neurips,https://proceedings.neurips.cc/paper/2020/file/2290a7385ed77cc5592dc2153229f082-Paper.pdf,Coupling-based Invertible Neural Networks Are Universal Diffeomorphism Approximators,"Takeshi Teshima, Isao Ishikawa, Koichi Tojo, Kenta Oono, Masahiro Ikeda, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2020/file/229aeb9e2ae66f2fac1149e5240b2fdd-Paper.pdf,Community detection using fast low-cardinality semidefinite programming,"Po-Wei Wang, J. Zico Kolter",
neurips,https://proceedings.neurips.cc/paper/2020/file/22bb543b251c39ccdad8063d486987bb-Paper.pdf,Modeling Noisy Annotations for Crowd Counting,"Jia Wan, Antoni Chan",
neurips,https://proceedings.neurips.cc/paper/2020/file/22eda830d1051274a2581d6466c06e6c-Paper.pdf,An operator view of policy gradient methods,"Dibya Ghosh, Marlos C. Machado, Nicolas Le Roux","We cast policy gradient methods as the repeated application of two operators: a policy improvement operator
I
I
, which maps any policy
π
π
to a better one
I
π
I
, and a projection operator
P
P
, which finds the best approximation of
I
π
I
in the set of realizable policies. We use this framework to introduce operator-based versions of well-known policy gradient methods such as REINFORCE and PPO, which leads to a better understanding of their original counterparts. We also use the understanding we develop of the role of
I
I
and
P
P
to propose a new global lower bound of the expected return. This new perspective allows us to further bridge the gap between policy-based and value-based methods, showing how REINFORCE and the Bellman optimality operator, for example, can be seen as two sides of the same coin."
neurips,https://proceedings.neurips.cc/paper/2020/file/22f791da07b0d8a2504c2537c560001c-Paper.pdf,"Demystifying Contrastive Self-Supervised Learning: Invariances, Augmentations and Dataset Biases","Senthil Purushwalkam, Abhinav Gupta",
neurips,https://proceedings.neurips.cc/paper/2020/file/23378a2d0a25c6ade2c1da1c06c5213f-Paper.pdf,Online MAP Inference of Determinantal Point Processes,"Aditya Bhaskara, Amin Karbasi, Silvio Lattanzi, Morteza Zadimoghaddam","In this paper, we provide an efficient approximation algorithm for finding the most likelihood configuration (MAP) of size
k
k
for Determinantal Point Processes (DPP) in the online setting where the data points arrive in an arbitrary order and the algorithm cannot discard the selected elements from its local memory. Given a tolerance additive error
η
η
, our \online algorithm achieves a
k
O
(
k
)
k
multiplicative approximation guarantee with an additive error
η
η
, using a memory footprint independent of the size of the data stream. We note that the exponential dependence on
k
k
in the approximation factor is unavoidable even in the offline setting. Our result readily implies a streaming algorithm with an improved memory bound compared to existing results."
neurips,https://proceedings.neurips.cc/paper/2020/file/234833147b97bb6aed53a8f4f1c7a7d8-Paper.pdf,Video Object Segmentation with Adaptive Feature Bank and Uncertain-Region Refinement,"Yongqing Liang, Xin Li, Navid Jafari, Jim Chen",
neurips,https://proceedings.neurips.cc/paper/2020/file/234b941e88b755b7a72a1c1dd5022f30-Paper.pdf,Inferring learning rules from animal decision-making,"Zoe Ashwood, Nicholas A. Roy, Ji Hyun Bak, Jonathan W. Pillow",
neurips,https://proceedings.neurips.cc/paper/2020/file/234e691320c0ad5b45ee3c96d0d7b8f8-Paper.pdf,Input-Aware Dynamic Backdoor Attack,"Tuan Anh Nguyen, Anh Tran",
neurips,https://proceedings.neurips.cc/paper/2020/file/23685a2431acad7789c1e3d43ea1522c-Paper.pdf,How hard is to distinguish graphs with graph neural networks?,Andreas Loukas,
neurips,https://proceedings.neurips.cc/paper/2020/file/236f119f58f5fd102c5a2ca609fdcbd8-Paper.pdf,Minimax Regret of Switching-Constrained Online Convex Optimization: No Phase Transition,"Lin Chen, Qian Yu, Hannah Lawrence, Amin Karbasi","We study the problem of switching-constrained online convex optimization (OCO), where the player has a limited number of opportunities to change her action. While the discrete analog of this online learning task has been studied extensively, previous work in the continuous setting has neither established the minimax rate nor algorithmically achieved it. In this paper, we show that
T
T
-round switching-constrained OCO with fewer than
K
K
switches has a minimax regret of
Θ
(
T
√
K
)
Θ
. In particular, it is at least
T
√
2
K
T
for one dimension and at least
T
√
K
T
for higher dimensions. The lower bound in higher dimensions is attained by an orthogonal subspace argument. In one dimension, a novel adversarial strategy yields the lower bound of
O
(
T
√
K
)
O
, but a precise minimax analysis including constants is more involved. To establish the tighter one-dimensional result, we introduce the \emph{fugal game} relaxation, whose minimax regret lower bounds that of switching-constrained OCO. We show that the minimax regret of the fugal game is at least
T
√
2
K
T
and thereby establish the optimal minimax lower bound in one dimension. To establish the dimension-independent upper bound, we next show that a mini-batching algorithm provides an
O
(
T
√
K
)
O
upper bound, and therefore conclude that the minimax regret of switching-constrained OCO is
Θ
(
T
√
K
)
Θ
for any
K
K
. This is in sharp contrast to its discrete counterpart, the switching-constrained prediction-from-experts problem, which exhibits a phase transition in minimax regret between the low-switching and high-switching regimes."
neurips,https://proceedings.neurips.cc/paper/2020/file/23937b42f9273974570fb5a56a6652ee-Paper.pdf,Dual Manifold Adversarial Robustness: Defense against Lp and non-Lp Adversarial Attacks,"Wei-An Lin, Chun Pong Lau, Alexander Levine, Rama Chellappa, Soheil Feizi",
neurips,https://proceedings.neurips.cc/paper/2020/file/23ad3e314e2a2b43b4c720507cec0723-Paper.pdf,Cross-Scale Internal Graph Neural Network for Image Super-Resolution,"Shangchen Zhou, Jiawei Zhang, Wangmeng Zuo, Chen Change Loy",
neurips,https://proceedings.neurips.cc/paper/2020/file/23af4b45f1e166141a790d1a3126e77a-Paper.pdf,Unsupervised Representation Learning by Invariance Propagation,"Feng Wang, Huaping Liu, Di Guo, Sun Fuchun",
neurips,https://proceedings.neurips.cc/paper/2020/file/240ac9371ec2671ae99847c3ae2e6384-Paper.pdf,Restoring Negative Information in Few-Shot Object Detection,"Yukuan Yang, Fangyun Wei, Miaojing Shi, Guoqi Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/24357dd085d2c4b1a88a7e0692e60294-Paper.pdf,Do Adversarially Robust ImageNet Models Transfer Better?,"Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, Aleksander Madry",
neurips,https://proceedings.neurips.cc/paper/2020/file/24368c745de15b3d2d6279667debcba3-Paper.pdf,Robust Correction of Sampling Bias using Cumulative Distribution Functions,"Bijan Mazaheri, Siddharth Jain, Jehoshua Bruck",
neurips,https://proceedings.neurips.cc/paper/2020/file/24389bfe4fe2eba8bf9aa9203a44cdad-Paper.pdf,Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach,"Alireza Fallah, Aryan Mokhtari, Asuman Ozdaglar",
neurips,https://proceedings.neurips.cc/paper/2020/file/243be2818a23c980ad664f30f48e5d19-Paper.pdf,Pixel-Level Cycle Association: A New Perspective for Domain Adaptive Semantic Segmentation,"Guoliang Kang, Yunchao Wei, Yi Yang, Yueting Zhuang, Alexander Hauptmann","Domain adaptive semantic segmentation aims to train a model performing satisfactory pixel-level predictions on the target with only out-of-domain (source) annotations. The conventional solution to this task is to minimize the discrepancy between source and target to enable effective knowledge transfer. Previous domain discrepancy minimization methods are mainly based on the adversarial training. They tend to consider the domain discrepancy globally, which ignore the pixel-wise relationships and are less discriminative. In this paper, we propose to build the pixel-level cycle association between source and target pixel pairs and contrastively strengthen their connections to diminish the domain gap and make the features more discriminative. To the best of our knowledge, this is a new perspective for tackling such a challenging task. Experiment results on two representative domain adaptation benchmarks, i.e. GTAV
→
→
Cityscapes and SYNTHIA
→
→
Cityscapes, verify the effectiveness of our proposed method and demonstrate that our method performs favorably against previous state-of-the-arts. Our method can be trained end-to-end in one stage and introduce no additional parameters, which is expected to serve as a general framework and help ease future research in domain adaptive semantic segmentation. Code is available at https://github.com/kgl-prml/Pixel-Level-Cycle-Association."
neurips,https://proceedings.neurips.cc/paper/2020/file/244edd7e85dc81602b7615cd705545f5-Paper.pdf,Classification with Valid and Adaptive Coverage,"Yaniv Romano, Matteo Sesia, Emmanuel Candes",
neurips,https://proceedings.neurips.cc/paper/2020/file/24aef8cb3281a2422a59b51659f1ad2e-Paper.pdf,Learning Global Transparent Models consistent with Local Contrastive Explanations,"Tejaswini Pedapati, Avinash Balakrishnan, Karthikeyan Shanmugam, Amit Dhurandhar",
neurips,https://proceedings.neurips.cc/paper/2020/file/24bcb4d0caa4120575bb45c8a156b651-Paper.pdf,Learning to Approximate a Bregman Divergence,"Ali Siahkamari, XIDE XIA, Venkatesh Saligrama, David Castañón, Brian Kulis","Bregman divergences generalize measures such as the squared Euclidean distance and the KL divergence, and arise throughout many areas of machine learning. In this paper, we focus on the problem of approximating an arbitrary Bregman divergence from supervision, and we provide a well-principled approach to analyzing such approximations. We develop a formulation and algorithm for learning arbitrary Bregman divergences based on approximating their underlying convex generating function via a piecewise linear function. We provide theoretical approximation bounds using our parameterization and show that the generalization error
O
p
(
m
−
1
/
2
)
O
for metric learning using our framework matches the known generalization error in the strictly less general Mahalanobis metric learning setting. We further demonstrate empirically that our method performs well in comparison to existing metric learning methods, particularly for clustering and ranking problems."
neurips,https://proceedings.neurips.cc/paper/2020/file/24bea84d52e6a1f8025e313c2ffff50a-Paper.pdf,Diverse Image Captioning with Context-Object Split Latent Spaces,"Shweta Mahajan, Stefan Roth",
neurips,https://proceedings.neurips.cc/paper/2020/file/24f2f931f12a4d9149876a5bef93e96a-Paper.pdf,Learning Disentangled Representations of Videos with Missing Data,"Armand Comas, Chi Zhang, Zlatan Feric, Octavia Camps, Rose Yu",
neurips,https://proceedings.neurips.cc/paper/2020/file/2517756c5a9be6ac007fe9bb7fb92611-Paper.pdf,Natural Graph Networks,"Pim de Haan, Taco S. Cohen, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2020/file/258be18e31c8188555c2ff05b4d542c3-Paper.pdf,Continual Learning with Node-Importance based Adaptive Group Sparse Regularization,"Sangwon Jung, Hongjoon Ahn, Sungmin Cha, Taesup Moon",
neurips,https://proceedings.neurips.cc/paper/2020/file/25ddc0f8c9d3e22e03d3076f98d83cb2-Paper.pdf,Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts,"Max Ryabinin, Anton Gusev","Many recent breakthroughs in deep learning were achieved by training increasingly larger models on massive datasets. However, training such models can be prohibitively expensive. For instance, the cluster used to train GPT-3 costs over
250
m
i
l
l
i
o
n
.
A
s
a
r
e
s
u
l
t
,
m
o
s
t
r
e
s
e
a
r
c
h
e
r
s
c
a
n
n
o
t
a
f
f
o
r
d
t
o
t
r
a
i
n
s
t
a
t
e
o
f
t
h
e
a
r
t
m
o
d
e
l
s
a
n
d
c
o
n
t
r
i
b
u
t
e
t
o
t
h
e
i
r
d
e
v
e
l
o
p
m
e
n
t
.
H
y
p
o
t
h
e
t
i
c
a
l
l
y
,
a
r
e
s
e
a
r
c
h
e
r
c
o
u
l
d
c
r
o
w
d
s
o
u
r
c
e
t
h
e
t
r
a
i
n
i
n
g
o
f
l
a
r
g
e
n
e
u
r
a
l
n
e
t
w
o
r
k
s
w
i
t
h
t
h
o
u
s
a
n
d
s
o
f
r
e
g
u
l
a
r
P
C
s
p
r
o
v
i
d
e
d
b
y
v
o
l
u
n
t
e
e
r
s
.
T
h
e
r
a
w
c
o
m
p
u
t
i
n
g
p
o
w
e
r
o
f
a
h
u
n
d
r
e
d
t
h
o
u
s
a
n
d
250
2500 desktops dwarfs that of a $250M server pod, but one cannot utilize that power efficiently with conventional distributed training methods. In this work, we propose Learning@home: a novel neural network training paradigm designed to handle large amounts of poorly connected participants. We analyze the performance, reliability, and architectural constraints of this paradigm and compare it against existing distributed training techniques."
neurips,https://proceedings.neurips.cc/paper/2020/file/26178fc759d2b89c45dd31962f81dc61-Paper.pdf,Bidirectional Convolutional Poisson Gamma Dynamical Systems,"wenchao chen, Chaojie Wang, Bo Chen, Yicheng Liu, Hao Zhang, Mingyuan Zhou",
neurips,https://proceedings.neurips.cc/paper/2020/file/26588e932c7ccfa1df309280702fe1b5-Paper.pdf,Deep Reinforcement and InfoMax Learning,"Bogdan Mazoure, Remi Tachet des Combes, Thang Long Doan, Philip Bachman, R Devon Hjelm",
neurips,https://proceedings.neurips.cc/paper/2020/file/26b58a41da329e0cbde0cbf956640a58-Paper.pdf,On ranking via sorting by estimated expected utility,"Clement Calauzenes, Nicolas Usunier",
neurips,https://proceedings.neurips.cc/paper/2020/file/26d88423fc6da243ffddf161ca712757-Paper.pdf,"Distribution-free binary classification: prediction sets, confidence intervals and calibration","Chirag Gupta, Aleksandr Podkopaev, Aaditya Ramdas",
neurips,https://proceedings.neurips.cc/paper/2020/file/26ed695e9b7b9f6463ef4bc1fd74fc87-Paper.pdf,Closing the Dequantization Gap: PixelCNN as a Single-Layer Flow,"Didrik Nielsen, Ole Winther",
neurips,https://proceedings.neurips.cc/paper/2020/file/27059a11c58ade9b03bde05c2ca7c285-Paper.pdf,Sequence to Multi-Sequence Learning via Conditional Chain Mapping for Mixture Signals,"Jing Shi, Xuankai Chang, Pengcheng Guo, Shinji Watanabe, Yusuke Fujita, Jiaming Xu, Bo Xu, Lei Xie",
neurips,https://proceedings.neurips.cc/paper/2020/file/272e11700558e27be60f7489d2d782e7-Paper.pdf,Variance reduction for Random Coordinate Descent-Langevin Monte Carlo,"ZHIYAN DING, Qin Li","A natural strategy to reduce computational cost in each iteration is to utilize random gradient approximations, such as random coordinate descent (RCD) or simultaneous perturbation stochastic approximation (SPSA).We show by a counterexamplethat blindly applying RCD does not achieve the goal in the most general setting. The high variance induced by the randomness means a larger number of iterations are needed, and this balances out the saving in each iteration."
neurips,https://proceedings.neurips.cc/paper/2020/file/274e6fcf4a583de4a81c6376f17673e7-Paper.pdf,Language as a Cognitive Tool to Imagine Goals in Curiosity Driven Exploration,"Cédric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clément Moulin-Frier, Peter Dominey, Pierre-Yves Oudeyer",
neurips,https://proceedings.neurips.cc/paper/2020/file/275d7fb2fd45098ad5c3ece2ed4a2824-Paper.pdf,All Word Embeddings from One Embedding,"Sho Takase, Sosuke Kobayashi",
neurips,https://proceedings.neurips.cc/paper/2020/file/2779fda014fbadb761f67dd708c1325e-Paper.pdf,Primal Dual Interpretation of the Proximal Stochastic Gradient Langevin Algorithm,"Adil Salim, Peter Richtarik","We consider the task of sampling with respect to a log concave probability distribution. The potential of the target distribution is assumed to be composite, i.e., written as the sum of a smooth convex term, and a nonsmooth convex term possibly taking infinite values. The target distribution can be seen as a minimizer of the Kullback-Leibler divergence defined on the Wasserstein space (i.e., the space of probability measures). In the first part of this paper, we establish a strong duality result for this minimization problem. In the second part of this paper, we use the duality gap arising from the first part to study the complexity of the Proximal Stochastic Gradient Langevin Algorithm (PSGLA), which can be seen as a generalization of the Projected Langevin Algorithm. Our approach relies on viewing PSGLA as a primal dual algorithm and covers many cases where the target distribution is not fully supported. In particular, we show that if the potential is strongly convex, the complexity of PSGLA is
\cO
(
1
/
ε
2
)
\cO
in terms of the 2-Wasserstein distance. In contrast, the complexity of the Projected Langevin Algorithm is
\cO
(
1
/
ε
12
)
\cO
in terms of total variation when the potential is convex."
neurips,https://proceedings.neurips.cc/paper/2020/file/2794f6a20ee0685f4006210f40799acd-Paper.pdf,How to Characterize The Landscape of Overparameterized Convolutional Neural Networks,"Yihong Gu, Weizhong Zhang, Cong Fang, Jason D. Lee, Tong Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/27b587bbe83aecf9a98c8fe6ab48cacc-Paper.pdf,On the Tightness of Semidefinite Relaxations for Certifying Robustness to Adversarial Examples,Richard Zhang,
neurips,https://proceedings.neurips.cc/paper/2020/file/27d8d40b22f812a1ba6c26f8ef7df480-Paper.pdf,Submodular Meta-Learning,"Arman Adibi, Aryan Mokhtari, Hamed Hassani",
neurips,https://proceedings.neurips.cc/paper/2020/file/27e9661e033a73a6ad8cefcde965c54d-Paper.pdf,Rethinking Pre-training and Self-training,"Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, Quoc Le",
neurips,https://proceedings.neurips.cc/paper/2020/file/28538c394c36e4d5ea8ff5ad60562a93-Paper.pdf,Unsupervised Sound Separation Using Mixture Invariant Training,"Scott Wisdom, Efthymios Tzinis, Hakan Erdogan, Ron Weiss, Kevin Wilson, John Hershey",
neurips,https://proceedings.neurips.cc/paper/2020/file/285baacbdf8fda1de94b19282acd23e2-Paper.pdf,Adaptive Discretization for Model-Based Reinforcement Learning,"Sean Sinclair, Tianyu Wang, Gauri Jain, Siddhartha Banerjee, Christina Yu","From an implementation standpoint, our algorithm has much lower storage and computational requirements due to maintaining a more efficient partition of the state and action spaces. We illustrate this via experiments on several canonical control problems, which shows that our algorithm empirically performs significantly better than fixed discretization in terms of both faster convergence and lower memory usage. Interestingly, we observe empirically that while fixed discretization model-based algorithms vastly outperform their model-free counterparts, the two achieve comparable performance with adaptive discretization."
neurips,https://proceedings.neurips.cc/paper/2020/file/285f89b802bcb2651801455c86d78f2a-Paper.pdf,CodeCMR: Cross-Modal Retrieval For Function-Level Binary Source Code Matching,"Zeping Yu, Wenxin Zheng, Jiaqi Wang, Qiyi Tang, Sen Nie, Shi Wu",
neurips,https://proceedings.neurips.cc/paper/2020/file/288cd2567953f06e460a33951f55daaf-Paper.pdf,On Warm-Starting Neural Network Training,"Jordan Ash, Ryan P. Adams",
neurips,https://proceedings.neurips.cc/paper/2020/file/28a7602724ba16600d5ccc644c19bf18-Paper.pdf,DAGs with No Fears: A Closer Look at Continuous Optimization for Learning Bayesian Networks,"Dennis Wei, Tian Gao, Yue Yu",
neurips,https://proceedings.neurips.cc/paper/2020/file/28e209b61a52482a0ae1cb9f5959c792-Paper.pdf,OOD-MAML: Meta-Learning for Few-Shot Out-of-Distribution Detection and Classification,"Taewon Jeong, Heeyoung Kim",
neurips,https://proceedings.neurips.cc/paper/2020/file/28f248e9279ac845995c4e9f8af35c2b-Paper.pdf,An Imitation from Observation Approach to Transfer Learning with Dynamics Mismatch,"Siddharth Desai, Ishan Durugkar, Haresh Karnan, Garrett Warnell, Josiah Hanna, Peter Stone",
neurips,https://proceedings.neurips.cc/paper/2020/file/291597a100aadd814d197af4f4bab3a7-Paper.pdf,Learning About Objects by Learning to Interact with Them,"Martin Lohmann, Jordi Salvador, Aniruddha Kembhavi, Roozbeh Mottaghi",
neurips,https://proceedings.neurips.cc/paper/2020/file/291dbc18539ba7e19b8abb7d85aa204e-Paper.pdf,Learning discrete distributions with infinite support,"Doron Cohen, Aryeh Kontorovich, Geoﬀrey Wolfer",
neurips,https://proceedings.neurips.cc/paper/2020/file/293835c2cc75b585649498ee74b395f5-Paper.pdf,Dissecting Neural ODEs,"Stefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita, Hajime Asama",
neurips,https://proceedings.neurips.cc/paper/2020/file/29405e2a4c22866a205f557559c7fa4b-Paper.pdf,Teaching a GAN What Not to Learn,"Siddarth Asokan, Chandra Seelamantula",
neurips,https://proceedings.neurips.cc/paper/2020/file/294e09f267683c7ddc6cc5134a7e68a8-Paper.pdf,Counterfactual Data Augmentation using Locally Factored Dynamics,"Silviu Pitis, Elliot Creager, Animesh Garg",
neurips,https://proceedings.neurips.cc/paper/2020/file/2952351097998ac1240cb2ab7333a3d2-Paper.pdf,Rethinking Learnable Tree Filter for Generic Feature Transform,"Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, Nanning Zheng",
neurips,https://proceedings.neurips.cc/paper/2020/file/29539ed932d32f1c56324cded92c07c2-Paper.pdf,Self-Supervised Relational Reasoning for Representation Learning,"Massimiliano Patacchiola, Amos J. Storkey",
neurips,https://proceedings.neurips.cc/paper/2020/file/29586cb449c90e249f1f09a0a4ee245a-Paper.pdf,Sufficient dimension reduction for classification using principal optimal transport direction,"Cheng Meng, Jun Yu, Jingyi Zhang, Ping Ma, Wenxuan Zhong",
neurips,https://proceedings.neurips.cc/paper/2020/file/2974788b53f73e7950e8aa49f3a306db-Paper.pdf,Fast Epigraphical Projection-based Incremental Algorithms for Wasserstein Distributionally Robust Support Vector Machine,"Jiajin Li, Caihua Chen, Anthony Man-Cho So",
neurips,https://proceedings.neurips.cc/paper/2020/file/299dc35e747eb77177d9cea10a802da2-Paper.pdf,Differentially Private Clustering: Tight Approximation Ratios,"Badih Ghazi, Ravi Kumar, Pasin Manurangsi","Our results also imply an improved algorithm for the Sample and Aggregate privacy framework. Furthermore, we show that one of the tools used in our 1-Cluster algorithm can be employed to get a faster quantum algorithm for ClosestPair in a moderate number of dimensions."
neurips,https://proceedings.neurips.cc/paper/2020/file/29a6aa8af3c942a277478a90aa4cae21-Paper.pdf,On the Power of Louvain in the Stochastic Block Model,"Vincent Cohen-Addad, Adrian Kosowski, Frederik Mallmann-Trenn, David Saulpic","In practice, the most popular approaches rely on local search algorithms; not only for the ease of implementation and the efficiency, but also because of the accuracy of these methods on many real world graphs. For example, the Louvain algorithm -- a local search based algorithm -- has quickly become the method of choice for clustering in social networks. However, explaining the success of these methods remains an open problem: in the worst-case, the runtime can be up to \Omega(n^2), much worse than what is typically observed in practice, and no guarantee on the quality of its output can be established."
neurips,https://proceedings.neurips.cc/paper/2020/file/29c0605a3bab4229e46723f89cf59d83-Paper.pdf,Fairness with Overlapping Groups; a Probabilistic Perspective,"Forest Yang, Mouhamadou Cisse, Sanmi Koyejo",
neurips,https://proceedings.neurips.cc/paper/2020/file/29e48b79ae6fc68e9b6480b677453586-Paper.pdf,AttendLight: Universal Attention-Based Reinforcement Learning Model for Traffic Signal Control,"Afshin Oroojlooy, Mohammadreza Nazari, Davood Hajinezhad, Jorge Silva",
neurips,https://proceedings.neurips.cc/paper/2020/file/2a084e55c87b1ebcdaad1f62fdbbac8e-Paper.pdf,Searching for Low-Bit Weights in Quantized Neural Networks,"Zhaohui Yang, Yunhe Wang, Kai Han, Chunjing XU, Chao Xu, Dacheng Tao, Chang Xu",
neurips,https://proceedings.neurips.cc/paper/2020/file/2a27b8144ac02f67687f76782a3b5d8f-Paper.pdf,Adaptive Reduced Rank Regression,"Qiong Wu, Felix MF Wong, Yanhua Li, Zhenming Liu, Varun Kanade",
neurips,https://proceedings.neurips.cc/paper/2020/file/2adcfc3929e7c03fac3100d3ad51da26-Paper.pdf,From Predictions to Decisions: Using Lookahead Regularization,"Nir Rosenfeld, Anna Hilgard, Sai Srivatsa Ravindranath, David C. Parkes",
neurips,https://proceedings.neurips.cc/paper/2020/file/2adee8815dd939548ee6b2772524b6f2-Paper.pdf,Sequential Bayesian Experimental Design with Variable Cost Structure,"Sue Zheng, David Hayden, Jason Pacheco, John W. Fisher III",
neurips,https://proceedings.neurips.cc/paper/2020/file/2b346a0aa375a07f5a90a344a61416c4-Paper.pdf,Predictive inference is free with the jackknife+-after-bootstrap,"Byol Kim, Chen Xu, Rina Barber",
neurips,https://proceedings.neurips.cc/paper/2020/file/2b64c2f19d868305aa8bbc2d72902cc5-Paper.pdf,Counterfactual Predictions under Runtime Confounding,"Amanda Coston, Edward Kennedy, Alexandra Chouldechova",
neurips,https://proceedings.neurips.cc/paper/2020/file/2ba596643cbbbc20318224181fa46b28-Paper.pdf,Learning Loss for Test-Time Augmentation,"Ildoo Kim, Younghoon Kim, Sungwoong Kim",
neurips,https://proceedings.neurips.cc/paper/2020/file/2ba61cc3a8f44143e1f2f13b2b729ab3-Paper.pdf,Balanced Meta-Softmax for Long-Tailed Visual Recognition,"Jiawei Ren, Cunjun Yu, shunan sheng, Xiao Ma, Haiyu Zhao, Shuai Yi, hongsheng Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/2bba9f4124283edd644799e0cecd45ca-Paper.pdf,Efficient Exploration of Reward Functions in Inverse Reinforcement Learning via Bayesian Optimization,"Sreejith Balakrishnan, Quoc Phong Nguyen, Bryan Kian Hsiang Low, Harold Soh",
neurips,https://proceedings.neurips.cc/paper/2020/file/2be5f9c2e3620eb73c2972d7552b6cb5-Paper.pdf,MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning,"Elise van der Pol, Daniel Worrall, Herke van Hoof, Frans Oliehoek, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2020/file/2c29d89cc56cdb191c60db2f0bae796b-Paper.pdf,How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods,"Jeya Vikranth Jeyakumar, Joseph Noor, Yu-Hsi Cheng, Luis Garcia, Mani Srivastava",
neurips,https://proceedings.neurips.cc/paper/2020/file/2c5201a7391fedbc40c3cc6aa057a029-Paper.pdf,On the Error Resistance of Hinge-Loss Minimization,Kunal Talwar,"Commonly used classification algorithms in machine learning, such as support vector machines, minimize a convex surrogate loss on training examples. In practice, these algorithms are surprisingly robust to errors in the training data. In this work, we identify a set of conditions on the data under which such surrogate loss minimization algorithms provably learn the correct classifier. This allows us to establish, in a unified framework, the robustness of these algorithms under various models on data as well as error. In particular, we show that if the data is linearly classifiable with a slightly non-trivial margin (i.e. a margin at least
C
÷
√
d
C
for
d
d
-dimensional unit vectors), and the class-conditional distributions are near isotropic and logconcave, then surrogate loss minimization has negligible error on the uncorrupted data even when a constant fraction of examples are adversarially mislabeled."
neurips,https://proceedings.neurips.cc/paper/2020/file/2c6a0bae0f071cbbf0bb3d5b11d90a82-Paper.pdf,Munchausen Reinforcement Learning,"Nino Vieillard, Olivier Pietquin, Matthieu Geist",
neurips,https://proceedings.neurips.cc/paper/2020/file/2c75cf2681788adaca63aa95ae028b22-Paper.pdf,Object Goal Navigation using Goal-Oriented Semantic Exploration,"Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, Russ R. Salakhutdinov",
neurips,https://proceedings.neurips.cc/paper/2020/file/2cb274e6ce940f47beb8011d8ecb1462-Paper.pdf,Efficient semidefinite-programming-based inference for binary and multi-class MRFs,"Chirag Pabbaraju, Po-Wei Wang, J. Zico Kolter",
neurips,https://proceedings.neurips.cc/paper/2020/file/2cd2915e69546904e4e5d4a2ac9e1652-Paper.pdf,Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing,"Zihang Dai, Guokun Lai, Yiming Yang, Quoc Le",
neurips,https://proceedings.neurips.cc/paper/2020/file/2cd4e8a2ce081c3d7c32c3cde4312ef7-Paper.pdf,Semantic Visual Navigation by Watching YouTube Videos,"Matthew Chang, Arjun Gupta, Saurabh Gupta",
neurips,https://proceedings.neurips.cc/paper/2020/file/2cfa3753d6a524711acb5fce38eeca1a-Paper.pdf,"Heavy-tailed Representations, Text Polarity Classification & Data Augmentation","Hamid Jalalzai, Pierre Colombo, Chloé Clavel, Eric Gaussier, Giovanna Varni, Emmanuel Vignon, Anne Sabourin",
neurips,https://proceedings.neurips.cc/paper/2020/file/2cfa8f9e50e0f510ede9d12338a5f564-Paper.pdf,SuperLoss: A Generic Loss for Robust Curriculum Learning,"Thibault Castells, Philippe Weinzaepfel, Jerome Revaud",
neurips,https://proceedings.neurips.cc/paper/2020/file/2d16ad1968844a4300e9a490588ff9f8-Paper.pdf,CogMol: Target-Specific and Selective Drug Design for COVID-19 Using Deep Generative Models,"Vijil Chenthamarakshan, Payel Das, Samuel Hoffman, Hendrik Strobelt, Inkit Padhi, Kar Wai Lim, Benjamin Hoover, Matteo Manica, Jannis Born, Teodoro Laino, Aleksandra Mojsilovic","The novel nature of SARS-CoV-2 calls for the development of efficient de novo drug design approaches. In this study, we propose an end-to-end framework, named CogMol (Controlled Generation of Molecules), for designing new drug-like small molecules targeting novel viral proteins with high affinity and off-target selectivity. CogMol combines adaptive pre-training of a molecular SMILES Variational Autoencoder (VAE) and an efficient multi-attribute controlled sampling scheme that uses guidance from attribute predictors trained on latent features. To generate novel and optimal drug-like molecules for unseen viral targets, CogMol leverages a protein-molecule binding affinity predictor that is trained using SMILES VAE embeddings and protein sequence embeddings learned unsupervised from a large corpus. We applied the CogMol framework to three SARS-CoV-2 target proteins: main protease, receptor-binding domain of the spike protein, and non-structural protein 9 replicase. The generated candidates are novel at both the molecular and chemical scaffold levels when compared to the training data. CogMol also includes insilico screening for assessing toxicity of parent molecules and their metabolites with a multi-task toxicity classifier, synthetic feasibility with a chemical retrosynthesis predictor, and target structure binding with docking simulations. Docking reveals favorable binding of generated molecules to the target protein structure, where 87--95\% of high affinity molecules showed docking free energy
<
<
-6 kcal/mol. When compared to approved drugs, the majority of designed compounds show low predicted parent molecule and metabolite toxicity and high predicted synthetic feasibility. In summary, CogMol can handle multi-constraint design of synthesizable, low-toxic, drug-like molecules with high target specificity and selectivity, even to novel protein target sequences, and does not need target-dependent fine-tuning of the framework or target structure information."
neurips,https://proceedings.neurips.cc/paper/2020/file/2df45244f09369e16ea3f9117ca45157-Paper.pdf,Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards,"Yijie Guo, Jongwook Choi, Marcin Moczulski, Shengyu Feng, Samy Bengio, Mohammad Norouzi, Honglak Lee",
neurips,https://proceedings.neurips.cc/paper/2020/file/2dfe1946b3003933b7f8ddd71f24dbb1-Paper.pdf,Liberty or Depth: Deep Bayesian Neural Nets Do Not Need Complex Weight Posterior Approximations,"Sebastian Farquhar, Lewis Smith, Yarin Gal",
neurips,https://proceedings.neurips.cc/paper/2020/file/2e1b24a664f5e9c18f407b2f9c73e821-Paper.pdf,Improving Sample Complexity Bounds for (Natural) Actor-Critic Algorithms,"Tengyu Xu, Zhe Wang, Yingbin Liang","The actor-critic (AC) algorithm is a popular method to find an optimal policy in reinforcement learning. In the infinite horizon scenario, the finite-sample convergence rate for the AC and natural actor-critic (NAC) algorithms has been established recently, but under independent and identically distributed (i.i.d.) sampling and single-sample update at each iteration. In contrast, this paper characterizes the convergence rate and sample complexity of AC and NAC under Markovian sampling, with mini-batch data for each iteration, and with actor having general policy class approximation. We show that the overall sample complexity for a mini-batch AC to attain an
ϵ
ϵ
-accurate stationary point improves the best known sample complexity of AC by an order of
O
(
ϵ
−
1
log
(
1
/
ϵ
)
)
O
, and the overall sample complexity for a mini-batch NAC to attain an
ϵ
ϵ
-accurate globally optimal point improves the existing sample complexity of NAC by an order of
O
(
ϵ
−
2
/
log
(
1
/
ϵ
)
)
O
. Moreover, the sample complexity of AC and NAC characterized in this work outperforms that of policy gradient (PG) and natural policy gradient (NPG) by a factor of
O
(
(
1
−
γ
)
−
3
)
O
and
O
(
(
1
−
γ
)
−
4
ϵ
−
2
/
log
(
1
/
ϵ
)
)
O
, respectively. This is the first theoretical study establishing that AC and NAC attain orderwise performance improvement over PG and NPG under infinite horizon due to the incorporation of critic."
neurips,https://proceedings.neurips.cc/paper/2020/file/2e255d2d6bf9bb33030246d31f1a79ca-Paper.pdf,Learning Differential Equations that are Easy to Solve,"Jacob Kelly, Jesse Bettencourt, Matthew J. Johnson, David K. Duvenaud",
neurips,https://proceedings.neurips.cc/paper/2020/file/2e2c4bf7ceaa4712a72dd5ee136dc9a8-Paper.pdf,Stability of Stochastic Gradient Descent on Nonsmooth Convex Losses,"Raef Bassily, Vitaly Feldman, Cristóbal Guzmán, Kunal Talwar","Our work is the first to address uniform stability of SGD on nonsmooth convex losses. Specifically, we provide sharp upper and lower bounds for several forms of SGD and full-batch GD on arbitrary Lipschitz nonsmooth convex losses. Our lower bounds show that, in the nonsmooth case, (S)GD can be inherently less stable than in the smooth case. On the other hand, our upper bounds show that (S)GD is sufficiently stable for deriving new and useful bounds on generalization error. Most notably, we obtain the first dimension-independent generalization bounds for multi-pass SGD in the nonsmooth case. In addition, our bound allow us to derive a new algorithm for differentially private nonsmooth stochastic convex optimization with optimal excess population risk. Our algorithm is simpler and more efficient than the best known algorithm for the nonsmooth case, due to Feldman et al. [2020]."
neurips,https://proceedings.neurips.cc/paper/2020/file/2e6d9c6052e99fcdfa61d9b9da273ca2-Paper.pdf,Influence-Augmented Online Planning for Complex Environments,"Jinke He, Miguel Suau de Castro, Frans Oliehoek",
neurips,https://proceedings.neurips.cc/paper/2020/file/2e85d72295b67c5b649290dfbf019285-Paper.pdf,PAC-Bayes Learning Bounds for Sample-Dependent Priors,"Pranjal Awasthi, Satyen Kale, Stefani Karp, Mehryar Mohri",
neurips,https://proceedings.neurips.cc/paper/2020/file/2f10c1578a0706e06b6d7db6f0b4a6af-Paper.pdf,Reward-rational (implicit) choice: A unifying formalism for reward learning,"Hong Jun Jeon, Smitha Milli, Anca Dragan",
neurips,https://proceedings.neurips.cc/paper/2020/file/2f2b265625d76a6704b08093c652fd79-Paper.pdf,Probabilistic Time Series Forecasting with Shape and Temporal Diversity,"Vincent LE GUEN, Nicolas THOME",
neurips,https://proceedings.neurips.cc/paper/2020/file/2f380b99d45812a211da102c04dc1ddb-Paper.pdf,Low Distortion Block-Resampling with Spatially Stochastic Networks,"Sarah Hong, Martin Arjovsky, Darryl Barnhart, Ian Thompson",
neurips,https://proceedings.neurips.cc/paper/2020/file/2f3bbb9730639e9ea48f309d9a79ff01-Paper.pdf,Continual Deep Learning by Functional Regularisation of Memorable Past,"Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard Turner, Mohammad Emtiyaz E. Khan",
neurips,https://proceedings.neurips.cc/paper/2020/file/2f73168bf3656f697507752ec592c437-Paper.pdf,Distance Encoding: Design Provably More Powerful Neural Networks for Graph Representation Learning,"Pan Li, Yanbang Wang, Hongwei Wang, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2020/file/2fd5d41ec6cfab47e32164d5624269b1-Paper.pdf,Fast Fourier Convolution,"Lu Chi, Borui Jiang, Yadong Mu",
neurips,https://proceedings.neurips.cc/paper/2020/file/3000311ca56a1cb93397bc676c0b7fff-Paper.pdf,Unsupervised Learning of Dense Visual Representations,"Pedro O. O. Pinheiro, Amjad Almahairi, Ryan Benmalek, Florian Golemo, Aaron C. Courville",
neurips,https://proceedings.neurips.cc/paper/2020/file/300891a62162b960cf02ce3827bb363c-Paper.pdf,Higher-Order Certification For Randomized Smoothing,"Jeet Mohapatra, Ching-Yun Ko, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, Luca Daniel","Randomized smoothing is a recently proposed defense against adversarial attacks that has achieved state-of-the-art provable robustness against
ℓ
2
ℓ
perturbations. A number of works have extended the guarantees to other metrics, such as
ℓ
1
ℓ
or
ℓ
∞
ℓ
, by using different smoothing measures. Although the current framework has been shown to yield near-optimal
ℓ
p
ℓ
radii, the total safety region certified by the current framework can be arbitrarily small compared to the optimal. In this work, we propose a framework to improve the certified safety region for these smoothed classifiers without changing the underlying smoothing scheme. The theoretical contributions are as follows: 1) We generalize the certification for randomized smoothing by reformulating certified radius calculation as a nested optimization problem over a class of functions. 2) We provide a method to calculate the certified safety region using zeroth-order and first-order information for Gaussian-smoothed classifiers. We also provide a framework that generalizes the calculation for certification using higher-order information. 3) We design efficient, high-confidence estimators for the relevant statistics of the first-order information. Combining the theoretical contribution 2) and 3) allows us to certify safety region that are significantly larger than ones provided by the current methods. On CIFAR and Imagenet, the new regions achieve significant improvements on general
ℓ
1
ℓ
certified radii and on the
ℓ
2
ℓ
certified radii for color-space attacks (
ℓ
2
ℓ
perturbation restricted to only one color/channel) while also achieving smaller improvements on the general
ℓ
2
ℓ
certified radii. As discussed in the future works section, our framework can also provide a way to circumvent the current impossibility results on achieving higher magnitudes of certified radii without requiring the use of data-dependent smoothing techniques."
neurips,https://proceedings.neurips.cc/paper/2020/file/305ddad049f65a2c241dbb6e6f746c54-Paper.pdf,Learning Structured Distributions From Untrusted Batches: Faster and Simpler,"Sitan Chen, Jerry Li, Ankur Moitra","In this paper, we find an appealing way to synthesize the techniques of [JO19] and [CLM19] to give the best of both worlds: an algorithm which runs in polynomial time and can exploit structure in the underlying distribution to achieve sublinear sample complexity. Along the way, we simplify the approach of [JO19] by avoiding the need for SDP rounding and giving a more direct interpretation of it through the lens of soft filtering, a powerful recent technique in high-dimensional robust estimation. We validate the usefulness of our algorithms in preliminary experimental evaluations."
neurips,https://proceedings.neurips.cc/paper/2020/file/309fee4e541e51de2e41f21bebb342aa-Paper.pdf,Hierarchical Quantized Autoencoders,"Will Williams, Sam Ringer, Tom Ash, David MacLeod, Jamie Dougherty, John Hughes",
neurips,https://proceedings.neurips.cc/paper/2020/file/30da227c6b5b9e2482b6b221c711edfd-Paper.pdf,Diversity can be Transferred: Output Diversification for White- and Black-box Attacks,"Yusuke Tashiro, Yang Song, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2020/file/30de24287a6d8f07b37c716ad51623a7-Paper.pdf,POLY-HOOT: Monte-Carlo Planning in Continuous Space MDPs with Non-Asymptotic Analysis,"Weichao Mao, Kaiqing Zhang, Qiaomin Xie, Tamer Basar",
neurips,https://proceedings.neurips.cc/paper/2020/file/30de9ece7cf3790c8c39ccff1a044209-Paper.pdf,AvE: Assistance via Empowerment,"Yuqing Du, Stas Tiomkin, Emre Kiciman, Daniel Polani, Pieter Abbeel, Anca Dragan",
neurips,https://proceedings.neurips.cc/paper/2020/file/30ee748d38e21392de740e2f9dc686b6-Paper.pdf,Variational Policy Gradient Method for Reinforcement Learning with General Utilities,"Junyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, Mengdi Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/30f0641c041f03d94e95a76b9d8bd58f-Paper.pdf,Reverse-engineering recurrent neural network solutions to a hierarchical inference task for mice,"Rylan Schaeffer, Mikail Khona, Leenoy Meshulam, Brain Laboratory International, Ila Fiete",
neurips,https://proceedings.neurips.cc/paper/2020/file/310614fca8fb8e5491295336298c340f-Paper.pdf,Temporal Positive-unlabeled Learning for Biomedical Hypothesis Generation via Risk Estimation,"Uchenna Akujuobi, Jun Chen, Mohamed Elhoseiny, Michael Spranger, Xiangliang Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/310cc7ca5a76a446f85c1a0d641ba96d-Paper.pdf,Efficient Low Rank Gaussian Variational Inference for Neural Networks,"Marcin Tomczak, Siddharth Swaroop, Richard Turner",
neurips,https://proceedings.neurips.cc/paper/2020/file/313f422ac583444ba6045cd122653b0e-Paper.pdf,Privacy Amplification via Random Check-Ins,"Borja Balle, Peter Kairouz, Brendan McMahan, Om Thakkar, Abhradeep Guha Thakurta",
neurips,https://proceedings.neurips.cc/paper/2020/file/31784d9fc1fa0d25d04eae50ac9bf787-Paper.pdf,Probabilistic Circuits for Variational Inference in Discrete Graphical Models,"Andy Shih, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2020/file/3181d59d19e76e902666df5c7821259a-Paper.pdf,Your Classifier can Secretly Suffice Multi-Source Domain Adaptation,"Naveen Venkat, Jogendra Nath Kundu, Durgesh Singh, Ambareesh Revanur, Venkatesh Babu R",
neurips,https://proceedings.neurips.cc/paper/2020/file/31fefc0e570cb3860f2a6d4b38c6490d-Paper.pdf,Labelling unlabelled videos from scratch with multi-modal self-supervision,"Yuki Asano, Mandela Patrick, Christian Rupprecht, Andrea Vedaldi",
neurips,https://proceedings.neurips.cc/paper/2020/file/3202111cf90e7c816a472aaceb72b0df-Paper.pdf,A Non-Asymptotic Analysis for Stein Variational Gradient Descent,"Anna Korba, Adil Salim, Michael Arbel, Giulia Luise, Arthur Gretton","We study the Stein Variational Gradient Descent (SVGD) algorithm, which optimises a set of particles to approximate a target probability distribution
π
∝
e
−
V
π
on
\R
d
\R
. In the population limit, SVGD performs gradient descent in the space of probability distributions on the KL divergence with respect to
π
π
, where the gradient is smoothed through a kernel integral operator. In this paper, we provide a novel finite time analysis for the SVGD algorithm. We provide a descent lemma establishing that the algorithm decreases the objective at each iteration, and rates of convergence. We also provide a convergence result of the finite particle system corresponding to the practical implementation of SVGD to its population version."
neurips,https://proceedings.neurips.cc/paper/2020/file/3214a6d842cc69597f9edf26df552e43-Paper.pdf,Robust Meta-learning for Mixed Linear Regression with Small Batches,"Weihao Kong, Raghav Somani, Sham Kakade, Sewoong Oh","A common challenge faced in practical supervised learning, such as medical image processing and robotic interactions, is that there are plenty of tasks but each task cannot afford to collect enough labeled examples to be learned in isolation. However, by exploiting the similarities across those tasks, one can hope to overcome such data scarcity. Under a canonical scenario where each task is drawn from a mixture of
k
k
linear regressions, we study a fundamental question: can abundant small-data tasks compensate for the lack of big-data tasks? Existing second moment based approaches of \cite{2020arXiv200208936K} show that such a trade-off is efficiently achievable, with the help of medium-sized tasks with
Ω
(
k
1
/
2
)
Ω
examples each. However, this algorithm is brittle in two important scenarios. The predictions can be arbitrarily bad
(
i
)
(
even with only a few outliers in the dataset; or
(
i
i
)
(
even if the medium-sized tasks are slightly smaller with
o
(
k
1
/
2
)
o
examples each. We introduce a spectral approach that is simultaneously robust under both scenarios. To this end, we first design a novel outlier-robust principal component analysis algorithm that achieves an optimal accuracy. This is followed by a sum-of-squares algorithm to exploit the information from higher order moments. Together, this approach is robust against outliers and achieves a graceful statistical trade-off; the lack of
Ω
(
k
1
/
2
)
Ω
-size tasks can be compensated for with smaller tasks, which can now be as small as
O
(
log
k
)
O
."
neurips,https://proceedings.neurips.cc/paper/2020/file/322f62469c5e3c7dc3e58f5a4d1ea399-Paper.pdf,Bayesian Deep Learning and a Probabilistic Perspective of Generalization,"Andrew G. Wilson, Pavel Izmailov",
neurips,https://proceedings.neurips.cc/paper/2020/file/32508f53f24c46f685870a075eaaa29c-Paper.pdf,Unsupervised Learning of Object Landmarks via Self-Training Correspondence,"Dimitrios Mallis, Enrique Sanchez, Matthew Bell, Georgios Tzimiropoulos",
neurips,https://proceedings.neurips.cc/paper/2020/file/3261769be720b0fefbfffec05e9d9202-Paper.pdf,Randomized tests for high-dimensional regression: A more efficient and powerful solution,"Yue Li, Ilmun Kim, Yuting Wei","We investigate the problem of testing the global null in the high-dimensional regression models when the feature dimension
p
p
grows proportionally to the number of observations
n
n
. Despite a number of prior work studying this problem, whether there exists a test that is model-agnostic, efficient to compute and enjoys a high power, still remains unsettled. In this paper, we answer this question in the affirmative by leveraging the random projection techniques, and propose a testing procedure that blends the classical
F
F
-test with a random projection step. When combined with a systematic choice of the projection dimension, the proposed procedure is proved to be minimax optimal and, meanwhile, reduces the computation and data storage requirements. We illustrate our results in various scenarios when the underlying feature matrix exhibits an intrinsic lower dimensional structure (such as approximate low-rank or has exponential/polynomial eigen-decay), and it turns out that the proposed test achieves sharp adaptive rates. Our theoretical findings are further validated by comparisons to other state-of-the-art tests on synthetic data."
neurips,https://proceedings.neurips.cc/paper/2020/file/328e5d4c166bb340b314d457a208dc83-Paper.pdf,Learning Representations from Audio-Visual Spatial Alignment,"Pedro Morgado, Yi Li, Nuno Nvasconcelos",
neurips,https://proceedings.neurips.cc/paper/2020/file/3295c76acbf4caaed33c36b1b5fc2cb1-Paper.pdf,Generative View Synthesis: From Single-view Semantics to Novel-view Images,"Tewodros Amberbir Habtegebrial, Varun Jampani, Orazio Gallo, Didier Stricker",
neurips,https://proceedings.neurips.cc/paper/2020/file/32bb90e8976aab5298d5da10fe66f21d-Paper.pdf,Towards More Practical Adversarial Attacks on Graph Neural Networks,"Jiaqi Ma, Shuangrui Ding, Qiaozhu Mei",
neurips,https://proceedings.neurips.cc/paper/2020/file/32cfdce9631d8c7906e8e9d6e68b514b-Paper.pdf,Multi-Task Reinforcement Learning with Soft Modularization,"Ruihan Yang, Huazhe Xu, YI WU, Xiaolong Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf,Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models,"Tom Heskes, Evi Sijben, Ioan Gabriel Bucur, Tom Claassen","In this paper, we propose a novel framework for computing Shapley values that generalizes recent work that aims to circumvent the independence assumption. By employing Pearl's do-calculus, we show how these `causal' Shapley values can be derived for general causal graphs without sacrificing any of their desirable properties. Moreover, causal Shapley values enable us to separate the contribution of direct and indirect effects. We provide a practical implementation for computing causal Shapley values based on causal chain graphs when only partial information is available and illustrate their utility on a real-world example."
neurips,https://proceedings.neurips.cc/paper/2020/file/32fcc8cfe1fa4c77b5c58dafd36d1a98-Paper.pdf,"On the training dynamics of deep networks with
L
2
L
regularization","Aitor Lewkowycz, Guy Gur-Ari","We study the role of
L
2
L
regularization in deep learning, and uncover simple relations between the performance of the model, the
L
2
L
coefficient, the learning rate, and the number of training steps. These empirical relations hold when the network is overparameterized. They can be used to predict the optimal regularization parameter of a given model. In addition, based on these observations we propose a dynamical schedule for the regularization parameter that improves performance and speeds up training. We test these proposals in modern image classification settings. Finally, we show that these empirical relations can be understood theoretically in the context of infinitely wide networks. We derive the gradient flow dynamics of such networks, and compare the role of
L
2
L
regularization in this context with that of linear models."
neurips,https://proceedings.neurips.cc/paper/2020/file/331316d4efb44682092a006307b9ae3a-Paper.pdf,Improved Algorithms for Convex-Concave Minimax Optimization,"Yuanhao Wang, Jian Li","This paper studies minimax optimization problems
min
\x
max
\y
f
(
\x
,
\y
)
min
, where
f
(
\x
,
\y
)
f
is
m
\x
m
-strongly convex with respect to
\x
\x
,
m
\y
m
-strongly concave with respect to
\y
\y
and
(
L
\x
,
L
\x
\y
,
L
\y
)
(
-smooth. Zhang et al. \cite{zhang2019lower} provided the following lower bound of the gradient complexity for any first-order method:
Ω
(
√
L
\x
m
\x
+
L
2
\x
\y
m
\x
m
\y
+
L
\y
m
\y
ln
(
1
/
ϵ
)
)
.
Ω
This paper proposes a new algorithm and proved a gradient complexity bound of
\Tilde
O
(
√
L
\x
m
\x
+
L
⋅
L
\x
\y
m
\x
m
\y
+
L
\y
m
\y
ln
(
1
/
ϵ
)
)
,
\Tilde
where
L
=
max
{
L
\x
,
L
\x
\y
,
L
\y
}
L
. This improves over the best known upper bound
\Tilde
O
(
√
\nicefrac
L
2
m
\x
m
\y
ln
3
(
1
/
ϵ
)
)
\Tilde
by Lin et al. \cite{lin2020near}. Our bound achieves linear convergence rate and tighter dependency on condition numbers, especially when
L
\x
\y
≪
L
L
(i.e., the weak interaction regime). Via simple reduction, our new bound also implies improved bounds for strongly convex-concave problems and convex-concave problems. When
f
f
is quadratic, we can further improve the bound to
O
(
√
L
\x
m
\x
+
L
2
\x
\y
m
\x
m
\y
+
L
\y
m
\y
(
L
2
m
\x
m
\y
)
o
(
1
)
ln
(
1
/
ϵ
)
)
O
, which matches the lower bound up to a sub-polynomial factor."
neurips,https://proceedings.neurips.cc/paper/2020/file/3341f6f048384ec73a7ba2e77d2db48b-Paper.pdf,Deep Variational Instance Segmentation,"Jialin Yuan, Chao Chen, Fuxin Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/335cd1b90bfa4ee70b39d08a4ae0cf2d-Paper.pdf,Learning Implicit Functions for Topology-Varying Dense 3D Shape Correspondence,"Feng Liu, Xiaoming Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/339a18def9898dd60a634b2ad8fbbd58-Paper.pdf,Deep Multimodal Fusion by Channel Exchanging,"Yikai Wang, Wenbing Huang, Fuchun Sun, Tingyang Xu, Yu Rong, Junzhou Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/33a5435d4f945aa6154b31a73bab3b73-Paper.pdf,Hierarchically Organized Latent Modules for Exploratory Search in Morphogenetic Systems,"Mayalen Etcheverry, Clément Moulin-Frier, Pierre-Yves Oudeyer",
neurips,https://proceedings.neurips.cc/paper/2020/file/33a854e247155d590883b93bca53848a-Paper.pdf,AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity,"Silviu-Marian Udrescu, Andrew Tan, Jiahai Feng, Orisvaldo Neto, Tailin Wu, Max Tegmark",
neurips,https://proceedings.neurips.cc/paper/2020/file/33c5f5bff65aa05a8cd3e5d2597f44ae-Paper.pdf,Delay and Cooperation in Nonstochastic Linear Bandits,"Shinji Ito, Daisuke Hatano, Hanna Sumita, Kei Takemura, Takuro Fukunaga, Naonori Kakimura, Ken-Ichi Kawarabayashi","This paper offers a nearly optimal algorithm for online linear optimization with delayed bandit feedback. Online linear optimization with bandit feedback, or nonstochastic linear bandits, provides a generic framework for sequential decision-making problems with limited information. This framework, however, assumes that feedback can be observed just after choosing the action, and, hence, does not apply directly to many practical applications, in which the feedback can often only be obtained after a while. To cope with such situations, we consider problem settings in which the feedback can be observed
d
d
rounds after the choice of an action, and propose an algorithm for which the expected regret is
~
O
(
√
m
(
m
+
d
)
T
)
O
, ignoring logarithmic factors in
m
m
and
T
T
, where
m
m
and
T
T
denote the dimensionality of the action set and the number of rounds, respectively. This algorithm achieves nearly optimal performance, as we are able to show that arbitrary algorithms suffer the regret of
Ω
(
√
m
(
m
+
d
)
T
)
Ω
in the worst case. To develop the algorithm, we introduce a technique we refer to as \textit{distribution truncation}, which plays an essential role in bounding the regret. We also apply our approach to cooperative bandits, as studied by Cesa-Bianchi et al. [17] and Bar-On and Mansour [12], and extend their results to the linear bandits setting."
neurips,https://proceedings.neurips.cc/paper/2020/file/33cc2b872dfe481abef0f61af181dfcf-Paper.pdf,Probabilistic Orientation Estimation with Matrix Fisher Distributions,"David Mohlin, Josephine Sullivan, Gérald Bianchi",
neurips,https://proceedings.neurips.cc/paper/2020/file/33cf42b38bbcf1dd6ba6b0f0cd005328-Paper.pdf,Minimax Dynamics of Optimally Balanced Spiking Networks of Excitatory and Inhibitory Neurons,"Qianyi Li, Cengiz Pehlevan",
neurips,https://proceedings.neurips.cc/paper/2020/file/33d3b157ddc0896addfb22fa2a519097-Paper.pdf,Telescoping Density-Ratio Estimation,"Benjamin Rhodes, Kai Xu, Michael U. Gutmann",
neurips,https://proceedings.neurips.cc/paper/2020/file/33dd6dba1d56e826aac1cbf23cdcca87-Paper.pdf,Towards Deeper Graph Neural Networks with Differentiable Group Normalization,"Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, Xia Hu",
neurips,https://proceedings.neurips.cc/paper/2020/file/33e75ff09dd601bbe69f351039152189-Paper.pdf,Stochastic Optimization for Performative Prediction,"Celestine Mendler-Dünner, Juan Perdomo, Tijana Zrnic, Moritz Hardt","In performative prediction, the choice of a model influences the distribution of future data, typically through actions taken based on the model's predictions. We initiate the study of stochastic optimization for performative prediction. What sets this setting apart from traditional stochastic optimization is the difference between merely updating model parameters and deploying the new model. The latter triggers a shift in the distribution that affects future data, while the former keeps the distribution as is. Assuming smoothness and strong convexity, we prove rates of convergence for both greedily deploying models after each stochastic update (greedy deploy) as well as for taking several updates before redeploying (lazy deploy). In both cases, our bounds smoothly recover the optimal
O
(
1
/
k
)
O
rate as the strength of performativity decreases. Furthermore, they illustrate how depending on the strength of performative effects, there exists a regime where either approach outperforms the other. We experimentally explore the trade-off on both synthetic data and a strategic classification simulator."
neurips,https://proceedings.neurips.cc/paper/2020/file/342285bb2a8cadef22f667eeb6a63732-Paper.pdf,Learning Differentiable Programs with Admissible Neural Heuristics,"Ameesh Shah, Eric Zhan, Jennifer Sun, Abhinav Verma, Yisong Yue, Swarat Chaudhuri",
neurips,https://proceedings.neurips.cc/paper/2020/file/342c472b95d00421be10e9512b532866-Paper.pdf,Improved guarantees and a multiple-descent curve for Column Subset Selection and the Nystrom method,"Michal Derezinski, Rajiv Khanna, Michael W. Mahoney",
neurips,https://proceedings.neurips.cc/paper/2020/file/3430095c577593aad3c39c701712bcfe-Paper.pdf,Domain Adaptation as a Problem of Inference on Graphical Models,"Kun Zhang, Mingming Gong, Petar Stojanov, Biwei Huang, QINGSONG LIU, Clark Glymour","This paper is concerned with data-driven unsupervised domain adaptation, where it is unknown in advance how the joint distribution changes across domains, i.e., what factors or modules of the data distribution remain invariant or change across domains. To develop an automated way of domain adaptation with multiple source domains, we propose to use a graphical model as a compact way to encode the change property of the joint distribution, which can be learned from data, and then view domain adaptation as a problem of Bayesian inference on the graphical models. Such a graphical model distinguishes between constant and varied modules of the distribution and specifies the properties of the changes across domains, which serves as prior knowledge of the changing modules for the purpose of deriving the posterior of the target variable
Y
Y
in the target domain. This provides an end-to-end framework of domain adaptation, in which additional knowledge about how the joint distribution changes, if available, can be directly incorporated to improve the graphical representation. We discuss how causality-based domain adaptation can be put under this umbrella. Experimental results on both synthetic and real data demonstrate the efficacy of the proposed framework for domain adaptation."
neurips,https://proceedings.neurips.cc/paper/2020/file/34609bdc08a07ace4e1526bbb1777673-Paper.pdf,Network size and size of the weights in memorization with two-layers neural networks,"Sebastien Bubeck, Ronen Eldan, Yin Tat Lee, Dan Mikulincer","In 1988, Eric B. Baum showed that two-layers neural networks with threshold activation function can perfectly memorize the binary labels of
n
n
points in general position in
\R
d
\R
using only
┌
n
/
d
┐
⌜
neurons. We observe that with ReLU networks, using four times as many neurons one can fit arbitrary real labels. Moreover, for approximate memorization up to error
ϵ
ϵ
, the neural tangent kernel can also memorize with only
O
(
n
d
⋅
log
(
1
/
ϵ
)
)
O
neurons (assuming that the data is well dispersed too). We show however that these constructions give rise to networks where the \emph{magnitude} of the neurons' weights are far from optimal. In contrast we propose a new training procedure for ReLU networks, based on {\em complex} (as opposed to {\em real}) recombination of the neurons, for which we show approximate memorization with both
O
(
n
d
⋅
log
(
1
/
ϵ
)
ϵ
)
O
neurons, as well as nearly-optimal size of the weights."
neurips,https://proceedings.neurips.cc/paper/2020/file/3465ab6e0c21086020e382f09a482ced-Paper.pdf,Certifying Strategyproof Auction Networks,"Michael Curry, Ping-yeh Chiang, Tom Goldstein, John Dickerson",
neurips,https://proceedings.neurips.cc/paper/2020/file/3472ab80b6dff70c54758fd6dfc800c2-Paper.pdf,Continual Learning of Control Primitives : Skill Discovery via Reset-Games,"Kelvin Xu, Siddharth Verma, Chelsea Finn, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2020/file/3493894fa4ea036cfc6433c3e2ee63b0-Paper.pdf,HOI Analysis: Integrating and Decomposing Human-Object Interaction,"Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, Cewu Lu",
neurips,https://proceedings.neurips.cc/paper/2020/file/3501672ebc68a5524629080e3ef60aef-Paper.pdf,Strongly local p-norm-cut algorithms for semi-supervised learning and local graph clustering,"Meng Liu, David F. Gleich",
neurips,https://proceedings.neurips.cc/paper/2020/file/350a7f5ee27d22dbe36698b10930ff96-Paper.pdf,Deep Direct Likelihood Knockoffs,"Mukund Sudarshan, Wesley Tansey, Rajesh Ranganath",
neurips,https://proceedings.neurips.cc/paper/2020/file/35464c848f410e55a13bb9d78e7fddd0-Paper.pdf,Meta-Neighborhoods,"Siyuan Shan, Yang Li, Junier B. Oliva",
neurips,https://proceedings.neurips.cc/paper/2020/file/354ac345fd8c6d7ef634d9a8e3d47b83-Paper.pdf,Neural Dynamic Policies for End-to-End Sensorimotor Learning,"Shikhar Bahl, Mustafa Mukadam, Abhinav Gupta, Deepak Pathak",
neurips,https://proceedings.neurips.cc/paper/2020/file/356dc40642abeb3a437e7e06f178701c-Paper.pdf,A new inference approach for training shallow and deep generalized linear models of noisy interacting neurons,"Gabriel Mahuas, Giulio Isacchini, Olivier Marre, Ulisse Ferrari, Thierry Mora",
neurips,https://proceedings.neurips.cc/paper/2020/file/357a6fdf7642bf815a88822c447d9dc4-Paper.pdf,Decision-Making with Auto-Encoding Variational Bayes,"Romain Lopez, Pierre Boyeau, Nir Yosef, Michael Jordan, Jeffrey Regier",
neurips,https://proceedings.neurips.cc/paper/2020/file/35adf1ae7eb5734122c84b7a9ea5cc13-Paper.pdf,Attribution Preservation in Network Compression for Reliable Network Interpretation,"Geondo Park, June Yong Yang, Sung Ju Hwang, Eunho Yang",
neurips,https://proceedings.neurips.cc/paper/2020/file/36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf,Feature Importance Ranking for Deep Learning,"Maksymilian Wojtas, Ke Chen",
neurips,https://proceedings.neurips.cc/paper/2020/file/36dcd524971019336af02550264b8a08-Paper.pdf,Causal Estimation with Functional Confounders,"Aahlad Puli, Adler Perotte, Rajesh Ranganath",
neurips,https://proceedings.neurips.cc/paper/2020/file/373e4c5d8edfa8b74fd4b6791d0cf6dc-Paper.pdf,Model Inversion Networks for Model-Based Optimization,"Aviral Kumar, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2020/file/37693cfc748049e45d87b8c7d8b9aacd-Paper.pdf,"Hausdorff Dimension, Heavy Tails, and Generalization in Neural Networks","Umut Simsekli, Ozan Sener, George Deligiannidis, Murat A. Erdogdu",
neurips,https://proceedings.neurips.cc/paper/2020/file/37740d59bb0eb7b4493725b2e0e5289b-Paper.pdf,Exact expressions for double descent and implicit regularization via surrogate random design,"Michal Derezinski, Feynman T. Liang, Michael W. Mahoney",
neurips,https://proceedings.neurips.cc/paper/2020/file/37aa5dfc44dddd0d19d4311e2c7a0240-Paper.pdf,Certifying Confidence via Randomized Smoothing,"Aounon Kumar, Alexander Levine, Soheil Feizi, Tom Goldstein",
neurips,https://proceedings.neurips.cc/paper/2020/file/37bc5e7fb6931a50b3464ec66179085f-Paper.pdf,Learning Physical Constraints with Neural Projections,"Shuqi Yang, Xingzhe He, Bo Zhu",
neurips,https://proceedings.neurips.cc/paper/2020/file/37d097caf1299d9aa79c2c2b843d2d78-Paper.pdf,Robust Optimization for Fairness with Noisy Protected Groups,"Serena Wang, Wenshuo Guo, Harikrishna Narasimhan, Andrew Cotter, Maya Gupta, Michael Jordan","Many existing fairness criteria for machine learning involve equalizing some metric across protected groups such as race or gender. However, practitioners trying to audit or enforce such group-based criteria can easily face the problem of noisy or biased protected group information. First, we study the consequences of naively relying on noisy protected group labels: we provide an upper bound on the fairness violations on the true groups
G
G
when the fairness criteria are satisfied on noisy groups
^
G
G
. Second, we introduce two new approaches using robust optimization that, unlike the naive approach of only relying on
^
G
G
, are guaranteed to satisfy fairness criteria on the true protected groups
G
G
while minimizing a training objective. We provide theoretical guarantees that one such approach converges to an optimal feasible solution. Using two case studies, we show empirically that the robust approaches achieve better true group fairness guarantees than the naive approach."
neurips,https://proceedings.neurips.cc/paper/2020/file/37e7897f62e8d91b1ce60515829ca282-Paper.pdf,Noise-Contrastive Estimation for Multivariate Point Processes,"Hongyuan Mei, Tom Wan, Jason Eisner",
neurips,https://proceedings.neurips.cc/paper/2020/file/37e79373884f0f0b70b5cb91fb947148-Paper.pdf,A Game-Theoretic Analysis of the Empirical Revenue Maximization Algorithm with Endogenous Sampling,"Xiaotie Deng, Ron Lavi, Tao Lin, Qi Qi, Wenwei WANG, Xiang Yan",
neurips,https://proceedings.neurips.cc/paper/2020/file/37f76c6fe3ab45e0cd7ecb176b5a046d-Paper.pdf,Neural Path Features and Neural Path Kernel : Understanding the role of gates in deep learning,"Chandrashekar Lakshminarayanan, Amit Vikram Singh","In this paper, we analytically characterise the role of gates and active sub-networks in deep learning. To this end, we encode the on/off state of the gates for a given input in a novel 'neural path feature' (NPF), and the weights of the DNN are encoded in a novel 'neural path value' (NPV). Further, we show that the output of network is indeed the inner product of NPF and NPV. The main result of the paper shows that the 'neural path kernel' associated with the NPF is a fundamental quantity that characterises the information stored in the gates of a DNN. We show via experiments (on MNIST and CIFAR-10) that in standard DNNs with ReLU activations NPFs are learnt during training and such learning is key for generalisation. Furthermore, NPFs and NPVs can be learnt in two separate networks and such learning also generalises well in experiments. In our experiments, we observe that almost all the information learnt by a DNN with ReLU activations is stored in the gates - a novel observation that underscores the need to investigate the role of the gates in DNNs."
neurips,https://proceedings.neurips.cc/paper/2020/file/3812f9a59b634c2a9c574610eaba5bed-Paper.pdf,Multiscale Deep Equilibrium Models,"Shaojie Bai, Vladlen Koltun, J. Zico Kolter",
neurips,https://proceedings.neurips.cc/paper/2020/file/385822e359afa26d52b5b286226f2cea-Paper.pdf,Sparse Graphical Memory for Robust Planning,"Scott Emmons, Ajay Jain, Misha Laskin, Thanard Kurutach, Pieter Abbeel, Deepak Pathak",
neurips,https://proceedings.neurips.cc/paper/2020/file/386854131f58a556343e056f03626e00-Paper.pdf,Second Order PAC-Bayesian Bounds for the Weighted Majority Vote,"Andres Masegosa, Stephan Lorenzen, Christian Igel, Yevgeny Seldin",
neurips,https://proceedings.neurips.cc/paper/2020/file/38a77aa456fc813af07bb428f2363c8d-Paper.pdf,Dirichlet Graph Variational Autoencoder,"Jia Li, Jianwei Yu, Jiajin Li, Honglei Zhang, Kangfei Zhao, Yu Rong, Hong Cheng, Junzhou Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/38a8e18d75e95ca619af8df0da1417f2-Paper.pdf,Modeling Task Effects on Meaning Representation in the Brain via Zero-Shot MEG Prediction,"Mariya Toneva, Otilia Stretcu, Barnabas Poczos, Leila Wehbe, Tom M. Mitchell",
neurips,https://proceedings.neurips.cc/paper/2020/file/39016cfe079db1bfb359ca72fcba3fd8-Paper.pdf,Counterfactual Vision-and-Language Navigation: Unravelling the Unseen,"Amin Parvaneh, Ehsan Abbasnejad, Damien Teney, Javen Qinfeng Shi, Anton van den Hengel",
neurips,https://proceedings.neurips.cc/paper/2020/file/3948ead63a9f2944218de038d8934305-Paper.pdf,Robust Quantization: One Model to Rule Them All,"moran shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan, Alex Bronstein, Uri Weiser",
neurips,https://proceedings.neurips.cc/paper/2020/file/397d6b4c83c91021fe928a8c4220386b-Paper.pdf,Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming,"Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan Uesato, Rudy R. Bunel, Shreya Shankar, Jacob Steinhardt, Ian Goodfellow, Percy S. Liang, Pushmeet Kohli","Convex relaxations have emerged as a promising approach for verifying properties of neural networks, but widely used using Linear Programming (LP) relaxations only provide meaningful certificates when networks are specifically trained to facilitate verification. This precludes many important applications which involve \emph{verification-agnostic} networks that are not trained specifically to promote verifiability. On the other hand, semidefinite programming (SDP) relaxations have shown success on verification-agnostic networks, such as adversarially trained image classifiers without additional regularization, but do not currently scale beyond small networks due to poor time and space asymptotics. In this work, we propose a first-order dual SDP algorithm that provides (1) any-time bounds (2) requires memory only linear in the total number of network activations and (3) has per-iteration complexity that scales linearly with the complexity of a forward/backward pass through the network. By exploiting iterative eigenvector methods, we express all solver operations in terms of forward and backward passes through the network, enabling efficient use of hardware optimized for deep learning. This allows us to dramatically improve the magnitude of
ℓ
∞
ℓ
perturbations for which we can verify robustness verification-agnostic networks (
1
%
→
88
%
1
on MNIST,
6
%
→
40
%
6
on CIFAR-10). We also demonstrate tight verification for a quadratic stability specification for the decoder of a variational autoencoder."
neurips,https://proceedings.neurips.cc/paper/2020/file/39d0a8908fbe6c18039ea8227f827023-Paper.pdf,Federated Accelerated Stochastic Gradient Descent,"Honglin Yuan, Tengyu Ma",
neurips,https://proceedings.neurips.cc/paper/2020/file/39d4b545fb02556829aab1db805021c3-Paper.pdf,Robust Density Estimation under Besov IPM Losses,"Ananya Uppal, Shashank Singh, Barnabas Poczos",
neurips,https://proceedings.neurips.cc/paper/2020/file/3a01fc0853ebeba94fde4d1cc6fb842a-Paper.pdf,An analytic theory of shallow networks dynamics for hinge loss classification,"Franco Pellegrini, Giulio Biroli",
neurips,https://proceedings.neurips.cc/paper/2020/file/3a029f04d76d32e79367c4b3255dda4d-Paper.pdf,Fixed-Support Wasserstein Barycenters: Computational Hardness and Fast Algorithm,"Tianyi Lin, Nhat Ho, Xi Chen, Marco Cuturi, Michael Jordan","We study the fixed-support Wasserstein barycenter problem (FS-WBP), which consists in computing the Wasserstein barycenter of
m
m
discrete probability measures supported on a finite metric space of size
n
n
. We show first that the constraint matrix arising from the standard linear programming (LP) representation of the FS-WBP is \textit{not totally unimodular} when
m
≥
3
m
and
n
≥
3
n
. This result resolves an open question pertaining to the relationship between the FS-WBP and the minimum-cost flow (MCF) problem since it proves that the FS-WBP in the standard LP form is not an MCF problem when
m
≥
3
m
and
n
≥
3
n
. We also develop a provably fast \textit{deterministic} variant of the celebrated iterative Bregman projection (IBP) algorithm, named \textsc{FastIBP}, with a complexity bound of
~
O
(
m
n
7
/
3
ε
−
4
/
3
)
O
, where
ε
∈
(
0
,
1
)
ε
is the desired tolerance. This complexity bound is better than the best known complexity bound of
~
O
(
m
n
2
ε
−
2
)
O
for the IBP algorithm in terms of
ε
ε
, and that of
~
O
(
m
n
5
/
2
ε
−
1
)
O
from accelerated alternating minimization algorithm or accelerated primal-dual adaptive gradient algorithm in terms of
n
n
. Finally, we conduct extensive experiments with both synthetic data and real images and demonstrate the favorable performance of the \textsc{FastIBP} algorithm in practice."
neurips,https://proceedings.neurips.cc/paper/2020/file/3a0772443a0739141292a5429b952fe6-Paper.pdf,Learning to Orient Surfaces by Self-supervised Spherical CNNs,"Riccardo Spezialetti, Federico Stella, Marlon Marcon, Luciano Silva, Samuele Salti, Luigi Di Stefano",
neurips,https://proceedings.neurips.cc/paper/2020/file/3a077e8acfc4a2b463c47f2125fdfac5-Paper.pdf,Adam with Bandit Sampling for Deep Learning,"Rui Liu, Tianyi Wu, Barzan Mozafari","Adam is a widely used optimization method for training deep learning models. It computes individual adaptive learning rates for different parameters. In this paper, we propose a generalization of Adam, called Adambs, that allows us to also adapt to different training examples based on their importance in the model's convergence. To achieve this, we maintain a distribution over all examples, selecting a mini-batch in each iteration by sampling according to this distribution, which we update using a multi-armed bandit algorithm. This ensures that examples that are more beneficial to the model training are sampled with higher probabilities. We theoretically show that Adambs improves the convergence rate of Adam---
O
(
√
log
n
T
)
O
instead of
O
(
√
n
T
)
O
in some cases. Experiments on various models and datasets demonstrate Adambs's fast convergence in practice."
neurips,https://proceedings.neurips.cc/paper/2020/file/3a30be93eb45566a90f4e95ee72a089a-Paper.pdf,Parabolic Approximation Line Search for DNNs,"Maximus Mutschler, Andreas Zell",
neurips,https://proceedings.neurips.cc/paper/2020/file/3a37abdeefe1dab1b30f7c5c7e581b93-Paper.pdf,Agnostic Learning of a Single Neuron with Gradient Descent,"Spencer Frei, Yuan Cao, Quanquan Gu","We consider the problem of learning the best-fitting single neuron as measured by the expected square loss
\E
(
x
,
y
)
∼
D
[
(
σ
(
w
⊤
x
)
−
y
)
2
]
\E
over some unknown joint distribution
D
D
by using gradient descent to minimize the empirical risk induced by a set of i.i.d. samples
S
∼
D
n
S
. The activation function
σ
σ
is an arbitrary Lipschitz and non-decreasing function, making the optimization problem nonconvex and nonsmooth in general, and covers typical neural network activation functions and inverse link functions in the generalized linear model setting. In the agnostic PAC learning setting, where no assumption on the relationship between the labels
y
y
and the input
x
x
is made, if the optimal population risk is
O
P
T
O
, we show that gradient descent achieves population risk
O
(
O
P
T
)
+
\eps
O
in polynomial time and sample complexity when
σ
σ
is strictly increasing. For the ReLU activation, our population risk guarantee is
O
(
O
P
T
1
/
2
)
+
\eps
O
. When labels take the form
y
=
σ
(
v
⊤
x
)
+
ξ
y
for zero-mean sub-Gaussian noise
ξ
ξ
, we show that the population risk guarantees for gradient descent improve to
O
P
T
+
\eps
O
. Our sample complexity and runtime guarantees are (almost) dimension independent, and when
σ
σ
is strictly increasing, require no distributional assumptions beyond boundedness. For ReLU, we show the same results under a nondegeneracy assumption for the marginal distribution of the input."
neurips,https://proceedings.neurips.cc/paper/2020/file/3a4496776767aaa99f9804d0905fe584-Paper.pdf,Statistical Efficiency of Thompson Sampling for Combinatorial Semi-Bandits,"Pierre Perrault, Etienne Boursier, Michal Valko, Vianney Perchet","We investigate stochastic combinatorial multi-armed bandit with semi-bandit feedback (CMAB). In CMAB, the question of the existence of an efficient policy with an optimal asymptotic regret (up to a factor poly-logarithmic with the action size) is still open for many families of distributions, including mutually independent outcomes, and more generally the multivariate \emph{sub-Gaussian} family. We propose to answer the above question for these two families by analyzing variants of the Combinatorial Thompson Sampling policy (CTS). For mutually independent outcomes in
[
0
,
1
]
[
, we propose a tight analysis of CTS using Beta priors. We then look at the more general setting of multivariate sub-Gaussian outcomes and propose a tight analysis of CTS using Gaussian priors. This last result gives us an alternative to the Efficient Sampling for Combinatorial Bandit policy (ESCB), which, although optimal, is not computationally efficient."
neurips,https://proceedings.neurips.cc/paper/2020/file/3a61ed715ee66c48bacf237fa7bb5289-Paper.pdf,Analytic Characterization of the Hessian in Shallow ReLU Models: A Tale of Symmetry,"Yossi Arjevani, Michael Field","We consider the optimization problem associated with fitting two-layers ReLU networks with respect to the squared loss, where labels are generated by a target network. We leverage the rich symmetry structure to analytically characterize the Hessian at various families of spurious minima in the natural regime where the number of inputs
d
d
and the number of hidden neurons
k
k
is finite. In particular, we prove that for
d
≥
k
d
standard Gaussian inputs: (a) of the
d
k
d
eigenvalues of the Hessian,
d
k
−
O
(
d
)
d
concentrate near zero, (b)
Ω
(
d
)
Ω
of the eigenvalues grow linearly with
k
k
. Although this phenomenon of extremely skewed spectrum has been observed many times before, to our knowledge, this is the first time it has been established {rigorously}. Our analytic approach uses techniques, new to the field, from symmetry breaking and representation theory, and carries important implications for our ability to argue about statistical generalization through local curvature."
neurips,https://proceedings.neurips.cc/paper/2020/file/3a93a609b97ec0ab0ff5539eb79ef33a-Paper.pdf,Generative causal explanations of black-box classifiers,"Matthew O'Shaughnessy, Gregory Canal, Marissa Connor, Christopher Rozell, Mark Davenport",
neurips,https://proceedings.neurips.cc/paper/2020/file/3ab6be46e1d6b21d59a3c3a0b9d0f6ef-Paper.pdf,Sub-sampling for Efficient Non-Parametric Bandit Exploration,"Dorian Baudry, Emilie Kaufmann, Odalric-Ambrym Maillard",
neurips,https://proceedings.neurips.cc/paper/2020/file/3ac48664b7886cf4e4ab4aba7e6b6bc9-Paper.pdf,Learning under Model Misspecification: Applications to Variational and Ensemble methods,Andres Masegosa,
neurips,https://proceedings.neurips.cc/paper/2020/file/3acb2a202ae4bea8840224e6fce16fd0-Paper.pdf,Language Through a Prism: A Spectral Approach for Multiscale Language Representations,"Alex Tamkin, Dan Jurafsky, Noah Goodman",
neurips,https://proceedings.neurips.cc/paper/2020/file/3ad7c2ebb96fcba7cda0cf54a2e802f5-Paper.pdf,DVERGE: Diversifying Vulnerabilities for Enhanced Robust Generation of Ensembles,"Huanrui Yang, Jingyang Zhang, Hongliang Dong, Nathan Inkawhich, Andrew Gardner, Andrew Touchet, Wesley Wilkes, Heath Berry, Hai Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/3b13b1eb44b05f57735764786fab9c2c-Paper.pdf,Towards practical differentially private causal graph discovery,"Lun Wang, Qi Pang, Dawn Song",
neurips,https://proceedings.neurips.cc/paper/2020/file/3b2acfe2e38102074656ed938abf4ac3-Paper.pdf,Independent Policy Gradient Methods for Competitive Reinforcement Learning,"Constantinos Daskalakis, Dylan J. Foster, Noah Golowich",
neurips,https://proceedings.neurips.cc/paper/2020/file/3bb585ea00014b0e3ebe4c6dd165a358-Paper.pdf,The Value Equivalence Principle for Model-Based Reinforcement Learning,"Christopher Grimm, Andre Barreto, Satinder Singh, David Silver",
neurips,https://proceedings.neurips.cc/paper/2020/file/3be0214185d6177a9aa6adea5a720b09-Paper.pdf,Structured Convolutions for Efficient Neural Network Design,"Yash Bhalgat, Yizhe Zhang, Jamie Menjay Lin, Fatih Porikli",
neurips,https://proceedings.neurips.cc/paper/2020/file/3c09bb10e2189124fdd8f467cc8b55a7-Paper.pdf,Latent World Models For Intrinsically Motivated Exploration,"Aleksandr Ermolov, Nicu Sebe",
neurips,https://proceedings.neurips.cc/paper/2020/file/3c0de3fec9ab8a3df01109251f137119-Paper.pdf,Estimating Rank-One Spikes from Heavy-Tailed Noise via Self-Avoiding Walks,"Jingqiu Ding, Samuel Hopkins, David Steurer","In this work, we exhibit an estimator that works for heavy-tailed noise up to the BBP threshold that is optimal even for Gaussian noise. We give a non-asymptotic analysis of our estimator which relies only on the variance of each entry remaining constant as the size of the matrix grows: higher moments may grow arbitrarily fast or even fail to exist. Previously, it was only known how to achieve these guarantees if higher-order moments of the noises are bounded by a constant independent of the size of the matrix."
neurips,https://proceedings.neurips.cc/paper/2020/file/3c56fe2f24038c4d22b9eb0aca78f590-Paper.pdf,Policy Improvement via Imitation of Multiple Oracles,"Ching-An Cheng, Andrey Kolobov, Alekh Agarwal",
neurips,https://proceedings.neurips.cc/paper/2020/file/3c8f9a173f749710d6377d3150cf90da-Paper.pdf,Training Generative Adversarial Networks by Solving Ordinary Differential Equations,"Chongli Qin, Yan Wu, Jost Tobias Springenberg, Andy Brock, Jeff Donahue, Timothy Lillicrap, Pushmeet Kohli",
neurips,https://proceedings.neurips.cc/paper/2020/file/3cc697419ea18cc98d525999665cb94a-Paper.pdf,Learning of Discrete Graphical Models with Neural Networks,"Abhijith Jayakumar, Andrey Lokhov, Sidhant Misra, Marc Vuffray",
neurips,https://proceedings.neurips.cc/paper/2020/file/3ce3bd7d63a2c9c81983cc8e9bd02ae5-Paper.pdf,RepPoints v2: Verification Meets Regression for Object Detection,"Yihong Chen, Zheng Zhang, Yue Cao, Liwei Wang, Stephen Lin, Han Hu",
neurips,https://proceedings.neurips.cc/paper/2020/file/3d2d8ccb37df977cb6d9da15b76c3f3a-Paper.pdf,Unfolding the Alternating Optimization for Blind Super Resolution,"zhengxiong luo, Yan Huang, Shang Li, Liang Wang, Tieniu Tan",
neurips,https://proceedings.neurips.cc/paper/2020/file/3d8e03e8b133b16f13a586f0c01b6866-Paper.pdf,Entrywise convergence of iterative methods for eigenproblems,"Vasileios Charisopoulos, Austin R. Benson, Anil Damle",
neurips,https://proceedings.neurips.cc/paper/2020/file/3d9dabe52805a1ea21864b09f3397593-Paper.pdf,Learning Object-Centric Representations of Multi-Object Scenes from Multiple Views,"Nanbo Li, Cian Eastwood, Robert Fisher",
neurips,https://proceedings.neurips.cc/paper/2020/file/3db54f5573cd617a0112d35dd1e6b1ef-Paper.pdf,A Catalyst Framework for Minimax Optimization,"Junchi Yang, Siqi Zhang, Negar Kiyavash, Niao He",
neurips,https://proceedings.neurips.cc/paper/2020/file/3def184ad8f4755ff269862ea77393dd-Paper.pdf,Self-supervised Co-Training for Video Representation Learning,"Tengda Han, Weidi Xie, Andrew Zisserman",
neurips,https://proceedings.neurips.cc/paper/2020/file/3df80af53dce8435cf9ad6c3e7a403fd-Paper.pdf,Gradient Estimation with Stochastic Softmax Tricks,"Max Paulus, Dami Choi, Daniel Tarlow, Andreas Krause, Chris J. Maddison",
neurips,https://proceedings.neurips.cc/paper/2020/file/3e5190eeb51ebe6c5bbc54ee8950c548-Paper.pdf,Meta-Learning Requires Meta-Augmentation,"Janarthanan Rajendran, Alexander Irpan, Eric Jang",
neurips,https://proceedings.neurips.cc/paper/2020/file/3e91970f771a2c473ae36b60d1146068-Paper.pdf,SLIP: Learning to predict in unknown dynamical systems with long-term memory,"Paria Rashidinejad, Jiantao Jiao, Stuart Russell",
neurips,https://proceedings.neurips.cc/paper/2020/file/3eb46aa5d93b7a5939616af91addfa88-Paper.pdf,Improving GAN Training with Probability Ratio Clipping and Sample Reweighting,"Yue Wu, Pan Zhou, Andrew G. Wilson, Eric Xing, Zhiting Hu",
neurips,https://proceedings.neurips.cc/paper/2020/file/3f13cf4ddf6fc50c0d39a1d5aeb57dd8-Paper.pdf,Bayesian Bits: Unifying Quantization and Pruning,"Mart van Baalen, Christos Louizos, Markus Nagel, Rana Ali Amjad, Ying Wang, Tijmen Blankevoort, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2020/file/3f1656d9668dffcf8119e3ecff873558-Paper.pdf,On Testing of Samplers,"Kuldeep S Meel, Yash Pralhad Pote, Sourav Chakraborty",
neurips,https://proceedings.neurips.cc/paper/2020/file/3f2dff7862a70f97a59a1fa02c3ec110-Paper.pdf,Gaussian Process Bandit Optimization of the Thermodynamic Variational Objective,"Vu Nguyen, Vaden Masrani, Rob Brekelmans, Michael Osborne, Frank Wood",
neurips,https://proceedings.neurips.cc/paper/2020/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf,MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers,"Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou",
neurips,https://proceedings.neurips.cc/paper/2020/file/3f8b2a81da929223ae025fcec26dde0d-Paper.pdf,Optimal Epoch Stochastic Gradient Descent Ascent Methods for Min-Max Optimization,"Yan Yan, Yi Xu, Qihang Lin, Wei Liu, Tianbao Yang",
neurips,https://proceedings.neurips.cc/paper/2020/file/3fb04953d95a94367bb133f862402bce-Paper.pdf,Woodbury Transformations for Deep Generative Flows,"You Lu, Bert Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/3fe230348e9a12c13120749e3f9fa4cd-Paper.pdf,Graph Contrastive Learning with Augmentations,"Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, Yang Shen",
neurips,https://proceedings.neurips.cc/paper/2020/file/3fe78a8acf5fda99de95303940a2420c-Paper.pdf,Gradient Surgery for Multi-Task Learning,"Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, Chelsea Finn",
neurips,https://proceedings.neurips.cc/paper/2020/file/3fe94a002317b5f9259f82690aeea4cd-Paper.pdf,Bayesian Probabilistic Numerical Integration with Tree-Based Models,"Harrison Zhu, Xing Liu, Ruya Kang, Zhichao Shen, Seth Flaxman, Francois-Xavier Briol",
neurips,https://proceedings.neurips.cc/paper/2020/file/405075699f065e43581f27d67bb68478-Paper.pdf,Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel,"Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M. Roy, Surya Ganguli",
neurips,https://proceedings.neurips.cc/paper/2020/file/412604be30f701b1b1e3124c252065e6-Paper.pdf,Graph Meta Learning via Local Subgraphs,"Kexin Huang, Marinka Zitnik",
neurips,https://proceedings.neurips.cc/paper/2020/file/415e1af7ea95f89f4e375162b21ae38c-Paper.pdf,Stochastic Deep Gaussian Processes over Graphs,"Naiqi Li, Wenjie Li, Jifeng Sun, Yinghua Gao, Yong Jiang, Shu-Tao Xia",
neurips,https://proceedings.neurips.cc/paper/2020/file/4175a4b46a45813fccf4bd34c779d817-Paper.pdf,Bayesian Causal Structural Learning with Zero-Inflated Poisson Bayesian Networks,"Junsouk Choi, Robert Chapkin, Yang Ni",
neurips,https://proceedings.neurips.cc/paper/2020/file/417fbbf2e9d5a28a855a11894b2e795a-Paper.pdf,Evaluating Attribution for Graph Neural Networks,"Benjamin Sanchez-Lengeling, Jennifer Wei, Brian Lee, Emily Reif, Peter Wang, Wesley Qian, Kevin McCloskey, Lucy Colwell , Alexander Wiltschko",
neurips,https://proceedings.neurips.cc/paper/2020/file/418db2ea5d227a9ea8db8e5357ca2084-Paper.pdf,On Second Order Behaviour in Augmented Neural ODEs,"Alexander Norcliffe, Cristian Bodnar, Ben Day, Nikola Simidjievski, Pietro Lió",
neurips,https://proceedings.neurips.cc/paper/2020/file/41c542dfe6e4fc3deb251d64cf6ed2e4-Paper.pdf,Neuron Shapley: Discovering the Responsible Neurons,"Amirata Ghorbani, James Y. Zou",
neurips,https://proceedings.neurips.cc/paper/2020/file/41d80bfc327ef980528426fc810a6d7a-Paper.pdf,Stochastic Normalizing Flows,"Hao Wu, Jonas Köhler, Frank Noe",
neurips,https://proceedings.neurips.cc/paper/2020/file/41e7637e7b6a9f27a98b84d3a185c7c0-Paper.pdf,GPU-Accelerated Primal Learning for Extremely Fast Large-Scale Classification,"John T. Halloran, David M. Rocke",
neurips,https://proceedings.neurips.cc/paper/2020/file/42299f06ee419aa5d9d07798b56779e2-Paper.pdf,Random Reshuffling is Not Always Better,Christopher M. De Sa,
neurips,https://proceedings.neurips.cc/paper/2020/file/426f990b332ef8193a61cc90516c1245-Paper.pdf,Model Agnostic Multilevel Explanations,"Karthikeyan Natesan Ramamurthy, Bhanukiran Vinzamuri, Yunfeng Zhang, Amit Dhurandhar",
neurips,https://proceedings.neurips.cc/paper/2020/file/42ae1544956fbe6e09242e6cd752444c-Paper.pdf,NeuMiss networks: differentiable programming for supervised learning with missing values.,"Marine Le Morvan, Julie Josse, Thomas Moreau, Erwan Scornet, Gael Varoquaux",
neurips,https://proceedings.neurips.cc/paper/2020/file/42cd63cb189c30ed03e42ce2c069566c-Paper.pdf,Revisiting Parameter Sharing for Automatic Neural Channel Number Search,"Jiaxing Wang, Haoli Bai, Jiaxiang Wu, Xupeng Shi, Junzhou Huang, Irwin King, Michael Lyu, Jian Cheng",
neurips,https://proceedings.neurips.cc/paper/2020/file/4311359ed4969e8401880e3c1836fbe1-Paper.pdf,Differentially-Private Federated Linear Bandits,"Abhimanyu Dubey, Alex `Sandy' Pentland",
neurips,https://proceedings.neurips.cc/paper/2020/file/43207fd5e34f87c48d584fc5c11befb8-Paper.pdf,Is Plug-in Solver Sample-Efficient for Feature-based Reinforcement Learning?,"Qiwen Cui, Lin Yang","It is believed that a model-based approach for reinforcement learning (RL) is the key to reduce sample complexity. However, the understanding of the sample optimality of model-based RL is still largely missing, even for the linear case. This work considers sample complexity of finding an
ϵ
ϵ
-optimal policy in a Markov decision process (MDP) that admits a linear additive feature representation, given only access to a generative model. We solve this problem via a plug-in solver approach, which builds an empirical model and plans in this empirical model via an arbitrary plug-in solver. We prove that under the anchor-state assumption, which implies implicit non-negativity in the feature space, the minimax sample complexity of finding an
ϵ
ϵ
-optimal policy in a
γ
γ
-discounted MDP is
O
(
K
/
(
1
−
γ
)
3
ϵ
2
)
O
, which only depends on the dimensionality
K
K
of the feature space and has no dependence on the state or action space. We further extend our results to a relaxed setting where anchor-states may not exist and show that a plug-in approach can be sample efficient as well, providing a flexible approach to design model-based algorithms for RL."
neurips,https://proceedings.neurips.cc/paper/2020/file/4324e8d0d37b110ee1a4f1633ac52df5-Paper.pdf,Learning Physical Graph Representations from Visual Scenes,"Daniel Bear, Chaofei Fan, Damian Mrowca, Yunzhu Li, Seth Alter, Aran Nayebi, Jeremy Schwartz, Li F. Fei-Fei, Jiajun Wu, Josh Tenenbaum, Daniel L. Yamins",
neurips,https://proceedings.neurips.cc/paper/2020/file/4379cf00e1a95a97a33dac10ce454ca4-Paper.pdf,Deep Graph Pose: a semi-supervised deep graphical model for improved animal pose tracking,"Anqi Wu, Estefany Kelly Buchanan, Matthew Whiteway, Michael Schartner, Guido Meijer, Jean-Paul Noel, Erica Rodriguez, Claire Everett, Amy Norovich, Evan Schaffer, Neeli Mishra, C. Daniel Salzman, Dora Angelaki, Andrés Bendesky, The International Brain Laboratory The International Brain Laboratory, John P. Cunningham, Liam Paninski",
neurips,https://proceedings.neurips.cc/paper/2020/file/438124b4c06f3a5caffab2c07863b617-Paper.pdf,Meta-learning from Tasks with Heterogeneous Attribute Spaces,"Tomoharu Iwata, Atsutoshi Kumagai",
neurips,https://proceedings.neurips.cc/paper/2020/file/439d8c975f26e5005dcdbf41b0d84161-Paper.pdf,Estimating decision tree learnability with polylogarithmic sample complexity,"Guy Blanc, Neha Gupta, Jane Lange, Li-Yang Tan","We show that top-down decision tree learning heuristics (such as ID3, C4.5, and CART) are amenable to highly efficient {\sl learnability estimation}: for monotone target functions, the error of the decision tree hypothesis constructed by these heuristics can be estimated with {\sl polylogarithmically} many labeled examples, exponentially smaller than the number necessary to run these heuristics, and indeed, exponentially smaller than information-theoretic minimum required to learn a good decision tree. This adds to a small but growing list of fundamental learning algorithms that have been shown to be amenable to learnability estimation. En route to this result, we design and analyze sample-efficient {\sl minibatch} versions of top-down decision tree learning heuristics and show that they achieve the same provable guarantees as the full-batch versions. We further give
active local'' versions of these heuristics: given a test point
x
⋆
x
, we show how the label
T
(
x
⋆
)
T
of the decision tree hypothesis
T
T
can be computed with polylogarithmically many labeled examples, exponentially smaller than the number necessary to learn~
T
T
."
neurips,https://proceedings.neurips.cc/paper/2020/file/439fca360bc99c315c5882c4432ae7a4-Paper.pdf,Sparse Symplectically Integrated Neural Networks,"Daniel DiPietro, Shiying Xiong, Bo Zhu",
neurips,https://proceedings.neurips.cc/paper/2020/file/43a7c24e2d1fe375ce60d84ac901819f-Paper.pdf,Continuous Object Representation Networks: Novel View Synthesis without Target View Supervision,"Nicolai Hani, Selim Engin, Jun-Jee Chao, Volkan Isler",
neurips,https://proceedings.neurips.cc/paper/2020/file/43bb733c1b62a5e374c63cb22fa457b4-Paper.pdf,Multimodal Generative Learning Utilizing Jensen-Shannon-Divergence,"Thomas Sutter, Imant Daunhawer, Julia Vogt",
neurips,https://proceedings.neurips.cc/paper/2020/file/43e4e6a6f341e00671e123714de019a8-Paper.pdf,Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers,"Kiwon Um, Robert Brand, Yun (Raymond) Fei, Philipp Holl, Nils Thuerey",
neurips,https://proceedings.neurips.cc/paper/2020/file/440924c5948e05070663f88e69e8242b-Paper.pdf,Reinforcement Learning with General Value Function Approximation: Provably Efficient Approach via Bounded Eluder Dimension,"Ruosong Wang, Russ R. Salakhutdinov, Lin Yang","Value function approximation has demonstrated phenomenal empirical success in reinforcement learning (RL). Nevertheless, despite a handful of recent progress on developing theory for RL with linear function approximation, the understanding of \emph{general} function approximation schemes largely remains missing. In this paper, we establish the first provably efficient RL algorithm with general value function approximation. We show that if the value functions admit an approximation with a function class
F
F
, our algorithm achieves a regret bound of
˜
O
(
p
o
l
y
(
d
H
)
√
T
)
O
where
d
d
is a complexity measure of
F
F
that depends on the eluder dimension~[Russo and Van Roy, 2013] and log-covering numbers,
H
H
is the planning horizon, and
T
T
is the number interactions with the environment. Our theory generalizes the linear MDP assumption to general function classes. Moreover, our algorithm is model-free and provides a framework to justify the effectiveness of algorithms used in practice."
neurips,https://proceedings.neurips.cc/paper/2020/file/440e7c3eb9bbcd4c33c3535354a51605-Paper.pdf,Predicting Training Time Without Training,"Luca Zancato, Alessandro Achille, Avinash Ravichandran, Rahul Bhotika, Stefano Soatto",
neurips,https://proceedings.neurips.cc/paper/2020/file/443dec3062d0286986e21dc0631734c9-Paper.pdf,How does This Interaction Affect Me? Interpretable Attribution for Feature Interactions,"Michael Tsang, Sirisha Rambhatla, Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/445e1050156c6ae8c082a8422bb7dfc0-Paper.pdf,Optimal Adaptive Electrode Selection to Maximize Simultaneously Recorded Neuron Yield,"John Choi, Krishan Kumar, Mohammad Khazali, Katie Wingel, Mahdi Choudhury, Adam S. Charles, Bijan Pesaran",
neurips,https://proceedings.neurips.cc/paper/2020/file/448d5eda79895153938a8431919f4c9f-Paper.pdf,Neurosymbolic Reinforcement Learning with Formally Verified Exploration,"Greg Anderson, Abhinav Verma, Isil Dillig, Swarat Chaudhuri",
neurips,https://proceedings.neurips.cc/paper/2020/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf,Wavelet Flow: Fast Training of High Resolution Normalizing Flows,"Jason J. Yu, Konstantinos G. Derpanis, Marcus A. Brubaker",
neurips,https://proceedings.neurips.cc/paper/2020/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf,Multi-task Batch Reinforcement Learning with Metric Learning,"Jiachen Li, Quan Vuong, Shuang Liu, Minghua Liu, Kamil Ciosek, Henrik Christensen, Hao Su",
neurips,https://proceedings.neurips.cc/paper/2020/file/44bf89b63173d40fb39f9842e308b3f9-Paper.pdf,On 1/n neural representation and robustness,"Josue Nassar, Piotr Sokol, Sueyeon Chung, Kenneth D. Harris, Il Memming Park",
neurips,https://proceedings.neurips.cc/paper/2020/file/44e76e99b5e194377e955b13fb12f630-Paper.pdf,Boundary thickness and robustness in learning models,"Yaoqing Yang, Rajiv Khanna, Yaodong Yu, Amir Gholami, Kurt Keutzer, Joseph E. Gonzalez, Kannan Ramchandran, Michael W. Mahoney",
neurips,https://proceedings.neurips.cc/paper/2020/file/44ece762ae7e41e3a0b1301488907eaa-Paper.pdf,Demixed shared component analysis of neural population data from multiple brain areas,"Yu Takagi, Steven Kennerley, Jun-ichiro Hirayama, Laurence Hunt",
neurips,https://proceedings.neurips.cc/paper/2020/file/44f683a84163b3523afe57c2e008bc8c-Paper.pdf,Learning Kernel Tests Without Data Splitting,"Jonas Kübler, Wittawat Jitkrittum, Bernhard Schölkopf, Krikamol Muandet",
neurips,https://proceedings.neurips.cc/paper/2020/file/44feb0096faa8326192570788b38c1d1-Paper.pdf,Unsupervised Data Augmentation for Consistency Training,"Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, Quoc Le",
neurips,https://proceedings.neurips.cc/paper/2020/file/456048afb7253926e1fbb7486e699180-Paper.pdf,Subgroup-based Rank-1 Lattice Quasi-Monte Carlo,"Yueming LYU, Yuan Yuan, Ivor Tsang","Quasi-Monte Carlo (QMC) is an essential tool for integral approximation, Bayesian inference, and sampling for simulation in science, etc. In the QMC area, the rank-1 lattice is important due to its simple operation, and nice property for point set construction. However, the construction of the generating vector of the rank-1 lattice is usually time-consuming through an exhaustive computer search. To address this issue, we propose a simple closed-form rank-1 lattice construction method based on group theory. Our method reduces the number of distinct pairwise distance values to generate a more regular lattice. We theoretically prove a lower and an upper bound of the minimum pairwise distance of any non-degenerate rank-1 lattice. Empirically, our methods can generate near-optimal rank-1 lattice compared with Korobov exhaustive search regarding the
l
1
l
-norm and
l
2
l
-norm minimum distance. Moreover, experimental results show that our method achieves superior approximation performance on the benchmark integration test problems and the kernel approximation problems."
neurips,https://proceedings.neurips.cc/paper/2020/file/45713f6ff2041d3fdfae927b82488db8-Paper.pdf,Minibatch vs Local SGD for Heterogeneous Distributed Learning,"Blake E. Woodworth, Kumar Kshitij Patel, Nati Srebro",
neurips,https://proceedings.neurips.cc/paper/2020/file/45c166d697d65080d54501403b433256-Paper.pdf,Multi-task Causal Learning with Gaussian Processes,"Virginia Aglietti, Theodoros Damoulas, Mauricio Álvarez, Javier González",
neurips,https://proceedings.neurips.cc/paper/2020/file/45f31d16b1058d586fc3be7207b58053-Paper.pdf,Proximity Operator of the Matrix Perspective Function and its Applications,Joong-Ho (Johann) Won,
neurips,https://proceedings.neurips.cc/paper/2020/file/45fbc6d3e05ebd93369ce542e8f2322d-Paper.pdf,Generative 3D Part Assembly via Dynamic Graph Learning,"jialei huang, Guanqi Zhan, Qingnan Fan, Kaichun Mo, Lin Shao, Baoquan Chen, Leonidas J. Guibas, Hao Dong",
neurips,https://proceedings.neurips.cc/paper/2020/file/460191c72f67e90150a093b4585e7eb4-Paper.pdf,Improving Natural Language Processing Tasks with Human Gaze-Guided Neural Attention,"Ekta Sood, Simon Tannert, Philipp Mueller, Andreas Bulling",
neurips,https://proceedings.neurips.cc/paper/2020/file/4607f7fff0dce694258e1c637512aa9d-Paper.pdf,The Power of Comparisons for Actively Learning Linear Classifiers,"Max Hopkins, Daniel Kane, Shachar Lovett",
neurips,https://proceedings.neurips.cc/paper/2020/file/464074179972cbbd75a39abc6954cd12-Paper.pdf,From Boltzmann Machines to Neural Networks and Back Again,"Surbhi Goel, Adam Klivans, Frederic Koehler","Graphical models are powerful tools for modeling high-dimensional data, but learning graphical models in the presence of latent variables is well-known to be difficult. In this work we give new results for learning Restricted Boltzmann Machines, probably the most well-studied class of latent variable models. Our results are based on new connections to learning two-layer neural networks under
ℓ
∞
ℓ
bounded input; for both problems, we give nearly optimal results under the conjectured hardness of sparse parity with noise. Using the connection between RBMs and feedforward networks, we also initiate the theoretical study of {\em supervised RBMs} \citep{hinton2012practical}, a version of neural-network learning that couples distributional assumptions induced from the underlying graphical model with the architecture of the unknown function class. We then give an algorithm for learning a natural class of supervised RBMs with better runtime than what is possible for its related class of networks without distributional assumptions."
neurips,https://proceedings.neurips.cc/paper/2020/file/46489c17893dfdcf028883202cefd6d1-Paper.pdf,Crush Optimism with Pessimism: Structured Bandits Beyond Asymptotic Optimality,"Kwang-Sung Jun, Chicheng Zhang","We study stochastic structured bandits for minimizing regret. The fact that the popular optimistic algorithms do not achieve the asymptotic instance-dependent regret optimality (asymptotic optimality for short) has recently alluded researchers. On the other hand, it is known that one can achieve bounded regret (i.e., does not grow indefinitely with
n
n
) in certain instances. Unfortunately, existing asymptotically optimal algorithms rely on forced sampling that introduces an
ω
(
1
)
ω
term w.r.t. the time horizon
n
n
in their regret, failing to adapt to the
easiness'' of the instance. In this paper, we focus on the finite hypothesis case and ask if one can achieve the asymptotic optimality while enjoying bounded regret whenever possible. We provide a positive answer by introducing a new algorithm called CRush Optimism with Pessimism (CROP) that eliminates optimistic hypotheses by pulling the informative arms indicated by a pessimistic hypothesis. Our finite-time analysis shows that CROP
(
i
)
(
achieves a constant-factor asymptotic optimality and, thanks to the forced-exploration-free design,
(
i
i
)
(
adapts to bounded regret, and
(
i
i
i
)
(
its regret bound scales not with
K
K
but with an effective number of arms
K
ψ
K
that we introduce. We also discuss a problem class where CROP can be exponentially better than existing algorithms in \textit{nonasymptotic} regimes. This problem class also reveals a surprising fact that even a clairvoyant oracle who plays according to the asymptotically optimal arm pull scheme may suffer a linear worst-case regret."
neurips,https://proceedings.neurips.cc/paper/2020/file/46a4378f835dc8040c8057beb6a2da52-Paper.pdf,Pruning neural networks without any data by iteratively conserving synaptic flow,"Hidenori Tanaka, Daniel Kunin, Daniel L. Yamins, Surya Ganguli",
neurips,https://proceedings.neurips.cc/paper/2020/file/473803f0f2ebd77d83ee60daaa61f381-Paper.pdf,Detecting Interactions from Neural Networks via Topological Analysis,"Zirui Liu, Qingquan Song, Kaixiong Zhou, Ting-Hsiang Wang, Ying Shan, Xia Hu",
neurips,https://proceedings.neurips.cc/paper/2020/file/475d66314dc56a0df8fb8f7c5dbbaf78-Paper.pdf,Neural Bridge Sampling for Evaluating Safety-Critical Autonomous Systems,"Aman Sinha, Matthew O'Kelly, Russ Tedrake, John C. Duchi",
neurips,https://proceedings.neurips.cc/paper/2020/file/477bdb55b231264bb53a7942fd84254d-Paper.pdf,Interpretable and Personalized Apprenticeship Scheduling: Learning Interpretable Scheduling Policies from Heterogeneous User Demonstrations,"Rohan Paleja, Andrew Silva, Letian Chen, Matthew Gombolay","Resource scheduling and coordination is an NP-hard optimization requiring an efficient allocation of agents to a set of tasks with upper- and lower bound temporal and resource constraints. Due to the large-scale and dynamic nature of resource coordination in hospitals and factories, human domain experts manually plan and adjust schedules on the fly. To perform this job, domain experts leverage heterogeneous strategies and rules-of-thumb honed over years of apprenticeship. What is critically needed is the ability to extract this domain knowledge in a heterogeneous and interpretable apprenticeship learning framework to scale beyond the power of a single human expert, a necessity in safety-critical domains. We propose a personalized and interpretable apprenticeship scheduling algorithm that infers an interpretable representation of all human task demonstrators by extracting decision-making criteria via an inferred, personalized embedding non-parametric in the number of demonstrator types. We achieve near-perfect LfD accuracy in synthetic domains and 88.22\% accuracy on a planning domain with real-world data, outperforming baselines. Finally, our user study showed our methodology produces more interpretable and easier-to-use models than neural networks (
p
<
0.05
p
)."
neurips,https://proceedings.neurips.cc/paper/2020/file/47951a40efc0d2f7da8ff1ecbfde80f4-Paper.pdf,Task-Agnostic Online Reinforcement Learning with an Infinite Mixture of Gaussian Processes,"Mengdi Xu, Wenhao Ding, Jiacheng Zhu, ZUXIN LIU, Baiming Chen, Ding Zhao",
neurips,https://proceedings.neurips.cc/paper/2020/file/47a3893cc405396a5c30d91320572d6d-Paper.pdf,Benchmarking Deep Learning Interpretability in Time Series Predictions,"Aya Abdelsalam Ismail, Mohamed Gunady, Hector Corrada Bravo, Soheil Feizi",
neurips,https://proceedings.neurips.cc/paper/2020/file/47a658229eb2368a99f1d032c8848542-Paper.pdf,Federated Principal Component Analysis,"Andreas Grammenos, Rodrigo Mendoza Smith, Jon Crowcroft, Cecilia Mascolo","We present a federated, asynchronous, and
(
ε
,
δ
)
(
-differentially private algorithm for
\PCA
\PCA
in the memory-limited setting. % Our algorithm incrementally computes local model updates using a streaming procedure and adaptively estimates its
r
r
leading principal components when only
O
(
d
r
)
O
memory is available with
d
d
being the dimensionality of the data. % We guarantee differential privacy via an input-perturbation scheme in which the covariance matrix of a dataset
\B
X
∈
\R
d
×
n
\B
is perturbed with a non-symmetric random Gaussian matrix with variance in
O
(
(
d
n
)
2
log
d
)
O
, thus improving upon the state-of-the-art. % Furthermore, contrary to previous federated or distributed algorithms for
\PCA
\PCA
, our algorithm is also invariant to permutations in the incoming data, which provides robustness against straggler or failed nodes. % Numerical simulations show that, while using limited-memory, our algorithm exhibits performance that closely matches or outperforms traditional non-federated algorithms, and in the absence of communication latency, it exhibits attractive horizontal scalability."
neurips,https://proceedings.neurips.cc/paper/2020/file/47ce0875420b2dbacfc5535f94e68433-Paper.pdf,(De)Randomized Smoothing for Certifiable Defense against Patch Attacks,"Alexander Levine, Soheil Feizi",
neurips,https://proceedings.neurips.cc/paper/2020/file/47d40767c7e9df50249ebfd9c7cfff77-Paper.pdf,SMYRF - Efficient Attention using Asymmetric Clustering,"Giannis Daras, Nikita Kitaev, Augustus Odena, Alexandros G. Dimakis","We propose a novel type of balanced clustering algorithm to approximate attention. Attention complexity is reduced from
O
(
N
2
)
O
to
O
(
N
log
N
)
O
, where N is the sequence length. Our algorithm, SMYRF, uses Locality Sensitive Hashing (LSH) in a novel way by defining new Asymmetric transformations and an adaptive scheme that produces balanced clusters. The biggest advantage of SMYRF is that it can be used as a drop-in replacement for dense attention layers without any retraining. On the contrary, prior fast attention methods impose constraints (e.g. tight queries and keys) and require re-training from scratch. We apply our method to pre-trained state-of-the-art Natural Language Processing and Computer Vision models and we report significant memory and speed benefits. Notably, SMYRF-BERT outperforms (slightly) BERT on GLUE, while using 50% less memory. We also show that SMYRF can be used interchangeably with dense attention before and after training. Finally, we use SMYRF to train GANs with attention in high resolutions. Using a single TPU, we train BigGAN on Celeba-HQ, with attention at resolution 128x128 and 256x256, capable of generating realistic human faces."
neurips,https://proceedings.neurips.cc/paper/2020/file/47fd3c87f42f55d4b233417d49c34783-Paper.pdf,Introducing Routing Uncertainty in Capsule Networks,"Fabio De Sousa Ribeiro, Georgios Leontidis, Stefanos Kollias",
neurips,https://proceedings.neurips.cc/paper/2020/file/481d462e46c2ab976294271a175b8929-Paper.pdf,A Simple and Efficient Smoothing Method for Faster Optimization and Local Exploration,"Kevin Scaman, Ludovic DOS SANTOS, Merwan Barlier, Igor Colin","This work proposes a novel smoothing method, called Bend, Mix and Release (BMR), that extends two well-known smooth approximations of the convex optimization literature: randomized smoothing and the Moreau envelope. The BMR smoothing method allows to trade-off between the computational simplicity of randomized smoothing (RS) and the approximation efficiency of the Moreau envelope (ME). More specifically, we show that BMR achieves up to a
√
d
d
multiplicative improvement compared to the approximation error of RS, where
d
d
is the dimension of the search space, while being less computation intensive than the ME. For non-convex objectives, BMR also has the desirable property to widen local minima, allowing optimization methods to reach small cracks and crevices of extremely irregular and non-convex functions, while being well-suited to a distributed setting. This novel smoothing method is then used to improve first-order non-smooth optimization (both convex and non-convex) by allowing for a local exploration of the search space. More specifically, our analysis sheds light on the similarities between evolution strategies and BMR, creating a link between exploration strategies of zeroth-order methods and the regularity of first-order optimization problems. Finally, we evidence the impact of BMR through synthetic experiments."
neurips,https://proceedings.neurips.cc/paper/2020/file/481fbfa59da2581098e841b7afc122f1-Paper.pdf,Hyperparameter Ensembles for Robustness and Uncertainty Quantification,"Florian Wenzel, Jasper Snoek, Dustin Tran, Rodolphe Jenatton",
neurips,https://proceedings.neurips.cc/paper/2020/file/48237d9f2dea8c74c2a72126cf63d933-Paper.pdf,Neutralizing Self-Selection Bias in Sampling for Sortition,"Bailey Flanigan, Paul Gölz, Anupam Gupta, Ariel D. Procaccia",
neurips,https://proceedings.neurips.cc/paper/2020/file/483101a6bc4e6c46a86222eb65fbcb6a-Paper.pdf,On the Convergence of Smooth Regularized Approximate Value Iteration Schemes,"Elena Smirnova, Elvis Dohmatob",
neurips,https://proceedings.neurips.cc/paper/2020/file/488e4104520c6aab692863cc1dba45af-Paper.pdf,Off-Policy Evaluation via the Regularized Lagrangian,"Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, Dale Schuurmans",
neurips,https://proceedings.neurips.cc/paper/2020/file/48db71587df6c7c442e5b76cc723169a-Paper.pdf,The LoCA Regret: A Consistent Metric to Evaluate Model-Based Behavior in Reinforcement Learning,"Harm Van Seijen, Hadi Nekoei, Evan Racah, Sarath Chandar",
neurips,https://proceedings.neurips.cc/paper/2020/file/48e59000d7dfcf6c1d96ce4a603ed738-Paper.pdf,Neural Power Units,"Niklas Heim, Tomas Pevny, Vasek Smidl",
neurips,https://proceedings.neurips.cc/paper/2020/file/48f7d3043bc03e6c48a6f0ebc0f258a8-Paper.pdf,Towards Scalable Bayesian Learning of Causal DAGs,"Jussi Viinikka, Antti Hyttinen, Johan Pensar, Mikko Koivisto",
neurips,https://proceedings.neurips.cc/paper/2020/file/490640b43519c77281cb2f8471e61a71-Paper.pdf,A Dictionary Approach to Domain-Invariant Learning in Deep Networks,"Ze Wang, Xiuyuan Cheng, Guillermo Sapiro, Qiang Qiu",
neurips,https://proceedings.neurips.cc/paper/2020/file/492114f6915a69aa3dd005aa4233ef51-Paper.pdf,Bootstrapping neural processes,"Juho Lee, Yoonho Lee, Jungtaek Kim, Eunho Yang, Sung Ju Hwang, Yee Whye Teh",
neurips,https://proceedings.neurips.cc/paper/2020/file/49562478de4c54fafd4ec46fdb297de5-Paper.pdf,Large-Scale Adversarial Training for Vision-and-Language Representation Learning,"Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, Jingjing Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/497476fe61816251905e8baafdf54c23-Paper.pdf,"Most ReLU Networks Suffer from
ℓ
2
ℓ
Adversarial Perturbations","Amit Daniely, Hadas Shacham","We consider ReLU networks with random weights, in which the dimension decreases at each layer. We show that for most such networks, most examples
x
x
admit an adversarial perturbation at an Euclidean distance of
O
(
∥
x
∥
√
d
)
O
, where
d
d
is the input dimension. Moreover, this perturbation can be found via gradient flow, as well as gradient descent with sufficiently small steps. This result can be seen as an explanation to the abundance of adversarial examples, and to the fact that they are found via gradient descent."
neurips,https://proceedings.neurips.cc/paper/2020/file/49856ed476ad01fcff881d57e161d73f-Paper.pdf,Compositional Visual Generation with Energy Based Models,"Yilun Du, Shuang Li, Igor Mordatch",
neurips,https://proceedings.neurips.cc/paper/2020/file/49ca03822497d26a3943d5084ed59130-Paper.pdf,Factor Graph Grammars,"David Chiang, Darcey Riley",
neurips,https://proceedings.neurips.cc/paper/2020/file/49f85a9ed090b20c8bed85a5923c669f-Paper.pdf,Erdos Goes Neural: an Unsupervised Learning Framework for Combinatorial Optimization on Graphs,"Nikolaos Karalias, Andreas Loukas",
neurips,https://proceedings.neurips.cc/paper/2020/file/4a4526b1ec301744aba9526d78fcb2a6-Paper.pdf,Autoregressive Score Matching,"Chenlin Meng, Lantao Yu, Yang Song, Jiaming Song, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2020/file/4a46fbfca3f1465a27b210f4bdfe6ab3-Paper.pdf,Debiasing Distributed Second Order Optimization with Surrogate Sketching and Scaled Regularization,"Michal Derezinski, Burak Bartan, Mert Pilanci, Michael W. Mahoney","In distributed second order optimization, a standard strategy is to average many local estimates, each of which is based on a small sketch or batch of the data. However, the local estimates on each machine are typically biased, relative to the full solution on all of the data, and this can limit the effectiveness of averaging. Here, we introduce a new technique for debiasing the local estimates, which leads to both theoretical and empirical improvements in the convergence rate of distributed second order methods. Our technique has two novel components: (1) modifying standard sketching techniques to obtain what we call a surrogate sketch; and (2) carefully scaling the global regularization parameter for local computations. Our surrogate sketches are based on determinantal point processes, a family of distributions for which the bias of an estimate of the inverse Hessian can be computed exactly. Based on this computation, we show that when the objective being minimized is
l
2
l
-regularized with parameter
λ
λ
and individual machines are each given a sketch of size
m
m
, then to eliminate the bias, local estimates should be computed using a shrunk regularization parameter given by
λ
′
=
λ
(
1
−
d
λ
m
)
λ
, where
d
λ
d
is the
λ
λ
-effective dimension of the Hessian (or, for quadratic problems, the data matrix)."
neurips,https://proceedings.neurips.cc/paper/2020/file/4a5876b450b45371f6cfe5047ac8cd45-Paper.pdf,Neural Controlled Differential Equations for Irregular Time Series,"Patrick Kidger, James Morrill, James Foster, Terry Lyons",
neurips,https://proceedings.neurips.cc/paper/2020/file/4a5cfa9281924139db466a8a19291aff-Paper.pdf,On Efficiency in Hierarchical Reinforcement Learning,"Zheng Wen, Doina Precup, Morteza Ibrahimi, Andre Barreto, Benjamin Van Roy, Satinder Singh",
neurips,https://proceedings.neurips.cc/paper/2020/file/4aaa76178f8567e05c8e8295c96171d8-Paper.pdf,On Correctness of Automatic Differentiation for Non-Differentiable Functions,"Wonyeol Lee, Hangyeol Yu, Xavier Rival, Hongseok Yang",
neurips,https://proceedings.neurips.cc/paper/2020/file/4afd521d77158e02aed37e2274b90c9c-Paper.pdf,Probabilistic Linear Solvers for Machine Learning,"Jonathan Wenger, Philipp Hennig",
neurips,https://proceedings.neurips.cc/paper/2020/file/4b0091f82f50ff7095647fe893580d60-Paper.pdf,Dynamic Regret of Policy Optimization in Non-Stationary Environments,"Yingjie Fei, Zhuoran Yang, Zhaoran Wang, Qiaomin Xie",
neurips,https://proceedings.neurips.cc/paper/2020/file/4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf,Multipole Graph Neural Operator for Parametric Partial Differential Equations,"Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhattacharya, Anima Anandkumar",
neurips,https://proceedings.neurips.cc/paper/2020/file/4b29fa4efe4fb7bc667c7b301b74d52d-Paper.pdf,BlockGAN: Learning 3D Object-aware Scene Representations from Unlabelled Images,"Thu H. Nguyen-Phuoc, Christian Richardt, Long Mai, Yongliang Yang, Niloy Mitra",
neurips,https://proceedings.neurips.cc/paper/2020/file/4b86ca48d90bd5f0978afa3a012503a4-Paper.pdf,Online Structured Meta-learning,"Huaxiu Yao, Yingbo Zhou, Mehrdad Mahdavi, Zhenhui (Jessie) Li, Richard Socher, Caiming Xiong",
neurips,https://proceedings.neurips.cc/paper/2020/file/4bb236de7787ceedafdff83bb8ea4710-Paper.pdf,Learning Strategic Network Emergence Games,"Rakshit Trivedi, Hongyuan Zha",
neurips,https://proceedings.neurips.cc/paper/2020/file/4be2c8f27b8a420492f2d44463933eb6-Paper.pdf,Towards Interpretable Natural Language Understanding with Explanations as Latent Variables,"Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, Jian Tang",
neurips,https://proceedings.neurips.cc/paper/2020/file/4bfbd52f4e8466dc12aaf30b7e057b66-Paper.pdf,The Mean-Squared Error of Double Q-Learning,"Wentao Weng, Harsh Gupta, Niao He, Lei Ying, R. Srikant",
neurips,https://proceedings.neurips.cc/paper/2020/file/4c2e5eaae9152079b9e95845750bb9ab-Paper.pdf,What Makes for Good Views for Contrastive Learning?,"Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, Phillip Isola",
neurips,https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf,Denoising Diffusion Probabilistic Models,"Jonathan Ho, Ajay Jain, Pieter Abbeel",
neurips,https://proceedings.neurips.cc/paper/2020/file/4cc05b35c2f937c5bd9e7d41d3686fff-Paper.pdf,Barking up the right tree: an approach to search over molecule synthesis DAGs,"John Bradshaw, Brooks Paige, Matt J. Kusner, Marwin Segler, José Miguel Hernández-Lobato",
neurips,https://proceedings.neurips.cc/paper/2020/file/4cc5400e63624c44fadeda99f57588a6-Paper.pdf,On Uniform Convergence and Low-Norm Interpolation Learning,"Lijia Zhou, Danica J. Sutherland, Nati Srebro",
neurips,https://proceedings.neurips.cc/paper/2020/file/4cea2358d3cc5f8cd32397ca9bc51b94-Paper.pdf,Bandit Samplers for Training Graph Neural Networks,"Ziqi Liu, Zhengwei Wu, Zhiqiang Zhang, Jun Zhou, Shuang Yang, Le Song, Yuan Qi",
neurips,https://proceedings.neurips.cc/paper/2020/file/4d410063822cd9be28f86701c0bc3a31-Paper.pdf,Sampling from a k-DPP without looking at all items,"Daniele Calandriello, Michal Derezinski, Michal Valko",
neurips,https://proceedings.neurips.cc/paper/2020/file/4d771504ddcd28037b4199740df767e6-Paper.pdf,Uncovering the Topology of Time-Varying fMRI Data using Cubical Persistence,"Bastian Rieck, Tristan Yates, Christian Bock, Karsten Borgwardt, Guy Wolf, Nicholas Turk-Browne, Smita Krishnaswamy",
neurips,https://proceedings.neurips.cc/paper/2020/file/4d7e0d72898ae7ea3593eb5ebf20c744-Paper.pdf,Hierarchical Poset Decoding for Compositional Generalization in Language,"Yinuo Guo, Zeqi Lin, Jian-Guang Lou, Dongmei Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/4d95d05a4fc4eadbc3b9dde67afdca39-Paper.pdf,Evaluating and Rewarding Teamwork Using Cooperative Game Abstractions,"Tom Yan, Christian Kroer, Alexander Peysakhovich",
neurips,https://proceedings.neurips.cc/paper/2020/file/4db73860ecb5533b5a6c710341d5bbec-Paper.pdf,Exchangeable Neural ODE for Set Modeling,"Yang Li, Haidong Yi, Christopher Bender, Siyuan Shan, Junier B. Oliva",
neurips,https://proceedings.neurips.cc/paper/2020/file/4dbf29d90d5780cab50897fb955e4373-Paper.pdf,Profile Entropy: A Fundamental Measure for the Learnability and Compressibility of Distributions,"Yi Hao, Alon Orlitsky",
neurips,https://proceedings.neurips.cc/paper/2020/file/4dc3ed26a29c9c3df3ec373524377a5b-Paper.pdf,CoADNet: Collaborative Aggregation-and-Distribution Networks for Co-Salient Object Detection,"Qijian Zhang, Runmin Cong, Junhui Hou, Chongyi Li, Yao Zhao",
neurips,https://proceedings.neurips.cc/paper/2020/file/4dd9cec1c21bc54eecb53786a2c5fa09-Paper.pdf,"Regularized linear autoencoders recover the principal components, eventually","Xuchan Bao, James Lucas, Sushant Sachdeva, Roger B. Grosse",
neurips,https://proceedings.neurips.cc/paper/2020/file/4dea382d82666332fb564f2e711cbc71-Paper.pdf,Semi-Supervised Partial Label Learning via Confidence-Rated Margin Maximization,"Wei Wang, Min-Ling Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/4df5bde009073d3ef60da64d736724d6-Paper.pdf,GramGAN: Deep 3D Texture Synthesis From 2D Exemplars,"Tiziano Portenier, Siavash Arjomand Bigdeli, Orcun Goksel",
neurips,https://proceedings.neurips.cc/paper/2020/file/4e0928de075538c593fbdabb0c5ef2c3-Paper.pdf,UWSOD: Toward Fully-Supervised-Level Capacity Weakly Supervised Object Detection,"Yunhang Shen, Rongrong Ji, Zhiwei Chen, Yongjian Wu, Feiyue Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/4e668929edb3bf915e1a3a9d96c3c97e-Paper.pdf,Learning Restricted Boltzmann Machines with Sparse Latent Variables,"Guy Bresler, Rares-Darius Buhai","Restricted Boltzmann Machines (RBMs) are a common family of undirected graphical models with latent variables. An RBM is described by a bipartite graph, with all observed variables in one layer and all latent variables in the other. We consider the task of learning an RBM given samples generated according to it. The best algorithms for this task currently have time complexity
~
O
(
n
2
)
O
for ferromagnetic RBMs (i.e., with attractive potentials) but
~
O
(
n
d
)
O
for general RBMs, where
n
n
is the number of observed variables and
d
d
is the maximum degree of a latent variable. Let the \textit{MRF neighborhood} of an observed variable be its neighborhood in the Markov Random Field of the marginal distribution of the observed variables. In this paper, we give an algorithm for learning general RBMs with time complexity
~
O
(
n
2
s
+
1
)
O
, where
s
s
is the maximum number of latent variables connected to the MRF neighborhood of an observed variable. This is an improvement when
s
<
log
2
(
d
−
1
)
s
, which corresponds to RBMs with sparse latent variables. Furthermore, we give a version of this learning algorithm that recovers a model with small prediction error and whose sample complexity is independent of the minimum potential in the Markov Random Field of the observed variables. This is of interest because the sample complexity of current algorithms scales with the inverse of the minimum potential, which cannot be controlled in terms of natural properties of the RBM."
neurips,https://proceedings.neurips.cc/paper/2020/file/4eab60e55fe4c7dd567a0be28016bff3-Paper.pdf,Sample Complexity of Asynchronous Q-Learning: Sharper Analysis and Variance Reduction,"Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, Yuxin Chen","Asynchronous Q-learning aims to learn the optimal action-value function (or Q-function) of a Markov decision process (MDP), based on a single trajectory of Markovian samples induced by a behavior policy. Focusing on a
γ
γ
-discounted MDP with state space S and action space A, we demonstrate that the
ℓ
∞
ℓ
-based sample complexity of classical asynchronous Q-learning --- namely, the number of samples needed to yield an entrywise
ϵ
ϵ
-accurate estimate of the Q-function --- is at most on the order of
1
μ
min
(
1
−
γ
)
5
ϵ
2
+
t
m
i
x
μ
min
(
1
−
γ
)
1
up to some logarithmic factor, provided that a proper constant learning rate is adopted. Here,
t
m
i
x
t
and
μ
min
μ
denote respectively the mixing time and the minimum state-action occupancy probability of the sample trajectory. The first term of this bound matches the complexity in the case with independent samples drawn from the stationary distribution of the trajectory. The second term reflects the expense taken for the empirical distribution of the Markovian trajectory to reach a steady state, which is incurred at the very beginning and becomes amortized as the algorithm runs. Encouragingly, the above bound improves upon the state-of-the-art result by a factor of at least |S||A|. Further, the scaling on the discount complexity can be improved by means of variance reduction."
neurips,https://proceedings.neurips.cc/paper/2020/file/4eb7d41ae6005f60fe401e56277ebd4e-Paper.pdf,Curriculum learning for multilevel budgeted combinatorial problems,"Adel Nabli, Margarida Carvalho","Learning heuristics for combinatorial optimization problems through graph neural networks have recently shown promising results on some classic NP-hard problems. These are single-level optimization problems with only one player. Multilevel combinatorial optimization problems are their generalization, encompassing situations with multiple players taking decisions sequentially. By framing them in a multi-agent reinforcement learning setting, we devise a value-based method to learn to solve multilevel budgeted combinatorial problems involving two players in a zero-sum game over a graph. Our framework is based on a simple curriculum: if an agent knows how to estimate the value of instances with budgets up to
B
B
, then solving instances with budget
B
+
1
B
can be done in polynomial time regardless of the direction of the optimization by checking the value of every possible afterstate. Thus, in a bottom-up approach, we generate datasets of heuristically solved instances with increasingly larger budgets to train our agent. We report results close to optimality on graphs up to
100
100
nodes and a
185
×
185
speedup on average compared to the quickest exact solver known for the Multilevel Critical Node problem, a max-min-max trilevel problem that has been shown to be at least
Σ
p
2
Σ
-hard."
neurips,https://proceedings.neurips.cc/paper/2020/file/4ebd440d99504722d80de606ea8507da-Paper.pdf,FedSplit: an algorithmic framework for fast federated optimization,"Reese Pathak, Martin J. Wainwright",
neurips,https://proceedings.neurips.cc/paper/2020/file/4ecb679fd35dcfd0f0894c399590be1a-Paper.pdf,Estimation and Imputation in Probabilistic Principal Component Analysis with Missing Not At Random Data,"Aude Sportisse, Claire Boyer, Julie Josse",
neurips,https://proceedings.neurips.cc/paper/2020/file/4ee78d4122ef8503fe01cdad3e9ea4ee-Paper.pdf,Correlation Robust Influence Maximization,"Louis Chen, Divya Padmanabhan, Chee Chin Lim, Karthik Natarajan",
neurips,https://proceedings.neurips.cc/paper/2020/file/4ef2f8259495563cb3a8ea4449ec4f9f-Paper.pdf,Neuronal Gaussian Process Regression,Johannes Friedrich,
neurips,https://proceedings.neurips.cc/paper/2020/file/4ef42b32bccc9485b10b8183507e5d82-Paper.pdf,Nonconvex Sparse Graph Learning under Laplacian Constrained Graphical Model,"Jiaxi Ying, José Vinícius de Miranda Cardoso , Daniel Palomar","In this paper, we consider the problem of learning a sparse graph from the Laplacian constrained Gaussian graphical model. This problem can be formulated as a penalized maximum likelihood estimation of the precision matrix under Laplacian structural constraints. Like in the classical graphical lasso problem, recent works made use of the
ℓ
1
ℓ
-norm with the goal of promoting sparsity in the Laplacian constrained precision matrix estimation. However, through empirical evidence, we observe that the
ℓ
1
ℓ
-norm is not effective in imposing a sparse solution in this problem. From a theoretical perspective, we prove that a large regularization parameter will surprisingly lead to a solution representing a fully connected graph instead of a sparse graph. To address this issue, we propose a nonconvex penalized maximum likelihood estimation method, and establish the order of the statistical error. Numerical experiments involving synthetic and real-world data sets demonstrate the effectiveness of the proposed method."
neurips,https://proceedings.neurips.cc/paper/2020/file/4eff0720836a198b6174eecf02cbfdbf-Paper.pdf,Synthetic Data Generators -- Sequential and Private,"Olivier Bousquet, Roi Livni, Shay Moran","We study the sample complexity of private synthetic data generation over an unbounded sized class of statistical queries, and show that any class that is privately proper PAC learnable admits a private synthetic data generator (perhaps non-efficient). A differentially private synthetic generator is an algorithm that receives an IID data and publishes synthetic data that is indistinguishable from the true data w.r.t a given fixed class of statistical queries. The synthetic data set can then be used by a data scientist without compromising the privacy of the original data set. Previous work on synthetic data generators focused on the case that the query class
\D
\D
is finite and obtained sample complexity bounds that scale logarithmically with the size
|
\D
|
|
. Here we construct a private synthetic data generator whose sample complexity is independent of the domain size, and we replace finiteness with the assumption that
\D
\D
is privately PAC learnable (a formally weaker task, hence we obtain equivalence between the two tasks). Our proof relies on a new type of synthetic data generator, Sequential Synthetic Data Generators, which we believe may be of interest of their own right. A sequential SDG is defined by a sequential game between a generator that proposes synthetic distributions and a discriminator that tries to distinguish between real and fake distributions. We characterize the classes that admit a sequential-SDG and show that they are exactly Littlestone classes. Given the online nature of the sequential setting, it is natural that Littlestone classes arise in this context. Nevertheless, the characterization of sequential--SDGs by Littlestone classes turns out to be technically challenging, and to the best of the author's knowledge, does not follow via simple reductions to online prediction."
neurips,https://proceedings.neurips.cc/paper/2020/file/4f00921114932db3f8662a41b44ee68f-Paper.pdf,Uncertainty Quantification for Inferring Hawkes Networks,"Haoyun Wang, Liyan Xie, Alex Cuozzo, Simon Mak, Yao Xie",
neurips,https://proceedings.neurips.cc/paper/2020/file/4f20f7f5d2e7a1b640ebc8244428558c-Paper.pdf,Implicit Distributional Reinforcement Learning,"Yuguang Yue, Zhendong Wang, Mingyuan Zhou",
neurips,https://proceedings.neurips.cc/paper/2020/file/4f87658ef0de194413056248a00ce009-Paper.pdf,Auxiliary Task Reweighting for Minimum-data Learning,"Baifeng Shi, Judy Hoffman, Kate Saenko, Trevor Darrell, Huijuan Xu",
neurips,https://proceedings.neurips.cc/paper/2020/file/4fbe073f17f161810fdf3dab1307b30f-Paper.pdf,Small Nash Equilibrium Certificates in Very Large Games,"Brian Zhang, Tuomas Sandholm",
neurips,https://proceedings.neurips.cc/paper/2020/file/4fc28b7093b135c21c7183ac07e928a6-Paper.pdf,Training Linear Finite-State Machines,"Arash Ardakani, Amir Ardakani, Warren Gross",
neurips,https://proceedings.neurips.cc/paper/2020/file/5034a5d62f91942d2a7aeaf527dfe111-Paper.pdf,Efficient active learning of sparse halfspaces with arbitrary bounded noise,"Chicheng Zhang, Jie Shen, Pranjal Awasthi","We study active learning of homogeneous
s
s
-sparse halfspaces in
R
d
R
under the setting where the unlabeled data distribution is isotropic log-concave and each label is flipped with probability at most
η
η
for a parameter
η
∈
[
0
,
1
2
)
η
, known as the bounded noise. Even in the presence of mild label noise, i.e.
η
η
is a small constant, this is a challenging problem and only recently have label complexity bounds of the form
~
O
(
s
⋅
p
o
l
y
l
o
g
(
d
,
1
ϵ
)
)
O
been established in [Zhang 2018] for computationally efficient algorithms. In contrast, under high levels of label noise, the label complexity bounds achieved by computationally efficient algorithms are much worse: the best known result [Awasthi et al. 2016] provides a computationally efficient algorithm with label complexity
~
O
(
(
s
l
n
d
/
ϵ
)
p
o
l
y
(
1
/
(
1
−
2
η
)
)
)
O
, which is label-efficient only when the noise rate
η
η
is a fixed constant. In this work, we substantially improve on it by designing a polynomial time algorithm for active learning of
s
s
-sparse halfspaces, with a label complexity of
~
O
(
s
(
1
−
2
η
)
4
p
o
l
y
l
o
g
(
d
,
1
ϵ
)
)
O
. This is the first efficient algorithm with label complexity polynomial in
1
1
−
2
η
1
in this setting, which is label-efficient even for
η
η
arbitrarily close to
1
2
1
. Our active learning algorithm and its theoretical guarantees also immediately translate to new state-of-the-art label and sample complexity results for full-dimensional active and passive halfspace learning under arbitrary bounded noise."
neurips,https://proceedings.neurips.cc/paper/2020/file/50905d7b2216bfeccb5b41016357176b-Paper.pdf,Swapping Autoencoder for Deep Image Manipulation,"Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei Efros, Richard Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/50c1f44e426560f3f2cdcb3e19e39903-Paper.pdf,Self-Supervised Few-Shot Learning on Point Clouds,"Charu Sharma, Manohar Kaul",
neurips,https://proceedings.neurips.cc/paper/2020/file/50cf0fe63e0ff857e1c9d01d827267ca-Paper.pdf,Faster Differentially Private Samplers via Rényi Divergence Analysis of Discretized Langevin MCMC,"Arun Ganesh, Kunal Talwar","Various differentially private algorithms instantiate the exponential mechanism, and require sampling from the distribution
exp
(
−
f
)
exp
for a suitable function
f
f
. When the domain of the distribution is high-dimensional, this sampling can be challenging. Using heuristic sampling schemes such as Gibbs sampling does not necessarily lead to provable privacy. When
f
f
is convex, techniques from log-concave sampling lead to polynomial-time algorithms, albeit with large polynomials. Langevin dynamics-based algorithms offer much faster alternatives under some distance measures such as statistical distance. In this work, we establish rapid convergence for these algorithms under distance measures more suitable for differential privacy. For smooth, strongly-convex
f
f
, we give the first results proving convergence in R\'enyi divergence. This gives us fast differentially private algorithms for such
f
f
. Our techniques and simple and generic and apply also to underdamped Langevin dynamics."
neurips,https://proceedings.neurips.cc/paper/2020/file/510f2318f324cf07fce24c3a4b89c771-Paper.pdf,Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE,"Ding Zhou, Xue-Xin Wei",
neurips,https://proceedings.neurips.cc/paper/2020/file/51200d29d1fc15f5a71c1dab4bb54f7c-Paper.pdf,RL Unplugged: A Suite of Benchmarks for Offline Reinforcement Learning,"Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio Gómez, Konrad Zolna, Rishabh Agarwal, Josh S. Merel, Daniel J. Mankowitz, Cosmin Paduraru, Gabriel Dulac-Arnold, Jerry Li, Mohammad Norouzi, Matthew Hoffman, Nicolas Heess, Nando de Freitas",
neurips,https://proceedings.neurips.cc/paper/2020/file/512c5cad6c37edb98ae91c8a76c3a291-Paper.pdf,Dual T: Reducing Estimation Error for Transition Matrix in Label-noise Learning,"Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Jiankang Deng, Gang Niu, Masashi Sugiyama","The transition matrix, denoting the transition relationship from clean labels to noisy labels, is essential to build statistically consistent classifiers in label-noise learning. Existing methods for estimating the transition matrix rely heavily on estimating the noisy class posterior. However, the estimation error for noisy class posterior could be large because of the randomness of label noise. The estimation error would lead the transition matrix to be poorly estimated. Therefore in this paper, we aim to solve this problem by exploiting the divide-and-conquer paradigm. Specifically, we introduce an intermediate class to avoid directly estimating the noisy class posterior. By this intermediate class, the original transition matrix can then be factorized into the product of two easy-to-estimated transition matrices. We term the proposed method as the dual
T
T
-estimator. Both theoretical analyses and empirical results illustrate the effectiveness of the dual
T
T
-estimator for estimating transition matrices, leading to better classification performances."
neurips,https://proceedings.neurips.cc/paper/2020/file/51311013e51adebc3c34d2cc591fefee-Paper.pdf,Interior Point Solving for LP-based prediction+optimisation,"Jayanta Mandi, Tias Guns",
neurips,https://proceedings.neurips.cc/paper/2020/file/5133aa1d673894d5a05b9d83809b9dbe-Paper.pdf,A simple normative network approximates local non-Hebbian learning in the cortex,"Siavash Golkar, David Lipshutz, Yanis Bahroun, Anirvan Sengupta, Dmitri Chklovskii",
neurips,https://proceedings.neurips.cc/paper/2020/file/517f24c02e620d5a4dac1db388664a63-Paper.pdf,Kernelized information bottleneck leads to biologically plausible 3-factor Hebbian learning in deep networks,"Roman Pogodin, Peter Latham",
neurips,https://proceedings.neurips.cc/paper/2020/file/518a38cc9a0173d0b2dc088166981cf8-Paper.pdf,Understanding the Role of Training Regimes in Continual Learning,"Seyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, Hassan Ghasemzadeh",
neurips,https://proceedings.neurips.cc/paper/2020/file/51cdbd2611e844ece5d80878eb770436-Paper.pdf,Fair regression with Wasserstein barycenters,"Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, Massimiliano Pontil",
neurips,https://proceedings.neurips.cc/paper/2020/file/51f4efbfb3e18f4ea053c4d3d282c4e2-Paper.pdf,Training Stronger Baselines for Learning to Optimize,"Tianlong Chen, Weiyi Zhang, Zhou Jingyang, Shiyu Chang, Sijia Liu, Lisa Amini, Zhangyang Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/5227fa9a19dce7ba113f50a405dcaf09-Paper.pdf,Exactly Computing the Local Lipschitz Constant of ReLU Networks,"Matt Jordan, Alexandros G. Dimakis",
neurips,https://proceedings.neurips.cc/paper/2020/file/524f141e189d2a00968c3d48cadd4159-Paper.pdf,Strictly Batch Imitation Learning by Energy-based Distribution Matching,"Daniel Jarrett, Ioana Bica, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2020/file/5265d33c184af566aeb7ef8afd0b9b03-Paper.pdf,"On the Ergodicity, Bias and Asymptotic Normality of Randomized Midpoint Sampling Method","Ye He, Krishnakumar Balasubramanian, Murat A. Erdogdu",
neurips,https://proceedings.neurips.cc/paper/2020/file/52aaa62e71f829d41d74892a18a11d59-Paper.pdf,A Single-Loop Smoothed Gradient Descent-Ascent Algorithm for Nonconvex-Concave Min-Max Problems,"Jiawei Zhang, Peijun Xiao, Ruoyu Sun, Zhiquan Luo","Nonconvex-concave min-max problem arises in many machine learning applications including minimizing a pointwise maximum of a set of nonconvex functions and robust adversarial training of neural networks. A popular approach to solve this problem is the gradient descent-ascent (GDA) algorithm which unfortunately can exhibit oscillation in case of nonconvexity. In this paper, we introduce a
smoothing"" scheme which can be combined with GDA to stabilize the oscillation and ensure convergence to a stationary solution. We prove that the stabilized GDA algorithm can achieve an
O
(
1
/
ϵ
2
)
O
iteration complexity for minimizing the pointwise maximum of a finite collection of nonconvex functions. Moreover, the smoothed GDA algorithm achieves an
O
(
1
/
ϵ
4
)
O
iteration complexity for general nonconvex-concave problems. Extensions of this stabilized GDA algorithm to multi-block cases are presented. To the best of our knowledge, this is the first algorithm to achieve
O
(
1
/
ϵ
2
)
O
for a class of nonconvex-concave problem. We illustrate the practical efficiency of the stabilized GDA algorithm on robust training."
neurips,https://proceedings.neurips.cc/paper/2020/file/52cf49fea5ff66588408852f65cf8272-Paper.pdf,Generating Correct Answers for Progressive Matrices Intelligence Tests,"Niv Pekar, Yaniv Benny, Lior Wolf",
neurips,https://proceedings.neurips.cc/paper/2020/file/52d2752b150f9c35ccb6869cbf074e48-Paper.pdf,HyNet: Learning Local Descriptor with Hybrid Similarity Measure and Triplet Loss,"Yurun Tian, Axel Barroso Laguna, Tony Ng, Vassileios Balntas, Krystian Mikolajczyk",
neurips,https://proceedings.neurips.cc/paper/2020/file/52f4691a4de70b3c441bca6c546979d9-Paper.pdf,Preference learning along multiple criteria: A game-theoretic perspective,"Kush Bhatia, Ashwin Pananjady, Peter Bartlett, Anca Dragan, Martin J. Wainwright","From a theoretical standpoint, we show that the Blackwell winner of a multi-criteria problem instance can be computed as the solution to a convex optimization problem. Furthermore, given random samples of pairwise comparisons, we show that a simple, ""plug-in"" estimator achieves (near-)optimal minimax sample complexity. Finally, we showcase the practical utility of our framework in a user study on autonomous driving, where we find that the Blackwell winner outperforms the von Neumann winner for the overall preferences."
neurips,https://proceedings.neurips.cc/paper/2020/file/5301c4d888f5204274439e6dcf5fdb54-Paper.pdf,Multi-Plane Program Induction with 3D Box Priors,"Yikai Li, Jiayuan Mao, Xiuming Zhang, Bill Freeman, Josh Tenenbaum, Noah Snavely, Jiajun Wu",
neurips,https://proceedings.neurips.cc/paper/2020/file/531d29a813ef9471aad0a5558d449a73-Paper.pdf,Online Neural Connectivity Estimation with Noisy Group Testing,"Anne Draelos, John Pearson",
neurips,https://proceedings.neurips.cc/paper/2020/file/537d9b6c927223c796cac288cced29df-Paper.pdf,Once-for-All Adversarial Training: In-Situ Tradeoff between Robustness and Accuracy for Free,"Haotao Wang, Tianlong Chen, Shupeng Gui, TingKuei Hu, Ji Liu, Zhangyang Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/53c04118df112c13a8c34b38343b9c10-Paper.pdf,Implicit Neural Representations with Periodic Activation Functions,"Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, Gordon Wetzstein",
neurips,https://proceedings.neurips.cc/paper/2020/file/53c5b2affa12eed84dfec9bfd83550b1-Paper.pdf,Rotated Binary Neural Network,"Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang Zhang, Yan Wang, Yongjian Wu, Feiyue Huang, Chia-Wen Lin",
neurips,https://proceedings.neurips.cc/paper/2020/file/54391c872fe1c8b4f98095c5d6ec7ec7-Paper.pdf,Community detection in sparse time-evolving graphs with a dynamical Bethe-Hessian,"Lorenzo Dall'Amico, Romain Couillet, Nicolas Tremblay",
neurips,https://proceedings.neurips.cc/paper/2020/file/543e83748234f7cbab21aa0ade66565f-Paper.pdf,Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness,"Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, Balaji Lakshminarayanan",
neurips,https://proceedings.neurips.cc/paper/2020/file/54e0e46b6647aa736c13ef9d09eab432-Paper.pdf,Adaptive Learning of Rank-One Models for Efficient Pairwise Sequence Alignment,"Govinda Kamath, Tavor Baharav, Ilan Shomorony",
neurips,https://proceedings.neurips.cc/paper/2020/file/54f3bc04830d762a3b56a789b6ff62df-Paper.pdf,Hierarchical nucleation in deep neural networks,"Diego Doimo, Aldo Glielmo, Alessio Ansuini, Alessandro Laio",
neurips,https://proceedings.neurips.cc/paper/2020/file/55053683268957697aa39fba6f231c68-Paper.pdf,Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains,"Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, Ren Ng",
neurips,https://proceedings.neurips.cc/paper/2020/file/551fdbb810aff145c114b93867dd8bfd-Paper.pdf,Graph Geometry Interaction Learning,"Shichao Zhu, Shirui Pan, Chuan Zhou, Jia Wu, Yanan Cao, Bin Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/55479c55ebd1efd3ff125f1337100388-Paper.pdf,Differentiable Augmentation for Data-Efficient GAN Training,"Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, Song Han",
neurips,https://proceedings.neurips.cc/paper/2020/file/555d6702c950ecb729a966504af0a635-Paper.pdf,Heuristic Domain Adaptation,"Shuhao Cui, Xuan Jin, Shuhui Wang, Yuan He, Qingming Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/55d491cf951b1b920900684d71419282-Paper.pdf,Learning Certified Individually Fair Representations,"Anian Ruoss, Mislav Balunovic, Marc Fischer, Martin Vechev",
neurips,https://proceedings.neurips.cc/paper/2020/file/5607fe8879e4fd269e88387e8cb30b7e-Paper.pdf,Part-dependent Label Noise: Towards Instance-dependent Label Noise,"Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2020/file/564127c03caab942e503ee6f810f54fd-Paper.pdf,Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization,"Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, H. Vincent Poor",
neurips,https://proceedings.neurips.cc/paper/2020/file/56577889b3c1cd083b6d7b32d32f99d5-Paper.pdf,An Improved Analysis of (Variance-Reduced) Policy Gradient and Natural Policy Gradient Methods,"Yanli Liu, Kaiqing Zhang, Tamer Basar, Wotao Yin",
neurips,https://proceedings.neurips.cc/paper/2020/file/565e8a413d0562de9ee4378402d2b481-Paper.pdf,Geometric Exploration for Online Control,"Orestis Plevrakis, Elad Hazan","We study the control of an \emph{unknown} linear dynamical system under general convex costs. The objective is minimizing regret vs the class of strongly-stable linear policies. In this work, we first consider the case of known cost functions, for which we design the first polynomial-time algorithm with
n
3
√
T
n
-regret, where
n
n
is the dimension of the state plus the dimension of control input. The
√
T
T
-horizon dependence is optimal, and improves upon the previous best known bound of
T
2
/
3
T
. The main component of our algorithm is a novel geometric exploration strategy: we adaptively construct a sequence of barycentric spanners in an over-parameterized policy space. Second, we consider the case of bandit feedback, for which we give the first polynomial-time algorithm with
p
o
l
y
(
n
)
√
T
p
-regret, building on Stochastic Bandit Convex Optimization."
neurips,https://proceedings.neurips.cc/paper/2020/file/566f0ea4f6c2e947f36795c8f58ba901-Paper.pdf,Automatic Curriculum Learning through Value Disagreement,"Yunzhi Zhang, Pieter Abbeel, Lerrel Pinto",
neurips,https://proceedings.neurips.cc/paper/2020/file/567b8f5f423af15818a068235807edc0-Paper.pdf,MRI Banding Removal via Adversarial Training,"Aaron Defazio, Tullie Murrell, Michael Recht",
neurips,https://proceedings.neurips.cc/paper/2020/file/569ff987c643b4bedf504efda8f786c2-Paper.pdf,The NetHack Learning Environment,"Heinrich Küttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, Tim Rocktäschel",
neurips,https://proceedings.neurips.cc/paper/2020/file/56dc0997d871e9177069bb472574eb29-Paper.pdf,Language and Visual Entity Relationship Graph for Agent Navigation,"Yicong Hong, Cristian Rodriguez, Yuankai Qi, Qi Wu, Stephen Gould",
neurips,https://proceedings.neurips.cc/paper/2020/file/56f9f88906aebf4ad985aaec7fa01313-Paper.pdf,ICAM: Interpretable Classification via Disentangled Representations and Feature Attribution Mapping,"Cher Bass, Mariana da Silva, Carole Sudre, Petru-Daniel Tudosiu, Stephen Smith, Emma Robinson",
neurips,https://proceedings.neurips.cc/paper/2020/file/572201a4497b0b9f02d4f279b09ec30d-Paper.pdf,Spectra of the Conjugate Kernel and Neural Tangent Kernel for linear-width neural networks,"Zhou Fan, Zhichao Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/5763abe87ed1938799203fb6e8650025-Paper.pdf,No-Regret Learning Dynamics for Extensive-Form Correlated Equilibrium,"Andrea Celli, Alberto Marchesi, Gabriele Farina, Nicola Gatti",
neurips,https://proceedings.neurips.cc/paper/2020/file/5781a2637b476d781eb3134581b32044-Paper.pdf,Estimating weighted areas under the ROC curve,"Andreas Maurer, Massimiliano Pontil",
neurips,https://proceedings.neurips.cc/paper/2020/file/57cd30d9088b0185cf0ebca1a472ff1d-Paper.pdf,Can Implicit Bias Explain Generalization? Stochastic Convex Optimization as a Case Study,"Assaf Dauber, Meir Feder, Tomer Koren, Roi Livni","We revisit this paradigm in arguably the simplest non-trivial setup, and study the implicit bias of Stochastic Gradient Descent (SGD) in the context of Stochastic Convex Optimization. As a first step, we provide a simple construction that rules out the existence of a \emph{distribution-independent} implicit regularizer that governs the generalization ability of SGD. We then demonstrate a learning problem that rules out a very general class of \emph{distribution-dependent} implicit regularizers from explaining generalization, which includes strongly convex regularizers as well as non-degenerate norm-based regularizations. Certain aspects of our constructions point out to significant difficulties in providing a comprehensive explanation of an algorithm's generalization performance by solely arguing about its implicit regularization properties."
neurips,https://proceedings.neurips.cc/paper/2020/file/57e5cb96e22546001f1d6520ff11d9ba-Paper.pdf,Generalized Hindsight for Reinforcement Learning,"Alexander Li, Lerrel Pinto, Pieter Abbeel",
neurips,https://proceedings.neurips.cc/paper/2020/file/588cb956d6bbe67078f29f8de420a13d-Paper.pdf,Critic Regularized Regression,"Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S. Merel, Jost Tobias Springenberg, Scott E. Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, Nando de Freitas",
neurips,https://proceedings.neurips.cc/paper/2020/file/5898d8095428ee310bf7fa3da1864ff7-Paper.pdf,Boosting Adversarial Training with Hypersphere Embedding,"Tianyu Pang, Xiao Yang, Yinpeng Dong, Kun Xu, Jun Zhu, Hang Su",
neurips,https://proceedings.neurips.cc/paper/2020/file/58ae23d878a47004366189884c2f8440-Paper.pdf,Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs,"Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, Danai Koutra",
neurips,https://proceedings.neurips.cc/paper/2020/file/58c54802a9fb9526cd0923353a34a7ae-Paper.pdf,Modeling Continuous Stochastic Processes with Dynamic Normalizing Flows,"Ruizhi Deng, Bo Chang, Marcus A. Brubaker, Greg Mori, Andreas Lehrmann",
neurips,https://proceedings.neurips.cc/paper/2020/file/5938b4d054136e5d59ada6ec9c295d7a-Paper.pdf,Efficient Online Learning of Optimal Rankings: Dimensionality Reduction via Gradient Descent,"Dimitris Fotakis, Thanasis Lianeas, Georgios Piliouras, Stratis Skoulakis","The widely studied Generalized Min-Sum-Set-Cover (GMSSC) problem serves as a formal model for the setting above. GMSSC is NP-hard and the standard application of no-regret online learning algorithms is computationally inefficient, because they operate in the space of rankings. In this work, we show how to achieve low regret for GMSSC in polynomial-time. We employ dimensionality reduction from rankings to the space of doubly stochastic matrices, where we apply Online Gradient Descent. A key step is to show how subgradients can be computed efficiently, by solving the dual of a configuration LP. Using deterministic and randomized rounding schemes, we map doubly stochastic matrices back to rankings with a small loss in the GMSSC objective."
neurips,https://proceedings.neurips.cc/paper/2020/file/593906af0d138e69f49d251d3e7cbed0-Paper.pdf,Training Normalizing Flows with the Information Bottleneck for Competitive Generative Classification,"Lynton Ardizzone, Radek Mackowiak, Carsten Rother, Ullrich Köthe",
neurips,https://proceedings.neurips.cc/paper/2020/file/595373f017b659cb7743291e920a8857-Paper.pdf,Detecting Hands and Recognizing Physical Contact in the Wild,"Supreeth Narasimhaswamy, Trung Nguyen, Minh Hoai Nguyen",
neurips,https://proceedings.neurips.cc/paper/2020/file/59587bffec1c7846f3e34230141556ae-Paper.pdf,On the Theory of Transfer Learning: The Importance of Task Diversity,"Nilesh Tripuraneni, Michael Jordan, Chi Jin","We provide new statistical guarantees for transfer learning via representation learning--when transfer is achieved by learning a feature representation shared across different tasks. This enables learning on new tasks using far less data than is required to learn them in isolation. Formally, we consider
t
+
1
t
tasks parameterized by functions of the form
f
j
∘
h
f
in a general function class
F
∘
H
F
, where each
f
j
f
is a task-specific function in
F
F
and
h
h
is the shared representation in
H
H
. Letting
C
(
⋅
)
C
denote the complexity measure of the function class, we show that for diverse training tasks (1) the sample complexity needed to learn the shared representation across the first
t
t
training tasks scales as
C
(
H
)
+
t
C
(
F
)
C
, despite no explicit access to a signal from the feature representation and (2) with an accurate estimate of the representation, the sample complexity needed to learn a new task scales only with
C
(
F
)
C
. Our results depend upon a new general notion of task diversity--applicable to models with general tasks, features, and losses--as well as a novel chain rule for Gaussian complexities. Finally, we exhibit the utility of our general framework in several models of importance in the literature."
neurips,https://proceedings.neurips.cc/paper/2020/file/597c7b407a02cc0a92167e7a371eca25-Paper.pdf,Finite-Time Analysis of Round-Robin Kullback-Leibler Upper Confidence Bounds for Optimal Adaptive Allocation with Multiple Plays and Markovian Rewards,Vrettos Moulos,
neurips,https://proceedings.neurips.cc/paper/2020/file/59a3adea76fadcb6dd9e54c96fc155d1-Paper.pdf,Neural Star Domain as Primitive Representation,"Yuki Kawana, Yusuke Mukuta, Tatsuya Harada",
neurips,https://proceedings.neurips.cc/paper/2020/file/59accb9fe696ce55e28b7d23a009e2d1-Paper.pdf,Off-Policy Interval Estimation with Lipschitz Value Iteration,"Ziyang Tang, Yihao Feng, Na Zhang, Jian Peng, Qiang Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/5a01f0597ac4bdf35c24846734ee9a76-Paper.pdf,Inverse Rational Control with Partially Observable Continuous Nonlinear Dynamics,"Minhae Kwon, Saurabh Daptardar, Paul R. Schrater, Xaq Pitkow",
neurips,https://proceedings.neurips.cc/paper/2020/file/5a16bce575f3ddce9c819de125ba0029-Paper.pdf,Deep Statistical Solvers,"Balthazar Donon, Zhengying Liu, Wenzhuo LIU, Isabelle Guyon, Antoine Marot, Marc Schoenauer",
neurips,https://proceedings.neurips.cc/paper/2020/file/5a29503a4909fcade36b1823e7cebcf5-Paper.pdf,Distributionally Robust Parametric Maximum Likelihood Estimation,"Viet Anh Nguyen, Xuhui Zhang, Jose Blanchet, Angelos Georghiou",
neurips,https://proceedings.neurips.cc/paper/2020/file/5a378f8490c8d6af8647a753812f6e31-Paper.pdf,Secretary and Online Matching Problems with Machine Learned Advice,"Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, Pavel Kolev",
neurips,https://proceedings.neurips.cc/paper/2020/file/5a5eab21ca2a8fef4af5e35709ecca15-Paper.pdf,Deep Transformation-Invariant Clustering,"Tom Monnier, Thibault Groueix, Mathieu Aubry",
neurips,https://proceedings.neurips.cc/paper/2020/file/5a66b9200f29ac3fa0ae244cc2a51b39-Paper.pdf,"Overfitting Can Be Harmless for Basis Pursuit, But Only to a Degree","Peizhong Ju, Xiaojun Lin, Jia Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/5a751d6a0b6ef05cfe51b86e5d1458e6-Paper.pdf,Improving Generalization in Reinforcement Learning with Mixture Regularization,"KAIXIN WANG, Bingyi Kang, Jie Shao, Jiashi Feng",
neurips,https://proceedings.neurips.cc/paper/2020/file/5a7b238ba0f6502e5d6be14424b20ded-Paper.pdf,Pontryagin Differentiable Programming: An End-to-End Learning and Control Framework,"Wanxin Jin, Zhaoran Wang, Zhuoran Yang, Shaoshuai Mou",
neurips,https://proceedings.neurips.cc/paper/2020/file/5b0fa0e4c041548bb6289e15d865a696-Paper.pdf,Learning from Aggregate Observations,"Yivan Zhang, Nontawat Charoenphakdee, Zhenguo Wu, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2020/file/5b8e9841e87fb8fc590434f5d933c92c-Paper.pdf,The Devil is in the Detail: A Framework for Macroscopic Prediction via Microscopic Models,"Yingxiang Yang, Negar Kiyavash, Le Song, Niao He",
neurips,https://proceedings.neurips.cc/paper/2020/file/5bca8566db79f3788be9efd96c9ed70d-Paper.pdf,Subgraph Neural Networks,"Emily Alsentzer, Samuel Finlayson, Michelle Li, Marinka Zitnik",
neurips,https://proceedings.neurips.cc/paper/2020/file/5bce843dd76db8c939d5323dd3e54ec9-Paper.pdf,Demystifying Orthogonal Monte Carlo and Beyond,"Han Lin, Haoxian Chen, Krzysztof M. Choromanski, Tianyi Zhang, Clement Laroche",
neurips,https://proceedings.neurips.cc/paper/2020/file/5bd844f11fa520d54fa5edec06ea2507-Paper.pdf,Optimal Robustness-Consistency Trade-offs for Learning-Augmented Online Algorithms,"Alexander Wei, Fred Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/5bf8aaef51c6e0d363cbe554acaf3f20-Paper.pdf,A Scalable Approach for Privacy-Preserving Collaborative Machine Learning,"Jinhyun So, Basak Guler, Salman Avestimehr","We consider a collaborative learning scenario in which multiple data-owners wish to jointly train a logistic regression model, while keeping their individual datasets private from the other parties. We propose COPML, a fully-decentralized training framework that achieves scalability and privacy-protection simultaneously. The key idea of COPML is to securely encode the individual datasets to distribute the computation load effectively across many parties and to perform the training computations as well as the model updates in a distributed manner on the securely encoded data. We provide the privacy analysis of COPML and prove its convergence. Furthermore, we experimentally demonstrate that COPML can achieve significant speedup in training over the benchmark protocols. Our protocol provides strong statistical privacy guarantees against colluding parties (adversaries) with unbounded computational power, while achieving up to
16
×
16
speedup in the training time against the benchmark protocols."
neurips,https://proceedings.neurips.cc/paper/2020/file/5c3b99e8f92532e5ad1556e53ceea00c-Paper.pdf,Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search,"Jaehyeon Kim, Sungwon Kim, Jungil Kong, Sungroh Yoon",
neurips,https://proceedings.neurips.cc/paper/2020/file/5c528e25e1fdeaf9d8160dc24dbf4d60-Paper.pdf,Towards Learning Convolutions from Scratch,Behnam Neyshabur,
neurips,https://proceedings.neurips.cc/paper/2020/file/5c9452254bccd24b8ad0bb1ab4408ad1-Paper.pdf,Cycle-Contrast for Self-Supervised Video Representation Learning,"Quan Kong, Wenpeng Wei, Ziwei Deng, Tomoaki Yoshinaga, Tomokazu Murakami",
neurips,https://proceedings.neurips.cc/paper/2020/file/5ca359ab1e9e3b9c478459944a2d9ca5-Paper.pdf,Posterior Re-calibration for Imbalanced Datasets,"Junjiao Tian, Yen-Cheng Liu, Nathaniel Glaser, Yen-Chang Hsu, Zsolt Kira",
neurips,https://proceedings.neurips.cc/paper/2020/file/5ca41a86596a5ed567d15af0be224952-Paper.pdf,Novelty Search in Representational Space for Sample Efficient Exploration,"Ruo Yu Tao, Vincent Francois-Lavet, Joelle Pineau",
neurips,https://proceedings.neurips.cc/paper/2020/file/5cb0e249689cd6d8369c4885435a56c2-Paper.pdf,Robust Reinforcement Learning via Adversarial training with Langevin Dynamics,"Parameswaran Kamalaruban, Yu-Ting Huang, Ya-Ping Hsieh, Paul Rolland, Cheng Shi, Volkan Cevher",
neurips,https://proceedings.neurips.cc/paper/2020/file/5cc3749a6e56ef6d656735dff9176074-Paper.pdf,Adversarial Blocking Bandits,"Nicholas Bishop, Hau Chan, Debmalya Mandal, Long Tran-Thanh",
neurips,https://proceedings.neurips.cc/paper/2020/file/5cc4bb753030a3d804351b2dfec0d8b5-Paper.pdf,Online Algorithms for Multi-shop Ski Rental with Machine Learned Advice,"Shufan Wang, Jian Li, Shiqiang Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/5cd5058bca53951ffa7801bcdf421651-Paper.pdf,Multi-label Contrastive Predictive Coding,"Jiaming Song, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2020/file/5d0cb12f8c9ad6845110317afc6e2183-Paper.pdf,Rotation-Invariant Local-to-Global Representation Learning for 3D Point Cloud,"SEOHYUN KIM, JaeYoo Park, Bohyung Han",
neurips,https://proceedings.neurips.cc/paper/2020/file/5d0d5594d24f0f955548f0fc0ff83d10-Paper.pdf,Learning Invariants through Soft Unification,"Nuri Cingillioglu, Alessandra Russo",
neurips,https://proceedings.neurips.cc/paper/2020/file/5d151d1059a6281335a10732fc49620e-Paper.pdf,One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL,"Saurabh Kumar, Aviral Kumar, Sergey Levine, Chelsea Finn",
neurips,https://proceedings.neurips.cc/paper/2020/file/5d40954183d62a82257835477ccad3d2-Paper.pdf,Variational Bayesian Monte Carlo with Noisy Likelihoods,Luigi Acerbi,
neurips,https://proceedings.neurips.cc/paper/2020/file/5d44ee6f2c3f71b73125876103c8f6c4-Paper.pdf,Finite-Sample Analysis of Contractive Stochastic Approximation Using Smooth Convex Envelopes,"Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, Karthikeyan Shanmugam",
neurips,https://proceedings.neurips.cc/paper/2020/file/5d79099fcdf499f12b79770834c0164a-Paper.pdf,Self-Supervised Generative Adversarial Compression,"Chong Yu, Jeff Pool",
neurips,https://proceedings.neurips.cc/paper/2020/file/5d97f4dd7c44b2905c799db681b80ce0-Paper.pdf,An efficient nonconvex reformulation of stagewise convex optimization problems,"Rudy R. Bunel, Oliver Hinder, Srinadh Bhojanapalli, Krishnamurthy Dvijotham",
neurips,https://proceedings.neurips.cc/paper/2020/file/5dbc8390f17e019d300d5a162c3ce3bc-Paper.pdf,From Finite to Countable-Armed Bandits,"Anand Kalvit, Assaf Zeevi",
neurips,https://proceedings.neurips.cc/paper/2020/file/5de8a36008b04a6167761fa19b61aa6c-Paper.pdf,Adversarial Distributional Training for Robust Deep Learning,"Yinpeng Dong, Zhijie Deng, Tianyu Pang, Jun Zhu, Hang Su",
neurips,https://proceedings.neurips.cc/paper/2020/file/5df0385cba256a135be596dbe28fa7aa-Paper.pdf,Meta-Learning Stationary Stochastic Process Prediction with Convolutional Neural Processes,"Andrew Foong, Wessel Bruinsma, Jonathan Gordon, Yann Dubois, James Requeima, Richard Turner",
neurips,https://proceedings.neurips.cc/paper/2020/file/5e1b18c4c6a6d31695acbae3fd70ecc6-Paper.pdf,Theory-Inspired Path-Regularized Differential Network Architecture Search,"Pan Zhou, Caiming Xiong, Richard Socher, Steven Chu Hong Hoi",
neurips,https://proceedings.neurips.cc/paper/2020/file/5e5dd00d770ef3e9154a4257edcb80b8-Paper.pdf,Conic Descent and its Application to Memory-efficient Optimization over Positive Semidefinite Matrices,"John C. Duchi, Oliver Hinder, Andrew Naber, Yinyu Ye",
neurips,https://proceedings.neurips.cc/paper/2020/file/5e98d23afe19a774d1b2dcbefd5103eb-Paper.pdf,Learning the Geometry of Wave-Based Imaging,"Konik Kothari, Maarten de Hoop, Ivan Dokmanić",
neurips,https://proceedings.neurips.cc/paper/2020/file/5ef20b89bab8fed38253e98a12f26316-Paper.pdf,Greedy inference with structure-exploiting lazy maps,"Michael Brennan, Daniele Bigoni, Olivier Zahm, Alessio Spantini, Youssef Marzouk",
neurips,https://proceedings.neurips.cc/paper/2020/file/5f0ad4db43d8723d18169b2e4817a160-Paper.pdf,Nimble: Lightweight and Parallel GPU Task Scheduling for Deep Learning,"Woosuk Kwon, Gyeong-In Yu, Eunji Jeong, Byung-Gon Chun",
neurips,https://proceedings.neurips.cc/paper/2020/file/5f14615696649541a025d3d0f8e0447f-Paper.pdf,Finding the Homology of Decision Boundaries with Active Learning,"Weizhi Li, Gautam Dasarathy, Karthikeyan Natesan Ramamurthy, Visar Berisha",
neurips,https://proceedings.neurips.cc/paper/2020/file/5f268dfb0fbef44de0f668a022707b86-Paper.pdf,Reinforced Molecular Optimization with Neighborhood-Controlled Grammars,"Chencheng Xu, Qiao Liu, Minlie Huang, Tao Jiang",
neurips,https://proceedings.neurips.cc/paper/2020/file/5f7695debd8cde8db5abcb9f161b49ea-Paper.pdf,Natural Policy Gradient Primal-Dual Method for Constrained Markov Decision Processes,"Dongsheng Ding, Kaiqing Zhang, Tamer Basar, Mihailo Jovanovic",
neurips,https://proceedings.neurips.cc/paper/2020/file/5f8b73c0d4b1bf60dd7173b660b87c29-Paper.pdf,"Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Evolvability","Sitan Chen, Frederic Koehler, Ankur Moitra, Morris Yau","In this paper, we revisit the problem of distribution-independently learning halfspaces under Massart noise with rate
η
η
. Recent work resolved a long-standing problem in this model of efficiently learning to error
η
+
ϵ
η
for any
ϵ
>
0
ϵ
, by giving an improper learner that partitions space into
poly
(
d
,
1
/
ϵ
)
poly
regions. Here we give a much simpler algorithm and settle a number of outstanding open questions: (1) We give the first \emph{proper} learner for Massart halfspaces that achieves
η
+
ϵ
η
. (2) Based on (1), we develop a blackbox knowledge distillation procedure to convert an arbitrarily complex classifier to an equally good proper classifier. (3) By leveraging a simple but overlooked connection to \emph{evolvability}, we show any SQ algorithm requires super-polynomially many queries to achieve
O
P
T
+
ϵ
O
. We then zoom out to study generalized linear models and give an efficient algorithm for learning under a challenging new corruption model generalizing Massart noise. Finally we study our algorithm for learning halfspaces under Massart noise empirically and find that it exhibits some appealing fairness properties as a byproduct of its strong provable robustness guarantees."
neurips,https://proceedings.neurips.cc/paper/2020/file/5fb37d5bbdbbae16dea2f3104d7f9439-Paper.pdf,Certified Defense to Image Transformations via Randomized Smoothing,"Marc Fischer, Maximilian Baader, Martin Vechev","We extend randomized smoothing to cover parameterized transformations (e.g., rotations, translations) and certify robustness in the parameter space (e.g., rotation angle). This is particularly challenging as interpolation and rounding effects mean that image transformations do not compose, in turn preventing direct certification of the perturbed image (unlike certification with
ℓ
p
ℓ
norms). We address this challenge by introducing three different defenses, each with a different guarantee (heuristic, distributional and individual) stemming from the method used to bound the interpolation error. Importantly, in the individual case, we show how to efficiently compute the inverse of an image transformation, enabling us to provide individual guarantees in the online setting. We provide an implementation of all methods at https://github.com/eth-sri/transformation-smoothing."
neurips,https://proceedings.neurips.cc/paper/2020/file/60495b4e033e9f60b32a6607b587aadd-Paper.pdf,Estimation of Skill Distribution from a Tournament,"Ali Jadbabaie, Anuran Makur, Devavrat Shah","In this paper, we study the problem of learning the skill distribution of a population of agents from observations of pairwise games in a tournament. These games are played among randomly drawn agents from the population. The agents in our model can be individuals, sports teams, or Wall Street fund managers. Formally, we postulate that the likelihoods of outcomes of games are governed by the parametric Bradley-Terry-Luce (or multinomial logit) model, where the probability of an agent beating another is the ratio between its skill level and the pairwise sum of skill levels, and the skill parameters are drawn from an unknown, non-parametric skill density of interest. The problem is, in essence, to learn a distribution from noisy, quantized observations. We propose a surprisingly simple and tractable algorithm that learns the skill density with near-optimal minimax mean squared error scaling as
n
−
1
+
ε
n
, for any
ε
>
0
ε
, so long as the density is smooth. Our approach brings together prior work on learning skill parameters from pairwise comparisons with kernel density estimation from non-parametric statistics. Furthermore, we prove information theoretic lower bounds which establish minimax optimality of the skill parameter estimation technique used in our algorithm. These bounds utilize a continuum version of Fano's method along with a careful covering argument. We apply our algorithm to various soccer leagues and world cups, cricket world cups, and mutual funds. We find that the entropy of a learnt distribution provides a quantitative measure of skill, which in turn provides rigorous explanations for popular beliefs about perceived qualities of sporting events, e.g., soccer league rankings. Finally, we apply our method to assess the skill distributions of mutual funds. Our results shed light on the abundance of low quality funds prior to the Great Recession of 2008, and the domination of the industry by more skilled funds after the financial crisis."
neurips,https://proceedings.neurips.cc/paper/2020/file/604b37ea63ea51fa5fb3d8a89ec056e6-Paper.pdf,Reparameterizing Mirror Descent as Gradient Descent,"Ehsan Amid, Manfred K. K. Warmuth",
neurips,https://proceedings.neurips.cc/paper/2020/file/604f2c31e67034642b288d76a8df11d5-Paper.pdf,General Control Functions for Causal Effect Estimation from IVs,"Aahlad Puli, Rajesh Ranganath",
neurips,https://proceedings.neurips.cc/paper/2020/file/607bc9ebe4abfcd65181bfbef6252830-Paper.pdf,Optimal Algorithms for Stochastic Multi-Armed Bandits with Heavy Tailed Rewards,"Kyungjae Lee, Hongjun Yang, Sungbin Lim, Songhwai Oh","In this paper, we consider stochastic multi-armed bandits (MABs) with heavy-tailed rewards, whose p-th moment is bounded by a constant nu_p for 1"
neurips,https://proceedings.neurips.cc/paper/2020/file/609a199881ca4ba9c95688235cd6ac5c-Paper.pdf,Certified Robustness of Graph Convolution Networks for Graph Classification under Topological Attacks,"Hongwei Jin, Zhan Shi, Venkata Jaya Shankar Ashish Peruri, Xinhua Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/609c5e5089a9aa967232aba2a4d03114-Paper.pdf,Zero-Resource Knowledge-Grounded Dialogue Generation,"Linxiao Li, Can Xu, Wei Wu, YUFAN ZHAO, Xueliang Zhao, Chongyang Tao",
neurips,https://proceedings.neurips.cc/paper/2020/file/609e9d4bcc8157c00808993f612f1acd-Paper.pdf,Targeted Adversarial Perturbations for Monocular Depth Prediction,"Alex Wong, Safa Cicek, Stefano Soatto",
neurips,https://proceedings.neurips.cc/paper/2020/file/60a70bb05b08d6cd95deb3bdb750dce8-Paper.pdf,Beyond the Mean-Field: Structured Deep Gaussian Processes Improve the Predictive Uncertainties,"Jakob Lindinger, David Reeb, Christoph Lippert, Barbara Rakitsch",
neurips,https://proceedings.neurips.cc/paper/2020/file/60cb558c40e4f18479664069d9642d5a-Paper.pdf,Offline Imitation Learning with a Misspecified Simulator,"Shengyi Jiang, Jingcheng Pang, Yang Yu","In real-world decision-making tasks, learning an optimal policy without a trial-and-error process is an appealing challenge. When expert demonstrations are available, imitation learning that mimics expert actions can learn a good policy efficiently. Learning in simulators is another commonly adopted approach to avoid real-world trials-and-errors. However, neither sufficient expert demonstrations nor high-fidelity simulators are easy to obtain. In this work, we investigate policy learning in the condition of a few expert demonstrations and a simulator with misspecified dynamics. Under a mild assumption that local states shall still be partially aligned under a dynamics mismatch, we propose imitation learning with horizon-adaptive inverse dynamics (HIDIL) that matches the simulator states with expert states in a
H
H
-step horizon and accurately recovers actions based on inverse dynamics policies. In the real environment, HIDIL can effectively derive adapted actions from the matched states. Experiments are conducted in four MuJoCo locomotion environments with modified friction, gravity, and density configurations. Experiment results show that HIDIL achieves significant improvement in terms of performance and stability in all of the real environments, compared with imitation learning methods and transferring methods in reinforcement learning."
neurips,https://proceedings.neurips.cc/paper/2020/file/60e1deb043af37db5ea4ce9ae8d2c9ea-Paper.pdf,Multi-Fidelity Bayesian Optimization via Deep Neural Networks,"Shibo Li, Wei Xing, Robert Kirby, Shandian Zhe",
neurips,https://proceedings.neurips.cc/paper/2020/file/6101903146e4bbf4999c449d78441606-Paper.pdf,PlanGAN: Model-based Planning With Sparse Rewards and Multiple Goals,"Henry Charlesworth, Giovanni Montana",
neurips,https://proceedings.neurips.cc/paper/2020/file/618491e20a9b686b79e158c293ab4f91-Paper.pdf,Bad Global Minima Exist and SGD Can Reach Them,"Shengchao Liu, Dimitris Papailiopoulos, Dimitris Achlioptas",
neurips,https://proceedings.neurips.cc/paper/2020/file/618790ae971abb5610b16c826fb72d01-Paper.pdf,Optimal Prediction of the Number of Unseen Species with Multiplicity,"Yi Hao, Ping Li","Based on a sample of size
n
n
, we consider estimating the number of symbols that appear at least
μ
μ
times in an independent sample of size
a
⋅
n
a
, where
a
a
is a given parameter. This formulation includes, as a special case, the well-known problem of inferring the number of unseen species introduced by [Fisher et al.] in 1943 and considered by many others. Of considerable interest in this line of works is the largest
a
a
for which the quantity can be accurately predicted. We completely resolve this problem by determining the limit of estimation to be
a
≈
(
log
n
)
/
μ
a
, with both lower and upper bounds matching up to constant factors. For the particular case of
μ
=
1
μ
, this implies the recent result by [Orlitsky et al.] on the unseen species problem. Experimental evaluations show that the proposed estimator performs exceptionally well in practice. Furthermore, the estimator is a simple linear combination of symbols' empirical counts, and hence linear-time computable."
neurips,https://proceedings.neurips.cc/paper/2020/file/61a10e6abb1149ad9d08f303267f9bc4-Paper.pdf,Characterizing Optimal Mixed Policies: Where to Intervene and What to Observe,"Sanghack Lee, Elias Bareinboim",
neurips,https://proceedings.neurips.cc/paper/2020/file/61c66a2f4e6e10dc9c16ddf9d19745d6-Paper.pdf,Factor Graph Neural Networks,"Zhen Zhang, Fan Wu, Wee Sun Lee",
neurips,https://proceedings.neurips.cc/paper/2020/file/61d77652c97ef636343742fc3dcf3ba9-Paper.pdf,A Closer Look at Accuracy vs. Robustness,"Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R. Salakhutdinov, Kamalika Chaudhuri",
neurips,https://proceedings.neurips.cc/paper/2020/file/62000dee5a05a6a71de3a6127a68778a-Paper.pdf,Curriculum Learning by Dynamic Instance Hardness,"Tianyi Zhou, Shengjie Wang, Jeffrey Bilmes",
neurips,https://proceedings.neurips.cc/paper/2020/file/6217b2f7e4634fa665d31d3b4df81b56-Paper.pdf,Spin-Weighted Spherical CNNs,"Carlos Esteves, Ameesh Makadia, Kostas Daniilidis",
neurips,https://proceedings.neurips.cc/paper/2020/file/62326dc7c4f7b849d6f013ba46489d6c-Paper.pdf,Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks,"David Bieber, Charles Sutton, Hugo Larochelle, Daniel Tarlow",
neurips,https://proceedings.neurips.cc/paper/2020/file/6244b2ba957c48bc64582cf2bcec3d04-Paper.pdf,AutoPrivacy: Automated Layer-wise Parameter Selection for Secure Neural Network Inference,"Qian Lou, Song Bian, Lei Jiang","Hybrid Privacy-Preserving Neural Network (HPPNN) implementing linear layers by Homomorphic Encryption (HE) and nonlinear layers by Garbled Circuit (GC) is one of the most promising secure solutions to emerging Machine Learning as a Service (MLaaS). Unfortunately, a HPPNN suffers from long inference latency, e.g.,
∼
100
∼
seconds per image, which makes MLaaS unsatisfactory. Because HE-based linear layers of a HPPNN cost
93
%
93
inference latency, it is critical to select a set of HE parameters to minimize computational overhead of linear layers. Prior HPPNNs over-pessimistically select huge HE parameters to maintain large noise budgets, since they use the same set of HE parameters for an entire network and ignore the error tolerance capability of a network. In this paper, for fast and accurate secure neural network inference, we propose an automated layer-wise parameter selector, AutoPrivacy, that leverages deep reinforcement learning to automatically determine a set of HE parameters for each linear layer in a HPPNN. The learning-based HE parameter selection policy outperforms conventional rule-based HE parameter selection policy. Compared to prior HPPNNs, AutoPrivacy-optimized HPPNNs reduce inference latency by
53
%
∼
70
%
53
with negligible loss of accuracy."
neurips,https://proceedings.neurips.cc/paper/2020/file/6271faadeedd7626d661856b7a004e27-Paper.pdf,Baxter Permutation Process,"Masahiro Nakano, Akisato Kimura, Takeshi Yamada, Naonori Ueda",
neurips,https://proceedings.neurips.cc/paper/2020/file/6275d7071d005260ab9d0766d6df1145-Paper.pdf,Characterizing emergent representations in a space of candidate learning rules for deep networks,"Yinan Cao, Christopher Summerfield, Andrew Saxe",
neurips,https://proceedings.neurips.cc/paper/2020/file/62d75fb2e3075506e8837d8f55021ab1-Paper.pdf,"Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation","Rasool Fakoor, Jonas W. Mueller, Nick Erickson, Pratik Chaudhari, Alexander J. Smola",
neurips,https://proceedings.neurips.cc/paper/2020/file/62da5a6d47be0029801ba74a17e47e1a-Paper.pdf,Adaptive Probing Policies for Shortest Path Routing,"Aditya Bhaskara, Sreenivas Gollapudi, Kostas Kollias, Kamesh Munagala","Inspired by traffic routing applications, we consider the problem of finding the shortest path from a source
s
s
to a destination
t
t
in a graph, when the lengths of the edges are unknown. Instead, we are given {\em hints} or predictions of the edge lengths from a collection of ML models, trained possibly on historical data and other contexts in the network. Additionally, we assume that the true length of any candidate path can be obtained by {\em probing} an up-to-date snapshot of the network. However, each probe introduces a latency, and thus the goal is to minimize the number of probes while finding a near-optimal path with high probability. We formalize this problem and show assumptions under which it admits to efficient approximation algorithms. We verify these assumptions and validate the performance of our algorithms on real data."
neurips,https://proceedings.neurips.cc/paper/2020/file/62db9e3397c76207a687c360e0243317-Paper.pdf,Approximate Heavily-Constrained Learning with Lagrange Multiplier Models,"Harikrishna Narasimhan, Andrew Cotter, Yichen Zhou, Serena Wang, Wenshuo Guo",
neurips,https://proceedings.neurips.cc/paper/2020/file/630eff1b380505a67570dff952ce4ad7-Paper.pdf,Faster Randomized Infeasible Interior Point Methods for Tall/Wide Linear Programs,"Agniva Chowdhury, Palma London, Haim Avron, Petros Drineas","Linear programming (LP) is used in many machine learning applications, such as
ℓ
1
ℓ
-regularized SVMs, basis pursuit, nonnegative matrix factorization, etc. Interior Point Methods (IPMs) are one of the most popular methods to solve LPs both in theory and in practice. Their underlying complexity is dominated by the cost of solving a system of linear equations at each iteration. In this paper, we consider \emph{infeasible} IPMs for the special case where the number of variables is much larger than the number of constraints (i.e., wide), or vice-versa (i.e., tall) by taking the dual. Using tools from Randomized Linear Algebra, we present a preconditioning technique that, when combined with the Conjugate Gradient iterative solver, provably guarantees that infeasible IPM algorithms (suitably modified to account for the error incurred by the approximate solver), converge to a feasible, approximately optimal solution, without increasing their iteration complexity. Our empirical evaluations verify our theoretical results on both real and synthetic data."
neurips,https://proceedings.neurips.cc/paper/2020/file/631e9c01c190fc1515b9fe3865abbb15-Paper.pdf,Sliding Window Algorithms for k-Clustering Problems,"Michele Borassi, Alessandro Epasto, Silvio Lattanzi, Sergei Vassilvitskii, Morteza Zadimoghaddam","The sliding window model of computation captures scenarios in which data is arriving continuously, but only the latest
w
w
elements should be used for analysis. The goal is to design algorithms that update the solution efficiently with each arrival rather than recomputing it from scratch. In this work, we focus on
k
k
-clustering problems such as
k
k
-means and
k
k
-median. In this setting, we provide simple and practical algorithms that offer stronger performance guarantees than previous results. Empirically, we show that our methods store only a small fraction of the data, are orders of magnitude faster, and find solutions with costs only slightly higher than those returned by algorithms with access to the full dataset."
neurips,https://proceedings.neurips.cc/paper/2020/file/634841a6831464b64c072c8510c7f35c-Paper.pdf,AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning,"Ximeng Sun, Rameswar Panda, Rogerio Feris, Kate Saenko",
neurips,https://proceedings.neurips.cc/paper/2020/file/636efd4f9aeb5781e9ea815cdd633e52-Paper.pdf,Approximate Cross-Validation for Structured Models,"Soumya Ghosh, Will Stephenson, Tin D. Nguyen, Sameer Deshpande, Tamara Broderick",
neurips,https://proceedings.neurips.cc/paper/2020/file/63c17d596f401acb520efe4a2a7a01ee-Paper.pdf,"Exemplar VAE: Linking Generative Models, Nearest Neighbor Retrieval, and Data Augmentation","Sajad Norouzi, David J. Fleet, Mohammad Norouzi",
neurips,https://proceedings.neurips.cc/paper/2020/file/63c3ddcc7b23daa1e42dc41f9a44a873-Paper.pdf,Debiased Contrastive Learning,"Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, Stefanie Jegelka",
neurips,https://proceedings.neurips.cc/paper/2020/file/63d5fb54a858dd033fe90e6e4a74b0f0-Paper.pdf,UCSG-NET- Unsupervised Discovering of Constructive Solid Geometry Tree,"Kacper Kania, Maciej Zieba, Tomasz Kajdanowicz",
neurips,https://proceedings.neurips.cc/paper/2020/file/63f44623dd8686aba388944c8810087f-Paper.pdf,Generalized Boosting,"Arun Suggala, Bingbin Liu, Pradeep Ravikumar",
neurips,https://proceedings.neurips.cc/paper/2020/file/641d77dd5271fca28764612a028d9c8e-Paper.pdf,COT-GAN: Generating Sequential Data via Causal Optimal Transport,"Tianlin Xu, Li Kevin Wenliang, Michael Munn, Beatrice Acciaio",
neurips,https://proceedings.neurips.cc/paper/2020/file/645e6bfdd05d1a69c5e47b20f0a91d46-Paper.pdf,Impossibility Results for Grammar-Compressed Linear Algebra,"Amir Abboud, Arturs Backurs, Karl Bringmann, Marvin Künnemann","In this paper we consider lossless compression schemes, and ask if we can run our computations on the compressed data as efficiently as if the original data was that small. That is, if an operation has time complexity T(input-size), can we perform it on the compressed representation in time T(n) rather than T(N)? We consider the most basic linear algebra operations: inner product, matrix-vector multiplication, and matrix multiplication. In particular, given two compressed vectors, can we compute their inner product in time O(n)? Or perhaps we must decompress first and then multiply, spending Omega(N) time?"
neurips,https://proceedings.neurips.cc/paper/2020/file/64714a86909d401f8feb83e8c2d94b23-Paper.pdf,Understanding spiking networks through convex optimization,"Allan Mancoo, Sander Keemink, Christian K. Machens",
neurips,https://proceedings.neurips.cc/paper/2020/file/6495cf7ca745a9443508b86951b8e33a-Paper.pdf,Better Full-Matrix Regret via Parameter-Free Online Learning,Ashok Cutkosky,
neurips,https://proceedings.neurips.cc/paper/2020/file/64986d86a17424eeac96b08a6d519059-Paper.pdf,Large-Scale Methods for Distributionally Robust Optimization,"Daniel Levy, Yair Carmon, John C. Duchi, Aaron Sidford","We propose and analyze algorithms for distributionally robust optimization of convex losses with conditional value at risk (CVaR) and
χ
2
χ
divergence uncertainty sets. We prove that our algorithms require a number of gradient evaluations independent of training set size and number of parameters, making them suitable for large-scale applications. For
χ
2
χ
uncertainty sets these are the first such guarantees in the literature, and for CVaR our guarantees scale linearly in the uncertainty level rather than quadratically as in previous work. We also provide lower bounds proving the worst-case optimality of our algorithms for CVaR and a penalized version of the
χ
2
χ
problem. Our primary technical contributions are novel bounds on the bias of batch robust risk estimation and the variance of a multilevel Monte Carlo gradient estimator due to [Blanchet & Glynn, 2015]. Experiments on MNIST and ImageNet confirm the theoretical scaling of our algorithms, which are 9-36 times more efficient than full-batch methods."
neurips,https://proceedings.neurips.cc/paper/2020/file/649d45bf179296e31731adfd4df25588-Paper.pdf,Analysis and Design of Thompson Sampling for Stochastic Partial Monitoring,"Taira Tsuchiya, Junya Honda, Masashi Sugiyama","We investigate finite stochastic partial monitoring, which is a general model for sequential learning with limited feedback. While Thompson sampling is one of the most promising algorithms on a variety of online decision-making problems, its properties for stochastic partial monitoring have not been theoretically investigated, and the existing algorithm relies on a heuristic approximation of the posterior distribution. To mitigate these problems, we present a novel Thompson-sampling-based algorithm, which enables us to exactly sample the target parameter from the posterior distribution. Besides, we prove that the new algorithm achieves the logarithmic problem-dependent expected pseudo-regret
O
(
log
T
)
O
for a linearized variant of the problem with local observability. This result is the first regret bound of Thompson sampling for partial monitoring, which also becomes the first logarithmic regret bound of Thompson sampling for linear bandits."
neurips,https://proceedings.neurips.cc/paper/2020/file/64a08e5f1e6c39faeb90108c430eb120-Paper.pdf,Bandit Linear Control,"Asaf Cassel, Tomer Koren",
neurips,https://proceedings.neurips.cc/paper/2020/file/64dcf3c521a00dbb4d2a10a27a95a9d8-Paper.pdf,Refactoring Policy for Compositional Generalizability using Self-Supervised Object Proposals,"Tongzhou Mu, Jiayuan Gu, Zhiwei Jia, Hao Tang, Hao Su",
neurips,https://proceedings.neurips.cc/paper/2020/file/652c208b21f13f6e995bfc1154a1a2e5-Paper.pdf,PEP: Parameter Ensembling by Perturbation,"Alireza Mehrtash, Purang Abolmaesumi, Polina Golland, Tina Kapur, Demian Wassermann, William Wells",
neurips,https://proceedings.neurips.cc/paper/2020/file/6547884cea64550284728eb26b0947ef-Paper.pdf,Theoretical Insights Into Multiclass Classification: A High-dimensional Asymptotic View,"Christos Thrampoulidis, Samet Oymak, Mahdi Soltanolkotabi",
neurips,https://proceedings.neurips.cc/paper/2020/file/65586803f1435736f42a541d3a924595-Paper.pdf,Adversarial Example Games,"Joey Bose, Gauthier Gidel, Hugo Berard, Andre Cianflone, Pascal Vincent, Simon Lacoste-Julien, Will Hamilton","The existence of adversarial examples capable of fooling trained neural network classifiers calls for a much better understanding of possible attacks to guide the development of safeguards against them. This includes attack methods in the challenging {\em non-interactive blackbox} setting, where adversarial attacks are generated without any access, including queries, to the target model. Prior attacks in this setting have relied mainly on algorithmic innovations derived from empirical observations (e.g., that momentum helps), lacking principled transferability guarantees. In this work, we provide a theoretical foundation for crafting transferable adversarial examples to entire hypothesis classes. We introduce \textit{Adversarial Example Games} (AEG), a framework that models the crafting of adversarial examples as a min-max game between a generator of attacks and a classifier. AEG provides a new way to design adversarial examples by adversarially training a generator and a classifier from a given hypothesis class (e.g., architecture). We prove that this game has an equilibrium, and that the optimal generator is able to craft adversarial examples that can attack any classifier from the corresponding hypothesis class. We demonstrate the efficacy of AEG on the MNIST and CIFAR-10 datasets, outperforming prior state-of-the-art approaches with an average relative improvement of
29.9
%
29.9
and
47.2
%
47.2
against undefended and robust models (Table
???
???
\&
???
???
) respectively."
neurips,https://proceedings.neurips.cc/paper/2020/file/657b96f0592803e25a4f07166fff289a-Paper.pdf,Residual Distillation: Towards Portable Deep Neural Networks without Shortcuts,"Guilin Li, Junlei Zhang, Yunhe Wang, Chuanjian Liu, Matthias Tan, Yunfeng Lin, Wei Zhang, Jiashi Feng, Tong Zhang","By transferring both features and gradients between different layers, shortcut connections explored by ResNets allow us to effectively train very deep neural networks up to hundreds of layers. However, the additional computation costs induced by those shortcuts are often overlooked. For example, during online inference, the shortcuts in ResNet-50 account for about 40 percent of the entire memory usage on feature maps, because the features in the preceding layers cannot be released until the subsequent calculation is completed. In this work, for the first time, we consider training the CNN models with shortcuts and deploying them without. In particular, we propose a novel joint-training framework to train plain CNN by leveraging the gradients of the ResNet counterpart. During forward step, the feature maps of the early stages of plain CNN are passed through later stages of both itself and the ResNet counterpart to calculate the loss. During backpropagation, gradients calculated from a mixture of these two parts are used to update the plainCNN network to solve the gradient vanishing problem. Extensive experiments on ImageNet/CIFAR10/CIFAR100 demonstrate that the plainCNN network without shortcuts generated by our approach can achieve the same level of accuracy as that of the ResNet baseline while achieving about
1.4
×
1.4
speed-up and
1.25
×
1.25
memory reduction. We also verified the feature transferability of our ImageNet pretrained plain-CNN network by fine-tuning it on MIT 67 and Caltech 101. Our results show that the performance of the plain-CNN is slightly higher than that of its baseline ResNet-50 on these two datasets. The codes are in: \href{https://github.com/leoozy/JointRD_Neurips2020}{https://github.com/leoozy/JointRD\_Neurips2020}"
neurips,https://proceedings.neurips.cc/paper/2020/file/65a99bb7a3115fdede20da98b08a370f-Paper.pdf,Provably Efficient Neural Estimation of Structural Equation Models: An Adversarial Approach,"Luofeng Liao, You-Lin Chen, Zhuoran Yang, Bo Dai, Mladen Kolar, Zhaoran Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/65ae450c5536606c266f49f1c08321f2-Paper.pdf,Security Analysis of Safe and Seldonian Reinforcement Learning Algorithms,"Pinar Ozisik, Philip S. Thomas",
neurips,https://proceedings.neurips.cc/paper/2020/file/65cf25ef90de99d93fa96dc49d0d8b3c-Paper.pdf,Learning to Play Sequential Games versus Unknown Opponents,"Pier Giuseppe Sessa, Ilija Bogunovic, Maryam Kamgarpour, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2020/file/66121d1f782d29b62a286909165517bc-Paper.pdf,Further Analysis of Outlier Detection with Deep Generative Models,"Ziyu Wang, Bin Dai, David Wipf, Jun Zhu",
neurips,https://proceedings.neurips.cc/paper/2020/file/661b1e76b95cc50a7a11a85619a67d95-Paper.pdf,Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning,"Guangxiang Zhu, Minghao Zhang, Honglak Lee, Chongjie Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/662a2e96162905620397b19c9d249781-Paper.pdf,Neural Networks Learning and Memorization with (almost) no Over-Parameterization,Amit Daniely,"Many results in recent years established polynomial time learnability of various models via neural networks algorithms (e.g. \cite{andoni2014learning, daniely2016toward, daniely2017sgd, cao2019generalization, ziwei2019polylogarithmic, zou2019improved, ma2019comparative, du2018gradient, arora2019fine, song2019quadratic, oymak2019towards, ge2019mildly, brutzkus2018sgd}). However, unless the model is linear separable~\cite{brutzkus2018sgd}, or the activation is a polynomial~\cite{ge2019mildly}, these results require very large networks -- much more than what is needed for the mere existence of a good predictor. In this paper we prove that SGD on depth two neural networks can memorize samples, learn polynomials with bounded weights, and learn certain kernel spaces, with {\em near optimal} network size, sample complexity, and runtime. In particular, we show that SGD on depth two network with
~
O
(
m
d
)
O
hidden neurons (and hence
~
O
(
m
)
O
parameters) can memorize
m
m
random labeled points in
\sphere
d
−
1
\sphere
."
neurips,https://proceedings.neurips.cc/paper/2020/file/6646b06b90bd13dabc11ddba01270d23-Paper.pdf,Exploiting Higher Order Smoothness in Derivative-free Optimization and Continuous Bandits,"Arya Akhavan, Massimiliano Pontil, Alexandre Tsybakov",
neurips,https://proceedings.neurips.cc/paper/2020/file/665d5cbb82b5785d9f344c46417c6c36-Paper.pdf,Towards a Combinatorial Characterization of Bounded-Memory Learning,"Alon Gonen, Shachar Lovett, Michal Moshkovitz",
neurips,https://proceedings.neurips.cc/paper/2020/file/66de6afdfb5fb3c21d0e3b5c3226bf00-Paper.pdf,"Chaos, Extremism and Optimism: Volume Analysis of Learning in Games","Yun Kuen Cheung, Georgios Piliouras","First, we examine these dynamics not in their original space (simplex of actions) but in a dual space (aggregate payoffs of actions). Second, we explore how the volume of a set of initial conditions evolves over time when it is pushed forward according to the algorithm. This is reminiscent of approaches in evolutionary game theory where replicator dynamics, the continuous-time analogue of MWU, is known to preserve volume in all games. Interestingly, when we examine discrete-time dynamics, the choices of the game and the algorithm both play a critical role. So whereas MWU expands volume in zero-sum games and is thus Lyapunov chaotic, we show that OMWU contracts volume, providing an alternative understanding for its known convergent behavior. Yet, we also prove a no-free-lunch type of theorem, in the sense that when examining coordination games the roles are reversed."
neurips,https://proceedings.neurips.cc/paper/2020/file/670c26185a3783678135b4697f7dbd1a-Paper.pdf,On Regret with Multiple Best Arms,"Yinglun Zhu, Robert Nowak",
neurips,https://proceedings.neurips.cc/paper/2020/file/672cf3025399742b1a047c8dc6b1e992-Paper.pdf,Matrix Completion with Hierarchical Graph Side Information,"Adel Elmahdy, Junhyung Ahn, Changho Suh, Soheil Mohajer",
neurips,https://proceedings.neurips.cc/paper/2020/file/6734fa703f6633ab896eecbdfad8953a-Paper.pdf,Is Long Horizon RL More Difficult Than Short Horizon RL?,"Ruosong Wang, Simon S. Du, Lin Yang, Sham Kakade","Learning to plan for long horizons is a central challenge in episodic reinforcement learning problems. A fundamental question is to understand how the difficulty of the problem scales as the horizon increases. Here the natural measure of sample complexity is a normalized one: we are interested in the \emph{number of episodes} it takes to provably discover a policy whose value is
ε
ε
near to that of the optimal value, where the value is measured by the \emph{normalized} cumulative reward in each episode. In a COLT 2018 open problem, Jiang and Agarwal conjectured that, for tabular, episodic reinforcement learning problems, there exists a sample complexity lower bound which exhibits a polynomial dependence on the horizon --- a conjecture which is consistent with all known sample complexity upper bounds. This work refutes this conjecture, proving that tabular, episodic reinforcement learning is possible with a sample complexity that scales only \emph{logarithmically} with the planning horizon. In other words, when the values are appropriately normalized (to lie in the unit interval), this results shows that long horizon RL is no more difficult than short horizon RL, at least in a minimax sense. Our analysis introduces two ideas: (i) the construction of an
ε
ε
-net for near-optimal policies whose log-covering number scales only logarithmically with the planning horizon, and (ii) the Online Trajectory Synthesis algorithm, which adaptively evaluates all policies in a given policy class and enjoys a sample complexity that scales logarithmically with the cardinality of the given policy class. Both may be of independent interest."
neurips,https://proceedings.neurips.cc/paper/2020/file/673de96b04fa3adcae1aacda704217ef-Paper.pdf,Hamiltonian Monte Carlo using an adjoint-differentiated Laplace approximation: Bayesian inference for latent Gaussian models and beyond,"Charles Margossian, Aki Vehtari, Daniel Simpson, Raj Agrawal",
neurips,https://proceedings.neurips.cc/paper/2020/file/6740526b78c0b230e41ae61d8ca07cf5-Paper.pdf,Adversarial Learning for Robust Deep Clustering,"Xu Yang, Cheng Deng, Kun Wei, Junchi Yan, Wei Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/6754e06e46dfa419d5afe3c9781cecad-Paper.pdf,Learning Mutational Semantics,"Brian Hie, Ellen Zhong, Bryan Bryson, Bonnie Berger",
neurips,https://proceedings.neurips.cc/paper/2020/file/67d16d00201083a2b118dd5128dd6f59-Paper.pdf,Learning to Learn Variational Semantic Memory,"Xiantong Zhen, Yingjun Du, Huan Xiong, Qiang Qiu, Cees Snoek, Ling Shao",
neurips,https://proceedings.neurips.cc/paper/2020/file/67e235e7f2fa8800d8375409b566e6b6-Paper.pdf,Myersonian Regression,"Allen Liu, Renato Leme, Jon Schneider","Motivated by pricing applications in online advertising, we study a variant of linear regression with a discontinuous loss function that we term Myersonian regression. In this variant, we wish to find a linear function
f
:
R
d
→
R
f
that well approximates a set of points
(
x
i
,
v
i
)
∈
R
d
×
[
0
,
1
]
(
in the following sense: we receive a loss of
v
i
v
when
f
(
x
i
)
>
v
i
f
and a loss of
v
i
−
f
(
x
i
)
v
when
f
(
x
i
)
≤
v
i
f
. This arises naturally in the economic application of designing a pricing policy for differentiated items (where the loss is the gap between the performance of our policy and the optimal Myerson prices). We show that Myersonian regression is NP-hard to solve exactly and furthermore that no fully polynomial-time approximation scheme exists for Myersonian regression conditioned on the Exponential Time Hypothesis being true. In contrast to this, we demonstrate a polynomial-time approximation scheme for Myersonian regression that obtains an
ϵ
m
ϵ
additive approximation to the optimal possible revenue and can be computed in time
O
(
exp
(
p
o
l
y
(
1
/
ϵ
)
)
\poly
(
m
,
n
)
)
O
. We show that this algorithm is stable and generalizes well over distributions of samples."
neurips,https://proceedings.neurips.cc/paper/2020/file/67ff32d40fb51f1a2fd2c4f1b1019785-Paper.pdf,Learnability with Indirect Supervision Signals,"Kaifu Wang, Qiang Ning, Dan Roth",
neurips,https://proceedings.neurips.cc/paper/2020/file/680390c55bbd9ce416d1d69a9ab4760d-Paper.pdf,Towards Safe Policy Improvement for Non-Stationary MDPs,"Yash Chandak, Scott Jordan, Georgios Theocharous, Martha White, Philip S. Thomas",
neurips,https://proceedings.neurips.cc/paper/2020/file/6811f9b2bf86bf64e3f320973119b959-Paper.pdf,Finer Metagenomic Reconstruction via Biodiversity Optimization,"Simon Foucart, David Koslicki",
neurips,https://proceedings.neurips.cc/paper/2020/file/6822951732be44edf818dc5a97d32ca6-Paper.pdf,Causal Discovery in Physical Systems from Videos,"Yunzhu Li, Antonio Torralba, Anima Anandkumar, Dieter Fox, Animesh Garg",
neurips,https://proceedings.neurips.cc/paper/2020/file/685ac8cadc1be5ac98da9556bc1c8d9e-Paper.pdf,Glyph: Fast and Accurately Training Deep Neural Networks on Encrypted Data,"Qian Lou, Bo Feng, Geoffrey Charles Fox, Lei Jiang","In this paper, we propose, Glyph, an FHE-based technique to fast and accurately train DNNs on encrypted data by switching between TFHE (Fast Fully Homomorphic Encryption over the Torus) and BGV cryptosystems. Glyph uses logic-operation-friendly TFHE to implement nonlinear activations, while adopts vectorial-arithmetic-friendly BGV to perform multiply-accumulations (MACs). Glyph further applies transfer learning on DNN training to improve test accuracy and reduce the number of MACs between ciphertext and ciphertext in convolutional layers. Our experimental results show Glyph obtains state-of-the-art accuracy, and reduces training latency by 69%~99% over prior FHE-based privacy-preserving techniques on encrypted datasets."
neurips,https://proceedings.neurips.cc/paper/2020/file/685bfde03eb646c27ed565881917c71c-Paper.pdf,Smoothed Analysis of Online and Differentially Private Learning,"Nika Haghtalab, Tim Roughgarden, Abhishek Shetty",
neurips,https://proceedings.neurips.cc/paper/2020/file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf,Self-Paced Deep Reinforcement Learning,"Pascal Klink, Carlo D'Eramo, Jan R. Peters, Joni Pajarinen",
neurips,https://proceedings.neurips.cc/paper/2020/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf,Kalman Filtering Attention for User Behavior Modeling in CTR Prediction,"Hu Liu, Jing LU, Xiwei Zhao, Sulong Xu, Hao Peng, Yutong Liu, Zehua Zhang, Jian Li, Junsheng Jin, Yongjun Bao, Weipeng Yan",
neurips,https://proceedings.neurips.cc/paper/2020/file/68d3743587f71fbaa5062152985aff40-Paper.pdf,Towards Maximizing the Representation Gap between In-Domain & Out-of-Distribution Examples,"Jay Nandy, Wynne Hsu, Mong Li Lee",
neurips,https://proceedings.neurips.cc/paper/2020/file/68dd09b9ff11f0df5624a690fe0f6729-Paper.pdf,Fully Convolutional Mesh Autoencoder using Efficient Spatially Varying Kernels,"Yi Zhou, Chenglei Wu, Zimo Li, Chen Cao, Yuting Ye, Jason Saragih, Hao Li, Yaser Sheikh",
neurips,https://proceedings.neurips.cc/paper/2020/file/690d83983a63aa1818423fd6edd3bfdb-Paper.pdf,GNNGuard: Defending Graph Neural Networks against Adversarial Attacks,"Xiang Zhang, Marinka Zitnik",
neurips,https://proceedings.neurips.cc/paper/2020/file/690f44c8c2b7ded579d01abe8fdb6110-Paper.pdf,Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction,"Tong He, John Collomosse, Hailin Jin, Stefano Soatto",
neurips,https://proceedings.neurips.cc/paper/2020/file/691dcb1d65f31967a874d18383b9da75-Paper.pdf,Optimal visual search based on a model of target detectability in natural images,"Shima Rashidi, Krista Ehinger, Andrew Turpin, Lars Kulik",
neurips,https://proceedings.neurips.cc/paper/2020/file/6925f2a16026e36e4fc112f82dd79406-Paper.pdf,Towards Convergence Rate Analysis of Random Forests for Classification,"Wei Gao, Zhi-Hua Zhou",
neurips,https://proceedings.neurips.cc/paper/2020/file/6933b5648c59d618bbb30986c84080fe-Paper.pdf,List-Decodable Mean Estimation via Iterative Multi-Filtering,"Ilias Diakonikolas, Daniel Kane, Daniel Kongsgaard","We study the problem of {\em list-decodable mean estimation} for bounded covariance distributions. Specifically, we are given a set
T
T
of points in
\R
d
\R
with the promise that an unknown
α
α
-fraction of points in
T
T
, where
0
<
α
<
1
/
2
0
, are drawn from an unknown mean and bounded covariance distribution
D
D
, and no assumptions are made on the remaining points. The goal is to output a small list of hypothesis vectors such that at least one of them is close to the mean of
D
D
. We give the first practically viable estimator for this problem. In more detail, our algorithm is sample and computationally efficient, and achieves information-theoretically near-optimal error. While the only prior algorithm for this setting inherently relied on the ellipsoid method, our algorithm is iterative and only uses spectral techniques. Our main technical innovation is the design of a soft outlier removal procedure for high-dimensional heavy-tailed datasets with a majority of outliers."
neurips,https://proceedings.neurips.cc/paper/2020/file/6950aa02ae8613af620668146dd11840-Paper.pdf,Exact Recovery of Mangled Clusters with Same-Cluster Queries,"Marco Bressan, Nicolò Cesa-Bianchi, Silvio Lattanzi, Andrea Paudice","We study the cluster recovery problem in the semi-supervised active clustering framework. Given a finite set of input points, and an oracle revealing whether any two points lie in the same cluster, our goal is to recover all clusters exactly using as few queries as possible. To this end, we relax the spherical
k
k
-means cluster assumption of Ashtiani et al.\ to allow for arbitrary ellipsoidal clusters with margin. This removes the assumption that the clustering is center-based (i.e., defined through an optimization problem), and includes all those cases where spherical clusters are individually transformed by any combination of rotations, axis scalings, and point deletions. We show that, even in this much more general setting, it is still possible to recover the latent clustering exactly using a number of queries that scales only logarithmically with the number of input points. More precisely, we design an algorithm that, given
n
n
points to be partitioned into
k
k
clusters, uses
O
(
k
3
ln
k
ln
n
)
O
oracle queries and
˜
O
(
k
n
+
k
3
)
O
time to recover the clustering with zero misclassification error. The
O
(
⋅
)
O
notation hides an exponential dependence on the dimensionality of the clusters, which we show to be necessary thus characterizing the query complexity of the problem. Our algorithm is simple, easy to implement, and can also learn the clusters using low-stretch separators, a class of ellipsoids with additional theoretical guarantees. Experiments on large synthetic datasets confirm that we can reconstruct clusterings exactly and efficiently."
neurips,https://proceedings.neurips.cc/paper/2020/file/69bfa2aa2b7b139ff581a806abf0a886-Paper.pdf,Steady State Analysis of Episodic Reinforcement Learning,Huang Bojun,"In this paper we proved that unique steady-state distributions pervasively exist in the learning environment of episodic learning tasks, and that the marginal distributions of the system state indeed approach to the steady state in essentially all episodic tasks. This observation supports an interestingly reversed mindset against conventional wisdom: While steady states are traditionally presumed to exist in continual learning and considered less relevant in episodic learning, it turns out they are guaranteed to exist for the latter under any behavior policy. We further developed interesting connections for important concepts that have been separately treated in episodic and continual RL. At the practical side, the existence of unique and approachable steady state implies a general, reliable, and efficient way to collect data in episodic RL algorithms. We applied this method to policy gradient algorithms, based on a new steady-state policy gradient theorem. We also proposed and experimentally evaluated a perturbation method to enforce faster convergence to steady state in real-world episodic RL tasks."
neurips,https://proceedings.neurips.cc/paper/2020/file/69d1fc78dbda242c43ad6590368912d4-Paper.pdf,Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures,"Julien Launay, Iacopo Poli, François Boniface, Florent Krzakala",
neurips,https://proceedings.neurips.cc/paper/2020/file/69eba34671b3ef1ef38ee85caae6b2a1-Paper.pdf,Bayesian Optimization for Iterative Learning,"Vu Nguyen, Sebastian Schulze, Michael Osborne",
neurips,https://proceedings.neurips.cc/paper/2020/file/6a508a60aa3bf9510ea6acb021c94b48-Paper.pdf,Minimax Bounds for Generalized Linear Models,"Kuan-Yun Lee, Thomas Courtade","We establish a new class of minimax prediction error bounds for generalized linear models. Our bounds significantly improve previous results when the design matrix is poorly structured, including natural cases where the matrix is wide or does not have full column rank. Apart from the typical
L
2
L
risks, we study a class of entropic risks which recovers the usual
L
2
L
prediction and estimation risks, and demonstrate that a tight analysis of Fisher information can uncover underlying structural dependency in terms of the spectrum of the design matrix. The minimax approach we take differs from the traditional metric entropy approach, and can be applied to many other settings."
neurips,https://proceedings.neurips.cc/paper/2020/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf,Projection Robust Wasserstein Distance and Riemannian Optimization,"Tianyi Lin, Chenyou Fan, Nhat Ho, Marco Cuturi, Michael Jordan",
neurips,https://proceedings.neurips.cc/paper/2020/file/6aaba9a124857622930ca4e50f5afed2-Paper.pdf,CoinDICE: Off-Policy Confidence Interval Estimation,"Bo Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesvari, Dale Schuurmans",
neurips,https://proceedings.neurips.cc/paper/2020/file/6abba5d8ab1f4f32243e174beb754661-Paper.pdf,Simple and Fast Algorithm for Binary Integer and Online Linear Programming,"Xiaocheng Li, Chunlin Sun, Yinyu Ye","In this paper, we develop a simple and fast online algorithm for solving a class of binary integer linear programs (LPs) arisen in the general resource allocation problem. The algorithm requires only one single pass through the input data and is free of doing any matrix inversion. It can be viewed as both an approximate algorithm for solving binary integer LPs and a fast algorithm for solving online LP problems. The algorithm is inspired by an equivalent form of the dual problem of the relaxed LP and it essentially performs (one-pass) projected stochastic subgradient descent in the dual space. We analyze the algorithm under two different models, stochastic input and random permutation, with minimal technical assumptions on the input data. The algorithm achieves
O
(
m
√
n
)
O
expected regret under the stochastic input model and
O
(
(
m
+
log
n
)
√
n
)
O
expected regret under the random permutation model, and it achieves
O
(
m
√
n
)
O
expected constraint violation under both models, where
n
n
is the number of decision variables and
m
m
is the number of constraints. In addition, we employ the notion of permutational Rademacher complexity and derive regret bounds for two earlier online LP algorithms for comparison. Both algorithms improve the regret bound with a factor of
√
m
m
by paying more computational cost. Furthermore, we demonstrate how to convert the possibly infeasible solution to a feasible one through a randomized procedure. Numerical experiments illustrate the general applicability and effectiveness of the algorithms."
neurips,https://proceedings.neurips.cc/paper/2020/file/6ad4174eba19ecb5fed17411a34ff5e6-Paper.pdf,Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction,"Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, Yi Ma","To learn intrinsic low-dimensional structures from high-dimensional data that most discriminate between classes, we propose the principle of {\em Maximal Coding Rate Reduction} (
MCR
2
MCR
), an information-theoretic measure that maximizes the coding rate difference between the whole dataset and the sum of each individual class. We clarify its relationships with most existing frameworks such as cross-entropy, information bottleneck, information gain, contractive and contrastive learning, and provide theoretical guarantees for learning diverse and discriminative features. The coding rate can be accurately computed from finite samples of degenerate subspace-like distributions and can learn intrinsic representations in supervised, self-supervised, and unsupervised settings in a unified manner. Empirically, the representations learned using this principle alone are significantly more robust to label corruptions in classification than those using cross-entropy, and can lead to state-of-the-art results in clustering mixed data from self-learned invariant features."
neurips,https://proceedings.neurips.cc/paper/2020/file/6affee954d76859baa2800e1c49e2c5d-Paper.pdf,Learning Rich Rankings,"Arjun Seshadri, Stephen Ragain, Johan Ugander",
neurips,https://proceedings.neurips.cc/paper/2020/file/6b39183e7053a0106e4376f4e9c5c74d-Paper.pdf,Color Visual Illusions: A Statistics-based Computational Model,"Elad Hirsch, Ayellet Tal",
neurips,https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela",
neurips,https://proceedings.neurips.cc/paper/2020/file/6b5617315c9ac918215fc7514bef514b-Paper.pdf,Universal guarantees for decision tree induction via a higher-order splitting criterion,"Guy Blanc, Neha Gupta, Jane Lange, Li-Yang Tan","We propose a simple extension of {\sl top-down decision tree learning heuristics} such as ID3, C4.5, and CART. Our algorithm achieves provable guarantees for all target functions
f
:
{
−
1
,
1
}
n
→
{
−
1
,
1
}
f
with respect to the uniform distribution, circumventing impossibility results showing that existing heuristics fare poorly even for simple target functions. The crux of our extension is a new splitting criterion that takes into account the correlations between
f
f
and {\sl small subsets} of its attributes. The splitting criteria of existing heuristics (e.g. Gini impurity and information gain), in contrast, are based solely on the correlations between
f
f
and its {\sl individual} attributes. Our algorithm satisfies the following guarantee: for all target functions
f
:
{
−
1
,
1
}
n
→
{
−
1
,
1
}
f
, sizes
s
∈
\N
s
, and error parameters
\eps
\eps
, it constructs a decision tree of size
s
~
O
(
(
log
s
)
2
/
\eps
2
)
s
that achieves error
≤
O
(
\opt
s
)
+
\eps
≤
, where
\opt
s
\opt
denotes the error of the optimal size-
s
s
decision tree for
f
f
. A key technical notion that drives our analysis is the {\sl noise stability} of
f
f
, a well-studied smoothness measure of
f
f
."
neurips,https://proceedings.neurips.cc/paper/2020/file/6b8b8e3bd6ad94b985c1b1f1b7a94cb2-Paper.pdf,Trade-offs and Guarantees of Adversarial Representation Learning for Information Obfuscation,"Han Zhao, Jianfeng Chi, Yuan Tian, Geoffrey J. Gordon",
neurips,https://proceedings.neurips.cc/paper/2020/file/6ba3af5d7b2790e73f0de32e5c8c1798-Paper.pdf,A Boolean Task Algebra for Reinforcement Learning,"Geraud Nangue Tasse, Steven James, Benjamin Rosman",
neurips,https://proceedings.neurips.cc/paper/2020/file/6bb56208f672af0dd65451f869fedfd9-Paper.pdf,Learning with Differentiable Pertubed Optimizers,"Quentin Berthet, Mathieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, Francis Bach",
neurips,https://proceedings.neurips.cc/paper/2020/file/6c1e55ec7c43dc51a37472ddcbd756fb-Paper.pdf,Optimal Learning from Verified Training Data,"Nicholas Bishop, Long Tran-Thanh, Enrico Gerding",
neurips,https://proceedings.neurips.cc/paper/2020/file/6c250b592dc94d4de38a79db4d2b18f2-Paper.pdf,Online Linear Optimization with Many Hints,"Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, Manish Purohit","We study an online linear optimization (OLO) problem in which the learner is provided access to
K
K
hint'' vectors in each round prior to making a decision. In this setting, we devise an algorithm that obtains logarithmic regret whenever there exists a convex combination of the
K
K
hints that has positive correlation with the cost vectors. This significantly extends prior work that considered only the case
K
=
1
K
. To accomplish this, we develop a way to combine many arbitrary OLO algorithms to obtain regret only a logarithmically worse factor than the minimum regret of the original algorithms in hindsight; this result is of independent interest."
neurips,https://proceedings.neurips.cc/paper/2020/file/6c81c83c4bd0b58850495f603ab45a93-Paper.pdf,Dynamical mean-field theory for stochastic gradient descent in Gaussian mixture classification,"Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, Lenka Zdeborová",
neurips,https://proceedings.neurips.cc/paper/2020/file/6cd9313ed34ef58bad3fdd504355e72c-Paper.pdf,Causal Discovery from Soft Interventions with Unknown Targets: Characterization and Learning,"Amin Jaber, Murat Kocaoglu, Karthikeyan Shanmugam, Elias Bareinboim",
neurips,https://proceedings.neurips.cc/paper/2020/file/6ce8d8f3b038f737cefcdafcf3752452-Paper.pdf,Exploiting the Surrogate Gap in Online Multiclass Classification,Dirk van der Hoeven,"We present \textproc{Gaptron}, a randomized first-order algorithm for online multiclass classification. In the full information setting we provide expected mistake bounds for \textproc{Gaptron} with respect to the logistic loss, hinge loss, and the smooth hinge loss with
O
(
K
)
O
regret, where the expectation is with respect to the learner's randomness and
K
K
is the number of classes. In the bandit classification setting we show that \textproc{Gaptron} is the first linear time algorithm with
O
(
K
√
T
)
O
expected regret. Additionally, the expected mistake bound of \textproc{Gaptron} does not depend on the dimension of the feature vector, contrary to previous algorithms with
O
(
K
√
T
)
O
regret in the bandit classification setting. We present a new proof technique that exploits the gap between the zero-one loss and surrogate losses rather than exploiting properties such as exp-concavity or mixability, which are traditionally used to prove logarithmic or constant regret bounds."
neurips,https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf,The Pitfalls of Simplicity Bias in Neural Networks,"Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, Praneeth Netrapalli",
neurips,https://proceedings.neurips.cc/paper/2020/file/6d0c932802f6953f70eb20931645fa40-Paper.pdf,Automatically Learning Compact Quality-aware Surrogates for Optimization Problems,"Kai Wang, Bryan Wilder, Andrew Perrault, Milind Tambe",
neurips,https://proceedings.neurips.cc/paper/2020/file/6d34d468ac8876333c4d7173b85efed9-Paper.pdf,Empirical Likelihood for Contextual Bandits,"Nikos Karampatziakis, John Langford, Paul Mineiro",
neurips,https://proceedings.neurips.cc/paper/2020/file/6d70cb65d15211726dcce4c0e971e21c-Paper.pdf,Can Q-Learning with Graph Networks Learn a Generalizable Branching Heuristic for a SAT Solver?,"Vitaly Kurin, Saad Godil, Shimon Whiteson, Bryan Catanzaro",
neurips,https://proceedings.neurips.cc/paper/2020/file/6d79e030371e47e6231337805a7a2685-Paper.pdf,Non-reversible Gaussian processes for identifying latent dynamical structure in neural data,"Virginia Rutten, Alberto Bernacchia, Maneesh Sahani, Guillaume Hennequin",
neurips,https://proceedings.neurips.cc/paper/2020/file/6d7d394c9d0c886e9247542e06ebb705-Paper.pdf,Listening to Sounds of Silence for Speech Denoising,"Ruilin Xu, Rundi Wu, Yuko Ishiwaka, Carl Vondrick, Changxi Zheng",
neurips,https://proceedings.neurips.cc/paper/2020/file/6dbbe6abe5f14af882ff977fc3f35501-Paper.pdf,BoxE: A Box Embedding Model for Knowledge Base Completion,"Ralph Abboud, Ismail Ceylan, Thomas Lukasiewicz, Tommaso Salvatori",
neurips,https://proceedings.neurips.cc/paper/2020/file/6dd4e10e3296fa63738371ec0d5df818-Paper.pdf,Coherent Hierarchical Multi-Label Classification Networks,"Eleonora Giunchiglia, Thomas Lukasiewicz",
neurips,https://proceedings.neurips.cc/paper/2020/file/6df182582740607da754e4515b70e32d-Paper.pdf,Walsh-Hadamard Variational Inference for Bayesian Deep Learning,"Simone Rossi, Sebastien Marmin, Maurizio Filippone",
neurips,https://proceedings.neurips.cc/paper/2020/file/6dfe08eda761bd321f8a9b239f6f4ec3-Paper.pdf,Federated Bayesian Optimization via Thompson Sampling,"Zhongxiang Dai, Bryan Kian Hsiang Low, Patrick Jaillet",
neurips,https://proceedings.neurips.cc/paper/2020/file/6e01383fd96a17ae51cc3e15447e7533-Paper.pdf,MultiON: Benchmarking Semantic Map Memory using Multi-Object Navigation,"Saim Wani, Shivansh Patel, Unnat Jain, Angel Chang, Manolis Savva","We propose the multiON task, which requires navigation to an episode-specific sequence of objects in a realistic environment. MultiON generalizes the ObjectGoal navigation task and explicitly tests the ability of navigation agents to locate previously observed goal objects. We perform a set of multiON experiments to examine how a variety of agent models perform across a spectrum of navigation task complexities. Our experiments show that: i) navigation performance degrades dramatically with escalating task complexity; ii) a simple semantic map agent performs surprisingly well relative to more complex neural image feature map agents; and iii) even oracle map agents achieve relatively low performance, indicating the potential for future work in training embodied navigation agents using maps."
neurips,https://proceedings.neurips.cc/paper/2020/file/6e17a5fd135fcaf4b49f2860c2474c7c-Paper.pdf,Neural Complexity Measures,"Yoonho Lee, Juho Lee, Sung Ju Hwang, Eunho Yang, Seungjin Choi",
neurips,https://proceedings.neurips.cc/paper/2020/file/6e69ebbfad976d4637bb4b39de261bf7-Paper.pdf,Optimal Iterative Sketching Methods with the Subsampled Randomized Hadamard Transform,"Jonathan Lacotte, Sifan Liu, Edgar Dobriban, Mert Pilanci",
neurips,https://proceedings.neurips.cc/paper/2020/file/6ef1173b096aa200158bfbc8af3ae8e3-Paper.pdf,Provably adaptive reinforcement learning in metric spaces,"Tongyi Cao, Akshay Krishnamurthy",
neurips,https://proceedings.neurips.cc/paper/2020/file/6f1d0705c91c2145201df18a1a0c7345-Paper.pdf,ShapeFlow: Learnable Deformation Flows Among 3D Shapes,"Chiyu Jiang, Jingwei Huang, Andrea Tagliasacchi, Leonidas J. Guibas",
neurips,https://proceedings.neurips.cc/paper/2020/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf,Self-Supervised Learning by Cross-Modal Audio-Video Clustering,"Humam Alwassel, Dhruv Mahajan, Bruno Korbar, Lorenzo Torresani, Bernard Ghanem, Du Tran",
neurips,https://proceedings.neurips.cc/paper/2020/file/6f3a770e5af1fd4cadc5f004b81e1040-Paper.pdf,Optimal Query Complexity of Secure Stochastic Convex Optimization,"Wei Tang, Chien-Ju Ho, Yang Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/6f5216f8d89b086c18298e043bfe48ed-Paper.pdf,DynaBERT: Dynamic BERT with Adaptive Width and Depth,"Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/6f5e4e86a87220e5d361ad82f1ebc335-Paper.pdf,Generalization Bound of Gradient Descent for Non-Convex Metric Learning,"MINGZHI DONG, Xiaochen Yang, Rui Zhu, Yujiang Wang, Jing-Hao Xue",
neurips,https://proceedings.neurips.cc/paper/2020/file/6fbd841e2e4b2938351a4f9b68f12e6b-Paper.pdf,Dynamic Submodular Maximization,Morteza Monemizadeh,"One of the basic primitives in the class of submodular optimization problems is the submodular maximization under a cardinality constraint. Here we are given a ground set
V
V
that is endowed with a monotone submodular function
f
:
2
V
→
\REAL
+
f
and a parameter
0
<
k
≤
n
0
and the goal is to return an optimal set
S
⊆
V
S
of at most
k
k
elements, i.e.,
f
(
S
)
f
is maximum among all subsets of
V
V
of size at most
k
k
. This basic primitive has many applications in machine learning as well as combinatorial optimization. Example applications are agglomerative clustering, exemplar-based clustering, categorical feature compression, document and corpus summarization, recommender systems, search result diversification, data subset selection, minimum spanning tree, max flow, global minimum cut, maximum matching, traveling salesman problem, max clique, max cut, set cover and knapsack, among the others. In this paper, we propose the first dynamic algorithm for this problem. Given a stream of inserts and deletes of elements of an underlying ground set
V
V
, we develop a dynamic algorithm that with high probability, maintains a
(
1
2
−
ϵ
)
(
-approximation of a cardinality-constrained monotone submodular maximization for any sequence of
z
z
updates (inserts and deletes) in time
O
(
k
2
z
ϵ
−
3
⋅
log
5
n
)
O
, where
n
n
is the maximum size of
V
V
at any time. That is, the amortized update time of our algorithm is
O
(
k
2
ϵ
−
3
⋅
log
5
n
)
O
."
neurips,https://proceedings.neurips.cc/paper/2020/file/6fd86e0ad726b778e37cf270fa0247d7-Paper.pdf,Inference for Batched Bandits,"Kelly Zhang, Lucas Janson, Susan Murphy",
neurips,https://proceedings.neurips.cc/paper/2020/file/6fd9a99a5abed788d9afc9d52d54e91b-Paper.pdf,Approximate Cross-Validation with Low-Rank Data in High Dimensions,"Will Stephenson, Madeleine Udell, Tamara Broderick","Many recent advances in machine learning are driven by a challenging trifecta: large data size
N
N
, high dimensions, and expensive algorithms. In this setting, cross-validation (CV) serves as an important tool for model assessment. Recent advances in approximate cross validation (ACV) provide accurate approximations to CV with only a single model fit, avoiding traditional CV's requirement for repeated runs of expensive algorithms. Unfortunately, these ACV methods can lose both speed and accuracy in high dimensions --- unless sparsity structure is present in the data. Fortunately, there is an alternative type of simplifying structure that is present in most data: approximate low rank (ALR). Guided by this observation, we develop a new algorithm for ACV that is fast and accurate in the presence of ALR data. Our first key insight is that the Hessian matrix --- whose inverse forms the computational bottleneck of existing ACV methods --- is ALR. We show that, despite our use of the \emph{inverse} Hessian, a low-rank approximation using the largest (rather than the smallest) matrix eigenvalues enables fast, reliable ACV. Our second key insight is that, in the presence of ALR data, error in existing ACV methods roughly grows with the (approximate, low) rank rather than with the (full, high) dimension. These insights allow us to prove theoretical guarantees on the quality of our proposed algorithm --- along with fast-to-compute upper bounds on its error. We demonstrate the speed and accuracy of our method, as well as the usefulness of our bounds, on a range of real and simulated data sets."
neurips,https://proceedings.neurips.cc/paper/2020/file/6fe43269967adbb64ec6149852b5cc3e-Paper.pdf,GANSpace: Discovering Interpretable GAN Controls,"Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris",
neurips,https://proceedings.neurips.cc/paper/2020/file/6fec24eac8f18ed793f5eaad3dd7977c-Paper.pdf,Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization,"Samuel Daulton, Maximilian Balandat, Eytan Bakshy",
neurips,https://proceedings.neurips.cc/paper/2020/file/703957b6dd9e3a7980e040bee50ded65-Paper.pdf,Neuron-level Structured Pruning using Polarization Regularizer,"Tao Zhuang, Zhixuan Zhang, Yuheng Huang, Xiaoyi Zeng, Kai Shuang, Xiang Li","Neuron-level structured pruning is a very effective technique to reduce the computation of neural networks without compromising prediction accuracy. In previous works, structured pruning is usually achieved by imposing L1 regularization on the scaling factors of neurons, and pruning the neurons whose scaling factors are below a certain threshold. The reasoning is that neurons with smaller scaling factors have weaker influence on network output. A scaling factor close to 0 actually suppresses a neuron. However, L1 regularization lacks discrimination between neurons because it pushes all scaling factors towards 0. A more reasonable pruning method is to only suppress unimportant neurons (with 0 scaling factors) and simultaneously keep important neurons intact (with larger scaling factor). To achieve this goal, we propose a new regularizer on scaling factors, namely polarization regularizer. Theoretically, we prove that polarization regularizer pushes some scaling factors to 0 and others to a value
a
>
0
a
. Experimentally, we show that structured pruning using polarization regularizer achieves much better results than using L1 regularizer. Experiments on CIFAR and ImageNet datasets show that polarization pruning achieves the state-of-the-art result to date."
neurips,https://proceedings.neurips.cc/paper/2020/file/70431e77d378d760c3c5456519f06efe-Paper.pdf,Limits on Testing Structural Changes in Ising Models,"Aditya Gangrade, Bobak Nazer, Venkatesh Saligrama",
neurips,https://proceedings.neurips.cc/paper/2020/file/7078971350bcefbc6ec2779c9b84a9bd-Paper.pdf,Field-wise Learning for Multi-field Categorical Data,"Zhibin Li, Jian Zhang, Yongshun Gong, Yazhou Yao, Qiang Wu",
neurips,https://proceedings.neurips.cc/paper/2020/file/70d85f35a1fdc0ab701ff78779306407-Paper.pdf,Continual Learning in Low-rank Orthogonal Subspaces,"Arslan Chaudhry, Naeemullah Khan, Puneet Dokania, Philip Torr",
neurips,https://proceedings.neurips.cc/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf,Unsupervised Learning of Visual Features by Contrasting Cluster Assignments,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin",
neurips,https://proceedings.neurips.cc/paper/2020/file/712a3c9878efeae8ff06d57432016ceb-Paper.pdf,"Sharpened Generalization Bounds based on Conditional Mutual Information and an Application to Noisy, Iterative Algorithms","Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel M. Roy, Gintare Karolina Dziugaite",
neurips,https://proceedings.neurips.cc/paper/2020/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf,Learning Deformable Tetrahedral Meshes for 3D Reconstruction,"Jun Gao, Wenzheng Chen, Tommy Xiang, Alec Jacobson, Morgan McGuire, Sanja Fidler",
neurips,https://proceedings.neurips.cc/paper/2020/file/713fd63d76c8a57b16fc433fb4ae718a-Paper.pdf,Information theoretic limits of learning a sparse rule,"Clément Luneau, jean barbier, Nicolas Macris",
neurips,https://proceedings.neurips.cc/paper/2020/file/7183145a2a3e0ce2b68cd3735186b1d5-Paper.pdf,Self-supervised learning through the eyes of a child,"Emin Orhan, Vaibhav Gupta, Brenden M. Lake",
neurips,https://proceedings.neurips.cc/paper/2020/file/71a58e8cb75904f24cde464161c3e766-Paper.pdf,Unsupervised Semantic Aggregation and Deformable Template Matching for Semi-Supervised Learning,"Tao Han, Junyu Gao, Yuan Yuan, Qi Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/71c1806ca28b555c76650f52bb0d2810-Paper.pdf,A game-theoretic analysis of networked system control for common-pool resource management using multi-agent reinforcement learning,"Arnu Pretorius, Scott Cameron, Elan van Biljon, Thomas Makkink, Shahil Mawjee, Jeremy du Plessis, Jonathan Shock, Alexandre Laterre, Karim Beguir",
neurips,https://proceedings.neurips.cc/paper/2020/file/71e9c6620d381d60196ebe694840aaaa-Paper.pdf,"What shapes feature representations? Exploring datasets, architectures, and training","Katherine Hermann, Andrew Lampinen",
neurips,https://proceedings.neurips.cc/paper/2020/file/7212a6567c8a6c513f33b858d868ff80-Paper.pdf,Optimal Best-arm Identification in Linear Bandits,"Yassir Jedra, Alexandre Proutiere",
neurips,https://proceedings.neurips.cc/paper/2020/file/7221e5c8ec6b08ef6d3f9ff3ce6eb1d1-Paper.pdf,Data Diversification: A Simple Strategy For Neural Machine Translation,"Xuan-Phi Nguyen, Shafiq Joty, Kui Wu, Ai Ti Aw",
neurips,https://proceedings.neurips.cc/paper/2020/file/722caafb4825ef5d8670710fa29087cf-Paper.pdf,Interstellar: Searching Recurrent Architecture for Knowledge Graph Embedding,"Yongqi Zhang, Quanming Yao, Lei Chen",
neurips,https://proceedings.neurips.cc/paper/2020/file/723e8f97fde15f7a8d5ff8d558ea3f16-Paper.pdf,CoSE: Compositional Stroke Embeddings,"Emre Aksan, Thomas Deselaers, Andrea Tagliasacchi, Otmar Hilliges",
neurips,https://proceedings.neurips.cc/paper/2020/file/7250eb93b3c18cc9daa29cf58af7a004-Paper.pdf,Learning Multi-Agent Coordination for Enhancing Target Coverage in Directional Sensor Networks,"Jing Xu, Fangwei Zhong, Yizhou Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/7261925973c9bf0a74d85ae968a57e5f-Paper.pdf,Biological credit assignment through dynamic inversion of feedforward networks,"Bill Podlaski, Christian K. Machens",
neurips,https://proceedings.neurips.cc/paper/2020/file/7288251b27c8f0e73f4d7f483b06a785-Paper.pdf,Discriminative Sounding Objects Localization via Self-supervised Audiovisual Matching,"Di Hu, Rui Qian, Minyue Jiang, Xiao Tan, Shilei Wen, Errui Ding, Weiyao Lin, Dejing Dou",
neurips,https://proceedings.neurips.cc/paper/2020/file/72ab54f9b8c11fae5b923d7f854ef06a-Paper.pdf,Learning Multi-Agent Communication through Structured Attentive Reasoning,"Murtaza Rangwala, Ryan Williams",
neurips,https://proceedings.neurips.cc/paper/2020/file/72b32a1f754ba1c09b3695e0cb6cde7f-Paper.pdf,Private Identity Testing for High-Dimensional Distributions,"Clément L. Canonne, Gautam Kamath, Audra McMillan, Jonathan Ullman, Lydia Zakynthinou",
neurips,https://proceedings.neurips.cc/paper/2020/file/72e6d3238361fe70f22fb0ac624a7072-Paper.pdf,"On the Optimal Weighted
ℓ
2
ℓ
Regularization in Overparameterized Linear Regression","Denny Wu, Ji Xu","We consider the linear model
\vy
=
\vX
\vbeta
⋆
+
\vepsilon
\vy
with
\vX
∈
R
n
×
p
\vX
in the overparameterized regime
p
>
n
p
. We estimate
\vbeta
⋆
\vbeta
via generalized (weighted) ridge regression:
^
\vbeta
λ
=
(
\vX
\t
\vX
+
λ
\vSigma
w
)
†
\vX
\t
\vy
\vbeta
, where
\vSigma
w
\vSigma
is the weighting matrix. Assuming a random effects model with general data covariance
\vSigma
x
\vSigma
and anisotropic prior on the true coefficients
\vbeta
⋆
\vbeta
, i.e.,
\bbE
\vbeta
⋆
\vbeta
\t
⋆
=
\vSigma
β
\bbE
, we provide an exact characterization of the prediction risk
E
(
y
−
\vx
\t
^
\vbeta
λ
)
2
E
in the proportional asymptotic limit
p
/
n
→
γ
∈
(
1
,
∞
)
p
. Our general setup leads to a number of interesting findings. We outline precise conditions that decide the sign of the optimal setting
λ
\opt
λ
for the ridge parameter
λ
λ
and confirm the implicit
ℓ
2
ℓ
regularization effect of overparameterization, which theoretically justifies the surprising empirical observation that
λ
\opt
λ
can be \textit{negative} in the overparameterized regime. We also characterize the double descent phenomenon for principal component regression (PCR) when
\vX
\vX
and
\vbeta
⋆
\vbeta
are both anisotropic. Finally, we determine the optimal weighting matrix
\vSigma
w
\vSigma
for both the ridgeless (
λ
→
0
λ
) and optimally regularized (
λ
=
λ
\opt
λ
) case, and demonstrate the advantage of the weighted objective over standard ridge regression and PCR."
neurips,https://proceedings.neurips.cc/paper/2020/file/731309c4bb223491a9f67eac5214fb2e-Paper.pdf,An Efficient Asynchronous Method for Integrating Evolutionary and Gradient-based Policy Search,"Kyunghyun Lee, Byeong-Uk Lee, Ukcheol Shin, In So Kweon",
neurips,https://proceedings.neurips.cc/paper/2020/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf,MetaSDF: Meta-Learning Signed Distance Functions,"Vincent Sitzmann, Eric Chan, Richard Tucker, Noah Snavely, Gordon Wetzstein",
neurips,https://proceedings.neurips.cc/paper/2020/file/735ddec196a9ca5745c05bec0eaa4bf9-Paper.pdf,Simple and Scalable Sparse k-means Clustering via Feature Ranking,"Zhiyue Zhang, Kenneth Lange, Jason Xu",
neurips,https://proceedings.neurips.cc/paper/2020/file/73634c1dcbe056c1f7dcf5969da406c8-Paper.pdf,Model-based Adversarial Meta-Reinforcement Learning,"Zichuan Lin, Garrett Thomas, Guangwen Yang, Tengyu Ma",
neurips,https://proceedings.neurips.cc/paper/2020/file/73740ea85c4ec25f00f9acbd859f861d-Paper.pdf,Graph Policy Network for Transferable Active Learning on Graphs,"Shengding Hu, Zheng Xiong, Meng Qu, Xingdi Yuan, Marc-Alexandre Côté, Zhiyuan Liu, Jian Tang",
neurips,https://proceedings.neurips.cc/paper/2020/file/738a6457be8432bab553e21b4235dd97-Paper.pdf,Towards a Better Global Loss Landscape of GANs,"Ruoyu Sun, Tiantian Fang, Alexander Schwing",
neurips,https://proceedings.neurips.cc/paper/2020/file/73a427badebe0e32caa2e1fc7530b7f3-Paper.pdf,Weighted QMIX: Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning,"Tabish Rashid, Gregory Farquhar, Bei Peng, Shimon Whiteson","QMIX is a popular
Q
Q
-learning algorithm for cooperative MARL in the centralised training and decentralised execution paradigm. In order to enable easy decentralisation, QMIX restricts the joint action
Q
Q
-values it can represent to be a monotonic mixing of each agent's utilities. However, this restriction prevents it from representing value functions in which an agent's ordering over its actions can depend on other agents' actions. To analyse this representational limitation, we first formalise the objective QMIX optimises, which allows us to view QMIX as an operator that first computes the
Q
Q
-learning targets and then projects them into the space representable by QMIX. This projection returns a representable
Q
Q
-value that minimises the unweighted squared error across all joint actions. We show in particular that this projection can fail to recover the optimal policy even with access to
Q
∗
Q
, which primarily stems from the equal weighting placed on each joint action. We rectify this by introducing a weighting into the projection, in order to place more importance on the better joint actions. We propose two weighting schemes and prove that they recover the correct maximal action for any joint action
Q
Q
-values, and therefore for
Q
∗
Q
as well. Based on our analysis and results in the tabular setting we introduce two scalable versions of our algorithm, Centrally-Weighted (CW) QMIX and Optimistically-Weighted (OW) QMIX and demonstrate improved performance on both predator-prey and challenging multi-agent StarCraft benchmark tasks (Samvelyan et al., 2019)."
neurips,https://proceedings.neurips.cc/paper/2020/file/73b817090081cef1bca77232f4532c5d-Paper.pdf,BanditPAM: Almost Linear Time k-Medoids Clustering via Multi-Armed Bandits,"Mo Tiwari, Martin J. Zhang, James Mayclin, Sebastian Thrun, Chris Piech, Ilan Shomorony",
neurips,https://proceedings.neurips.cc/paper/2020/file/73d02e4344f71a0b0d51a925246990e7-Paper.pdf,"UDH: Universal Deep Hiding for Steganography, Watermarking, and Light Field Messaging","Chaoning Zhang, Philipp Benz, Adil Karjauv, Geng Sun, In So Kweon","Neural networks have been shown effective in deep steganography for hiding a full image in another. However, the reason for its success remains not fully clear. Under the existing cover (
C
C
) dependent deep hiding (DDH) pipeline, it is challenging to analyze how the secret (
S
S
) image is encoded since the encoded message cannot be analyzed independently. We propose a novel universal deep hiding (UDH) meta-architecture to disentangle the encoding of
S
S
from
C
C
. We perform extensive analysis and demonstrate that the success of deep steganography can be attributed to a frequency discrepancy between
C
C
and the encoded secret image. Despite
S
S
being hidden in a cover-agnostic manner, strikingly, UDH achieves a performance comparable to the existing DDH. Beyond hiding one image, we push the limits of deep steganography. Exploiting its property of being \emph{universal}, we propose universal watermarking as a timely solution to address the concern of the exponentially increasing amount of images/videos. UDH is robust to a pixel intensity shift on the container image, which makes it suitable for challenging application of light field messaging (LFM). This is the first work demonstrating the success of (DNN-based) hiding a full image for watermarking and LFM. Code: \url{https://github.com/ChaoningZhang/Universal-Deep-Hiding}"
neurips,https://proceedings.neurips.cc/paper/2020/file/73f95ee473881dea4afd89c06165fa66-Paper.pdf,Evidential Sparsification of Multimodal Latent Spaces in Conditional Variational Autoencoders,"Masha Itkina, Boris Ivanovic, Ransalu Senanayake, Mykel J. Kochenderfer, Marco Pavone",
neurips,https://proceedings.neurips.cc/paper/2020/file/747c1bcceb6109a4ef936bc70cfe67de-Paper.pdf,An Unbiased Risk Estimator for Learning with Augmented Classes,"Yu-Jie Zhang, Peng Zhao, Lanjihong Ma, Zhi-Hua Zhou",
neurips,https://proceedings.neurips.cc/paper/2020/file/747d3443e319a22747fbb873e8b2f9f2-Paper.pdf,AutoBSS: An Efficient Algorithm for Block Stacking Style Search,"Yikang Zhang, Jian Zhang, Zhao Zhong",
neurips,https://proceedings.neurips.cc/paper/2020/file/747e32ab0fea7fbd2ad9ec03daa3f840-Paper.pdf,Pushing the Limits of Narrow Precision Inferencing at Cloud Scale with Microsoft Floating Point,"Bita Darvish Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov , Anna Vinogradsky, Sarah Massengill , Lita Yang, Ray Bittner, Alessandro Forin, Haishan Zhu, Taesik Na, Prerak Patel, Shuai Che, Lok Chand Koppaka , XIA SONG, Subhojit Som, Kaustav Das, Saurabh T, Steve Reinhardt , Sitaram Lanka, Eric Chung, Doug Burger",
neurips,https://proceedings.neurips.cc/paper/2020/file/74dbd1111727a31a2b825d615d80b2e7-Paper.pdf,Stochastic Optimization with Laggard Data Pipelines,"Naman Agarwal, Rohan Anil, Tomer Koren, Kunal Talwar, Cyril Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/74de5f915765ea59816e770a8e686f38-Paper.pdf,Self-supervised Auxiliary Learning with Meta-paths for Heterogeneous Graphs,"Dasol Hwang, Jinyoung Park, Sunyoung Kwon, KyungMin Kim, Jung-Woo Ha, Hyunwoo J. Kim",
neurips,https://proceedings.neurips.cc/paper/2020/file/7503cfacd12053d309b6bed5c89de212-Paper.pdf,GPS-Net: Graph-based Photometric Stereo Network,"Zhuokun Yao, Kun Li, Ying Fu, Haofeng Hu, Boxin Shi",
neurips,https://proceedings.neurips.cc/paper/2020/file/7504adad8bb96320eb3afdd4df6e1f60-Paper.pdf,Consistent Structural Relation Learning for Zero-Shot Segmentation,"Peike Li, Yunchao Wei, Yi Yang",
neurips,https://proceedings.neurips.cc/paper/2020/file/751d51528afe5e6f7fe95dece4ed32ba-Paper.pdf,Model Selection in Contextual Stochastic Bandit Problems,"Aldo Pacchiano, My Phan, Yasin Abbasi Yadkori, Anup Rao, Julian Zimmert, Tor Lattimore, Csaba Szepesvari","We study bandit model selection in stochastic environments. Our approach relies on a master algorithm that selects between candidate base algorithms. We develop a master-base algorithm abstraction that can work with general classes of base algorithms and different type of adversarial master algorithms. Our methods rely on a novel and generic smoothing transformation for bandit algorithms that permits us to obtain optimal
O
(
√
T
)
O
model selection guarantees for stochastic contextual bandit problems as long as the optimal base algorithm satisfies a high probability regret guarantee. We show through a lower bound that even when one of the base algorithms has
O
(
log
T
)
O
regret, in general it is impossible to get better than
Ω
(
√
T
)
Ω
regret in model selection, even asymptotically. Using our techniques, we address model selection in a variety of problems such as misspecified linear contextual bandits \citep{lattimore2019learning}, linear bandit with unknown dimension \citep{Foster-Krishnamurthy-Luo-2019} and reinforcement learning with unknown feature maps. Our algorithm requires the knowledge of the optimal base regret to adjust the master learning rate. We show that without such prior knowledge any master can suffer a regret larger than the optimal base regret."
neurips,https://proceedings.neurips.cc/paper/2020/file/751f6b6b02bf39c41025f3bcfd9948ad-Paper.pdf,Truncated Linear Regression in High Dimensions,"Constantinos Daskalakis, Dhruv Rohatgi, Emmanouil Zampetakis",
neurips,https://proceedings.neurips.cc/paper/2020/file/7520fa31d14f45add6d61e52df5a03ff-Paper.pdf,Incorporating Pragmatic Reasoning Communication into Emergent Language,"Yipeng Kang, Tonghan Wang, Gerard de Melo",
neurips,https://proceedings.neurips.cc/paper/2020/file/753a043674f0193523abc1bbce678686-Paper.pdf,Deep Subspace Clustering with Data Augmentation,"Mahdi Abavisani, Alireza Naghizadeh, Dimitris Metaxas, Vishal Patel",
neurips,https://proceedings.neurips.cc/paper/2020/file/75800f73fa80f935216b8cfbedf77bfa-Paper.pdf,An Empirical Process Approach to the Union Bound: Practical Algorithms for Combinatorial and Linear Bandits,"Julian Katz-Samuels, Lalit Jain, zohar karnin, Kevin G. Jamieson",
neurips,https://proceedings.neurips.cc/paper/2020/file/75877cb75154206c4e65e76b88a12712-Paper.pdf,Can Graph Neural Networks Count Substructures?,"Zhengdao Chen, Lei Chen, Soledad Villar, Joan Bruna",
neurips,https://proceedings.neurips.cc/paper/2020/file/75a7c30fc0063c4952d7eb044a3c0897-Paper.pdf,A Bayesian Perspective on Training Speed and Model Selection,"Clare Lyle, Lisa Schut, Robin Ru, Yarin Gal, Mark van der Wilk",
neurips,https://proceedings.neurips.cc/paper/2020/file/75c58d36157505a600e0695ed0b3a22d-Paper.pdf,On the Modularity of Hypernetworks,"Tomer Galanti, Lior Wolf","In the context of learning to map an input
I
I
to a function
h
I
:
X
→
R
h
, two alternative methods are compared: (i) an embedding-based method, which learns a fixed function in which
I
I
is encoded as a conditioning signal
e
(
I
)
e
and the learned function takes the form
h
I
(
x
)
=
q
(
x
,
e
(
I
)
)
h
, and (ii) hypernetworks, in which the weights
θ
I
θ
of the function
h
I
(
x
)
=
g
(
x
;
θ
I
)
h
are given by a hypernetwork
f
f
as
θ
I
=
f
(
I
)
θ
. In this paper, we define the property of modularity as the ability to effectively learn a different function for each input instance
I
I
. For this purpose, we adopt an expressivity perspective of this property and extend the theory of~\cite{devore} and provide a lower bound on the complexity (number of trainable parameters) of neural networks as function approximators, by eliminating the requirements for the approximation method to be robust. Our results are then used to compare the complexities of
q
q
and
g
g
, showing that under certain conditions and when letting the functions
e
e
and
f
f
be as large as we wish,
g
g
can be smaller than
q
q
by orders of magnitude. This sheds light on the modularity of hypernetworks in comparison with the embedding-based method. Besides, we show that for a structured target function, the overall number of trainable parameters in a hypernetwork is smaller by orders of magnitude than the number of trainable parameters of a standard neural network and an embedding method."
neurips,https://proceedings.neurips.cc/paper/2020/file/75df63609809c7a2052fdffe5c00a84e-Paper.pdf,Doubly Robust Off-Policy Value and Gradient Estimation for Deterministic Policies,"Nathan Kallus, Masatoshi Uehara",
neurips,https://proceedings.neurips.cc/paper/2020/file/75ebb02f92fc30a8040bbd625af999f1-Paper.pdf,Provably Efficient Neural GTD for Off-Policy Learning,"Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, Mingyi Hong","This paper studies a gradient temporal difference (GTD) algorithm using neural network (NN) function approximators to minimize the mean squared Bellman error (MSBE). For off-policy learning, we show that the minimum MSBE problem can be recast into a min-max optimization involving a pair of over-parameterized primal-dual NNs. The resultant formulation can then be tackled using a neural GTD algorithm. We analyze the convergence of the proposed algorithm with a 2-layer ReLU NN architecture using
m
m
neurons and prove that it computes an approximate optimal solution to the minimum MSBE problem as
m
→
∞
m
."
neurips,https://proceedings.neurips.cc/paper/2020/file/7612936dcc85282c6fa4dd9d4ffe57f1-Paper.pdf,Learning Discrete Energy-based Models via Auxiliary-variable Local Exploration,"Hanjun Dai, Rishabh Singh, Bo Dai, Charles Sutton, Dale Schuurmans",
neurips,https://proceedings.neurips.cc/paper/2020/file/766d856ef1a6b02f93d894415e6bfa0e-Paper.pdf,Stable and expressive recurrent vision models,"Drew Linsley, Alekh Karkada Ashok, Lakshmi Narasimhan Govindarajan, Rex Liu, Thomas Serre",
neurips,https://proceedings.neurips.cc/paper/2020/file/766e428d1e232bbdd58664b41346196c-Paper.pdf,Entropic Optimal Transport between Unbalanced Gaussian Measures has a Closed Form,"Hicham Janati, Boris Muzellec, Gabriel Peyré, Marco Cuturi",
neurips,https://proceedings.neurips.cc/paper/2020/file/768e78024aa8fdb9b8fe87be86f64745-Paper.pdf,BRP-NAS: Prediction-based NAS using GCNs,"Lukasz Dudziak, Thomas Chau, Mohamed Abdelfattah, Royson Lee, Hyeji Kim, Nicholas Lane",
neurips,https://proceedings.neurips.cc/paper/2020/file/769c3bce651ce5feaa01ce3b75986420-Paper.pdf,Deep Shells: Unsupervised Shape Correspondence with Optimal Transport,"Marvin Eisenberger, Aysim Toker, Laura Leal-Taixé, Daniel Cremers",
neurips,https://proceedings.neurips.cc/paper/2020/file/76cf99d3614e23eabab16fb27e944bf9-Paper.pdf,ISTA-NAS: Efficient and Consistent Neural Architecture Search by Sparse Coding,"Yibo Yang, Hongyang Li, Shan You, Fei Wang, Chen Qian, Zhouchen Lin",
neurips,https://proceedings.neurips.cc/paper/2020/file/76dc611d6ebaafc66cc0879c71b5db5c-Paper.pdf,Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D,"Ankit Goyal, Kaiyu Yang, Dawei Yang, Jia Deng",
neurips,https://proceedings.neurips.cc/paper/2020/file/770f8e448d07586afbf77bb59f698587-Paper.pdf,Regularizing Black-box Models for Improved Interpretability,"Gregory Plumb, Maruan Al-Shedivat, Ángel Alexander Cabrera, Adam Perer, Eric Xing, Ameet Talwalkar",
neurips,https://proceedings.neurips.cc/paper/2020/file/77133be2e96a577bd4794928976d2ae2-Paper.pdf,Trust the Model When It Is Confident: Masked Model-based Actor-Critic,"Feiyang Pan, Jia He, Dandan Tu, Qing He","In this work, we find that better model usage can make a huge difference. We show theoretically that if the use of model-generated data is restricted to state-action pairs where the model error is small, the performance gap between model and real rollouts can be reduced. It motivates us to use model rollouts only when the model is confident about its predictions. We propose Masked Model-based Actor-Critic (M2AC), a novel policy optimization algorithm that maximizes a model-based lower-bound of the true value function. M2AC implements a masking mechanism based on the model's uncertainty estimation to decide whether the model should be used or not. Consequently, the new algorithm tends to give robust policy improvements. Experiments on continuous control benchmarks demonstrate that M2AC has strong performance even when using long model rollouts in very noisy environments, and significantly outperforms previous state-of-the-art methods."
neurips,https://proceedings.neurips.cc/paper/2020/file/77305c2f862ad1d353f55bf38e5a5183-Paper.pdf,Semi-Supervised Neural Architecture Search,"Renqian Luo, Xu Tan, Rui Wang, Tao Qin, Enhong Chen, Tie-Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/77330e1330ae2b086e5bfcae50d9ffae-Paper.pdf,Consistency Regularization for Certified Robustness of Smoothed Classifiers,"Jongheon Jeong, Jinwoo Shin",
neurips,https://proceedings.neurips.cc/paper/2020/file/774412967f19ea61d448977ad9749078-Paper.pdf,Robust Multi-Agent Reinforcement Learning with Model Uncertainty,"Kaiqing Zhang, TAO SUN, Yunzhe Tao, Sahika Genc, Sunil Mallya, Tamer Basar",
neurips,https://proceedings.neurips.cc/paper/2020/file/778609db5dc7e1a8315717a9cdd8fd6f-Paper.pdf,SIRI: Spatial Relation Induced Network For Spatial Description Resolution,"peiyao wang, Weixin Luo, Yanyu Xu, Haojie Li, Shugong Xu, Jianyu Yang, Shenghua Gao",
neurips,https://proceedings.neurips.cc/paper/2020/file/780261c4b9a55cd803080619d0cc3e11-Paper.pdf,Adaptive Shrinkage Estimation for Streaming Graphs,"Nesreen Ahmed, Nick Duffield",
neurips,https://proceedings.neurips.cc/paper/2020/file/781397bc0630d47ab531ea850bddcf63-Paper.pdf,Make One-Shot Video Object Segmentation Efficient Again,"Tim Meinhardt, Laura Leal-Taixé",
neurips,https://proceedings.neurips.cc/paper/2020/file/781877bda0783aac5f1cf765c128b437-Paper.pdf,Depth Uncertainty in Neural Networks,"Javier Antoran, James Allingham, José Miguel Hernández-Lobato",
neurips,https://proceedings.neurips.cc/paper/2020/file/786ab8c4d7ee758f80d57e65582e609d-Paper.pdf,Non-Euclidean Universal Approximation,"Anastasis Kratsios, Ievgen Bilokopytov",
neurips,https://proceedings.neurips.cc/paper/2020/file/78719f11fa2df9917de3110133506521-Paper.pdf,Constraining Variational Inference with Geometric Jensen-Shannon Divergence,"Jacob Deasy, Nikola Simidjievski, Pietro Lió","We examine the problem of controlling divergences for latent space regularisation in variational autoencoders. Specifically, when aiming to reconstruct example
x
∈
R
m
x
via latent space
z
∈
R
n
z
(
n
≤
m
n
), while balancing this against the need for generalisable latent representations. We present a regularisation mechanism based on the {\em skew-geometric Jensen-Shannon divergence}
(
JS
G
α
)
(
. We find a variation in
JS
G
α
JS
, motivated by limiting cases, which leads to an intuitive interpolation between forward and reverse KL in the space of both distributions and divergences. We motivate its potential benefits for VAEs through low-dimensional examples, before presenting quantitative and qualitative results. Our experiments demonstrate that skewing our variant of
JS
G
α
JS
, in the context of
JS
G
α
JS
-VAEs, leads to better reconstruction and generation when compared to several baseline VAEs. Our approach is entirely unsupervised and utilises only one hyperparameter which can be easily interpreted in latent space."
neurips,https://proceedings.neurips.cc/paper/2020/file/7880d7226e872b776d8b9f23975e2a3d-Paper.pdf,Gibbs Sampling with People,"Peter Harrison, Raja Marjieh, Federico Adolfi, Pol van Rijn, Manuel Anglada-Tort, Ofer Tchernichovski, Pauline Larrouy-Maestri, Nori Jacoby",
neurips,https://proceedings.neurips.cc/paper/2020/file/788d986905533aba051261497ecffcbb-Paper.pdf,HM-ANN: Efficient Billion-Point Nearest Neighbor Search on Heterogeneous Memory,"Jie Ren, Minjia Zhang, Dong Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/789ba2ae4d335e8a2ad283a3f7effced-Paper.pdf,FrugalML: How to use ML Prediction APIs more accurately and cheaply,"Lingjiao Chen, Matei Zaharia, James Y. Zou",
neurips,https://proceedings.neurips.cc/paper/2020/file/78f7d96ea21ccae89a7b581295f34135-Paper.pdf,Sharp Representation Theorems for ReLU Networks with Precise Dependence on Depth,"Guy Bresler, Dheeraj Nagaraj","This constitutes a fine-grained characterization of the representation power of feedforward networks of arbitrary depth D and number of neurons N, in contrast to existing representation results which either require D growing quickly with N or assume that the function being represented is highly smooth. In the latter case similar rates can be obtained with a single nonlinear layer. Our results confirm the prevailing hypothesis that deeper networks are better at representing less smooth functions, and indeed, the main technical novelty is to fully exploit the fact that deep networks can produce highly oscillatory functions with few activation functions."
neurips,https://proceedings.neurips.cc/paper/2020/file/7967cc8e3ab559e68cc944c44b1cf3e8-Paper.pdf,Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning,"Filippos Christianos, Lukas Schäfer, Stefano Albrecht",
neurips,https://proceedings.neurips.cc/paper/2020/file/798d1c2813cbdf8bcdb388db0e32d496-Paper.pdf,Monotone operator equilibrium networks,"Ezra Winston, J. Zico Kolter",
neurips,https://proceedings.neurips.cc/paper/2020/file/79a3308b13cd31f096d8a4a34f96b66b-Paper.pdf,When and How to Lift the Lockdown? Global COVID-19 Scenario Analysis and Policy Assessment using Compartmental Gaussian Processes,"Zhaozhi Qian, Ahmed M. Alaa, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2020/file/79f56e5e3e0e999b3c139f225838d41f-Paper.pdf,Unsupervised Learning of Lagrangian Dynamics from Images for Prediction and Control,"Yaofeng Desmond Zhong, Naomi Leonard",
neurips,https://proceedings.neurips.cc/paper/2020/file/7a006957be65e608e863301eb98e1808-Paper.pdf,High-Dimensional Sparse Linear Bandits,"Botao Hao, Tor Lattimore, Mengdi Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/7a1d9028a78f418cb8f01909a348d9b2-Paper.pdf,Non-Stochastic Control with Bandit Feedback,"Paula Gradu, John Hallman, Elad Hazan",
neurips,https://proceedings.neurips.cc/paper/2020/file/7a22c0c0a4515485e31f95fd372050c9-Paper.pdf,Generalized Leverage Score Sampling for Neural Networks,"Jason D. Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, zheng Yu",
neurips,https://proceedings.neurips.cc/paper/2020/file/7a43ed4e82d06a1e6b2e88518fb8c2b0-Paper.pdf,An Optimal Elimination Algorithm for Learning a Best Arm,"Avinatan Hassidim, Ron Kupfer, Yaron Singer","We consider the classic problem of
(
ϵ
,
δ
)
(
-\texttt{PAC} learning a best arm where the goal is to identify with confidence
1
−
δ
1
an arm whose mean is an
ϵ
ϵ
-approximation to that of the highest mean arm in a multi-armed bandit setting. This problem is one of the most fundamental problems in statistics and learning theory, yet somewhat surprisingly its worst case sample complexity is not well understood. In this paper we propose a new approach for
(
ϵ
,
δ
)
(
-\texttt{PAC} learning a best arm. This approach leads to an algorithm whose sample complexity converges to \emph{exactly} the optimal sample complexity of
(
ϵ
,
δ
)
(
-learning the mean of
n
n
arms separately and we complement this result with a conditional matching lower bound. More specifically:
\begin{itemize}
\item The algorithm's sample complexity converges to \emph{exactly} $\frac{n}{2\epsilon^2}\log \frac{1}{\delta}$ as $n$ grows and $\delta \geq \frac{1}{n}$;  
%
\item We prove that no elimination algorithm obtains sample complexity arbitrarily lower than $\frac{n}{2\epsilon^2}\log \frac{1}{\delta}$.  Elimination algorithms is a broad class of $(\epsilon,\delta)$-\texttt{PAC} best arm learning algorithms that includes many algorithms in the literature.   
\end{itemize}
\begin{itemize}\item The algorithm's sample complexity converges to \emph{exactly} $\frac{n}{2\epsilon^2}\log \frac{1}{\delta}$ as $n$ grows and $\delta \geq \frac{1}{n}$;  %\item We prove that no elimination algorithm obtains sample complexity arbitrarily lower than $\frac{n}{2\epsilon^2}\log \frac{1}{\delta}$.  Elimination algorithms is a broad class of $(\epsilon,\delta)$-\texttt{PAC} best arm learning algorithms that includes many algorithms in the literature.   \end{itemize}
When
n
n
is independent of
δ
δ
our approach yields an algorithm whose sample complexity converges to
2
n
ϵ
2
log
1
δ
2
as
n
n
grows. In comparison with the best known algorithm for this problem our approach improves the sample complexity by a factor of over 1500 and over 6000 when
δ
≥
1
n
δ
."
neurips,https://proceedings.neurips.cc/paper/2020/file/7a53928fa4dd31e82c6ef826f341daec-Paper.pdf,Efficient Projection-free Algorithms for Saddle Point Problems,"Cheng Chen, Luo Luo, Weinan Zhang, Yong Yu","The Frank-Wolfe algorithm is a classic method for constrained optimization problems. It has recently been popular in many machine learning applications because its projection-free property leads to more efficient iterations. In this paper, we study projection-free algorithms for convex-strongly-concave saddle point problems with complicated constraints. Our method combines Conditional Gradient Sliding with Mirror-Prox and show that it only requires
~
\cO
(
1
/
√
ϵ
)
\cO
gradient evaluations and
~
\cO
(
1
/
ϵ
2
)
\cO
linear optimizations in the batch setting. We also extend our method to the stochastic setting and propose first stochastic projection-free algorithms for saddle point problems. Experimental results demonstrate the effectiveness of our algorithms and verify our theoretical guarantees."
neurips,https://proceedings.neurips.cc/paper/2020/file/7a674153c63cff1ad7f0e261c369ab2c-Paper.pdf,A mathematical model for automatic differentiation in machine learning,"Jérôme Bolte, Edouard Pauwels",
neurips,https://proceedings.neurips.cc/paper/2020/file/7a677bb4477ae2dd371add568dd19e23-Paper.pdf,Unsupervised Text Generation by Learning from Search,"Jingjing Li, Zichao Li, Lili Mou, Xin Jiang, Michael Lyu, Irwin King",
neurips,https://proceedings.neurips.cc/paper/2020/file/7a685d9edd95508471a9d3d6fcace432-Paper.pdf,Learning Compositional Rules via Neural Program Synthesis,"Maxwell Nye, Armando Solar-Lezama, Josh Tenenbaum, Brenden M. Lake",
neurips,https://proceedings.neurips.cc/paper/2020/file/7a6a74cbe87bc60030a4bd041dd47b78-Paper.pdf,Incorporating BERT into Parallel Sequence Decoding with Adapters,"Junliang Guo, Zhirui Zhang, Linli Xu, Hao-Ran Wei, Boxing Chen, Enhong Chen","While large scale pre-trained language models such as BERT have achieved great success on various natural language understanding tasks, how to efficiently and effectively incorporate them into sequence-to-sequence models and the corresponding text generation tasks remains a non-trivial problem. In this paper, we propose to address this problem by taking two different BERT models as the encoder and decoder respectively, and fine-tuning them by introducing simple and lightweight adapter modules, which are inserted between BERT layers and tuned on the task-specific dataset. In this way, we obtain a flexible and efficient model which is able to jointly leverage the information contained in the source-side and target-side BERT models, while bypassing the catastrophic forgetting problem. Each component in the framework can be considered as a plug-in unit, making the framework flexible and task agnostic. Our framework is based on a parallel sequence decoding algorithm named Mask-Predict considering the bi-directional and conditional independent nature of BERT, and can be adapted to traditional autoregressive decoding easily. We conduct extensive experiments on neural machine translation tasks where the proposed method consistently outperforms autoregressive baselines while reducing the inference latency by half, and achieves
36.49
36.49
/
33.57
33.57
BLEU scores on IWSLT14 German-English/WMT14 German-English translation. When adapted to autoregressive decoding, the proposed method achieves
30.60
30.60
/
43.56
43.56
BLEU scores on WMT14 English-German/English-French translation, on par with the state-of-the-art baseline models."
neurips,https://proceedings.neurips.cc/paper/2020/file/7a8b8402b2f0fc78cf726ee484a0a2b7-Paper.pdf,Estimating Fluctuations in Neural Representations of Uncertain Environments,"Sahand Farhoodi, Mark Plitt, Lisa Giocomo, Uri Eden",
neurips,https://proceedings.neurips.cc/paper/2020/file/7a9a322cbe0d06a98667fdc5160dc6f8-Paper.pdf,"Discover, Hallucinate, and Adapt: Open Compound Domain Adaptation for Semantic Segmentation","KwanYong Park, Sanghyun Woo, Inkyu Shin, In So Kweon",
neurips,https://proceedings.neurips.cc/paper/2020/file/7ac52e3f2729d1b3f6d2b7e8f6467226-Paper.pdf,"SURF: A Simple, Universal, Robust, Fast Distribution Learning Algorithm","Yi Hao, Ayush Jain, Alon Orlitsky, Vaishakh Ravindrakumar","Sample- and computationally-efficient distribution estimation is a fundamental tenet in statistics and machine learning. We present
\SURF
\SURF
, an algorithm for approximating distributions by piecewise polynomials.
\SURF
\SURF
is: simple, replacing prior complex optimization techniques by straight-forward empirical probability approximation of each potential polynomial piece through simple empirical-probability interpolation, and using plain divide-and-conquer to merge the pieces; universal, as well-known polynomial-approximation results imply that it accurately approximates a large class of common distributions; robust to distribution mis-specification as for any degree
d
≤
8
d
, it estimates any distribution to an
ℓ
1
ℓ
distance
<
3
<
times that of the nearest degree-
d
d
piecewise polynomial, improving known factor upper bounds of 3 for single polynomials and 15 for polynomials with arbitrarily many pieces; fast, using optimal sample complexity, running in near sample-linear time, and if given sorted samples it may be parallelized to run in sub-linear time. In experiments,
\SURF
\SURF
outperforms state-of-the art algorithms."
neurips,https://proceedings.neurips.cc/paper/2020/file/7b41bfa5085806dfa24b8c9de0ce567f-Paper.pdf,Understanding Approximate Fisher Information for Fast Convergence of Natural Gradient Descent in Wide Neural Networks,"Ryo Karakida, Kazuki Osawa",
neurips,https://proceedings.neurips.cc/paper/2020/file/7b497aa1b2a83ec63d1777a88676b0c2-Paper.pdf,General Transportability of Soft Interventions: Completeness Results,"Juan Correa, Elias Bareinboim","The challenge of generalizing causal knowledge across different environments is pervasive in scientific explorations, including in AI, ML, and Data Science. Experiments are usually performed in one environment (e.g., in a lab, on Earth) with the intent, almost invariably, of being used elsewhere (e.g., outside the lab, on Mars), where the conditions are likely to be different. In the causal inference literature, this generalization task has been formalized under the rubric of transportability (Pearl and Bareinboim, 2011), where a number of criteria and algorithms have been developed for various settings. Despite the generality of such results, transportability theory has been confined to atomic, do()-interventions. In practice, many real-world applications require more complex, stochastic interventions; for instance, in reinforcement learning, agents need to continuously adapt to the changing conditions of an uncertain and unknown environment. In this paper, we extend transportability theory to encompass these more complex types of interventions, which are known as ""soft,"" both relative to the input as well as the target distribution of the analysis. Specifically, we develop a graphical condition that is both necessary and sufficient for deciding soft-transportability. Second, we develop an algorithm to determine whether a non-atomic intervention is computable from a combination of the distributions available across domains. As a corollary, we show that the
σ
σ
-calculus is complete for the task of soft-transportability."
neurips,https://proceedings.neurips.cc/paper/2020/file/7ba0691b7777b6581397456412a41390-Paper.pdf,GAIT-prop: A biologically plausible learning rule derived from backpropagation of error,"Nasir Ahmad, Marcel A. J. van Gerven, Luca Ambrogioni",
neurips,https://proceedings.neurips.cc/paper/2020/file/7bab7650be60b0738e22c3b8745f937d-Paper.pdf,Lipschitz Bounds and Provably Robust Training by Laplacian Smoothing,"Vishaal Krishnan, Abed AlRahman Al Makdah, Fabio Pasqualetti","In this work we propose a graph-based learning framework to train models with provable robustness to adversarial perturbations. In contrast to regularization-based approaches, we formulate the adversarially robust learning problem as one of loss minimization with a Lipschitz constraint, and show that the saddle point of the associated Lagrangian is characterized by a Poisson equation with weighted Laplace operator. Further, the weighting for the Laplace operator is given by the Lagrange multiplier for the Lipschitz constraint, which modulates the sensitivity of the minimizer to perturbations. We then design a provably robust training scheme using graph-based discretization of the input space and a primal-dual algorithm to converge to the Lagrangian's saddle point. Our analysis establishes a novel connection between elliptic operators with constraint-enforced weighting and adversarial learning. We also study the complementary problem of improving the robustness of minimizers with a margin on their loss, formulated as a loss-constrained minimization problem of the Lipschitz constant. We propose a technique to obtain robustified minimizers, and evaluate fundamental Lipschitz lower bounds by approaching Lipschitz constant minimization via a sequence of gradient
p
p
-norm minimization problems. Ultimately, our results show that, for a desired nominal performance, there exists a fundamental lower bound on the sensitivity to adversarial perturbations that depends only on the loss function and the data distribution, and that improvements in robustness beyond this bound can only be made at the expense of nominal performance. Our training schemes provably achieve these bounds both under constraints on performance and~robustness."
neurips,https://proceedings.neurips.cc/paper/2020/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf,SCOP: Scientific Control for Reliable Neural Network Pruning,"Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing XU, Chao Xu, Chang Xu",
neurips,https://proceedings.neurips.cc/paper/2020/file/7bd28f15a49d5e5848d6ec70e584e625-Paper.pdf,Provably Consistent Partial-Label Learning,"Lei Feng, Jiaqi Lv, Bo Han, Miao Xu, Gang Niu, Xin Geng, Bo An, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2020/file/7cac11e2f46ed46c339ec3d569853759-Paper.pdf,"Robust, Accurate Stochastic Optimization for Variational Inference","Akash Kumar Dhaka, Alejandro Catalina, Michael R. Andersen, Måns Magnusson, Jonathan Huggins, Aki Vehtari",
neurips,https://proceedings.neurips.cc/paper/2020/file/7cc538b1337957dae283c30ad46def38-Paper.pdf,Discovering conflicting groups in signed networks,"Ruo-Chun Tzeng, Bruno Ordozgoiti, Aristides Gionis","Signed networks are graphs where edges are annotated with a positive or negative sign, indicating whether an edge interaction is friendly or antagonistic. Signed networks can be used to study a variety of social phenomena, such as mining polarized discussions in social media, or modeling relations of trust and distrust in online review platforms. In this paper we study the problem of detecting
k
k
conflicting groups in a signed network. Our premise is that each group is positively connected internally and negatively connected with the other
k
−
1
k
groups. An important aspect of our formulation is that we are not searching for a complete partition of the signed network, instead, we allow other nodes to be neutral with respect to the conflict structure we are searching. As a result, the problem we tackle differs from previously studied problems, such as correlation clustering and
k
k
-way partitioning. To solve the conflicting-group discovery problem, we derive a novel formulation in which each conflicting group is naturally characterized by the solution to the maximum discrete Rayleigh's quotient (\maxdrq) problem. We present two spectral methods for finding approximate solutions to the \maxdrq problem, which we analyze theoretically. Our experimental evaluation shows that, compared to state-of-the-art baselines, our methods find solutions of higher quality, are faster, and recover ground truth conflicting groups with higher accuracy."
neurips,https://proceedings.neurips.cc/paper/2020/file/7cc980b0f894bd0cf05c37c246f215f3-Paper.pdf,Learning Some Popular Gaussian Graphical Models without Condition Number Bounds,"Jonathan Kelner, Frederic Koehler, Raghu Meka, Ankur Moitra","Here we give the first fixed polynomial-time algorithms for learning attractive GGMs and walk-summable GGMs with a logarithmic number of samples without any such assumptions. In particular, our algorithms can tolerate strong dependencies among the variables. Our result for structure recovery in walk-summable GGMs is derived from a more general result for efficient sparse linear regression in walk-summable models without any norm dependencies. We complement our results with experiments showing that many existing algorithms fail even in some simple settings where there are long dependency chains. Our algorithms do not."
neurips,https://proceedings.neurips.cc/paper/2020/file/7d265aa7147bd3913fb84c7963a209d1-Paper.pdf,Sense and Sensitivity Analysis: Simple Post-Hoc Analysis of Bias Due to Unobserved Confounding,"Victor Veitch, Anisha Zaveri",
neurips,https://proceedings.neurips.cc/paper/2020/file/7d3d5bcad324d3edc08e40738e663554-Paper.pdf,Mix and Match: An Optimistic Tree-Search Approach for Learning Models from Mixture Distributions,"Matthew Faw, Rajat Sen, Karthikeyan Shanmugam, Constantine Caramanis, Sanjay Shakkottai",
neurips,https://proceedings.neurips.cc/paper/2020/file/7d420e2b2939762031eed0447a9be19f-Paper.pdf,Understanding Double Descent Requires A Fine-Grained Bias-Variance Decomposition,"Ben Adlam, Jeffrey Pennington",
neurips,https://proceedings.neurips.cc/paper/2020/file/7d97667a3e056acab9aaf653807b4a03-Paper.pdf,VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain,"Jinsung Yoon, Yao Zhang, James Jordon, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2020/file/7e05d6f828574fbc975a896b25bb011e-Paper.pdf,The Smoothed Possibility of Social Choice,Lirong Xia,
neurips,https://proceedings.neurips.cc/paper/2020/file/7e0a0209b929d097bd3e8ef30567a5c1-Paper.pdf,A Decentralized Parallel Algorithm for Training Generative Adversarial Nets,"Mingrui Liu, Wei Zhang, Youssef Mroueh, Xiaodong Cui, Jarret Ross, Tianbao Yang, Payel Das",
neurips,https://proceedings.neurips.cc/paper/2020/file/7ec0dbeee45813422897e04ad8424a5e-Paper.pdf,Phase retrieval in high dimensions: Statistical and computational phase transitions,"Antoine Maillard, Bruno Loureiro, Florent Krzakala, Lenka Zdeborová","We consider the phase retrieval problem of reconstructing a
n
n
-dimensional real or complex signal
X
⋆
X
from
m
m
(possibly noisy) observations
Y
μ
=
|
∑
n
i
=
1
Φ
μ
i
X
⋆
i
/
√
n
|
Y
, for a large class of correlated real and complex random sensing matrices
Φ
Φ
, in a high-dimensional setting where
m
,
n
→
∞
m
while
α
=
m
/
n
=
Θ
(
1
)
α
. First, we derive sharp asymptotics for the lowest possible estimation error achievable statistically and we unveil the existence of sharp phase transitions for the weak- and full-recovery thresholds as a function of the singular values of the matrix
Φ
Φ
. This is achieved by providing a rigorous proof of a result first obtained by the replica method from statistical mechanics. In particular, the information-theoretic transition to perfect recovery for full-rank matrices appears at
α
=
1
α
(real case) and
α
=
2
α
(complex case). Secondly, we analyze the performance of the best-known polynomial time algorithm for this problem --- approximate message-passing--- establishing the existence of statistical-to-algorithmic gap depending, again, on the spectral properties of
Φ
Φ
. Our work provides an extensive classification of the statistical and algorithmic thresholds in high-dimensional phase retrieval for a broad class of random matrices."
neurips,https://proceedings.neurips.cc/paper/2020/file/7ec2442aa04c157590b2fa1a7d093a33-Paper.pdf,Fair Performance Metric Elicitation,"Gaurush Hiranandani, Harikrishna Narasimhan, Sanmi Koyejo",
neurips,https://proceedings.neurips.cc/paper/2020/file/7f141cf8e7136ce8701dc6636c2a6fe4-Paper.pdf,Hybrid Variance-Reduced SGD Algorithms For Minimax Problems with Nonconvex-Linear Function,"Quoc Tran Dinh, Deyi Liu, Lam Nguyen","We develop a novel and single-loop variance-reduced algorithm to solve a class of stochastic nonconvex-convex minimax problems involving a nonconvex-linear objective function, which has various applications in different fields such as ma- chine learning and robust optimization. This problem class has several compu- tational challenges due to its nonsmoothness, nonconvexity, nonlinearity, and non-separability of the objective functions. Our approach relies on a new combi- nation of recent ideas, including smoothing and hybrid biased variance-reduced techniques. Our algorithm and its variants can achieve
O
(
T
−
2
/
3
)
O
-convergence rate and the best-known oracle complexity under standard assumptions, where T is the iteration counter. They have several computational advantages compared to exist- ing methods such as simple to implement and less parameter tuning requirements. They can also work with both single sample or mini-batch on derivative estimators, and with constant or diminishing step-sizes. We demonstrate the benefits of our algorithms over existing methods through two numerical examples, including a nonsmooth and nonconvex-non-strongly concave minimax model."
neurips,https://proceedings.neurips.cc/paper/2020/file/7f2be1b45d278ac18804b79207a24c53-Paper.pdf,Belief-Dependent Macro-Action Discovery in POMDPs using the Value of Information,"Genevieve Flaspohler, Nicholas A. Roy, John W. Fisher III",
neurips,https://proceedings.neurips.cc/paper/2020/file/7f2cba89a7116c7c6b0a769572d5fad9-Paper.pdf,Soft Contrastive Learning for Visual Localization,"Janine Thoma, Danda Pani Paudel, Luc V. Gool",
neurips,https://proceedings.neurips.cc/paper/2020/file/7f6caf1f0ba788cd7953d817724c2b6e-Paper.pdf,Fine-Grained Dynamic Head for Object Detection,"Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Hongbin Sun, Jian Sun, Nanning Zheng",
neurips,https://proceedings.neurips.cc/paper/2020/file/7fa215c9efebb3811a7ef58409907899-Paper.pdf,LoCo: Local Contrastive Representation Learning,"Yuwen Xiong, Mengye Ren, Raquel Urtasun",
neurips,https://proceedings.neurips.cc/paper/2020/file/7fc63ff01769c4fa7d9279e97e307829-Paper.pdf,Modeling and Optimization Trade-off in Meta-learning,"Katelyn Gao, Ozan Sener",
neurips,https://proceedings.neurips.cc/paper/2020/file/7fd3b80fb1884e2927df46a7139bb8bf-Paper.pdf,SnapBoost: A Heterogeneous Boosting Machine,"Thomas Parnell, Andreea Anghel, Małgorzata Łazuka, Nikolas Ioannou, Sebastian Kurella, Peshal Agarwal, Nikolaos Papandreou, Haralampos Pozidis",
neurips,https://proceedings.neurips.cc/paper/2020/file/803ef56843860e4a48fc4cdb3065e8ce-Paper.pdf,On Adaptive Distance Estimation,"Yeshwanth Cherapanamjeri, Jelani Nelson","We provide a static data structure for distance estimation which supports {\it adaptive} queries. Concretely, given a dataset
X
=
{
x
i
}
n
i
=
1
X
of
n
n
points in
R
d
R
and
0
<
p
≤
2
0
, we construct a randomized data structure with low memory consumption and query time which, when later given any query point
q
∈
R
d
q
, outputs a
(
1
+
ε
)
(
-approximation of
∥
q
−
x
i
∥
p
‖
with high probability for all
i
∈
[
n
]
i
. The main novelty is our data structure's correctness guarantee holds even when the sequence of queries can be chosen adaptively: an adversary is allowed to choose the
j
j
th query point
q
j
q
in a way that depends on the answers reported by the data structure for
q
1
,
…
,
q
j
−
1
q
. Previous randomized Monte Carlo methods do not provide error guarantees in the setting of adaptively chosen queries. Our memory consumption is
~
O
(
n
d
/
ε
2
)
O
, slightly more than the
O
(
n
d
)
O
required to store
X
X
in memory explicitly, but with the benefit that our time to answer queries is only
~
O
(
ε
−
2
(
n
+
d
)
)
O
, much faster than the naive
Θ
(
n
d
)
Θ
time obtained from a linear scan in the case of
n
n
and
d
d
very large. Here
~
O
O
hides
log
(
n
d
/
ε
)
log
factors. We discuss applications to nearest neighbor search and nonparametric estimation. Our method is simple and likely to applicable to other domains: we describe a generic approach for transforming randomized Monte Carlo data structures which do not support adaptive queries to ones that do, and show that for the problem at hand it can be applied to standard nonadaptive solutions to
ℓ
p
ℓ
norm estimation with negligible overhead in query time and a factor
d
d
overhead in memory."
neurips,https://proceedings.neurips.cc/paper/2020/file/804741413d7fe0e515b19a7ffc7b3027-Paper.pdf,Stage-wise Conservative Linear Bandits,"Ahmadreza Moradipari, Christos Thrampoulidis, Mahnoosh Alizadeh","We study stage-wise conservative linear stochastic bandits: an instance of bandit optimization, which accounts for (unknown) safety constraints that appear in applications such as online advertising and medical trials. At each stage, the learner must choose actions that not only maximize cumulative reward across the entire time horizon, but further satisfy a linear baseline constraint that takes the form of a lower bound on the instantaneous reward. For this problem, we present two novel algorithms, stage-wise conservative linear Thompson Sampling (SCLTS) and stage-wise conservative linear UCB (SCLUCB), that respect the baseline constraints and enjoy probabilistic regret bounds of order
O
(
√
T
log
3
/
2
T
)
O
and
O
(
√
T
log
T
)
O
, respectively. Notably, the proposed algorithms can be adjusted with only minor modifications to tackle different problem variations, such as, constraints with bandit-feedback, or an unknown sequence of baseline rewards. We discuss these and other improvements over the state-of-the art. For instance, compared to existing solutions, we show that SCLTS plays the (non-optimal) baseline action at most
O
(
log
T
)
O
times (compared to
O
(
√
T
)
O
). Finally, we make connections to another studied form of safety-constraints that takes the form of an upper bound on the instantaneous reward. While this incurs additional complexity to the learning process as the optimal action is not guaranteed to belong to the safe-set at each round, we show that SCLUCB can properly adjust in this setting via a simple modification."
neurips,https://proceedings.neurips.cc/paper/2020/file/806beafe154032a5b818e97b4420ad98-Paper.pdf,RELATE: Physically Plausible Multi-Object Scene Synthesis Using Structured Latent Spaces,"Sebastien Ehrhardt, Oliver Groth, Aron Monszpart, Martin Engelcke, Ingmar Posner, Niloy Mitra, Andrea Vedaldi",
neurips,https://proceedings.neurips.cc/paper/2020/file/80b618ebcac7aa97a6dac2ba65cb7e36-Paper.pdf,Metric-Free Individual Fairness in Online Learning,"Yahav Bechavod, Christopher Jung, Steven Z. Wu",
neurips,https://proceedings.neurips.cc/paper/2020/file/8169e05e2a0debcb15458f2cc1eff0ea-Paper.pdf,GreedyFool: Distortion-Aware Sparse Adversarial Attack,"Xiaoyi Dong, Dongdong Chen, Jianmin Bao, Chuan Qin, Lu Yuan, Weiming Zhang, Nenghai Yu, Dong Chen",
neurips,https://proceedings.neurips.cc/paper/2020/file/8171ac2c5544a5cb54ac0f38bf477af4-Paper.pdf,VAEM: a Deep Generative Model for Heterogeneous Mixed Type Data,"Chao Ma, Sebastian Tschiatschek, Richard Turner, José Miguel Hernández-Lobato, Cheng Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/819f46e52c25763a55cc642422644317-Paper.pdf,RetroXpert: Decompose Retrosynthesis Prediction Like A Chemist,"Chaochao Yan, Qianggang Ding, Peilin Zhao, Shuangjia Zheng, JINYU YANG, Yang Yu, Junzhou Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/81e3225c6ad49623167a4309eb4b2e75-Paper.pdf,Sample-Efficient Optimization in the Latent Space of Deep Generative Models via Weighted Retraining,"Austin Tripp, Erik Daxberger, José Miguel Hernández-Lobato",
neurips,https://proceedings.neurips.cc/paper/2020/file/81e793dc8317a3dbc3534ed3f242c418-Paper.pdf,Improved Sample Complexity for Incremental Autonomous Exploration in MDPs,"Jean Tarbouriech, Matteo Pirotta, Michal Valko, Alessandro Lazaric","We study the problem of exploring an unknown environment when no reward function is provided to the agent. Building on the incremental exploration setting introduced by Lim and Auer (2012), we define the objective of learning the set of
ϵ
ϵ
-optimal goal-conditioned policies attaining all states that are incrementally reachable within
L
L
steps (in expectation) from a reference state
s
0
s
. In this paper, we introduce a novel model-based approach that interleaves discovering new states from
s
0
s
and improving the accuracy of a model estimate that is used to compute goal-conditioned policies. The resulting algorithm, DisCo, achieves a sample complexity scaling as
˜
O
ϵ
(
L
5
S
L
+
ϵ
Γ
L
+
ϵ
A
ϵ
−
2
)
O
, where
A
A
is the number of actions,
S
L
+
ϵ
S
is the number of states that are incrementally reachable from
s
0
s
in
L
+
ϵ
L
steps, and
Γ
L
+
ϵ
Γ
is the branching factor of the dynamics over such states. This improves over the algorithm proposed in (Lim and Auer, 2012) in both
ϵ
ϵ
and
L
L
at the cost of an extra
Γ
L
+
ϵ
Γ
factor, which is small in most environments of interest. Furthermore, DisCo is the first algorithm that can return an
ϵ
/
c
min
ϵ
-optimal policy for any cost-sensitive shortest-path problem defined on the
L
L
-reachable states with minimum cost
c
min
c
. Finally, we report preliminary empirical results confirming our theoretical findings."
neurips,https://proceedings.neurips.cc/paper/2020/file/81f7acabd411274fcf65ce2070ed568a-Paper.pdf,"TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning","Han Cai, Chuang Gan, Ligeng Zhu, Song Han",
neurips,https://proceedings.neurips.cc/paper/2020/file/82039d16dce0aab3913b6a7ac73deff7-Paper.pdf,"RD
2
2
: Reward Decomposition with Representation Decomposition","Zichuan Lin, Derek Yang, Li Zhao, Tao Qin, Guangwen Yang, Tie-Yan Liu","Reward decomposition, which aims to decompose the full reward into multiple sub-rewards, has been proven beneficial for improving sample efficiency in reinforcement learning. Existing works on discovering reward decomposition are mostly policy dependent, which constrains diverse or disentangled behavior between different policies induced by different sub-rewards. In this work, we propose a set of novel reward decomposition principles by constraining uniqueness and compactness of different state features/representations relevant to different sub-rewards. Our principles encourage sub-rewards with minimal relevant features, while maintaining the uniqueness of each sub-reward. We derive a deep learning algorithm based on our principle, and term our method as RD
2
2
, since we learn reward decomposition and representation decomposition jointly. RD
2
2
is evaluated on a toy case, where we have the true reward structure, and some Atari environments where reward structure exists but is unknown to the agent to demonstrate the effectiveness of RD
2
2
against existing reward decomposition methods."
neurips,https://proceedings.neurips.cc/paper/2020/file/821fa74b50ba3f7cba1e6c53e8fa6845-Paper.pdf,Self-paced Contrastive Learning with Hybrid Memory for Domain Adaptive Object Re-ID,"Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, hongsheng Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/8248a99e81e752cb9b41da3fc43fbe7f-Paper.pdf,Fairness constraints can help exact inference in structured prediction,"Kevin Bello, Jean Honorio","Many inference problems in structured prediction can be modeled as maximizing a score function on a space of labels, where graphs are a natural representation to decompose the total score into a sum of unary (nodes) and pairwise (edges) scores. Given a generative model with an undirected connected graph G and true vector of binary labels
¯
y
y
, it has been previously shown that when G has good expansion properties, such as complete graphs or d-regular expanders, one can exactly recover
¯
y
y
(with high probability and in polynomial time) from a single noisy observation of each edge and node. We analyze the previously studied generative model by Globerson et al. (2015) under a notion of statistical parity. That is, given a fair binary node labeling, we ask the question whether it is possible to recover the fair assignment, with high probability and in polynomial time, from single edge and node observations. We find that, in contrast to the known trade-offs between fairness and model performance, the addition of the fairness constraint improves the probability of exact recovery. We effectively explain this phenomenon and empirically show how graphs with poor expansion properties, such as grids, are now capable of achieving exact recovery. Finally, as a byproduct of our analysis, we provide a tighter minimum-eigenvalue bound than that which can be derived from Weyl's inequality."
neurips,https://proceedings.neurips.cc/paper/2020/file/82674fc29bc0d9895cee346548c2cb5c-Paper.pdf,Instance-based Generalization in Reinforcement Learning,"Martin Bertran, Natalia Martinez, Mariano Phielipp, Guillermo Sapiro",
neurips,https://proceedings.neurips.cc/paper/2020/file/8289889263db4a40463e3f358bb7c7a1-Paper.pdf,Smooth And Consistent Probabilistic Regression Trees,"Sami Alkhoury, Emilie Devijver, Marianne Clausel, Myriam Tami, Eric Gaussier, georges Oppenheim",
neurips,https://proceedings.neurips.cc/paper/2020/file/82b04cd5aa016d979fe048f3ddf0e8d3-Paper.pdf,Computing Valid p-value for Optimal Changepoint by Selective Inference using Dynamic Programming,"Vo Nguyen Le Duy, Hiroki Toda, Ryota Sugiyama, Ichiro Takeuchi",
neurips,https://proceedings.neurips.cc/paper/2020/file/82e9e7a12665240d13d0b928be28f230-Paper.pdf,Factorized Neural Processes for Neural Processes: K-Shot Prediction of Neural Responses,"Ronald (James) Cotton, Fabian Sinz, Andreas Tolias","In recent years, artificial neural networks have achieved state-of-the-art performance for predicting the responses of neurons in the visual cortex to natural stimuli. However, they require a time consuming parameter optimization process for accurately modeling the tuning function of newly observed neurons, which prohibits many applications including real-time, closed-loop experiments. We overcome this limitation by formulating the problem as
K
K
-shot prediction to directly infer a neuron's tuning function from a small set of stimulus-response pairs using a Neural Process. This required us to developed a Factorized Neural Process, which embeds the observed set into a latent space partitioned into the receptive field location and the tuning function properties. We show on simulated responses that the predictions and reconstructed receptive fields from the Factorized Neural Process approach ground truth with increasing number of trials. Critically, the latent representation that summarizes the tuning function of a neuron is inferred in a quick, single forward pass through the network. Finally, we validate this approach on real neural data from visual cortex and find that the predictive accuracy is comparable to --- and for small
K
K
even greater than --- optimization based approaches, while being substantially faster. We believe this novel deep learning systems identification framework will facilitate better real-time integration of artificial neural network modeling into neuroscience experiments."
neurips,https://proceedings.neurips.cc/paper/2020/file/83004190b1793d7aa15f8d0d49a13eba-Paper.pdf,Winning the Lottery with Continuous Sparsification,"Pedro Savarese, Hugo Silva, Michael Maire","The search for efficient, sparse deep neural network models is most prominently performed by pruning: training a dense, overparameterized network and removing parameters, usually via following a manually-crafted heuristic. Additionally, the recent Lottery Ticket Hypothesis conjectures that, for a typically-sized neural network, it is possible to find small sub-networks which, when trained from scratch on a comparable budget, match the performance of the original dense counterpart. We revisit fundamental aspects of pruning algorithms, pointing out missing ingredients in previous approaches, and develop a method, Continuous Sparsification, which searches for sparse networks based on a novel approximation of an intractable
ℓ
0
ℓ
regularization. We compare against dominant heuristic-based methods on pruning as well as ticket search -- finding sparse subnetworks that can be successfully re-trained from an early iterate. Empirical results show that we surpass the state-of-the-art for both objectives, across models and datasets, including VGG trained on CIFAR-10 and ResNet-50 trained on ImageNet. In addition to setting a new standard for pruning, Continuous Sparsification also offers fast parallel ticket search, opening doors to new applications of the Lottery Ticket Hypothesis."
neurips,https://proceedings.neurips.cc/paper/2020/file/837a7924b8c0aa866e41b2721f66135c-Paper.pdf,Adversarial robustness via robust low rank representations,"Pranjal Awasthi, Himanshu Jain, Ankit Singh Rawat, Aravindan Vijayaraghavan",Our first contribution is for certified robustness to perturbations measured in L_2 norm. We exploit low rank data representations to provide improved guarantees over state-of-the-art randomized smoothing-based approaches on standard benchmark datasets such as CIFAR-10 and CIFAR-100.
neurips,https://proceedings.neurips.cc/paper/2020/file/8396b14c5dff55d13eea57487bf8ed26-Paper.pdf,Joints in Random Forests,"Alvaro Correia, Robert Peharz, Cassio P. de Campos",
neurips,https://proceedings.neurips.cc/paper/2020/file/83adc9225e4deb67d7ce42d58fe5157c-Paper.pdf,Compositional Generalization by Learning Analytical Expressions,"Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, Dongmei Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/83d3d4b6c9579515e1679aca8cbc8033-Paper.pdf,JAX MD: A Framework for Differentiable Physics,"Samuel Schoenholz, Ekin Dogus Cubuk",
neurips,https://proceedings.neurips.cc/paper/2020/file/83eaa6722798a773dd55e8fc7443aa09-Paper.pdf,An implicit function learning approach for parametric modal regression,"Yangchen Pan, Ehsan Imani, Amir-massoud Farahmand, Martha White",
neurips,https://proceedings.neurips.cc/paper/2020/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf,SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static Images,"Chen-Hsuan Lin, Chaoyang Wang, Simon Lucey",
neurips,https://proceedings.neurips.cc/paper/2020/file/8493eeaccb772c0878f99d60a0bd2bb3-Paper.pdf,Coresets for Robust Training of Deep Neural Networks against Noisy Labels,"Baharan Mirzasoleiman, Kaidi Cao, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2020/file/84c230a5b1bc3495046ef916957c7238-Paper.pdf,Adapting to Misspecification in Contextual Bandits,"Dylan J. Foster, Claudio Gentile, Mehryar Mohri, Julian Zimmert","A major research direction in contextual bandits is to develop algorithms that are computationally efficient, yet support flexible, general-purpose function approximation. Algorithms based on modeling rewards have shown strong empirical performance, yet typically require a well-specified model, and can fail when this assumption does not hold. Can we design algorithms that are efficient and flexible, yet degrade gracefully in the face of model misspecification? We introduce a new family of oracle-efficient algorithms for
ε
ε
-misspecified contextual bandits that adapt to unknown model misspecification---both for finite and infinite action settings. Given access to an \emph{online oracle} for square loss regression, our algorithm attains optimal regret and---in particular---optimal dependence on the misspecification level, with \emph{no prior knowledge}. Specializing to linear contextual bandits with infinite actions in
d
d
dimensions, we obtain the first algorithm that achieves the optimal
\bigoht
(
d
√
T
+
ε
√
d
T
)
\bigoht
regret bound for unknown
ε
ε
. On a conceptual level, our results are enabled by a new optimization-based perspective on the regression oracle reduction framework of Foster and Rakhlin (2020), which we believe will be useful more broadly."
neurips,https://proceedings.neurips.cc/paper/2020/file/84c578f202616448a2f80e6f56d5f16d-Paper.pdf,Convergence of Meta-Learning with Task-Specific Adaptation over Partial Parameters,"Kaiyi Ji, Jason D. Lee, Yingbin Liang, H. Vincent Poor","Although model-agnostic meta-learning (MAML) is a very successful algorithm in meta-learning practice, it can have high computational cost because it updates all model parameters over both the inner loop of task-specific adaptation and the outer-loop of meta initialization training. A more efficient algorithm ANIL (which refers to almost no inner loop) was proposed recently by Raghu et al. 2019, which adapts only a small subset of parameters in the inner loop and thus has substantially less computational cost than MAML as demonstrated by extensive experiments. However, the theoretical convergence of ANIL has not been studied yet. In this paper, we characterize the convergence rate and the computational complexity for ANIL under two representative inner-loop loss geometries, i.e., strongly-convexity and nonconvexity. Our results show that such a geometric property can significantly affect the overall convergence performance of ANIL. For example, ANIL achieves a faster convergence rate for a strongly-convex inner-loop loss as the number
N
N
of inner-loop gradient descent steps increases, but a slower convergence rate for a nonconvex inner-loop loss as
N
N
increases. Moreover, our complexity analysis provides a theoretical quantification on the improved efficiency of ANIL over MAML. The experiments on standard few-shot meta-learning benchmarks validate our theoretical findings."
neurips,https://proceedings.neurips.cc/paper/2020/file/84ddfb34126fc3a48ee38d7044e87276-Paper.pdf,MetaPerturb: Transferable Regularizer for Heterogeneous Tasks and Architectures,"Jeong Un Ryu, JaeWoong Shin, Hae Beom Lee, Sung Ju Hwang",
neurips,https://proceedings.neurips.cc/paper/2020/file/84fec9a8e45846340fdf5c7c9f7ed66c-Paper.pdf,Learning to solve TV regularised problems with unrolled algorithms,"Hamza Cherkaoui, Jeremias Sulam, Thomas Moreau",
neurips,https://proceedings.neurips.cc/paper/2020/file/8511df98c02ab60aea1b2356c013bc0f-Paper.pdf,Object-Centric Learning with Slot Attention,"Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, Thomas Kipf",
neurips,https://proceedings.neurips.cc/paper/2020/file/85690f81aadc1749175c187784afc9ee-Paper.pdf,Improving robustness against common corruptions by covariate shift adaptation,"Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, Matthias Bethge",
neurips,https://proceedings.neurips.cc/paper/2020/file/858e47701162578e5e627cd93ab0938a-Paper.pdf,Deep Smoothing of the Implied Volatility Surface,"Damien Ackerer, Natasa Tagasovska, Thibault Vatter",
neurips,https://proceedings.neurips.cc/paper/2020/file/85934679f30131d812a8c7475a7d0f74-Paper.pdf,Probabilistic Inference with Algebraic Constraints: Theoretical Limits and Practical Approximations,"Zhe Zeng, Paolo Morettin, Fanqi Yan, Antonio Vergari, Guy Van den Broeck",
neurips,https://proceedings.neurips.cc/paper/2020/file/85b42dd8aae56e01379be5736db5b496-Paper.pdf,Provable Online CP/PARAFAC Decomposition of a Structured Tensor via Dictionary Learning,"Sirisha Rambhatla, Xingguo Li, Jarvis Haupt",
neurips,https://proceedings.neurips.cc/paper/2020/file/85b9a5ac91cd629bd3afe396ec07270a-Paper.pdf,Look-ahead Meta Learning for Continual Learning,"Gunshi Gupta, Karmesh Yadav, Liam Paull",
neurips,https://proceedings.neurips.cc/paper/2020/file/85c9f9efab89cee90a95cb98f15feacd-Paper.pdf,A polynomial-time algorithm for learning nonparametric causal graphs,"Ming Gao, Yi Ding, Bryon Aragam","We establish finite-sample guarantees for a polynomial-time algorithm for learning a nonlinear, nonparametric directed acyclic graphical (DAG) model from data. The analysis is model-free and does not assume linearity, additivity, independent noise, or faithfulness. Instead, we impose a condition on the residual variances that is closely related to previous work on linear models with equal variances. Compared to an optimal algorithm with oracle knowledge of the variable ordering, the additional cost of the algorithm is linear in the dimension
d
d
and the number of samples
n
n
. Finally, we compare the proposed algorithm to existing approaches in a simulation study."
neurips,https://proceedings.neurips.cc/paper/2020/file/85fc37b18c57097425b52fc7afbb6969-Paper.pdf,Sparse Learning with CART,Jason Klusowski,
neurips,https://proceedings.neurips.cc/paper/2020/file/8606bdb6f1fa707fc6ca309943eea443-Paper.pdf,Proximal Mapping for Deep Regularization,"Mao Li, Yingyi Ma, Xinhua Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/860b37e28ec7ba614f00f9246949561d-Paper.pdf,Identifying Causal-Effect Inference Failure with Uncertainty-Aware Models,"Andrew Jesson, Sören Mindermann, Uri Shalit, Yarin Gal",
neurips,https://proceedings.neurips.cc/paper/2020/file/861637a425ef06e6d539aaaff113d1d5-Paper.pdf,Hierarchical Granularity Transfer Learning,"Shaobo Min, Hongtao Xie, Hantao Yao, Xuran Deng, Zheng-Jun Zha, Yongdong Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/865dfbde8a344b44095495f3591f7407-Paper.pdf,Deep active inference agents using Monte-Carlo methods,"Zafeirios Fountas, Noor Sajid, Pedro Mediano, Karl Friston",
neurips,https://proceedings.neurips.cc/paper/2020/file/866d90e0921ac7b024b47d672445a086-Paper.pdf,Consistent Estimation of Identifiable Nonparametric Mixture Models from Grouped Observations,"Alexander Ritchie, Robert A. Vandermeulen, Clayton Scott",
neurips,https://proceedings.neurips.cc/paper/2020/file/8682cc30db9c025ecd3fee433f8ab54c-Paper.pdf,Manifold structure in graph embeddings,Patrick Rubin-Delanchy,
neurips,https://proceedings.neurips.cc/paper/2020/file/86b94dae7c6517ec1ac767fd2c136580-Paper.pdf,Adaptive Learned Bloom Filter (Ada-BF): Efficient Utilization of the Classifier with Application to Real-Time Information Filtering on the Web,"Zhenwei Dai, Anshumali Shrivastava",
neurips,https://proceedings.neurips.cc/paper/2020/file/86c51678350f656dcc7f490a43946ee5-Paper.pdf,MCUNet: Tiny Deep Learning on IoT Devices,"Ji Lin, Wei-Ming Chen, Yujun Lin, john cohn, Chuang Gan, Song Han",
neurips,https://proceedings.neurips.cc/paper/2020/file/86d7c8a08b4aaa1bc7c599473f5dddda-Paper.pdf,In search of robust measures of generalization,"Gintare Karolina Dziugaite, Alexandre Drouin, Brady Neal, Nitarshan Rajkumar, Ethan Caballero, Linbo Wang, Ioannis Mitliagkas, Daniel M. Roy",
neurips,https://proceedings.neurips.cc/paper/2020/file/8763d72bba4a7ade23f9ae1f09f4efc7-Paper.pdf,Task-agnostic Exploration in Reinforcement Learning,"Xuezhou Zhang, Yuzhe Ma, Adish Singla","Efficient exploration is one of the main challenges in reinforcement learning (RL). Most existing sample-efficient algorithms assume the existence of a single reward function during exploration. In many practical scenarios, however, there is not a single underlying reward function to guide the exploration, for instance, when an agent needs to learn many skills simultaneously, or multiple conflicting objectives need to be balanced. To address these challenges, we propose the \textit{task-agnostic RL} framework: In the exploration phase, the agent first collects trajectories by exploring the MDP without the guidance of a reward function. After exploration, it aims at finding near-optimal policies for
N
N
tasks, given the collected trajectories augmented with \textit{sampled rewards} for each task. We present an efficient task-agnostic RL algorithm, \textsc{UCBZero}, that finds
ϵ
ϵ
-optimal policies for
N
N
arbitrary tasks after at most
~
O
(
log
(
N
)
H
5
S
A
/
ϵ
2
)
O
exploration episodes. We also provide an
Ω
(
log
(
N
)
H
2
S
A
/
ϵ
2
)
Ω
lower bound, showing that the
log
log
dependency on
N
N
is unavoidable. Furthermore, we provide an
N
N
-independent sample complexity bound of \textsc{UCBZero} in the statistically easier setting when the ground truth reward functions are known."
neurips,https://proceedings.neurips.cc/paper/2020/file/8767bccb1ff4231a9962e3914f4f1f8f-Paper.pdf,Multi-task Additive Models for Robust Estimation and Automatic Structure Discovery,"Yingjie Wang, Hong Chen, Feng Zheng, Chen Xu, Tieliang Gong, Yanhong Chen",
neurips,https://proceedings.neurips.cc/paper/2020/file/87736972ed2fb48230f1052699dedbe7-Paper.pdf,Provably Efficient Reward-Agnostic Navigation with Linear Value Iteration,"Andrea Zanette, Alessandro Lazaric, Mykel J. Kochenderfer, Emma Brunskill",
neurips,https://proceedings.neurips.cc/paper/2020/file/884d247c6f65a96a7da4d1105d584ddd-Paper.pdf,Softmax Deep Double Deterministic Policy Gradients,"Ling Pan, Qingpeng Cai, Longbo Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/885b2c7a6deb4fea10f319c4ce993e02-Paper.pdf,Online Decision Based Visual Tracking via Reinforcement Learning,"ke Song, Wei Zhang, Ran Song, Yibin Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/887caadc3642e304ede659b734f79b00-Paper.pdf,Efficient Marginalization of Discrete and Structured Latent Variables via Sparsity,"Gonçalo Correia, Vlad Niculae, Wilker Aziz, André Martins",
neurips,https://proceedings.neurips.cc/paper/2020/file/88855547570f7ff053fff7c54e5148cc-Paper.pdf,DeepI2I: Enabling Deep Hierarchical Image-to-Image Translation by Transferring from GANs,"yaxing wang, Lu Yu, Joost van de Weijer",
neurips,https://proceedings.neurips.cc/paper/2020/file/8929c70f8d710e412d38da624b21c3c8-Paper.pdf,Distributional Robustness with IPMs and links to Regularization and GANs,Hisham Husain,"Robustness to adversarial attacks is an important concern due to the fragility of deep neural networks to small perturbations, and has received an abundance of attention in recent years. Distributional Robust Optimization (DRO), a particularly promising way of addressing this challenge, studies robustness via divergence-based uncertainty sets and has provided valuable insights into robustification strategies such as regularisation. In the context of machine learning, majority of existing results have chosen
f
f
-divergences, Wasserstein distances and more recently, the Maximum Mean Discrepancy (MMD) to construct uncertainty sets. We extend this line of work for the purposes of understanding robustness via regularization by studying uncertainty sets constructed with Integral Probability Metrics (IPMs) - a large family of divergences including the MMD, Total Variation and Wasserstein distances. Our main result shows that DRO under \textit{any} choice of IPM corresponds to a family of regularization penalties, which recover and improve upon existing results in the setting of MMD and Wasserstein distances. Due to the generality of our result, we show that other choices of IPMs correspond to other commonly used penalties in machine learning. Furthermore, we extend our results to shed light on adversarial generative modelling via
f
f
-GANs, constituting the first study of distributional robustness for the
f
f
-GAN objective. Our results unveil the inductive properties of the discriminator set with regards to robustness, allowing us to give positive comments for a number of existing penalty-based GAN methods such as Wasserstein-, MMD- and Sobolev-GANs. In summary, our results intimately link GANs to distributional robustness, extend previous results on DRO and contribute to our understanding of the link between regularization and robustness at large."
neurips,https://proceedings.neurips.cc/paper/2020/file/89562dccfeb1d0394b9ae7e09544dc70-Paper.pdf,A shooting formulation of deep learning,"François-Xavier Vialard, Roland Kwitt, Susan Wei, Marc Niethammer",
neurips,https://proceedings.neurips.cc/paper/2020/file/8965f76632d7672e7d3cf29c87ecaa0c-Paper.pdf,CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances,"Jihoon Tack, Sangwoo Mo, Jongheon Jeong, Jinwoo Shin",
neurips,https://proceedings.neurips.cc/paper/2020/file/8977ecbb8cb82d77fb091c7a7f186163-Paper.pdf,Learning Implicit Credit Assignment for Cooperative Multi-Agent Reinforcement Learning,"Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, Yuk Ying Chung",
neurips,https://proceedings.neurips.cc/paper/2020/file/8989e07fc124e7a9bcbdebcc8ace2bc0-Paper.pdf,MATE: Plugging in Model Awareness to Task Embedding for Meta Learning,"Xiaohan Chen, Zhangyang Wang, Siyu Tang, Krikamol Muandet",
neurips,https://proceedings.neurips.cc/paper/2020/file/89ae0fe22c47d374bc9350ef99e01685-Paper.pdf,"Restless-UCB, an Efficient and Low-complexity Algorithm for Online Restless Bandits","Siwei Wang, Longbo Huang, John C. S. Lui","We study the online restless bandit problem, where the state of each arm evolves according to a Markov chain, and the reward of pulling an arm depends on both the pulled arm and the current state of the corresponding Markov chain. In this paper, we propose Restless-UCB, a learning policy that follows the explore-then-commit framework. In Restless-UCB, we present a novel method to construct offline instances, which only requires
O
(
N
)
O
time-complexity (
N
N
is the number of arms) and is exponentially better than the complexity of existing learning policy. We also prove that Restless-UCB achieves a regret upper bound of
~
O
(
(
N
+
M
3
)
T
2
3
)
O
, where
M
M
is the Markov chain state space size and
T
T
is the time horizon. Compared to existing algorithms, our result eliminates the exponential factor (in
M
,
N
M
) in the regret upper bound, due to a novel exploitation of the sparsity in transitions in general restless bandit problems. As a result, our analysis technique can also be adopted to tighten the regret bounds of existing algorithms. Finally, we conduct experiments based on real-world dataset, to compare the Restless-UCB policy with state-of-the-art benchmarks. Our results show that Restless-UCB outperforms existing algorithms in regret, and significantly reduces the running time."
neurips,https://proceedings.neurips.cc/paper/2020/file/89b9e0a6f6d1505fe13dea0f18a2dcfa-Paper.pdf,Predictive Information Accelerates Learning in RL,"Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee, John Canny, Sergio Guadarrama",
neurips,https://proceedings.neurips.cc/paper/2020/file/8a1276c25f5efe85f0fc4020fbf5b4f8-Paper.pdf,"Robust and Heavy-Tailed Mean Estimation Made Simple, via Regret Minimization","Sam Hopkins, Jerry Li, Fred Zhang","In this paper, we provide a meta-problem and a duality theorem that lead to a new unified view on robust and heavy-tailed mean estimation in high dimensions. We show that the meta-problem can be solved either by a variant of the Filter algorithm from the recent literature on robust estimation or by the quantum entropy scoring scheme (QUE), due to Dong, Hopkins and Li (NeurIPS '19). By leveraging our duality theorem, these results translate into simple and efficient algorithms for both robust and heavy-tailed settings. Furthermore, the QUE-based procedure has run-time that matches the fastest known algorithms on both fronts."
neurips,https://proceedings.neurips.cc/paper/2020/file/8a50bae297807da9e97722a0b3fd8f27-Paper.pdf,High-Fidelity Generative Image Compression,"Fabian Mentzer, George D. Toderici, Michael Tschannen, Eirikur Agustsson",
neurips,https://proceedings.neurips.cc/paper/2020/file/8a7129b8f3edd95b7d969dfc2c8e9d9d-Paper.pdf,A Statistical Mechanics Framework for Task-Agnostic Sample Design in Machine Learning,"Bhavya Kailkhura, Jayaraman Thiagarajan, Qunwei Li, Jize Zhang, Yi Zhou, Timo Bremer",
neurips,https://proceedings.neurips.cc/paper/2020/file/8ab70731b1553f17c11a3bbc87e0b605-Paper.pdf,Counterexample-Guided Learning of Monotonic Neural Networks,"Aishwarya Sivaraman, Golnoosh Farnadi, Todd Millstein, Guy Van den Broeck",
neurips,https://proceedings.neurips.cc/paper/2020/file/8ab9bb97ce35080338be74dc6375e0ed-Paper.pdf,A Novel Approach for Constrained Optimization in Graphical Models,"Sara Rouhani, Tahrima Rahman, Vibhav Gogate","We consider the following constrained maximization problem in discrete probabilistic graphical models (PGMs). Given two (possibly identical) PGMs
M
1
M
and
M
2
M
defined over the same set of variables and a real number
q
q
, find an assignment of values to all variables such that the probability of the assignment is maximized w.r.t.
M
1
M
and is smaller than
q
q
w.r.t.
M
2
M
. We show that several explanation and robust estimation queries over graphical models are special cases of this problem. We propose a class of approximate algorithms for solving this problem. Our algorithms are based on a graph concept called
k
k
-separator and heuristic algorithms for multiple choice knapsack and subset-sum problems. Our experiments show that our algorithms are superior to the following approach: encode the problem as a mixed integer linear program (MILP) and solve the latter using a state-of-the-art MILP solver such as SCIP."
neurips,https://proceedings.neurips.cc/paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf,Global Convergence of Deep Networks with One Wide Layer Followed by Pyramidal Topology,"Quynh N. Nguyen, Marco Mondelli",
neurips,https://proceedings.neurips.cc/paper/2020/file/8b4066554730ddfaa0266346bdc1b202-Paper.pdf,On the Trade-off between Adversarial and Backdoor Robustness,"Cheng-Hsin Weng, Yan-Ting Lee, Shan-Hung (Brandon) Wu",
neurips,https://proceedings.neurips.cc/paper/2020/file/8b5c8441a8ff8e151b191c53c1842a38-Paper.pdf,Implicit Graph Neural Networks,"Fangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, Laurent El Ghaoui",
neurips,https://proceedings.neurips.cc/paper/2020/file/8b9e7ab295e87570551db122a04c6f7c-Paper.pdf,Rethinking Importance Weighting for Deep Learning under Distribution Shift,"Tongtong Fang, Nan Lu, Gang Niu, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2020/file/8ba6c657b03fc7c8dd4dff8e45defcd2-Paper.pdf,Guiding Deep Molecular Optimization with Genetic Exploration,"Sungsoo Ahn, Junsu Kim, Hankook Lee, Jinwoo Shin",
neurips,https://proceedings.neurips.cc/paper/2020/file/8bdb5058376143fa358981954e7626b8-Paper.pdf,Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks,"Wenrui Zhang, Peng Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/8c00dee24c9878fea090ed070b44f1ab-Paper.pdf,TSPNet: Hierarchical Feature Learning via Temporal Semantic Pyramid for Sign Language Translation,"DONGXU LI, Chenchen Xu, Xin Yu, Kaihao Zhang, Benjamin Swift, Hanna Suominen, Hongdong Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/8c3c27ac7d298331a1bdfd0a5e8703d3-Paper.pdf,Neural Topographic Factor Analysis for fMRI Data,"Eli Sennesh, Zulqarnain Khan, Yiyu Wang, J Benjamin Hutchinson, Ajay Satpute, Jennifer Dy, Jan-Willem van de Meent",
neurips,https://proceedings.neurips.cc/paper/2020/file/8c53d30ad023ce50140181f713059ddf-Paper.pdf,Neural Architecture Generator Optimization,"Robin Ru, Pedro Esperança, Fabio Maria Carlucci",
neurips,https://proceedings.neurips.cc/paper/2020/file/8ccf1fb8b09a8212bafea305cf5d5e9f-Paper.pdf,A Bandit Learning Algorithm and Applications to Auction Design,Kim Thang Nguyen,"We consider online bandit learning in which at every time step, an algorithm has to make a decision and then observe only its reward. The goal is to design efficient (polynomial-time) algorithms that achieve a total reward approximately close to that of the best fixed decision in hindsight. In this paper, we introduce a new notion of
(
λ
,
μ
)
(
-concave functions and present a bandit learning algorithm that achieves a performance guarantee which is characterized as a function of the concavity parameters
λ
λ
and
μ
μ
. The algorithm is based on the mirror descent algorithm in which the update directions follow the gradient of the multilinear extensions of the reward functions. The regret bound induced by our algorithm is
˜
O
(
√
T
)
O
which is nearly optimal. We apply our algorithm to auction design, specifically to welfare maximization, revenue maximization, and no-envy learning in auctions. In welfare maximization, we show that a version of fictitious play in smooth auctions guarantees a competitive regret bound which is determined by the smooth parameters. In revenue maximization, we consider the simultaneous second-price auctions with reserve prices in multi-parameter environments. We give a bandit algorithm which achieves the total revenue at least
1
/
2
1
times that of the best fixed reserve prices in hindsight. In no-envy learning, we study the bandit item selection problem where the player valuation is submodular and provide an efficient
1
/
2
1
-approximation no-envy algorithm."
neurips,https://proceedings.neurips.cc/paper/2020/file/8ce6fc704072e351679ac97d4a985574-Paper.pdf,MetaPoison: Practical General-purpose Clean-label Data Poisoning,"W. Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, Tom Goldstein",
neurips,https://proceedings.neurips.cc/paper/2020/file/8d2355364e9a2ba1f82f975414937b43-Paper.pdf,Sample Efficient Reinforcement Learning via Low-Rank Matrix Estimation,"Devavrat Shah, Dogyoon Song, Zhi Xu, Yuzhe Yang","We consider the question of learning
Q
Q
-function in a sample efficient manner for reinforcement learning with continuous state and action spaces under a generative model. If
Q
Q
-function is Lipschitz continuous, then the minimal sample complexity for estimating
ϵ
ϵ
-optimal
Q
Q
-function is known to scale as
Ω
(
1
ϵ
d
1
+
d
2
+
2
)
Ω
per classical non-parametric learning theory, where
d
1
d
and
d
2
d
denote the dimensions of the state and action spaces respectively. The
Q
Q
-function, when viewed as a kernel, induces a Hilbert-Schmidt operator and hence possesses square-summable spectrum. This motivates us to consider a parametric class of
Q
Q
-functions parameterized by its ""rank""
r
r
, which contains all Lipschitz
Q
Q
-functions as
r
→
∞
r
. As our key contribution, we develop a simple, iterative learning algorithm that finds
ϵ
ϵ
-optimal
Q
Q
-function with sample complexity of
˜
O
(
1
ϵ
max
(
d
1
,
d
2
)
+
2
)
O
when the optimal
Q
Q
-function has low rank
r
r
and the discounting factor
γ
γ
is below a certain threshold. Thus, this provides an exponential improvement in sample complexity. To enable our result, we develop a novel Matrix Estimation algorithm that faithfully estimates an unknown low-rank matrix in the
ℓ
∞
ℓ
sense even in the presence of arbitrary bounded noise, which might be of interest in its own right. Empirical results on several stochastic control tasks confirm the efficacy of our ""low-rank"" algorithms."
neurips,https://proceedings.neurips.cc/paper/2020/file/8d30aa96e72440759f74bd2306c1fa3d-Paper.pdf,Training Generative Adversarial Networks with Limited Data,"Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, Timo Aila",
neurips,https://proceedings.neurips.cc/paper/2020/file/8d3215ae97598264ad6529613774a038-Paper.pdf,Deeply Learned Spectral Total Variation Decomposition,"Tamara G. Grossmann, Yury Korolev, Guy Gilboa, Carola Schoenlieb",
neurips,https://proceedings.neurips.cc/paper/2020/file/8dc5983b8c4ef1d8fcd5f325f9a65511-Paper.pdf,FracTrain: Fractionally Squeezing Bit Savings Both Temporally and Spatially for Efficient DNN Training,"Yonggan Fu, Haoran You, Yang Zhao, Yue Wang, Chaojian Li, Kailash Gopalakrishnan, Zhangyang Wang, Yingyan Lin",
neurips,https://proceedings.neurips.cc/paper/2020/file/8dcf2420e78a64333a59674678fb283b-Paper.pdf,Improving Neural Network Training in Low Dimensional Random Bases,"Frithjof Gressmann, Zach Eaton-Rosen, Carlo Luschi",
neurips,https://proceedings.neurips.cc/paper/2020/file/8df6a65941e4c9da40a4fb899de65c55-Paper.pdf,Safe Reinforcement Learning via Curriculum Induction,"Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, Alekh Agarwal",
neurips,https://proceedings.neurips.cc/paper/2020/file/8e2c381d4dd04f1c55093f22c59c3a08-Paper.pdf,Leverage the Average: an Analysis of KL Regularization in Reinforcement Learning,"Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, Remi Munos, Matthieu Geist",
neurips,https://proceedings.neurips.cc/paper/2020/file/8e3308c853e47411c761429193511819-Paper.pdf,How Robust are the Estimated Effects of Nonpharmaceutical Interventions against COVID-19?,"Mrinank Sharma, Sören Mindermann, Jan Brauner, Gavin Leech, Anna Stephenson, Tomáš Gavenčiak, Jan Kulveit, Yee Whye Teh, Leonid Chindelevitch, Yarin Gal",
neurips,https://proceedings.neurips.cc/paper/2020/file/8ee7730e97c67473a424ccfeff49ab20-Paper.pdf,Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses,"Kaivalya Rawal, Himabindu Lakkaraju",
neurips,https://proceedings.neurips.cc/paper/2020/file/8f4576ad85410442a74ee3a7683757b3-Paper.pdf,Generalization error in high-dimensional perceptrons: Approaching Bayes error with convex optimization,"Benjamin Aubin, Florent Krzakala, Yue Lu, Lenka Zdeborová","We consider a commonly studied supervised classification of a synthetic dataset whose labels are generated by feeding a one-layer non-linear neural network with random iid inputs. We study the generalization performances of standard classifiers in the high-dimensional regime where
α
=
n
d
α
is kept finite in the limit of a high dimension
d
d
and number of samples
n
n
. Our contribution is three-fold: First, we prove a formula for the generalization error achieved by
ℓ
2
ℓ
regularized classifiers that minimize a convex loss. This formula was first obtained by the heuristic replica method of statistical physics. Secondly, focussing on commonly used loss functions and optimizing the
ℓ
2
ℓ
regularization strength, we observe that while ridge regression performance is poor, logistic and hinge regression are surprisingly able to approach the Bayes-optimal generalization error extremely closely. As
α
→
∞
α
they lead to Bayes-optimal rates, a fact that does not follow from predictions of margin-based generalization error bounds. Third, we design an optimal loss and regularizer that provably leads to Bayes-optimal generalization error."
neurips,https://proceedings.neurips.cc/paper/2020/file/8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf,Projection Efficient Subgradient Method and Optimal Nonsmooth Frank-Wolfe Method,"Kiran K. Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh","We consider the classical setting of optimizing a nonsmooth Lipschitz continuous convex function over a convex constraint set, when having access to a (stochastic) first-order oracle (FO) for the function and a projection oracle (PO) for the constraint set. It is well known that to achieve
ϵ
ϵ
-suboptimality in high-dimensions,
Θ
(
ϵ
−
2
)
Θ
FO calls are necessary. This is achieved by the projected subgradient method (PGD). However, PGD also entails
O
(
ϵ
−
2
)
O
PO calls, which may be computationally costlier than FO calls (e.g. nuclear norm constraints). Improving this PO calls complexity of PGD is largely unexplored, despite the fundamental nature of this problem and extensive literature. We present first such improvement. This only requires a mild assumption that the objective function, when extended to a slightly larger neighborhood of the constraint set, still remains Lipschitz and accessible via FO. In particular, we introduce MOPES method, which carefully combines Moreau-Yosida smoothing and accelerated first-order schemes. This is guaranteed to find a feasible
ϵ
ϵ
-suboptimal solution using only
O
(
ϵ
−
1
)
O
PO calls and optimal
O
(
ϵ
−
2
)
O
FO calls. Further, instead of a PO if we only have a linear minimization oracle (LMO, a la Frank-Wolfe) to access the constraint set, an extension of our method, MOLES, finds a feasible
ϵ
ϵ
-suboptimal solution using
O
(
ϵ
−
2
)
O
LMO calls and FO calls---both match known lower bounds, resolving a question left open since White (1993). Our experiments confirm that these methods achieve significant speedups over the state-of-the-art, for a problem with costly PO and LMO calls."
neurips,https://proceedings.neurips.cc/paper/2020/file/8fb134f258b1f7865a6ab2d935a897c9-Paper.pdf,PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks,"Minh Vu, My T. Thai",
neurips,https://proceedings.neurips.cc/paper/2020/file/8fc687aa152e8199fe9e73304d407bca-Paper.pdf,Few-Cost Salient Object Detection with Adversarial-Paced Learning,"Dingwen Zhang, HaiBin Tian, Jungong Han",
neurips,https://proceedings.neurips.cc/paper/2020/file/8fcd9e5482a62a5fa130468f4cf641ef-Paper.pdf,Minimax Estimation of Conditional Moment Models,"Nishanth Dikkala, Greg Lewis, Lester Mackey, Vasilis Syrgkanis",
neurips,https://proceedings.neurips.cc/paper/2020/file/8fdd149fcaa7058caccc9c4ad5b0d89a-Paper.pdf,Causal Imitation Learning With Unobserved Confounders,"Junzhe Zhang, Daniel Kumor, Elias Bareinboim",
neurips,https://proceedings.neurips.cc/paper/2020/file/90525e70b7842930586545c6f1c9310c-Paper.pdf,Your GAN is Secretly an Energy-based Model and You Should Use Discriminator Driven Latent Sampling,"Tong Che, Ruixiang ZHANG, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, Yoshua Bengio","We show that the sum of the implicit generator log-density
log
p
g
log
of a GAN with the logit score of the discriminator defines an energy function which yields the true data density when the generator is imperfect but the discriminator is optimal, thus making it possible to improve on the typical generator (with implicit density
p
g
p
). To make that practical, we show that sampling from this modified density can be achieved by sampling in latent space according to an energy-based model induced by the sum of the latent prior log-density and the discriminator output score. This can be achieved by running a Langevin MCMC in latent space and then applying the generator function, which we call Discriminator Driven Latent Sampling~(DDLS). We show that DDLS is highly efficient compared to previous methods which work in the high-dimensional pixel space and can be applied to improve on previously trained GANs of many types. We evaluate DDLS on both synthetic and real-world datasets qualitatively and quantitatively. On CIFAR-10, DDLS substantially improves the Inception Score of an off-the-shelf pre-trained SN-GAN~\citep{sngan} from
8.22
8.22
to
9.09
9.09
which is even comparable to the class-conditional BigGAN~\citep{biggan} model. This achieves a new state-of-the-art in unconditional image synthesis setting without introducing extra parameters or additional training."
neurips,https://proceedings.neurips.cc/paper/2020/file/90599c8fdd2f6e7a03ad173e2f535751-Paper.pdf,Learning Black-Box Attackers with Transferable Priors and Query Feedback,"Jiancheng YANG, Yangzhou Jiang, Xiaoyang Huang, Bingbing Ni, Chenglong Zhao",
neurips,https://proceedings.neurips.cc/paper/2020/file/908c9a564a86426585b29f5335b619bc-Paper.pdf,Locally Differentially Private (Contextual) Bandits Learning,"Kai Zheng, Tianle Cai, Weiran Huang, Zhenguo Li, Liwei Wang","We study locally differentially private (LDP) bandits learning in this paper. First, we propose simple black-box reduction frameworks that can solve a large family of context-free bandits learning problems with LDP guarantee. Based on our frameworks, we can improve previous best results for private bandits learning with one-point feedback, such as private Bandits Convex Optimization etc, and obtain the first results for Bandits Convex Optimization (BCO) with multi-point feedback under LDP. LDP guarantee and black-box nature make our frameworks more attractive in real applications compared with previous specifically designed and relatively weaker differentially private (DP) algorithms. Further, we also extend our algorithm to Generalized Linear Bandits with regret bound
~
\mc
O
(
T
3
/
4
/
ε
)
\mc
under
(
ε
,
δ
)
(
-LDP and it is conjectured to be optimal. Note given existing
Ω
(
T
)
Ω
lower bound for DP contextual linear bandits (Shariff & Sheffet, NeurIPS 2018), our result shows a fundamental difference between LDP and DP for contextual bandits."
neurips,https://proceedings.neurips.cc/paper/2020/file/90c34175923a36ab7a5de4b981c1972f-Paper.pdf,Invertible Gaussian Reparameterization: Revisiting the Gumbel-Softmax,"Andres Potapczynski, Gabriel Loaiza-Ganem, John P. Cunningham",
neurips,https://proceedings.neurips.cc/paper/2020/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf,Kernel Based Progressive Distillation for Adder Neural Networks,"Yixing Xu, Chang Xu, Xinghao Chen, Wei Zhang, Chunjing XU, Yunhe Wang","Adder Neural Networks (ANNs) which only contain additions bring us a new way of developing deep neural networks with low energy consumption. Unfortunately, there is an accuracy drop when replacing all convolution filters by adder filters. The main reason here is the optimization difficulty of ANNs using
ℓ
1
ℓ
-norm, in which the estimation of gradient in back propagation is inaccurate. In this paper, we present a novel method for further improving the performance of ANNs without increasing the trainable parameters via a progressive kernel based knowledge distillation (PKKD) method. A convolutional neural network (CNN) with the same architecture is simultaneously initialized and trained as a teacher network, features and weights of ANN and CNN will be transformed to a new space to eliminate the accuracy drop. The similarity is conducted in a higher-dimensional space to disentangle the difference of their distributions using a kernel based method. Finally, the desired ANN is learned based on the information from both the ground-truth and teacher, progressively. The effectiveness of the proposed method for learning ANN with higher performance is then well-verified on several benchmarks. For instance, the ANN-50 trained using the proposed PKKD method obtains a 76.8\% top-1 accuracy on ImageNet dataset, which is 0.6\% higher than that of the ResNet-50."
neurips,https://proceedings.neurips.cc/paper/2020/file/9161ab7a1b61012c4c303f10b4c16b2c-Paper.pdf,Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization,"Paul Barde, Julien Roy, Wonseok Jeon, Joelle Pineau, Chris Pal, Derek Nowrouzezahrai",
neurips,https://proceedings.neurips.cc/paper/2020/file/91c77393975889bd08f301c9e13a44b7-Paper.pdf,Agree to Disagree: Adaptive Ensemble Knowledge Distillation in Gradient Space,"Shangchen Du, Shan You, Xiaojie Li, Jianlong Wu, Fei Wang, Chen Qian, Changshui Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/91cff01af640a24e7f9f7a5ab407889f-Paper.pdf,The Wasserstein Proximal Gradient Algorithm,"Adil Salim, Anna Korba, Giulia Luise",
neurips,https://proceedings.neurips.cc/paper/2020/file/92049debbe566ca5782a3045cf300a3c-Paper.pdf,Universally Quantized Neural Compression,"Eirikur Agustsson, Lucas Theis",
neurips,https://proceedings.neurips.cc/paper/2020/file/9239be5f9dc4058ec647f14fd04b1290-Paper.pdf,Temporal Variability in Implicit Online Learning,"Nicolò Campolongo, Francesco Orabona",
neurips,https://proceedings.neurips.cc/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf,Investigating Gender Bias in Language Models Using Causal Mediation Analysis,"Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, Stuart Shieber",
neurips,https://proceedings.neurips.cc/paper/2020/file/92977ae4d2ba21425a59afb269c2a14e-Paper.pdf,Off-Policy Imitation Learning from Observations,"Zhuangdi Zhu, Kaixiang Lin, Bo Dai, Jiayu Zhou",
neurips,https://proceedings.neurips.cc/paper/2020/file/92a08bf918f44ccd961477be30023da1-Paper.pdf,Escaping Saddle-Point Faster under Interpolation-like Conditions,"Abhishek Roy, Krishnakumar Balasubramanian, Saeed Ghadimi, Prasant Mohapatra","In this paper, we show that under over-parametrization several standard stochastic optimization algorithms escape saddle-points and converge to local-minimizers much faster. One of the fundamental aspects of over-parametrized models is that they are capable of interpolating the training data. We show that, under interpolation-like assumptions satisfied by the stochastic gradients in an over-parametrization setting, the first-order oracle complexity of Perturbed Stochastic Gradient Descent (PSGD) algorithm to reach an
ϵ
ϵ
-local-minimizer, matches the corresponding deterministic rate of
O
(
1
/
ϵ
2
)
O
. We next analyze Stochastic Cubic-Regularized Newton (SCRN) algorithm under interpolation-like conditions, and show that the oracle complexity to reach an
ϵ
ϵ
-local-minimizer under interpolation-like conditions, is
O
(
1
/
ϵ
2.5
)
O
. While this obtained complexity is better than the corresponding complexity of either PSGD, or SCRN without interpolation-like assumptions, it does not match the rate of
O
(
1
/
ϵ
1.5
)
O
corresponding to deterministic Cubic-Regularized Newton method. It seems further Hessian-based interpolation-like assumptions are necessary to bridge this gap. We also discuss the corresponding improved complexities in the zeroth-order settings."
neurips,https://proceedings.neurips.cc/paper/2020/file/92bf5e6240737e0326ea59846a83e076-Paper.pdf,Matérn Gaussian Processes on Riemannian Manifolds,"Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, Marc Deisenroth (he/him)",
neurips,https://proceedings.neurips.cc/paper/2020/file/92c3b916311a5517d9290576e3ea37ad-Paper.pdf,Improved Techniques for Training Score-Based Generative Models,"Yang Song, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf,wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,"Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, Michael Auli",
neurips,https://proceedings.neurips.cc/paper/2020/file/9308b0d6e5898366a4a986bc33f3d3e7-Paper.pdf,A Maximum-Entropy Approach to Off-Policy Evaluation in Average-Reward MDPs,"Nevena Lazic, Dong Yin, Mehrdad Farajtabar, Nir Levine, Dilan Gorur, Chris Harris, Dale Schuurmans",
neurips,https://proceedings.neurips.cc/paper/2020/file/9332c513ef44b682e9347822c2e457ac-Paper.pdf,"Instead of Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast Gradients","William Moses, Valentin Churavy",
neurips,https://proceedings.neurips.cc/paper/2020/file/937936029af671cf479fa893db91cbdd-Paper.pdf,Does Unsupervised Architecture Representation Learning Help Neural Architecture Search?,"Shen Yan, Yu Zheng, Wei Ao, Xiao Zeng, Mi Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/9381fc93ad66f9ec4b2eef71147a6665-Paper.pdf,Value-driven Hindsight Modelling,"Arthur Guez, Fabio Viola, Theophane Weber, Lars Buesing, Steven Kapturowski, Doina Precup, David Silver, Nicolas Heess",
neurips,https://proceedings.neurips.cc/paper/2020/file/939314105ce8701e67489642ef4d49e8-Paper.pdf,Dynamic Regret of Convex and Smooth Functions,"Peng Zhao, Yu-Jie Zhang, Lijun Zhang, Zhi-Hua Zhou","We investigate online convex optimization in non-stationary environments and choose the dynamic regret as the performance measure, defined as the difference between cumulative loss incurred by the online algorithm and that of any feasible comparator sequence. Let
T
T
be the time horizon and
P
T
P
be the path-length that essentially reflects the non-stationarity of environments, the state-of-the-art dynamic regret is
O
(
√
T
(
1
+
P
T
)
)
O
. Although this bound is proved to be minimax optimal for convex functions, in this paper, we demonstrate that it is possible to further enhance the dynamic regret by exploiting the smoothness condition. Specifically, we propose novel online algorithms that are capable of leveraging smoothness and replace the dependence on
T
T
in the dynamic regret by problem-dependent quantities: the variation in gradients of loss functions, the cumulative loss of the comparator sequence, and the minimum of the previous two terms. These quantities are at most
O
(
T
)
O
while could be much smaller in benign environments. Therefore, our results are adaptive to the intrinsic difficulty of the problem, since the bounds are tighter than existing results for easy problems and meanwhile guarantee the same rate in the worst case."
neurips,https://proceedings.neurips.cc/paper/2020/file/93d9033636450402d67cd55e60b3f926-Paper.pdf,On Convergence of Nearest Neighbor Classifiers over Feature Transformations,"Luka Rimanic, Cedric Renggli, Bo Li, Ce Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/93fb39474c51b8a82a68413e2a5ae17a-Paper.pdf,Mitigating Manipulation in Peer Review via Randomized Reviewer Assignments,"Steven Jecmen, Hanrui Zhang, Ryan Liu, Nihar Shah, Vincent Conitzer, Fei Fang",
neurips,https://proceedings.neurips.cc/paper/2020/file/949686ecef4ee20a62d16b4a2d7ccca3-Paper.pdf,Contrastive learning of global and local features for medical image segmentation with limited annotations,"Krishna Chaitanya, Ertunc Erdil, Neerav Karani, Ender Konukoglu",
neurips,https://proceedings.neurips.cc/paper/2020/file/94aef38441efa3380a3bed3faf1f9d5d-Paper.pdf,Self-Supervised Graph Transformer on Large-Scale Molecular Data,"Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying WEI, Wenbing Huang, Junzhou Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/94c28dcfc97557df0df6d1f7222fc384-Paper.pdf,Generative Neurosymbolic Machines,"Jindong Jiang, Sungjin Ahn",
neurips,https://proceedings.neurips.cc/paper/2020/file/94c4dd41f9dddce696557d3717d98d82-Paper.pdf,How many samples is a good initial point worth in Low-rank Matrix Recovery?,"Jialun Zhang, Richard Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/94cb02feb750f20bad8a85dfe7e18d11-Paper.pdf,CSER: Communication-efficient SGD with Error Reset,"Cong Xie, Shuai Zheng, Sanmi Koyejo, Indranil Gupta, Mu Li, Haibin Lin","The scalability of Distributed Stochastic Gradient Descent (SGD) is today limited by communication bottlenecks. We propose a novel SGD variant: \underline{C}ommunication-efficient \underline{S}GD with \underline{E}rror \underline{R}eset, or \underline{CSER}. The key idea in CSER is first a new technique called
error reset'' that adapts arbitrary compressors for SGD, producing bifurcated local models with periodic reset of resulting local residual errors. Second we introduce partial synchronization for both the gradients and the models, leveraging advantages from them. We prove the convergence of CSER for smooth non-convex problems. Empirical results show that when combined with highly aggressive compressors, the CSER algorithms accelerate the distributed training by nearly
10
×
10
for CIFAR-100, and by
4.5
×
4.5
for ImageNet."
neurips,https://proceedings.neurips.cc/paper/2020/file/94d2a3c6dd19337f2511cdf8b4bf907e-Paper.pdf,Efficient estimation of neural tuning during naturalistic behavior,"Edoardo Balzani, Kaushik Lakshminarasimhan, Dora Angelaki, Cristina Savin",
neurips,https://proceedings.neurips.cc/paper/2020/file/94e70705efae423efda1088614128d0b-Paper.pdf,High-recall causal discovery for autocorrelated time series with latent confounders,"Andreas Gerhardus, Jakob Runge",
neurips,https://proceedings.neurips.cc/paper/2020/file/951124d4a093eeae83d9726a20295498-Paper.pdf,Forget About the LiDAR: Self-Supervised Depth Estimators with MED Probability Volumes,"Juan Luis GonzalezBello, Munchurl Kim","Self-supervised depth estimators have recently shown results comparable to the supervised methods on the challenging single image depth estimation (SIDE) task, by exploiting the geometrical relations between target and reference views in the training data. However, previous methods usually learn forward or backward image synthesis, but not depth estimation, as they cannot effectively neglect occlusions between the target and the reference images. Previous works rely on rigid photometric assumptions or on the SIDE network to infer depth and occlusions, resulting in limited performance. On the other hand, we propose a method to ""Forget About the LiDAR"" (FAL), with Mirrored Exponential Disparity (MED) probability volumes for the training of monocular depth estimators from stereo images. Our MED representation allows us to obtain geometrically inspired occlusion maps with our novel Mirrored Occlusion Module (MOM), which does not impose a learning burden on our FAL-net. Contrary to the previous methods that learn SIDE from stereo pairs by regressing disparity in the linear space, our FAL-net regresses disparity by binning it into the exponential space, which allows for better detection of distant and nearby objects. We define a two-step training strategy for our FAL-net: It is first trained for view synthesis and then fine-tuned for depth estimation with our MOM. Our FAL-net is remarkably light-weight and outperforms the previous state-of-the-art methods with 8
×
×
fewer parameters and 3
×
×
faster inference speeds on the challenging KITTI dataset. We present extensive experimental results on the KITTI, CityScapes, and Make3D datasets to verify our method's effectiveness. To the authors' best knowledge, the presented method performs the best among all the previous self-supervised methods until now."
neurips,https://proceedings.neurips.cc/paper/2020/file/9523147e5a6707baf674941812ee5c94-Paper.pdf,Joint Contrastive Learning with Infinite Possibilities,"Qi Cai, Yu Wang, Yingwei Pan, Ting Yao, Tao Mei",
neurips,https://proceedings.neurips.cc/paper/2020/file/9529fbba677729d3206b3b9073d1e9ca-Paper.pdf,Robust Gaussian Covariance Estimation in Nearly-Matrix Multiplication Time,"Jerry Li, Guanghao Ye","Robust covariance estimation is the following, well-studied problem in high dimensional statistics: given
N
N
samples from a
d
d
-dimensional Gaussian
N
(
0
,
Σ
)
N
, but where an
ε
ε
-fraction of the samples have been arbitrarily corrupted, output
ˆ
Σ
Σ
minimizing the total variation distance between
N
(
0
,
Σ
)
N
and
N
(
0
,
ˆ
Σ
)
N
. This corresponds to learning
Σ
Σ
in a natural affine-invariant variant of the Frobenius norm known as the \emph{Mahalanobis norm}. Previous work of Cheng et al demonstrated an algorithm that, given
N
=
˜
Ω
(
d
2
/
ε
2
)
N
samples, achieved a near-optimal error of
O
(
ε
log
1
/
ε
)
O
, and moreover, their algorithm ran in time
˜
O
(
T
(
N
,
d
)
log
κ
/
p
o
l
y
(
ε
)
)
O
, where
T
(
N
,
d
)
T
is the time it takes to multiply a
d
×
N
d
matrix by its transpose, and
κ
κ
is the condition number of
Σ
Σ
. When
ε
ε
is relatively small, their polynomial dependence on
1
/
ε
1
in the runtime is prohibitively large. In this paper, we demonstrate a novel algorithm that achieves the same statistical guarantees, but which runs in time
˜
O
(
T
(
N
,
d
)
log
κ
)
O
. In particular, our runtime has no dependence on
ε
ε
. When
Σ
Σ
is reasonably conditioned, our runtime matches that of the fastest algorithm for covariance estimation without outliers, up to poly-logarithmic factors, showing that we can get robustness essentially
for free.''"
neurips,https://proceedings.neurips.cc/paper/2020/file/95424358822e753eb993c97ee76a9076-Paper.pdf,Adversarially-learned Inference via an Ensemble of Discrete Undirected Graphical Models,"Adarsh Keshav Jeewajee, Leslie Kaelbling",
neurips,https://proceedings.neurips.cc/paper/2020/file/9547ad6b651e2087bac67651aa92cd0d-Paper.pdf,GS-WGAN: A Gradient-Sanitized Approach for Learning Differentially Private Generators,"Dingfan Chen, Tribhuvanesh Orekondy, Mario Fritz",
neurips,https://proceedings.neurips.cc/paper/2020/file/9578a63fbe545bd82cc5bbe749636af1-Paper.pdf,SurVAE Flows: Surjections to Bridge the Gap between VAEs and Flows,"Didrik Nielsen, Priyank Jaini, Emiel Hoogeboom, Ole Winther, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2020/file/95a6fc111fa11c3ab209a0ed1b9abeb6-Paper.pdf,Learning Causal Effects via Weighted Empirical Risk Minimization,"Yonghan Jung, Jin Tian, Elias Bareinboim",
neurips,https://proceedings.neurips.cc/paper/2020/file/95b431e51fc53692913da5263c214162-Paper.pdf,Revisiting the Sample Complexity of Sparse Spectrum Approximation of Gaussian Processes,"Minh Hoang, Nghia Hoang, Hai Pham, David Woodruff",
neurips,https://proceedings.neurips.cc/paper/2020/file/95c7dfc5538e1ce71301cf92a9a96bd0-Paper.pdf,Incorporating Interpretable Output Constraints in Bayesian Neural Networks,"Wanqian Yang, Lars Lorch, Moritz Graule, Himabindu Lakkaraju, Finale Doshi-Velez",
neurips,https://proceedings.neurips.cc/paper/2020/file/95e62984b87e90645a5cf77037395959-Paper.pdf,Multi-Stage Influence Function,"Hongge Chen, Si Si, Yang Li, Ciprian Chelba, Sanjiv Kumar, Duane Boning, Cho-Jui Hsieh",
neurips,https://proceedings.neurips.cc/paper/2020/file/95f2b84de5660ddf45c8a34933a2e66f-Paper.pdf,Probabilistic Fair Clustering,"Seyed Esmaeili, Brian Brubach, Leonidas Tsepenekas, John Dickerson",
neurips,https://proceedings.neurips.cc/paper/2020/file/95f8d9901ca8878e291552f001f67692-Paper.pdf,Stochastic Segmentation Networks: Modelling Spatially Correlated Aleatoric Uncertainty,"Miguel Monteiro, Loic Le Folgoc, Daniel Coelho de Castro, Nick Pawlowski, Bernardo Marques, Konstantinos Kamnitsas, Mark van der Wilk, Ben Glocker",
neurips,https://proceedings.neurips.cc/paper/2020/file/962e56a8a0b0420d87272a682bfd1e53-Paper.pdf,ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based on Nonlinear ICA,"Ilyes Khemakhem, Ricardo Monti, Diederik Kingma, Aapo Hyvarinen",
neurips,https://proceedings.neurips.cc/paper/2020/file/964d1775b722eff11b8ecd9e9ed5bd9e-Paper.pdf,Testing Determinantal Point Processes,"Khashayar Gatmiry, Maryam Aliakbarpour, Stefanie Jegelka","Determinantal point processes (DPPs) are popular probabilistic models of diversity. In this paper, we investigate DPPs from a new perspective: property testing of distributions. Given sample access to an unknown distribution
q
q
over the subsets of a ground set, we aim to distinguish whether
q
q
is a DPP distribution or
ϵ
ϵ
-far from all DPP distributions in
ℓ
1
ℓ
-distance. In this work, we propose the first algorithm for testing DPPs. Furthermore, we establish a matching lower bound on the sample complexity of DPP testing. This lower bound also extends to showing a new hardness result for the problem of testing the more general class of log-submodular distributions."
neurips,https://proceedings.neurips.cc/paper/2020/file/96671501524948bc3937b4b30d0e57b9-Paper.pdf,CogLTX: Applying BERT to Long Texts,"Ming Ding, Chang Zhou, Hongxia Yang, Jie Tang",
neurips,https://proceedings.neurips.cc/paper/2020/file/967990de5b3eac7b87d49a13c6834978-Paper.pdf,f-GAIL: Learning f-Divergence for Generative Adversarial Imitation Learning,"Xin Zhang, Yanhua Li, Ziming Zhang, Zhi-Li Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/968b15768f3d19770471e9436d97913c-Paper.pdf,Non-parametric Models for Non-negative Functions,"Ulysse Marteau-Ferey, Francis Bach, Alessandro Rudi",
neurips,https://proceedings.neurips.cc/paper/2020/file/968c9b4f09cbb7d7925f38aea3484111-Paper.pdf,Uncertainty Aware Semi-Supervised Learning on Graph Data,"Xujiang Zhao, Feng Chen, Shu Hu, Jin-Hee Cho",
neurips,https://proceedings.neurips.cc/paper/2020/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf,ConvBERT: Improving BERT with Span-based Dynamic Convolution,"Zi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan",
neurips,https://proceedings.neurips.cc/paper/2020/file/96e07156db854ca7b00b5df21716b0c6-Paper.pdf,Practical No-box Adversarial Attacks against DNNs,"Qizhang Li, Yiwen Guo, Hao Chen",
neurips,https://proceedings.neurips.cc/paper/2020/file/96ea64f3a1aa2fd00c72faacf0cb8ac9-Paper.pdf,Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model,"Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, Yuxin Chen","We investigate the sample efficiency of reinforcement learning in a
γ
γ
-discounted infinite-horizon Markov decision process (MDP) with state space S and action space A, assuming access to a generative model. Despite a number of prior work tackling this problem, a complete picture of the trade-offs between sample complexity and statistical accuracy is yet to be determined. In particular, prior results suffer from a sample size barrier, in the sense that their claimed statistical guarantees hold only when the sample size exceeds at least
|
S
|
|
A
|
/
(
1
−
γ
)
2
|
(up to some log factor). The current paper overcomes this barrier by certifying the minimax optimality of model-based reinforcement learning as soon as the sample size exceeds the order of
|
S
|
|
A
|
/
(
1
−
γ
)
|
(modulo some log factor). More specifically, a perturbed model-based planning algorithm provably finds an
ϵ
ϵ
-optimal policy with an order of
|
S
|
|
A
|
/
(
(
1
−
γ
)
3
ϵ
2
)
|
samples (up to log factor) for any
0
<
ϵ
<
1
/
(
1
−
γ
)
0
. Along the way, we derive improved (instance-dependent) guarantees for model-based policy evaluation. To the best of our knowledge, this work provides the first minimax-optimal guarantee in a generative model that accommodates the entire range of sample sizes (beyond which finding a meaningful policy is information theoretically impossible)."
neurips,https://proceedings.neurips.cc/paper/2020/file/96f2d6069db8ad895c34e2285d25c0ed-Paper.pdf,Walking in the Shadow: A New Perspective on Descent Directions for Constrained Minimization,"Hassan Mortagy, Swati Gupta, Sebastian Pokutta",
neurips,https://proceedings.neurips.cc/paper/2020/file/96fca94df72984fc97ee5095410d4dec-Paper.pdf,Path Sample-Analytic Gradient Estimators for Stochastic Binary Networks,"Alexander Shekhovtsov, Viktor Yanush, Boris Flach",
neurips,https://proceedings.neurips.cc/paper/2020/file/970627414218ccff3497cb7a784288f5-Paper.pdf,Reward Propagation Using Graph Convolutional Networks,"Martin Klissarov, Doina Precup",
neurips,https://proceedings.neurips.cc/paper/2020/file/970af30e481057c48f87e101b61e6994-Paper.pdf,"LoopReg: Self-supervised Learning of Implicit Surface Correspondences, Pose and Shape for 3D Human Mesh Registration","Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt, Gerard Pons-Moll",
neurips,https://proceedings.neurips.cc/paper/2020/file/9715d04413f296eaf3c30c47cec3daa6-Paper.pdf,Fully Dynamic Algorithm for Constrained Submodular Optimization,"Silvio Lattanzi, Slobodan Mitrović, Ashkan Norouzi-Fard, Jakub M. Tarnawski, Morteza Zadimoghaddam","The task of maximizing a monotone submodular function under a cardinality constraint is at the core of many machine learning and data mining applications, including data summarization, sparse regression and coverage problems. We study this classic problem in the fully dynamic setting, where elements can be both inserted and removed. Our main result is a randomized algorithm that maintains an efficient data structure with a poly-logarithmic amortized update time and yields a
(
1
/
2
−
e
p
s
i
l
o
n
)
(
-approximate solution. We complement our theoretical analysis with an empirical study of the performance of our algorithm."
neurips,https://proceedings.neurips.cc/paper/2020/file/9719a00ed0c5709d80dfef33795dcef3-Paper.pdf,Robust Optimal Transport with Applications in Generative Modeling and Domain Adaptation,"Yogesh Balaji, Rama Chellappa, Soheil Feizi",
neurips,https://proceedings.neurips.cc/paper/2020/file/972cda1e62b72640cb7ac702714a115f-Paper.pdf,Autofocused oracles for model-based design,"Clara Fannjiang, Jennifer Listgarten",
neurips,https://proceedings.neurips.cc/paper/2020/file/972ededf6c4d7c1405ef53f27d961eda-Paper.pdf,Debiasing Averaged Stochastic Gradient Descent to handle missing values,"Aude Sportisse, Claire Boyer, Aymeric Dieuleveut, Julie Josse","Stochastic gradient algorithm is a key ingredient of many machine learning methods, particularly appropriate for large-scale learning. However, a major caveat of large data is their incompleteness. We propose an averaged stochastic gradient algorithm handling missing values in linear models. This approach has the merit to be free from the need of any data distribution modeling and to account for heterogeneous missing proportion. In both streaming and finite-sample settings, we prove that this algorithm achieves convergence rate of
O
(
1
n
)
O
at the iteration
n
n
, the same as without missing values. We show the convergence behavior and the relevance of the algorithm not only on synthetic data but also on real data sets, including those collected from medical register."
neurips,https://proceedings.neurips.cc/paper/2020/file/9739efc4f01292e764c86caa59af353e-Paper.pdf,Trajectory-wise Multiple Choice Learning for Dynamics Generalization in Reinforcement Learning,"Younggyo Seo, Kimin Lee, Ignasi Clavera Gilaberte, Thanard Kurutach, Jinwoo Shin, Pieter Abbeel",
neurips,https://proceedings.neurips.cc/paper/2020/file/975a1c8b9aee1c48d32e13ec30be7905-Paper.pdf,CompRess: Self-Supervised Learning by Compressing Representations,"Soroush Abbasi Koohpayegani, Ajinkya Tejankar, Hamed Pirsiavash",
neurips,https://proceedings.neurips.cc/paper/2020/file/977f8b33d303564416bf9f4ab1c39720-Paper.pdf,Sample complexity and effective dimension for regression on manifolds,"Andrew McRae, Justin Romberg, Mark Davenport",
neurips,https://proceedings.neurips.cc/paper/2020/file/979a3f14bae523dc5101c52120c535e9-Paper.pdf,The phase diagram of approximation rates for deep neural networks,"Dmitry Yarotsky, Anton Zhevnerchuk",
neurips,https://proceedings.neurips.cc/paper/2020/file/97e401a02082021fd24957f852e0e475-Paper.pdf,Timeseries Anomaly Detection using Temporal Hierarchical One-Class Network,"Lifeng Shen, Zhuocong Li, James Kwok",
neurips,https://proceedings.neurips.cc/paper/2020/file/97e49161287e7a4f9b745366e4f9431b-Paper.pdf,EcoLight: Intersection Control in Developing Regions Under Extreme Budget and Network Constraints,"Sachin Chauhan, Kashish Bansal, Rijurekha Sen",
neurips,https://proceedings.neurips.cc/paper/2020/file/9813b270ed0288e7c0388f0fd4ec68f5-Paper.pdf,Reconstructing Perceptive Images from Brain Activity by Shape-Semantic GAN,"Tao Fang, Yu Qi, Gang Pan",
neurips,https://proceedings.neurips.cc/paper/2020/file/985e9a46e10005356bbaf194249f6856-Paper.pdf,Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design,"Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2020/file/9873eaad153c6c960616c89e54fe155a-Paper.pdf,A Spectral Energy Distance for Parallel Speech Synthesis,"Alexey Gritsenko, Tim Salimans, Rianne van den Berg, Jasper Snoek, Nal Kalchbrenner",
neurips,https://proceedings.neurips.cc/paper/2020/file/98b17f068d5d9b7668e19fb8ae470841-Paper.pdf,Simulating a Primary Visual Cortex at the Front of CNNs Improves Robustness to Image Perturbations,"Joel Dapello, Tiago Marques, Martin Schrimpf, Franziska Geiger, David Cox, James J. DiCarlo",
neurips,https://proceedings.neurips.cc/paper/2020/file/98b297950041a42470269d56260243a1-Paper.pdf,Learning from Positive and Unlabeled Data with Arbitrary Positive Shift,"Zayd Hammoudeh, Daniel Lowd",
neurips,https://proceedings.neurips.cc/paper/2020/file/98b418276d571e623651fc1d471c7811-Paper.pdf,Deep Energy-based Modeling of Discrete-Time Physics,"Takashi Matsubara, Ai Ishikawa, Takaharu Yaguchi",
neurips,https://proceedings.neurips.cc/paper/2020/file/98dce83da57b0395e163467c9dae521b-Paper.pdf,Quantifying Learnability and Describability of Visual Concepts Emerging in Representation Learning,"Iro Laina, Ruth Fong, Andrea Vedaldi",
neurips,https://proceedings.neurips.cc/paper/2020/file/98f2d76d4d9caf408180b5abfa83ae87-Paper.pdf,Self-Learning Transformations for Improving Gaze and Head Redirection,"Yufeng Zheng, Seonwook Park, Xucong Zhang, Shalini De Mello, Otmar Hilliges",
neurips,https://proceedings.neurips.cc/paper/2020/file/9909794d52985cbc5d95c26e31125d1a-Paper.pdf,Language-Conditioned Imitation Learning for Robot Manipulation Tasks,"Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, Heni Ben Amor",
neurips,https://proceedings.neurips.cc/paper/2020/file/992f0fed0720dbb9d4e060d03ed531ba-Paper.pdf,POMDPs in Continuous Time and Discrete Spaces,"Bastian Alt, Matthias Schultheis, Heinz Koeppl",
neurips,https://proceedings.neurips.cc/paper/2020/file/993edc98ca87f7e08494eec37fa836f7-Paper.pdf,Exemplar Guided Active Learning,"Jason S. Hartford, Kevin Leyton-Brown, Hadas Raviv, Dan Padnos, Shahar Lev, Barak Lenz",
neurips,https://proceedings.neurips.cc/paper/2020/file/994d1cad9132e48c993d58b492f71fc1-Paper.pdf,Grasp Proposal Networks: An End-to-End Solution for Visual Learning of Robotic Grasps,"Chaozheng Wu, Jian Chen, Qiaoyu Cao, Jianchi Zhang, Yunxin Tai, Lin Sun, Kui Jia",
neurips,https://proceedings.neurips.cc/paper/2020/file/99503bdd3c5a4c4671ada72d6fd81433-Paper.pdf,Node Embeddings and Exact Low-Rank Representations of Complex Networks,"Sudhanshu Chanpuriya, Cameron Musco, Konstantinos Sotiropoulos, Charalampos Tsourakakis","In this work we show that the results of Seshadhri et al. are intimately connected to the model they use rather than the low-dimensional structure of complex networks. Specifically, we prove that a minor relaxation of their model can generate sparse graphs with high triangle density. Surprisingly, we show that this same model leads to exact low-dimensional factorizations of many real-world networks. We give a simple algorithm based on logistic principal component analysis (LPCA) that succeeds in finding such exact embeddings. Finally, we perform a large number of experiments that verify the ability of very low-dimensional embeddings to capture local structure in real-world networks."
neurips,https://proceedings.neurips.cc/paper/2020/file/995ca733e3657ff9f5f3c823d73371e1-Paper.pdf,Fictitious Play for Mean Field Games: Continuous Time Analysis and Applications,"Sarah Perrin, Julien Perolat, Mathieu Lauriere, Matthieu Geist, Romuald Elie, Olivier Pietquin","In this paper, we deepen the analysis of continuous time Fictitious Play learning algorithm to the consideration of various finite state Mean Field Game settings (finite horizon,
γ
γ
-discounted), allowing in particular for the introduction of an additional common noise. We first present a theoretical convergence analysis of the continuous time Fictitious Play process and prove that the induced exploitability decreases at a rate
O
(
1
t
)
O
. Such analysis emphasizes the use of exploitability as a relevant metric for evaluating the convergence towards a Nash equilibrium in the context of Mean Field Games. These theoretical contributions are supported by numerical experiments provided in either model-based or model-free settings. We provide hereby for the first time converging learning dynamics for Mean Field Games in the presence of common noise."
neurips,https://proceedings.neurips.cc/paper/2020/file/99607461cdb9c26e2bd5f31b12dcf27a-Paper.pdf,Steering Distortions to Preserve Classes and Neighbors in Supervised Dimensionality Reduction,"Benoît Colange, Jaakko Peltonen, Michael Aupetit, Denys Dutykh, Sylvain Lespinats",
neurips,https://proceedings.neurips.cc/paper/2020/file/999df4ce78b966de17aee1dc87111044-Paper.pdf,On Infinite-Width Hypernetworks,"Etai Littwin, Tomer Galanti, Lior Wolf, Greg Yang",
neurips,https://proceedings.neurips.cc/paper/2020/file/99ba5c4097c6b8fef5ed774a1a6714b8-Paper.pdf,Interferobot: aligning an optical interferometer by a reinforcement learning agent,"Dmitry Sorokin, Alexander Ulanov, Ekaterina Sazhina, Alexander Lvovsky",
neurips,https://proceedings.neurips.cc/paper/2020/file/99c83c904d0d64fbef50d919a5c66a80-Paper.pdf,Program Synthesis with Pragmatic Communication,"Yewen Pu, Kevin Ellis, Marta Kryven, Josh Tenenbaum, Armando Solar-Lezama",
neurips,https://proceedings.neurips.cc/paper/2020/file/99cad265a1768cc2dd013f0e740300ae-Paper.pdf,Principal Neighbourhood Aggregation for Graph Nets,"Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Liò, Petar Veličković",
neurips,https://proceedings.neurips.cc/paper/2020/file/99e314b1b43706773153e7ef375fc68c-Paper.pdf,Reliable Graph Neural Networks via Robust Aggregation,"Simon Geisler, Daniel Zügner, Stephan Günnemann",
neurips,https://proceedings.neurips.cc/paper/2020/file/99f6a934a7cf277f2eaece8e3ce619b2-Paper.pdf,Instance Selection for GANs,"Terrance DeVries, Michal Drozdzal, Graham W. Taylor",
neurips,https://proceedings.neurips.cc/paper/2020/file/9a02387b02ce7de2dac4b925892f68fb-Paper.pdf,Linear Disentangled Representations and Unsupervised Action Estimation,"Matthew Painter, Adam Prugel-Bennett, Jonathon Hare",
neurips,https://proceedings.neurips.cc/paper/2020/file/9a11883317fde3aef2e2432a58c86779-Paper.pdf,Video Frame Interpolation without Temporal Priors,"Youjian Zhang, Chaoyue Wang, Dacheng Tao",
neurips,https://proceedings.neurips.cc/paper/2020/file/9a32ef65c42085537062753ec435750f-Paper.pdf,Learning compositional functions via multiplicative weight updates,"Jeremy Bernstein, Jiawei Zhao, Markus Meister, Ming-Yu Liu, Anima Anandkumar, Yisong Yue",
neurips,https://proceedings.neurips.cc/paper/2020/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Paper.pdf,Sample Complexity of Uniform Convergence for Multicalibration,"Eliran Shabat, Lee Cohen, Yishay Mansour",
neurips,https://proceedings.neurips.cc/paper/2020/file/9a96a2c73c0d477ff2a6da3bf538f4f4-Paper.pdf,Differentiable Neural Architecture Search in Equivalent Space with Exploration Enhancement,"Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, Zongyuan Ge, Steven Su",
neurips,https://proceedings.neurips.cc/paper/2020/file/9ac1382fd8fc4b631594aa135d16ad75-Paper.pdf,The interplay between randomness and structure during learning in RNNs,"Friedrich Schuessler, Francesca Mastrogiuseppe, Alexis Dubreuil, Srdjan Ostojic, Omri Barak",
neurips,https://proceedings.neurips.cc/paper/2020/file/9afe487de556e59e6db6c862adfe25a4-Paper.pdf,A Generalized Neural Tangent Kernel Analysis for Two-layer Neural Networks,"Zixiang Chen, Yuan Cao, Quanquan Gu, Tong Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/9b10a919ddeb07e103dc05ff523afe38-Paper.pdf,Instance-wise Feature Grouping,"Aria Masoomi, Chieh Wu, Tingting Zhao, Zifeng Wang, Peter Castaldi, Jennifer Dy",
neurips,https://proceedings.neurips.cc/paper/2020/file/9b22a40256b079f338827b0ff1f4792b-Paper.pdf,Robust Disentanglement of a Few Factors at a Time using rPU-VAE,"Benjamin Estermann, Markus Marks, Mehmet Fatih Yanik",
neurips,https://proceedings.neurips.cc/paper/2020/file/9b3a9fb4db30fc6594ec3990cbc09932-Paper.pdf,PC-PG: Policy Cover Directed Exploration for Provable Policy Gradient Learning,"Alekh Agarwal, Mikael Henaff, Sham Kakade, Wen Sun",
neurips,https://proceedings.neurips.cc/paper/2020/file/9b72e31dac81715466cd580a448cf823-Paper.pdf,Group Contextual Encoding for 3D Point Clouds,"Xu Liu, Chengtao Li, Jian Wang, Jingbo Wang, Boxin Shi, Xiaodong He",
neurips,https://proceedings.neurips.cc/paper/2020/file/9b7c8d13e4b2f08895fb7bcead930b46-Paper.pdf,Latent Bandits Revisited,"Joey Hong, Branislav Kveton, Manzil Zaheer, Yinlam Chow, Amr Ahmed, Craig Boutilier",
neurips,https://proceedings.neurips.cc/paper/2020/file/9b8619251a19057cff70779273e95aa6-Paper.pdf,Is normalization indispensable for training deep neural network?,"Jie Shao, Kai Hu, Changhu Wang, Xiangyang Xue, Bhiksha Raj",
neurips,https://proceedings.neurips.cc/paper/2020/file/9b8b50fb590c590ffbf1295ce92258dc-Paper.pdf,Optimization and Generalization of Shallow Neural Networks with Quadratic Activation Functions,"Stefano Sarao Mannelli, Eric Vanden-Eijnden, Lenka Zdeborová",We consider a teacher-student scenario where the teacher has the same structure as the student with a hidden layer of smaller width m*<=m.
neurips,https://proceedings.neurips.cc/paper/2020/file/9bc99c590be3511b8d53741684ef574c-Paper.pdf,Intra Order-preserving Functions for Calibration of Multi-Class Neural Networks,"Amir Rahimi, Amirreza Shaban, Ching-An Cheng, Richard Hartley, Byron Boots",
neurips,https://proceedings.neurips.cc/paper/2020/file/9bde76f262285bb1eaeb7b40c758b53e-Paper.pdf,Linear Time Sinkhorn Divergences using Positive Features,"Meyer Scetbon, Marco Cuturi","Although Sinkhorn divergences are now routinely used in data sciences to compare probability distributions, the computational effort required to compute them remains expensive, growing in general quadratically in the size
n
n
of the support of these distributions. Indeed, solving optimal transport (OT) with an entropic regularization requires computing a
n
×
n
n
kernel matrix (the neg-exponential of a
n
×
n
n
pairwise ground cost matrix) that is repeatedly applied to a vector. We propose to use instead ground costs of the form
c
(
x
,
y
)
=
−
log
\dotp
φ
(
x
)
φ
(
y
)
c
where
φ
φ
is a map from the ground space onto the positive orthant
\RR
r
+
\RR
, with
r
≪
n
r
. This choice yields, equivalently, a kernel
k
(
x
,
y
)
=
\dotp
φ
(
x
)
φ
(
y
)
k
, and ensures that the cost of Sinkhorn iterations scales as
O
(
n
r
)
O
. We show that usual cost functions can be approximated using this form. Additionaly, we take advantage of the fact that our approach yields approximation that remain fully differentiable with respect to input distributions, as opposed to previously proposed adaptive low-rank approximations of the kernel matrix, to train a faster variant of OT-GAN~\cite{salimans2018improving}."
neurips,https://proceedings.neurips.cc/paper/2020/file/9c22c0b51b3202246463e986c7e205df-Paper.pdf,VarGrad: A Low-Variance Gradient Estimator for Variational Inference,"Lorenz Richter, Ayman Boustati, Nikolas Nüsken, Francisco Ruiz, Omer Deniz Akyildiz",
neurips,https://proceedings.neurips.cc/paper/2020/file/9c449771d0edc923c2713a7462cefa3b-Paper.pdf,A Convolutional Auto-Encoder for Haplotype Assembly and Viral Quasispecies Reconstruction,"Ziqi Ke, Haris Vikalo",
neurips,https://proceedings.neurips.cc/paper/2020/file/9cafd121ba982e6de30ffdf5ada9ce2e-Paper.pdf,Promoting Stochasticity for Expressive Policies via a Simple and Efficient Regularization Method,"Qi Zhou, Yufei Kuang, Zherui Qiu, Houqiang Li, Jie Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/9cd013fe250ebffc853b386569ab18c0-Paper.pdf,Adversarial Counterfactual Learning and Evaluation for Recommender System,"Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan",
neurips,https://proceedings.neurips.cc/paper/2020/file/9cd78264cf2cd821ba651485c111a29a-Paper.pdf,Memory-Efficient Learning of Stable Linear Dynamical Systems for Prediction and Control,"Giorgos ('Yorgos') Mamakoukas, Orest Xherija, Todd Murphey","Learning a stable Linear Dynamical System (LDS) from data involves creating models that both minimize reconstruction error and enforce stability of the learned representation. We propose a novel algorithm for learning stable LDSs. Using a recent characterization of stable matrices, we present an optimization method that ensures stability at every step and iteratively improves the reconstruction error using gradient directions derived in this paper. When applied to LDSs with inputs, our approach---in contrast to current methods for learning stable LDSs---updates both the state and control matrices, expanding the solution space and allowing for models with lower reconstruction error. We apply our algorithm in simulations and experiments to a variety of problems, including learning dynamic textures from image sequences and controlling a robotic manipulator. Compared to existing approaches, our proposed method achieves an \textit{orders-of-magnitude} improvement in reconstruction error and superior results in terms of control performance. In addition, it is \textit{provably} more memory efficient, with an
O
(
n
2
)
O
space complexity compared to
O
(
n
4
)
O
of competing alternatives, thus scaling to higher-dimensional systems when the other methods fail. The code of the proposed algorithm and animations of the results can be found at https://github.com/giorgosmamakoukas/MemoryEfficientStableLDS."
neurips,https://proceedings.neurips.cc/paper/2020/file/9d4c03631b8b0c85ae08bf05eda37d0f-Paper.pdf,Evolving Normalization-Activation Layers,"Hanxiao Liu, Andy Brock, Karen Simonyan, Quoc Le",
neurips,https://proceedings.neurips.cc/paper/2020/file/9d58963592071dbf38a0fa114269959c-Paper.pdf,ScaleCom: Scalable Sparsified Gradient Compression for Communication-Efficient Distributed Training,"Chia-Yu Chen, Jiamin Ni, Songtao Lu, Xiaodong Cui, Pin-Yu Chen, Xiao Sun, Naigang Wang, Swagath Venkataramani, Vijayalakshmi (Viji) Srinivasan, Wei Zhang, Kailash Gopalakrishnan",
neurips,https://proceedings.neurips.cc/paper/2020/file/9d684c589d67031a627ad33d59db65e5-Paper.pdf,RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder,"Cheng Chi, Fangyun Wei, Han Hu","Existing object detection frameworks are usually built on a single format of object/part representation, i.e., anchor/proposal rectangle boxes in RetinaNet and Faster R-CNN, center points in FCOS and RepPoints, and corner points in CornerNet. While these different representations usually drive the frameworks to perform well in different aspects, e.g., better classification or finer localization, it is in general difficult to combine these representations in a single framework to make good use of each strength, due to the heterogeneous or non-grid feature extraction by different representations. This paper presents an attention-based decoder module similar as that in Transformer~\cite{vaswani2017attention} to bridge other representations into a typical object detector built on a single representation format, in an end-to-end fashion. The other representations act as a set of \emph{key} instances to strengthen the main \emph{query} representation features in the vanilla detectors. Novel techniques are proposed towards efficient computation of the decoder module, including a \emph{key sampling} approach and a \emph{shared location embedding} approach. The proposed module is named \emph{bridging visual representations} (BVR). It can perform in-place and we demonstrate its broad effectiveness in bridging other representations into prevalent object detection frameworks, including RetinaNet, Faster R-CNN, FCOS and ATSS, where about
1.5
∼
3.0
1.5
AP improvements are achieved. In particular, we improve a state-of-the-art framework with a strong backbone by about
2.0
2.0
AP, reaching
52.7
52.7
AP on COCO test-dev. The resulting network is named RelationNet++. The code is available at \url{https://github.com/microsoft/RelationNet2}."
neurips,https://proceedings.neurips.cc/paper/2020/file/9d702ffd99ad9c70ac37e506facc8c38-Paper.pdf,Efficient Learning of Discrete Graphical Models,"Marc Vuffray, Sidhant Misra, Andrey Lokhov",
neurips,https://proceedings.neurips.cc/paper/2020/file/9d7311ba459f9e45ed746755a32dcd11-Paper.pdf,Near-Optimal SQ Lower Bounds for Agnostically Learning Halfspaces and ReLUs under Gaussian Marginals,"Ilias Diakonikolas, Daniel Kane, Nikos Zarifis","We study the fundamental problems of agnostically learning halfspaces and ReLUs under Gaussian marginals. In the former problem, given labeled examples
(
\bx
,
y
)
(
from an unknown distribution on
\R
d
×
{
±
1
}
\R
, whose marginal distribution on
\bx
\bx
is the standard Gaussian and the labels
y
y
can be arbitrary, the goal is to output a hypothesis with 0-1 loss
\opt
+
\eps
\opt
, where
\opt
\opt
is the 0-1 loss of the best-fitting halfspace. In the latter problem, given labeled examples
(
\bx
,
y
)
(
from an unknown distribution on
\R
d
×
\R
\R
, whose marginal distribution on
\bx
\bx
is the standard Gaussian and the labels
y
y
can be arbitrary, the goal is to output a hypothesis with square loss
\opt
+
\eps
\opt
, where
\opt
\opt
is the square loss of the best-fitting ReLU. We prove Statistical Query (SQ) lower bounds of
d
\poly
(
1
/
\eps
)
d
for both of these problems. Our SQ lower bounds provide strong evidence that current upper bounds for these tasks are essentially best possible."
neurips,https://proceedings.neurips.cc/paper/2020/file/9d740bd0f36aaa312c8d504e28c42163-Paper.pdf,Neurosymbolic Transformers for Multi-Agent Communication,"Jeevana Priya Inala, Yichen Yang, James Paulos, Yewen Pu, Osbert Bastani, Vijay Kumar, Martin Rinard, Armando Solar-Lezama",
neurips,https://proceedings.neurips.cc/paper/2020/file/9d752cb08ef466fc480fba981cfa44a1-Paper.pdf,Fairness in Streaming Submodular Maximization: Algorithms and Hardness,"Marwa El Halabi, Slobodan Mitrović, Ashkan Norouzi-Fard, Jakab Tardos, Jakub M. Tarnawski",
neurips,https://proceedings.neurips.cc/paper/2020/file/9d94c8981a48d12adfeecfe1ae6e0ec1-Paper.pdf,Smoothed Geometry for Robust Attribution,"Zifan Wang, Haofan Wang, Shakul Ramkumar, Piotr Mardziel, Matt Fredrikson, Anupam Datta",
neurips,https://proceedings.neurips.cc/paper/2020/file/9da187a7a191431db943a9a5a6fec6f4-Paper.pdf,Fast Adversarial Robustness Certification of Nearest Prototype Classifiers for Arbitrary Seminorms,"Sascha Saralajew, Lars Holdijk, Thomas Villmann",
neurips,https://proceedings.neurips.cc/paper/2020/file/9db6faeef387dc789777227a8bed4d52-Paper.pdf,Multi-agent active perception with prediction rewards,"Mikko Lauri, Frans Oliehoek",
neurips,https://proceedings.neurips.cc/paper/2020/file/9dd16e049becf4d5087c90a83fea403b-Paper.pdf,A Local Temporal Difference Code for Distributional Reinforcement Learning,"Pablo Tano, Peter Dayan, Alexandre Pouget",
neurips,https://proceedings.neurips.cc/paper/2020/file/9ddb9dd5d8aee9a76bf217a2a3c54833-Paper.pdf,Learning with Optimized Random Features: Exponential Speedup by Quantum Machine Learning without Sparsity and Low-Rank Assumptions,"Hayata Yamasaki, Sathyawageeswar Subramanian, Sho Sonoda, Masato Koashi",
neurips,https://proceedings.neurips.cc/paper/2020/file/9de6d14fff9806d4bcd1ef555be766cd-Paper.pdf,CaSPR: Learning Canonical Spatiotemporal Point Cloud Representations,"Davis Rempe, Tolga Birdal, Yongheng Zhao, Zan Gojcic, Srinath Sridhar, Leonidas J. Guibas",
neurips,https://proceedings.neurips.cc/paper/2020/file/9df81829c4ebc9c427b9afe0438dce5a-Paper.pdf,Deep Automodulators,"Ari Heljakka, Yuxin Hou, Juho Kannala, Arno Solin",
neurips,https://proceedings.neurips.cc/paper/2020/file/9e1a36515d6704d7eb7a30d783400e5d-Paper.pdf,Convolutional Tensor-Train LSTM for Spatio-Temporal Learning,"Jiahao Su, Wonmin Byeon, Jean Kossaifi, Furong Huang, Jan Kautz, Anima Anandkumar",
neurips,https://proceedings.neurips.cc/paper/2020/file/9e5f64cde99af96fdca0e02a3d24faec-Paper.pdf,The Potts-Ising model for discrete multivariate data,"Zahra Razaee, Arash Amini","Modeling dependencies in multivariate discrete data is a challenging problem, especially in high dimensions. The Potts model is a versatile such model, suitable when each coordinate is a categorical variable. However, the full Potts model has too many parameters to be accurately fit when the number of categories is large. We introduce a variation on the Potts model that allows for general categorical marginals and Ising-type multivariate dependence. This reduces the number of parameters from
Ω
(
d
2
K
2
)
Ω
in the full Potts model to
O
(
d
2
+
K
d
)
O
, where
K
K
is the number of categories and
d
d
is the dimension of the data. We show that the complexity of fitting this new Potts-Ising model is the same as that of an Ising model. In particular, adopting the neighborhood regression framework, the model can be fit by solving
d
d
separate logistic regressions. We demonstrate the ability of the model to capture multivariate dependencies by comparing with existing approaches."
neurips,https://proceedings.neurips.cc/paper/2020/file/9e9a30b74c49d07d8150c8c83b1ccf07-Paper.pdf,Interpretable multi-timescale models for predicting fMRI responses to continuous natural speech,"Shailee Jain, Vy Vo, Shivangi Mahto, Amanda LeBel, Javier S. Turek, Alexander Huth",
neurips,https://proceedings.neurips.cc/paper/2020/file/9ec0cfdc84044494e10582436e013e64-Paper.pdf,Group-Fair Online Allocation in Continuous Time,"Semih Cayci, Swati Gupta, Atilla Eryilmaz","The theory of discrete-time online learning has been successfully applied in many problems that involve sequential decision-making under uncertainty. However, in many applications including contractual hiring in online freelancing platforms and server allocation in cloud computing systems, the outcome of each action is observed only after a random and action-dependent time. Furthermore, as a consequence of certain ethical and economic concerns, the controller may impose deadlines on the completion of each task, and require fairness across different groups in the allocation of total time budget
B
B
. In order to address these applications, we consider continuous-time online learning problem with fairness considerations, and present a novel framework based on continuous-time utility maximization. We show that this formulation recovers reward-maximizing, max-min fair and proportionally fair allocation rules across different groups as special cases. We characterize the optimal offline policy, which allocates the total time between different actions in an optimally fair way (as defined by the utility function), and impose deadlines to maximize time-efficiency. In the absence of any statistical knowledge, we propose a novel online learning algorithm based on dual ascent optimization for time averages, and prove that it achieves
~
O
(
B
−
1
/
2
)
O
regret bound."
neurips,https://proceedings.neurips.cc/paper/2020/file/9ec51f6eb240fb631a35864e13737bca-Paper.pdf,Decentralized TD Tracking with Linear Function Approximation and its Finite-Time Analysis,"Gang Wang, Songtao Lu, Georgios Giannakis, Gerald Tesauro, Jian Sun",
neurips,https://proceedings.neurips.cc/paper/2020/file/9ecff5455677b38d19f49ce658ef0608-Paper.pdf,Understanding Gradient Clipping in Private SGD: A Geometric Perspective,"Xiangyi Chen, Steven Z. Wu, Mingyi Hong",
neurips,https://proceedings.neurips.cc/paper/2020/file/9ed27554c893b5bad850a422c3538c15-Paper.pdf,O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers,"Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, Sanjiv Kumar","Recently, Transformer networks have redefined the state of the art in many NLP tasks. However, these models suffer from quadratic computational cost in the input sequence length
n
n
to compute pairwise attention in each layer. This has prompted recent research into sparse Transformers that sparsify the connections in the attention layers. While empirically promising for long sequences, fundamental questions remain unanswered: Can sparse Transformers approximate any arbitrary sequence-to-sequence function, similar to their dense counterparts? How does the sparsity pattern and the sparsity level affect their performance? In this paper, we address these questions and provide a unifying framework that captures existing sparse attention models. We propose sufficient conditions under which we prove that a sparse attention model can universally approximate any sequence-to-sequence function. Surprisingly, our results show that sparse Transformers with only
O
(
n
)
O
connections per attention layer can approximate the same function class as the dense model with
n
2
n
connections. Lastly, we present experiments comparing different patterns/levels of sparsity on standard NLP tasks."
neurips,https://proceedings.neurips.cc/paper/2020/file/9eed867b73ab1eab60583c9d4a789b1b-Paper.pdf,Identifying signal and noise structure in neural population activity with Gaussian process factor models,"Stephen Keeley, Mikio Aoi, Yiyi Yu, Spencer Smith, Jonathan W. Pillow",
neurips,https://proceedings.neurips.cc/paper/2020/file/9efb1a59d7b58e69996cf0e32cb71098-Paper.pdf,Equivariant Networks for Hierarchical Structures,"Renhao Wang, Marjan Albooyeh, Siamak Ravanbakhsh",
neurips,https://proceedings.neurips.cc/paper/2020/file/9f067d8d6df2d4b8c64fb4c084d6c208-Paper.pdf,"MinMax Methods for Optimal Transport and Beyond: Regularization, Approximation and Numerics","Luca De Gennaro Aquino, Stephan Eckstein",
neurips,https://proceedings.neurips.cc/paper/2020/file/9f1d5659d5880fb427f6e04ae500fc25-Paper.pdf,A Discrete Variational Recurrent Topic Model without the Reparametrization Trick,"Mehdi Rezaee, Francis Ferraro",
neurips,https://proceedings.neurips.cc/paper/2020/file/9f29450d2eb58feb555078bdefe28aa5-Paper.pdf,Transferable Graph Optimizers for ML Compilers,"Yanqi Zhou, Sudip Roy, Amirali Abdolrashidi, Daniel Wong, Peter Ma, Qiumin Xu, Hanxiao Liu, Phitchaya Phothilimtha, Shen Wang, Anna Goldie, Azalia Mirhoseini, James Laudon",
neurips,https://proceedings.neurips.cc/paper/2020/file/9f319422ca17b1082ea49820353f14ab-Paper.pdf,Learning with Operator-valued Kernels in Reproducing Kernel Krein Spaces,"Akash Saha, Balamurugan Palaniappan",
neurips,https://proceedings.neurips.cc/paper/2020/file/9f60ab2b55468f104055b16df8f69e81-Paper.pdf,Learning Bounds for Risk-sensitive Learning,"Jaeho Lee, Sejun Park, Jinwoo Shin",
neurips,https://proceedings.neurips.cc/paper/2020/file/9f655cc8884fda7ad6d8a6fb15cc001e-Paper.pdf,Simplifying Hamiltonian and Lagrangian Neural Networks via Explicit Constraints,"Marc Finzi, Ke Alexander Wang, Andrew G. Wilson","Reasoning about the physical world requires models that are endowed with the right inductive biases to learn the underlying dynamics. Recent works improve generalization for predicting trajectories by learning the Hamiltonian or Lagrangian of a system rather than the differential equations directly. While these methods encode the constraints of the systems using generalized coordinates, we show that embedding the system into Cartesian coordinates and enforcing the constraints explicitly with Lagrange multipliers dramatically simplifies the learning problem. We introduce a series of challenging chaotic and extended-body systems, including systems with
N
N
-pendulums, spring coupling, magnetic fields, rigid rotors, and gyroscopes, to push the limits of current approaches. Our experiments show that Cartesian coordinates with explicit constraints lead to a 100x improvement in accuracy and data efficiency."
neurips,https://proceedings.neurips.cc/paper/2020/file/9f6992966d4c363ea0162a056cb45fe5-Paper.pdf,Beyond accuracy: quantifying trial-by-trial behaviour of CNNs and humans by measuring error consistency,"Robert Geirhos, Kristof Meding, Felix A. Wichmann",
neurips,https://proceedings.neurips.cc/paper/2020/file/9fa04f87c9138de23e92582b4ce549ec-Paper.pdf,Provably Efficient Reinforcement Learning with Kernel and Neural Function Approximations,"Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, Michael Jordan","Reinforcement learning (RL) algorithms combined with modern function approximators such as kernel functions and deep neural networks have achieved significant empirical successes in large-scale application problems with a massive number of states. From a theoretical perspective, however, RL with functional approximation poses a fundamental challenge to developing algorithms with provable computational and statistical efficiency, due to the need to take into consideration both the exploration-exploitation tradeoff that is inherent in RL and the bias-variance tradeoff that is innate in statistical estimation. To address such a challenge, focusing on the episodic setting where the action-value functions are represented by a kernel function or over-parametrized neural network, we propose the first provable RL algorithm with both polynomial runtime and sample complexity, without additional assumptions on the data-generating model. In particular, for both the kernel and neural settings, we prove that an optimistic modification of the least-squares value iteration algorithm incurs an
~
O
(
δ
\cF
H
2
√
T
)
O
regret, where
δ
\cF
δ
characterizes the intrinsic complexity of the function class
\cF
\cF
,
H
H
is the length of each episode, and
T
T
is the total number of episodes. Our regret bounds are independent of the number of states and therefore even allows it to diverge, which exhibits the benefit of function approximation."
neurips,https://proceedings.neurips.cc/paper/2020/file/9fa83fec3cf3810e5680ed45f7124dce-Paper.pdf,Constant-Expansion Suffices for Compressed Sensing with Generative Priors,"Constantinos Daskalakis, Dhruv Rohatgi, Emmanouil Zampetakis",
neurips,https://proceedings.neurips.cc/paper/2020/file/9fe8593a8a330607d76796b35c64c600-Paper.pdf,RANet: Region Attention Network for Semantic Segmentation,"Dingguo Shen, Yuanfeng Ji, Ping Li, Yi Wang, Di Lin",
neurips,https://proceedings.neurips.cc/paper/2020/file/a03fa30821986dff10fc66647c84c9c3-Paper.pdf,"A random matrix analysis of random Fourier features: beyond the Gaussian kernel, a precise phase transition, and the corresponding double descent","Zhenyu Liao, Romain Couillet, Michael W. Mahoney","This article characterizes the exact asymptotics of random Fourier feature (RFF) regression, in the realistic setting where the number of data samples
n
n
, their dimension
p
p
, and the dimension of feature space
N
N
are all large and comparable. In this regime, the random RFF Gram matrix no longer converges to the well-known limiting Gaussian kernel matrix (as it does when
N
→
∞
N
alone), but it still has a tractable behavior that is captured by our analysis. This analysis also provides accurate estimates of training and test regression errors for large
n
,
p
,
N
n
. Based on these estimates, a precise characterization of two qualitatively different phases of learning, including the phase transition between them, is provided; and the corresponding double descent test error curve is derived from this phase transition behavior. These results do not depend on strong assumptions on the data distribution, and they perfectly match empirical results on real-world data sets."
neurips,https://proceedings.neurips.cc/paper/2020/file/a03fec24df877cc65c037673397ad5c0-Paper.pdf,Learning sparse codes from compressed representations with biologically plausible local wiring constraints,"Kion Fallah, Adam Willats, Ninghao Liu, Christopher Rozell",
neurips,https://proceedings.neurips.cc/paper/2020/file/a0443c8c8c3372d662e9173c18faaa2c-Paper.pdf,Self-Imitation Learning via Generalized Lower Bound Q-learning,Yunhao Tang,
neurips,https://proceedings.neurips.cc/paper/2020/file/a08e32d2f9a8b78894d964ec7fd4172e-Paper.pdf,Private Learning of Halfspaces: Simplifying the Construction and Reducing the Sample Complexity,"Haim Kaplan, Yishay Mansour, Uri Stemmer, Eliad Tsfadia","We present a differentially private learner for halfspaces over a finite grid
G
G
in
\R
d
\R
with sample complexity
≈
d
2.5
⋅
2
log
∗
|
G
|
≈
, which improves the state-of-the-art result of [Beimel et al., COLT 2019] by a
d
2
d
factor. The building block for our learner is a new differentially private algorithm for approximately solving the linear feasibility problem: Given a feasible collection of
m
m
linear constraints of the form
A
x
≥
b
A
, the task is to {\em privately} identify a solution
x
x
that satisfies {\em most} of the constraints. Our algorithm is iterative, where each iteration determines the next coordinate of the constructed solution
x
x
."
neurips,https://proceedings.neurips.cc/paper/2020/file/a09e75c5c86a7bf6582d2b4d75aad615-Paper.pdf,Directional Pruning of Deep Neural Networks,"Shih-Kang Chao, Zhanyu Wang, Yue Xing, Guang Cheng","In the light of the fact that the stochastic gradient descent (SGD) often finds a flat minimum valley in the training loss, we propose a novel directional pruning method which searches for a sparse minimizer in or close to that flat region. The proposed pruning method does not require retraining or the expert knowledge on the sparsity level. To overcome the computational formidability of estimating the flat directions, we propose to use a carefully tuned
ℓ
1
ℓ
proximal gradient algorithm which can provably achieve the directional pruning with a small learning rate after sufficient training. The empirical results demonstrate the promising results of our solution in highly sparse regime (92% sparsity) among many existing pruning methods on the ResNet50 with the ImageNet, while using only a slightly higher wall time and memory footprint than the SGD. Using the VGG16 and the wide ResNet 28x10 on the CIFAR-10 and CIFAR-100, we demonstrate that our solution reaches the same minima valley as the SGD, and the minima found by our solution and the SGD do not deviate in directions that impact the training loss. The code that reproduces the results of this paper is available at https://github.com/donlan2710/gRDA-Optimizer/tree/master/directional_pruning."
neurips,https://proceedings.neurips.cc/paper/2020/file/a0dc078ca0d99b5ebb465a9f1cad54ba-Paper.pdf,Smoothly Bounding User Contributions in Differential Privacy,"Alessandro Epasto, Mohammad Mahdian, Jieming Mao, Vahab Mirrokni, Lijie Ren","For a better trade-off between utility and privacy guarantee, we propose a method which smoothly bounds user contributions by setting appropriate weights on data points and apply it to estimating the mean/quantiles, linear regression, and empirical risk minimization. We show that our algorithm provably outperforms the sample limiting algorithm. We conclude with experimental evaluations which validate our theoretical results."
neurips,https://proceedings.neurips.cc/paper/2020/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf,Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping,"Minjia Zhang, Yuxiong He","In this work, we propose a method based on progressive layer dropping that speeds the training of Transformer-based language models, not at the cost of excessive hardware resources but from model architecture change and training technique boosted efficiency. Extensive experiments on BERT show that the proposed method achieves a 25% reduction of computation cost in FLOPS and a 24% reduction in the end-to-end wall-clock training time. Furthermore, we show that our pre-trained models are equipped with strong knowledge transferability, achieving similar or even higher accuracy in downstream tasks to baseline models."
neurips,https://proceedings.neurips.cc/paper/2020/file/a18aa23ee676d7f5ffb34cf16df3e08c-Paper.pdf,Online Planning with Lookahead Policies,"Yonathan Efroni, Mohammad Ghavamzadeh, Shie Mannor","Real Time Dynamic Programming (RTDP) is an online algorithm based on Dynamic Programming (DP) that acts by 1-step greedy planning. Unlike DP, RTDP does not require access to the entire state space, i.e., it explicitly handles the exploration. This fact makes RTDP particularly appealing when the state space is large and it is not possible to update all states simultaneously. In this we devise a multi-step greedy RTDP algorithm, which we call
h
h
-RTDP, that replaces the 1-step greedy policy with a
h
h
-step lookahead policy. We analyze
h
h
-RTDP in its exact form and establish that increasing the lookahead horizon,
h
h
, results in an improved sample complexity, with the cost of additional computations. This is the first work that proves improved sample complexity as a result of {\em increasing} the lookahead horizon in online planning. We then analyze the performance of
h
h
-RTDP in three approximate settings: approximate model, approximate value updates, and approximate state representation. For these cases, we prove that the asymptotic performance of
h
h
-RTDP remains the same as that of a corresponding approximate DP algorithm, the best one can hope for without further assumptions on the approximation errors."
neurips,https://proceedings.neurips.cc/paper/2020/file/a19883fca95d0e5ec7ee6c94c6c32028-Paper.pdf,Learning Deep Attribution Priors Based On Prior Knowledge,"Ethan Weinberger, Joseph Janizek, Su-In Lee",
neurips,https://proceedings.neurips.cc/paper/2020/file/a1ada9947e0d683b4625f94c74104d73-Paper.pdf,Using noise to probe recurrent neural network structure and prune synapses,"Eli Moore, Rishidev Chaudhuri",
neurips,https://proceedings.neurips.cc/paper/2020/file/a1c3ae6c49a89d92aef2d423dadb477f-Paper.pdf,NanoFlow: Scalable Normalizing Flows with Sublinear Parameter Complexity,"Sang-gil Lee, Sungwon Kim, Sungroh Yoon",
neurips,https://proceedings.neurips.cc/paper/2020/file/a1d4c20b182ad7137ab3606f0e3fc8a4-Paper.pdf,Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge,"Chaoyang He, Murali Annavaram, Salman Avestimehr",
neurips,https://proceedings.neurips.cc/paper/2020/file/a23156abfd4a114c35b930b836064e8b-Paper.pdf,Neural FFTs for Universal Texture Image Synthesis,"Morteza Mardani, Guilin Liu, Aysegul Dundar, Shiqiu Liu, Andrew Tao, Bryan Catanzaro",
neurips,https://proceedings.neurips.cc/paper/2020/file/a26398dca6f47b49876cbaffbc9954f9-Paper.pdf,Graph Cross Networks with Vertex Infomax Pooling,"Maosen Li, Siheng Chen, Ya Zhang, Ivor Tsang",
neurips,https://proceedings.neurips.cc/paper/2020/file/a267f936e54d7c10a2bb70dbe6ad7a89-Paper.pdf,Instance-optimality in differential privacy via approximate inverse sensitivity mechanisms,"Hilal Asi, John C. Duchi",
neurips,https://proceedings.neurips.cc/paper/2020/file/a2f04745390fd6897d09772b2cd1f581-Paper.pdf,Calibration of Shared Equilibria in General Sum Partially Observable Markov Games,"Nelson Vadori, Sumitra Ganesh, Prashant Reddy, Manuela Veloso",
neurips,https://proceedings.neurips.cc/paper/2020/file/a322852ce0df73e204b7e67cbbef0d0a-Paper.pdf,MOPO: Model-based Offline Policy Optimization,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y. Zou, Sergey Levine, Chelsea Finn, Tengyu Ma",
neurips,https://proceedings.neurips.cc/paper/2020/file/a32d7eeaae19821fd9ce317f3ce952a7-Paper.pdf,Building powerful and equivariant graph neural networks with structural message-passing,"Clément Vignac, Andreas Loukas, Pascal Frossard",
neurips,https://proceedings.neurips.cc/paper/2020/file/a36b598abb934e4528412e5a2127b931-Paper.pdf,Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning,"Sebastian Curi, Felix Berkenkamp, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2020/file/a376802c0811f1b9088828288eb0d3f0-Paper.pdf,Practical Low-Rank Communication Compression in Decentralized Deep Learning,"Thijs Vogels, Sai Praneeth Karimireddy, Martin Jaggi",
neurips,https://proceedings.neurips.cc/paper/2020/file/a378383b89e6719e15cd1aa45478627c-Paper.pdf,Mutual exclusivity as a challenge for deep neural networks,"Kanishk Gandhi, Brenden M. Lake",
neurips,https://proceedings.neurips.cc/paper/2020/file/a3842ed7b3d0fe3ac263bcabd2999790-Paper.pdf,3D Shape Reconstruction from Vision and Touch,"Edward Smith, Roberto Calandra, Adriana Romero, Georgia Gkioxari, David Meger, Jitendra Malik, Michal Drozdzal",
neurips,https://proceedings.neurips.cc/paper/2020/file/a3a3e8b30dd6eadfc78c77bb2b8e6b60-Paper.pdf,GradAug: A New Regularization Method for Deep Neural Networks,"Taojiannan Yang, Sijie Zhu, Chen Chen",
neurips,https://proceedings.neurips.cc/paper/2020/file/a3bf6e4db673b6449c2f7d13ee6ec9c0-Paper.pdf,An Equivalence between Loss Functions and Non-Uniform Sampling in Experience Replay,"Scott Fujimoto, David Meger, Doina Precup",
neurips,https://proceedings.neurips.cc/paper/2020/file/a3c788c57e423fa9c177544a4d5d1239-Paper.pdf,Learning Utilities and Equilibria in Non-Truthful Auctions,"Hu Fu, Tao Lin","In non-truthful auctions, agents' utility for a strategy depends on the strategies of the opponents and also the prior distribution over their private types; the set of Bayes Nash equilibria generally has an intricate dependence on the prior. Using the First Price Auction as our main demonstrating example, we show that
~
O
(
n
/
ϵ
2
)
O
samples from the prior with
n
n
agents suffice for an algorithm to learn the interim utilities for all monotone bidding strategies. As a consequence, this number of samples suffice for learning all approximate equilibria. We give almost matching (up to polylog factors) lower bound on the sample complexity for learning utilities. We also consider a setting where agents must pay a search cost to discover their own types. Drawing on a connection between this setting and the first price auction, discovered recently by Kleinberg et al. (2016), we show that
~
O
(
n
/
ϵ
2
)
O
samples suffice for utilities and equilibria to be estimated in a near welfare-optimal descending auction in this setting. En route, we improve the sample complexity bound, recently obtained by Guo et al. (2019), for the Pandora's Box problem, which is a classical model for sequential consumer search."
neurips,https://proceedings.neurips.cc/paper/2020/file/a3f390d88e4c41f2747bfa2f1b5f87db-Paper.pdf,Rational neural networks,"Nicolas Boulle, Yuji Nakatsukasa, Alex Townsend",
neurips,https://proceedings.neurips.cc/paper/2020/file/a42a596fc71e17828440030074d15e74-Paper.pdf,DISK: Learning local features with policy gradient,"Michał Tyszkiewicz, Pascal Fua, Eduard Trulls",
neurips,https://proceedings.neurips.cc/paper/2020/file/a4a83056b58ff983d12c72bb17996243-Paper.pdf,"Transfer Learning via
ℓ
1
ℓ
Regularization","Masaaki Takada, Hironori Fujisawa","Machine learning algorithms typically require abundant data under a stationary environment. However, environments are nonstationary in many real-world applications. Critical issues lie in how to effectively adapt models under an ever-changing environment. We propose a method for transferring knowledge from a source domain to a target domain via
ℓ
1
ℓ
regularization in high dimension. We incorporate
ℓ
1
ℓ
regularization of differences between source and target parameters in addition to an ordinary
ℓ
1
ℓ
regularization. Hence, our method yields sparsity for both the estimates themselves and changes of the estimates. The proposed method has a tight estimation error bound under a stationary environment, and the estimate remains unchanged from the source estimate under small residuals. Moreover, the estimate is consistent with the underlying function, even when the source estimate is mistaken due to nonstationarity. Empirical results demonstrate that the proposed method effectively balances stability and plasticity."
neurips,https://proceedings.neurips.cc/paper/2020/file/a4a8a31750a23de2da88ef6a491dfd5c-Paper.pdf,GOCor: Bringing Globally Optimized Correspondence Volumes into Your Neural Network,"Prune Truong, Martin Danelljan, Luc V. Gool, Radu Timofte",
neurips,https://proceedings.neurips.cc/paper/2020/file/a4c42bfd5f5130ddf96e34a036c75e0a-Paper.pdf,Deep Inverse Q-learning with Constraints,"Gabriel Kalweit, Maria Huegle, Moritz Werling, Joschka Boedecker",
neurips,https://proceedings.neurips.cc/paper/2020/file/a4df48d0b71376788fee0b92746fd7d5-Paper.pdf,Optimistic Dual Extrapolation for Coherent Non-monotone Variational Inequalities,"Chaobing Song, Zhengyuan Zhou, Yichao Zhou, Yong Jiang, Yi Ma","The optimization problems associated with training generative adversarial neural networks can be largely reduced to certain {\em non-monotone} variational inequality problems (VIPs), whereas existing convergence results are mostly based on monotone or strongly monotone assumptions. In this paper, we propose {\em optimistic dual extrapolation (OptDE)}, a method that only performs {\em one} gradient evaluation per iteration. We show that OptDE is provably convergent to {\em a strong solution} under different coherent non-monotone assumptions. In particular, when a {\em weak solution} exists, the convergence rate of our method is
O
(
1
/
ϵ
2
)
O
, which matches the best existing result of the methods with two gradient evaluations. Further, when a {\em
σ
σ
-weak solution} exists, the convergence guarantee is improved to the linear rate
O
(
log
1
ϵ
)
O
. Along the way--as a byproduct of our inquiries into non-monotone variational inequalities--we provide the near-optimal
O
(
1
ϵ
log
1
ϵ
)
O
convergence guarantee in terms of restricted strong merit function for monotone variational inequalities. We also show how our results can be naturally generalized to the stochastic setting, and obtain corresponding new convergence results. Taken together, our results contribute to the broad landscape of variational inequality--both non-monotone and monotone alike--by providing a novel and more practical algorithm with the state-of-the-art convergence guarantees."
neurips,https://proceedings.neurips.cc/paper/2020/file/a512294422de868f8474d22344636f16-Paper.pdf,Prediction with Corrupted Expert Advice,"Idan Amir, Idan Attias, Tomer Koren, Yishay Mansour, Roi Livni",
neurips,https://proceedings.neurips.cc/paper/2020/file/a516a87cfcaef229b342c437fe2b95f7-Paper.pdf,Human Parsing Based Texture Transfer from Single Image to 3D Human via Cross-View Consistency,"Fang Zhao, Shengcai Liao, Kaihao Zhang, Ling Shao",
neurips,https://proceedings.neurips.cc/paper/2020/file/a51fb975227d6640e4fe47854476d133-Paper.pdf,Knowledge Augmented Deep Neural Networks for Joint Facial Expression and Action Unit Recognition,"Zijun Cui, Tengfei Song, Yuru Wang, Qiang Ji",
neurips,https://proceedings.neurips.cc/paper/2020/file/a5481cd6d7517aa3fc6476dc7d9019ab-Paper.pdf,Point process models for sequence detection in high-dimensional neural spike trains,"Alex Williams, Anthony Degleris, Yixin Wang, Scott Linderman",
neurips,https://proceedings.neurips.cc/paper/2020/file/a554f89dd61cabd2ff833d3468e2008a-Paper.pdf,Adversarial Attacks on Linear Contextual Bandits,"Evrard Garcelon, Baptiste Roziere, Laurent Meunier, Jean Tarbouriech, Olivier Teytaud, Alessandro Lazaric, Matteo Pirotta",
neurips,https://proceedings.neurips.cc/paper/2020/file/a5585a4d4b12277fee5cad0880611bc6-Paper.pdf,Meta-Consolidation for Continual Learning,"Joseph K J, Vineeth N Balasubramanian","We assume that weights of a neural network, for solving task, come from a meta-distribution. This meta-distribution is learned and consolidated incrementally. We operate in the challenging online continual learning setting, where a data point is seen by the model only once."
neurips,https://proceedings.neurips.cc/paper/2020/file/a576eafbce762079f7d1f77fca1c5cc2-Paper.pdf,Organizing recurrent network dynamics by task-computation to enable continual learning,"Lea Duncker, Laura Driscoll, Krishna V. Shenoy, Maneesh Sahani, David Sussillo",
neurips,https://proceedings.neurips.cc/paper/2020/file/a58149d355f02887dfbe55ebb2b64ba3-Paper.pdf,Lifelong Policy Gradient Learning of Factored Policies for Faster Training Without Forgetting,"Jorge Mendez, Boyu Wang, Eric Eaton",
neurips,https://proceedings.neurips.cc/paper/2020/file/a59afb1b7d82ec353921a55c579ee26d-Paper.pdf,Kernel Methods Through the Roof: Handling Billions of Points Efficiently,"Giacomo Meanti, Luigi Carratino, Lorenzo Rosasco, Alessandro Rudi",
neurips,https://proceedings.neurips.cc/paper/2020/file/a5bad363fc47f424ddf5091c8471480a-Paper.pdf,Spike and slab variational Bayes for high dimensional logistic regression,"Kolyan Ray, Botond Szabo, Gabriel Clara","Variational Bayes (VB) is a popular scalable alternative to Markov chain Monte Carlo for Bayesian inference. We study a mean-field spike and slab VB approximation of widely used Bayesian model selection priors in sparse high-dimensional logistic regression. We provide non-asymptotic theoretical guarantees for the VB posterior in both
ℓ
2
ℓ
and prediction loss for a sparse truth, giving optimal (minimax) convergence rates. Since the VB algorithm does not depend on the unknown truth to achieve optimality, our results shed light on effective prior choices. We confirm the improved performance of our VB algorithm over common sparse VB approaches in a numerical study."
neurips,https://proceedings.neurips.cc/paper/2020/file/a5bfc9e07964f8dddeb95fc584cd965d-Paper.pdf,Maximum-Entropy Adversarial Data Augmentation for Improved Generalization and Robustness,"Long Zhao, Ting Liu, Xi Peng, Dimitris Metaxas",
neurips,https://proceedings.neurips.cc/paper/2020/file/a6292668b36ef412fa3c4102d1311a62-Paper.pdf,Fast geometric learning with symbolic matrices,"Jean Feydy, Alexis Glaunès, Benjamin Charlier, Michael Bronstein",
neurips,https://proceedings.neurips.cc/paper/2020/file/a64bd53139f71961c5c31a9af03d775e-Paper.pdf,MESA: Boost Ensemble Imbalanced Learning with MEta-SAmpler,"Zhining Liu, Pengfei Wei, Jing Jiang, Wei Cao, Jiang Bian, Yi Chang",
neurips,https://proceedings.neurips.cc/paper/2020/file/a684eceee76fc522773286a895bc8436-Paper.pdf,CoinPress: Practical Private Mean and Covariance Estimation,"Sourav Biswas, Yihe Dong, Gautam Kamath, Jonathan Ullman",
neurips,https://proceedings.neurips.cc/paper/2020/file/a6a767bbb2e3513233f942e0ff24272c-Paper.pdf,Planning with General Objective Functions: Going Beyond Total Rewards,"Ruosong Wang, Peilin Zhong, Simon S. Du, Russ R. Salakhutdinov, Lin Yang","Standard sequential decision-making paradigms aim to maximize the cumulative reward when interacting with the unknown environment., i.e., maximize
∑
H
h
=
1
r
h
∑
where
H
H
is the planning horizon. However, this paradigm fails to model important practical applications, e.g., safe control that aims to maximize the lowest reward, i.e., maximize
min
H
h
=
1
r
h
min
. In this paper, based on techniques in sketching algorithms, we propose a novel planning algorithm in deterministic systems which deals with a large class of objective functions of the form
f
(
r
1
,
r
2
,
.
.
.
r
H
)
f
that are of interest to practical applications. We show that efficient planning is possible if
f
f
is symmetric under permutation of coordinates and satisfies certain technical conditions. Complementing our algorithm, we further prove that removing any of the conditions will make the problem intractable in the worst case and thus demonstrate the necessity of our conditions."
neurips,https://proceedings.neurips.cc/paper/2020/file/a6b964c0bb675116a15ef1325b01ff45-Paper.pdf,Scattering GCN: Overcoming Oversmoothness in Graph Convolutional Networks,"Yimeng Min, Frederik Wenkel, Guy Wolf",
neurips,https://proceedings.neurips.cc/paper/2020/file/a6d259bfbfa2062843ef543e21d7ec8e-Paper.pdf,"KFC: A Scalable Approximation Algorithm for
k
k
−center Fair Clustering","Elfarouk Harb, Ho Shan Lam","In this paper, we study the problem of fair clustering on the
k
−
k
center objective. In fair clustering, the input is
N
N
points, each belonging to at least one of
l
l
protected groups, e.g. male, female, Asian, Hispanic. The objective is to cluster the
N
N
points into
k
k
clusters to minimize a classical clustering objective function. However, there is an additional constraint that each cluster needs to be fair, under some notion of fairness. This ensures that no group is either
over-represented'' or
under-represented'' in any cluster. Our work builds on the work of Chierichetti et al. (NIPS 2017), Bera et al. (NeurIPS 2019), Ahmadian et al. (KDD 2019), and Bercea et al. (APPROX 2019). We obtain a randomized
3
−
3
approximation algorithm for the
k
−
k
center objective function, beating the previous state of the art (
4
−
4
approximation). We test our algorithm on real datasets, and show that our algorithm is effective in finding good clusters without over-representation or under-representation, surpassing the current state of the art in runtime speed, clustering cost, while achieving similar fairness violations."
neurips,https://proceedings.neurips.cc/paper/2020/file/a6e4f250fb5c56aaf215a236c64e5b0a-Paper.pdf,Leveraging Predictions in Smoothed Online Convex Optimization via Gradient-based Algorithms,"Yingying Li, Na Li","We consider online convex optimization with time-varying stage costs and additional switching costs. Since the switching costs introduce coupling across all stages, multi-step-ahead (long-term) predictions are incorporated to improve the online performance. However, longer-term predictions tend to suffer from lower quality. Thus, a critical question is: how to reduce the impact of long-term prediction errors on the online performance? To address this question, we introduce a gradient-based online algorithm, Receding Horizon Inexact Gradient (RHIG), and analyze its performance by dynamic regrets in terms of the temporal variation of the environment and the prediction errors. RHIG only considers at most
W
W
-step-ahead predictions to avoid being misled by worse predictions in the longer term. The optimal choice of
W
W
suggested by our regret bounds depends on the tradeoff between the variation of the environment and the prediction accuracy. Additionally, we apply RHIG to a well-established stochastic prediction error model and provide expected regret and concentration bounds under correlated prediction errors. Lastly, we numerically test the performance of RHIG on quadrotor tracking problems."
neurips,https://proceedings.neurips.cc/paper/2020/file/a70145bf8b173e4496b554ce57969e24-Paper.pdf,Learning the Linear Quadratic Regulator from Nonlinear Observations,"Zakaria Mhammedi, Dylan J. Foster, Max Simchowitz, Dipendra Misra, Wen Sun, Akshay Krishnamurthy, Alexander Rakhlin, John Langford",
neurips,https://proceedings.neurips.cc/paper/2020/file/a7453a5f026fb6831d68bdc9cb0edcae-Paper.pdf,Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate,"Zhiyuan Li, Kaifeng Lyu, Sanjeev Arora","Recent works (e.g., (Li \& Arora, 2020)) suggest that the use of popular normalization schemes (including Batch Normalization) in today's deep learning can move it far from a traditional optimization viewpoint, e.g., use of exponentially increasing learning rates. The current paper highlights other ways in which behavior of normalized nets departs from traditional viewpoints, and then initiates a formal framework for studying their mathematics via suitable adaptation of the conventional framework namely, modeling SGD-induced training trajectory via a suitable stochastic differential equation (SDE) with a noise term that captures gradient noise. This yields: (a) A new \textquotedblleft intrinsic learning rate\textquotedblright\ parameter that is the product of the normal learning rate
η
η
and weight decay factor
λ
λ
. Analysis of the SDE shows how the effective speed of learning varies and equilibrates over time under the control of intrinsic LR. (b) A challenge---via theory and experiments---to popular belief that good generalization requires large learning rates at the start of training. (c) New experiments, backed by mathematical intuition, suggesting the number of steps to equilibrium (in function space) scales as the inverse of the intrinsic learning rate, as opposed to the exponential time convergence bound implied by SDE analysis. We name it the \emph{Fast Equilibrium Conjecture} and suggest it holds the key to why Batch Normalization is effective."
neurips,https://proceedings.neurips.cc/paper/2020/file/a7789ef88d599b8df86bbee632b2994d-Paper.pdf,Scalable Graph Neural Networks via Bidirectional Propagation,"Ming Chen, Zhewei Wei, Bolin Ding, Yaliang Li, Ye Yuan, Xiaoyong Du, Ji-Rong Wen",
neurips,https://proceedings.neurips.cc/paper/2020/file/a7968b4339a1b85b7dbdb362dc44f9c4-Paper.pdf,Distribution Aligning Refinery of Pseudo-label for Imbalanced Semi-supervised Learning,"Jaehyung Kim, Youngbum Hur, Sejun Park, Eunho Yang, Sung Ju Hwang, Jinwoo Shin",
neurips,https://proceedings.neurips.cc/paper/2020/file/a7b23e6eefbe6cf04b8e62a6f0915550-Paper.pdf,Assisted Learning: A Framework for Multi-Organization Learning,"Xun Xian, Xinran Wang, Jie Ding, Reza Ghanadan",
neurips,https://proceedings.neurips.cc/paper/2020/file/a7d8ae4569120b5bec12e7b6e9648b86-Paper.pdf,The Strong Screening Rule for SLOPE,"Johan Larsson, Malgorzata Bogdan, Jonas Wallin",
neurips,https://proceedings.neurips.cc/paper/2020/file/a7da6ba0505a41b98bd85907244c4c30-Paper.pdf,STLnet: Signal Temporal Logic Enforced Multivariate Recurrent Neural Networks,"Meiyi Ma, Ji Gao, Lu Feng, John Stankovic",
neurips,https://proceedings.neurips.cc/paper/2020/file/a7f0d2b95c60161b3f3c82f764b1d1c9-Paper.pdf,Election Coding for Distributed Learning: Protecting SignSGD against Byzantine Attacks,"Jy-yong Sohn, Dong-Jun Han, Beongjun Choi, Jaekyun Moon",
neurips,https://proceedings.neurips.cc/paper/2020/file/a822554e5403b1d370db84cfbc530503-Paper.pdf,Reducing Adversarially Robust Learning to Non-Robust PAC Learning,"Omar Montasser, Steve Hanneke, Nati Srebro",
neurips,https://proceedings.neurips.cc/paper/2020/file/a851bd0d418b13310dd1e5e3ac7318ab-Paper.pdf,Top-k Training of GANs: Improving GAN Performance by Throwing Away Bad Samples,"Samarth Sinha, Zhengli Zhao, Anirudh Goyal ALIAS PARTH GOYAL, Colin A. Raffel, Augustus Odena",
neurips,https://proceedings.neurips.cc/paper/2020/file/a878dbebc902328b41dbf02aa87abb58-Paper.pdf,Black-Box Optimization with Local Generative Surrogates,"Sergey Shirobokov, Vladislav Belavin, Michael Kagan, Andrei Ustyuzhanin, Atilim Gunes Baydin",
neurips,https://proceedings.neurips.cc/paper/2020/file/a87c11b9100c608b7f8e98cfa316ff7b-Paper.pdf,Efficient Generation of Structured Objects with Constrained Adversarial Networks,"Luca Di Liello, Pierfrancesco Ardino, Jacopo Gobbi, Paolo Morettin, Stefano Teso, Andrea Passerini",
neurips,https://proceedings.neurips.cc/paper/2020/file/a87d27f712df362cd22c7a8ef823e987-Paper.pdf,Hard Example Generation by Texture Synthesis for Cross-domain Shape Similarity Learning,"Huan Fu, Shunming Li, Rongfei Jia, Mingming Gong, Binqiang Zhao, Dacheng Tao",
neurips,https://proceedings.neurips.cc/paper/2020/file/a89b71bb5227c75d463dd82a03115738-Paper.pdf,Recovery of sparse linear classifiers from mixture of responses,"Venkata Gandikota, Arya Mazumdar, Soumyabrata Pal",
neurips,https://proceedings.neurips.cc/paper/2020/file/a8acc28734d4fe90ea24353d901ae678-Paper.pdf,Efficient Distance Approximation for Structured High-Dimensional Distributions via Learning,"Arnab Bhattacharyya, Sutanu Gayen, Kuldeep S Meel, N. V. Vinodchandran","The distance approximation algorithms immediately imply new tolerant closeness testers for the corresponding classes of distributions. Prior to our work, only non-tolerant testers were known for both Bayes net distributions and Ising models, and no testers with quantitative guarantee were known for interventional distributions. To best of our knowledge, efficient distance approximation algorithms for Gaussian distributions were not present in the literature. Our algorithms are designed using a conceptually simple but general framework that is applicable to a variety of scenarios."
neurips,https://proceedings.neurips.cc/paper/2020/file/a8e5a72192378802318bf51063153729-Paper.pdf,A Single Recipe for Online Submodular Maximization with Adversarial or Stochastic Constraints,"Omid Sadeghi, Prasanna Raut, Maryam Fazel","In this paper, we consider an online optimization problem in which the reward functions are DR-submodular, and in addition to maximizing the total reward, the sequence of decisions must satisfy some convex constraints on average. Specifically, at each round
t
∈
{
1
,
…
,
T
}
t
, upon committing to an action
x
t
x
, a DR-submodular utility function
f
t
(
⋅
)
f
and a convex constraint function
g
t
(
⋅
)
g
are revealed, and the goal is to maximize the overall utility while ensuring the average of the constraint functions
1
T
∑
T
t
=
1
g
t
(
x
t
)
1
is non-positive. Such cumulative constraints arise naturally in applications where the average resource consumption is required to remain below a prespecified threshold. We study this problem under an adversarial model and a stochastic model for the convex constraints, where the functions
g
t
g
can vary arbitrarily or according to an i.i.d. process over time slots
t
∈
{
1
,
…
,
T
}
t
, respectively. We propose a single algorithm which achieves sub-linear (with respect to
T
T
) regret as well as sub-linear constraint violation bounds in both settings, without prior knowledge of the regime. Prior works have studied this problem in the special case of linear constraint functions. Our results not only improve upon the existing bounds under linear cumulative constraints, but also give the first sub-linear bounds for general convex long-term constraints."
neurips,https://proceedings.neurips.cc/paper/2020/file/a8ef1979aeec2737ae3830ec543ed0df-Paper.pdf,Learning Sparse Prototypes for Text Generation,"Junxian He, Taylor Berg-Kirkpatrick, Graham Neubig",
neurips,https://proceedings.neurips.cc/paper/2020/file/a9078e8653368c9c291ae2f8b74012e7-Paper.pdf,Implicit Rank-Minimizing Autoencoder,"Li Jing, Jure Zbontar, yann lecun",
neurips,https://proceedings.neurips.cc/paper/2020/file/a914ecef9c12ffdb9bede64bb703d877-Paper.pdf,Storage Efficient and Dynamic Flexible Runtime Channel Pruning via Deep Reinforcement Learning,"Jianda Chen, Shangyu Chen, Sinno Jialin Pan",
neurips,https://proceedings.neurips.cc/paper/2020/file/a96b65a721e561e1e3de768ac819ffbb-Paper.pdf,Task-Oriented Feature Distillation,"Linfeng Zhang, Yukang Shi, Zuoqiang Shi, Kaisheng Ma, Chenglong Bao",
neurips,https://proceedings.neurips.cc/paper/2020/file/a979ca2444b34449a2c80b012749e9cd-Paper.pdf,Entropic Causal Inference: Identifiability and Finite Sample Results,"Spencer Compton, Murat Kocaoglu, Kristjan Greenewald, Dmitriy Katz",
neurips,https://proceedings.neurips.cc/paper/2020/file/a97da629b098b75c294dffdc3e463904-Paper.pdf,Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement,"Ben Eysenbach, XINYANG GENG, Sergey Levine, Russ R. Salakhutdinov",
neurips,https://proceedings.neurips.cc/paper/2020/file/a992995ef4f0439b258f2360dbb85511-Paper.pdf,Variance-Reduced Off-Policy TDC Learning: Non-Asymptotic Convergence Analysis,"Shaocong Ma, Yi Zhou, Shaofeng Zou","Variance reduction techniques have been successfully applied to temporal-difference (TD) learning and help to improve the sample complexity in policy evaluation. However, the existing work applied variance reduction to either the less popular one time-scale TD algorithm or the two time-scale GTD algorithm but with a finite number of i.i.d.\ samples, and both algorithms apply to only the on-policy setting. In this work, we develop a variance reduction scheme for the two time-scale TDC algorithm in the off-policy setting and analyze its non-asymptotic convergence rate over both i.i.d.\ and Markovian samples. In the i.i.d setting, our algorithm achieves an improved sample complexity
\calO
(
ϵ
−
3
5
log
ϵ
−
1
)
\calO
over the state-of-the-art result
\calO
(
ϵ
−
1
log
ϵ
−
1
)
\calO
. In the Markovian setting, our algorithm achieves the state-of-the-art sample complexity
\calO
(
ϵ
−
1
log
ϵ
−
1
)
\calO
that is near-optimal. Experiments demonstrate that the proposed variance-reduced TDC achieves a smaller asymptotic convergence error than both the conventional TDC and the variance-reduced TD."
neurips,https://proceedings.neurips.cc/paper/2020/file/a9b7ba70783b617e9998dc4dd82eb3c5-Paper.pdf,AdaTune: Adaptive Tensor Program Compilation Made Efficient,"Menghao Li, Minjia Zhang, Chi Wang, Mingqin Li","In this paper, we present a new method, called AdaTune, that significantly reduces the optimization time of tensor programs for high-performance deep learning inference. In particular, we propose an adaptive evaluation method that statistically early terminates a costly hardware measurement without losing much accuracy. We further devise a surrogate model with uncertainty quantification that allows the optimization to adapt to hardware and model heterogeneity better. Finally, we introduce a contextual optimizer that provides adaptive control of the exploration and exploitation to improve the transformation space searching effectiveness. We evaluate and compare the levels of optimization obtained by a state-of-the-art DL compiler and AdaTune. The experiment results show that AdaTune obtains up to 115% higher GFLOPS than the baseline under the same optimization time budget. Furthermore, AdaTune provides 1.3--3.9X speedup in optimization time over the state-of-the-art to reach the same optimization quality for a range of models across different hardware architectures."
neurips,https://proceedings.neurips.cc/paper/2020/file/a9df2255ad642b923d95503b9a7958d8-Paper.pdf,When Do Neural Networks Outperform Kernel Methods?,"Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, Andrea Montanari","How can we reconcile the above claims? For which tasks do NNs outperform RKHS? If covariates are nearly isotropic, RKHS methods suffer from the curse of dimensionality, while NNs can overcome it by learning the best low-dimensional representation. Here we show that this curse of dimensionality becomes milder if the covariates display the same low-dimensional structure as the target function, and we precisely characterize this tradeoff. Building on these results, we present the spiked covariates model that can capture in a unified framework both behaviors observed in earlier works."
neurips,https://proceedings.neurips.cc/paper/2020/file/a9e18cb5dd9d3ab420946fa19ebbbf52-Paper.pdf,STEER : Simple Temporal Regularization For Neural ODE,"Arnab Ghosh, Harkirat Behl, Emilien Dupont, Philip Torr, Vinay Namboodiri",
neurips,https://proceedings.neurips.cc/paper/2020/file/aa0d2a804a3510442f2fd40f2100b054-Paper.pdf,A Variational Approach for Learning from Positive and Unlabeled Data,"Hui Chen, Fangqing Liu, Yin Wang, Liyue Zhao, Hao Wu",
neurips,https://proceedings.neurips.cc/paper/2020/file/aa108f56a10e75c1f20f27723ecac85f-Paper.pdf,"Efficient Clustering Based On A Unified View Of
K
K
-means And Ratio-cut","Shenfei Pei, Feiping Nie, Rong Wang, Xuelong Li","Spectral clustering and
k
k
-means, both as two major traditional clustering methods, are still attracting a lot of attention, although a variety of novel clustering algorithms have been proposed in recent years. Firstly, a unified framework of
k
k
-means and ratio-cut is revisited, and a novel and efficient clustering algorithm is then proposed based on this framework. The time and space complexity of our method are both linear with respect to the number of samples, and are independent of the number of clusters to construct, more importantly. These properties mean that it is easily scalable and applicable to large practical problems. Extensive experiments on 12 real-world benchmark and 8 facial datasets validate the advantages of the proposed algorithms compared to the state-of-the-art clustering algorithms. In particular, over 15x and 7x speed-up can be obtained with respect to
k
k
-means on the synthetic dataset of 1 million samples and the benchmark dataset (CelebA) of 200k samples, respectively [GitHub]."
neurips,https://proceedings.neurips.cc/paper/2020/file/aa1f5f73327ba40d47ebce155e785aaf-Paper.pdf,Recurrent Switching Dynamical Systems Models for Multiple Interacting Neural Populations,"Joshua Glaser, Matthew Whiteway, John P. Cunningham, Liam Paninski, Scott Linderman",
neurips,https://proceedings.neurips.cc/paper/2020/file/aa2a77371374094fe9e0bc1de3f94ed9-Paper.pdf,Coresets via Bilevel Optimization for Continual Learning and Streaming,"Zalán Borsos, Mojmir Mutny, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2020/file/aa475604668730af60a0a87cc92604da-Paper.pdf,Generalized Independent Noise Condition for Estimating Latent Variable Causal Graphs,"Feng Xie, Ruichu Cai, Biwei Huang, Clark Glymour, Zhifeng Hao, Kun Zhang","Causal discovery aims to recover causal structures or models underlying the observed data. Despite its success in certain domains, most existing methods focus on causal relations between observed variables, while in many scenarios the observed ones may not be the underlying causal variables (e.g., image pixels), but are generated by latent causal variables or confounders that are causally related. To this end, in this paper, we consider Linear, Non-Gaussian Latent variable Models (LiNGLaMs), in which latent confounders are also causally related, and propose a Generalized Independent Noise (GIN) condition to estimate such latent variable graphs. Specifically, for two observed random vectors
Y
Y
and
Z
Z
, GIN holds if and only if
ω
⊺
Y
ω
and
Z
Z
are statistically independent, where
ω
ω
is a parameter vector characterized from the cross-covariance between
Y
Y
and
Z
Z
. From the graphical view, roughly speaking, GIN implies that causally earlier latent common causes of variables in
Y
Y
d-separate
Y
Y
from
Z
Z
. Interestingly, we find that the independent noise condition, i.e., if there is no confounder, causes are independent from the error of regressing the effect on the causes, can be seen as a special case of GIN. Moreover, we show that GIN helps locate latent variables and identify their causal structure, including causal directions. We further develop a recursive learning algorithm to achieve these goals. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method."
neurips,https://proceedings.neurips.cc/paper/2020/file/aa85e45da94cb0d78853c50ba636a15a-Paper.pdf,Understanding and Exploring the Network with Stochastic Architectures,"Zhijie Deng, Yinpeng Dong, Shifeng Zhang, Jun Zhu",
neurips,https://proceedings.neurips.cc/paper/2020/file/aaa5ebec57257fa776a1990c2bd025c1-Paper.pdf,All-or-nothing statistical and computational phase transitions in sparse spiked matrix estimation,"jean barbier, Nicolas Macris, Cynthia Rush",
neurips,https://proceedings.neurips.cc/paper/2020/file/aab085461de182608ee9f607f3f7d18f-Paper.pdf,Deep Evidential Regression,"Alexander Amini, Wilko Schwarting, Ava Soleimany, Daniela Rus",
neurips,https://proceedings.neurips.cc/paper/2020/file/aaf2979785deb27864047e0ea40ef1b7-Paper.pdf,Analytical Probability Distributions and Exact Expectation-Maximization for Deep Generative Networks,"Randall Balestriero, Sebastien PARIS, Richard Baraniuk",
neurips,https://proceedings.neurips.cc/paper/2020/file/ab452534c5ce28c4fbb0e102d4a4fb2e-Paper.pdf,Bayesian Pseudocoresets,"Dionysis Manousakas, Zuheng Xu, Cecilia Mascolo, Trevor Campbell",
neurips,https://proceedings.neurips.cc/paper/2020/file/ab6b331e94c28169d15cca0cb3bbc73e-Paper.pdf,"See, Hear, Explore: Curiosity via Audio-Visual Association","Victoria Dean, Shubham Tulsiani, Abhinav Gupta",
neurips,https://proceedings.neurips.cc/paper/2020/file/ab7314887865c4265e896c6e209d1cd6-Paper.pdf,Adversarial Training is a Form of Data-dependent Operator Norm Regularization,"Kevin Roth, Yannic Kilcher, Thomas Hofmann","We establish a theoretical link between adversarial training and operator norm regularization for deep neural networks. Specifically, we prove that
l
p
l
-norm constrained projected gradient ascent based adversarial training with an
l
q
l
-norm loss on the logits of clean and perturbed inputs is equivalent to data-dependent (p, q) operator norm regularization. This fundamental connection confirms the long-standing argument that a network’s sensitivity to adversarial examples is tied to its spectral properties and hints at novel ways to robustify and defend against adversarial attacks. We provide extensive empirical evidence on state-of-the-art network architectures to support our theoretical results."
neurips,https://proceedings.neurips.cc/paper/2020/file/ab73f542b6d60c4de151800b8abc0a6c-Paper.pdf,A Biologically Plausible Neural Network for Slow Feature Analysis,"David Lipshutz, Charles Windolf, Siavash Golkar, Dmitri Chklovskii",
neurips,https://proceedings.neurips.cc/paper/2020/file/ab7a710458b8378b523e39143a6764d6-Paper.pdf,Learning Feature Sparse Principal Subspace,"Lai Tian, Feiping Nie, Rong Wang, Xuelong Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/aba3b6fd5d186d28e06ff97135cade7f-Paper.pdf,Online Adaptation for Consistent Mesh Reconstruction in the Wild,"Xueting Li, Sifei Liu, Shalini De Mello, Kihwan Kim, Xiaolong Wang, Ming-Hsuan Yang, Jan Kautz",
neurips,https://proceedings.neurips.cc/paper/2020/file/abb451a12cf1a9d93292e81f0d4fdd7a-Paper.pdf,Online learning with dynamics: A minimax perspective,"Kush Bhatia, Karthik Sridharan","Our main results provide sufficient conditions for online learnability for this setup with corresponding rates. The rates are characterized by: 1) a complexity term capturing the expressiveness of the underlying policy class under the dynamics of state change, and 2) a dynamics stability term measuring the deviation of the instantaneous loss from a certain counterfactual loss. Further, we provide matching lower bounds which show that both the complexity terms are indeed necessary."
neurips,https://proceedings.neurips.cc/paper/2020/file/abc99d6b9938aa86d1f30f8ee0fd169f-Paper.pdf,Learning to Select Best Forecast Tasks for Clinical Outcome Prediction,"Yuan Xue, Nan Du, Anne Mottram, Martin Seneviratne, Andrew M. Dai",
neurips,https://proceedings.neurips.cc/paper/2020/file/abd1c782880cc59759f4112fda0b8f98-Paper.pdf,Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping,"Eduard Gorbunov, Marina Danilova, Alexander Gasnikov",
neurips,https://proceedings.neurips.cc/paper/2020/file/abd987257ff0eddc2bc6602538cb3c43-Paper.pdf,Adaptive Experimental Design with Temporal Interference: A Maximum Likelihood Approach,"Peter W. Glynn, Ramesh Johari, Mohammad Rasouli",
neurips,https://proceedings.neurips.cc/paper/2020/file/ac10ec1ace51b2d973cd87973a98d3ab-Paper.pdf,From Trees to Continuous Embeddings and Back: Hyperbolic Hierarchical Clustering,"Ines Chami, Albert Gu, Vaggos Chatziafratis, Christopher Ré",
neurips,https://proceedings.neurips.cc/paper/2020/file/ac10ff1941c540cd87c107330996f4f6-Paper.pdf,The Autoencoding Variational Autoencoder,"Taylan Cemgil, Sumedh Ghaisas, Krishnamurthy Dvijotham, Sven Gowal, Pushmeet Kohli",
neurips,https://proceedings.neurips.cc/paper/2020/file/ac3870fcad1cfc367825cda0101eee62-Paper.pdf,A Fair Classifier Using Kernel Density Estimation,"Jaewoong Cho, Gyeongjo Hwang, Changho Suh",
neurips,https://proceedings.neurips.cc/paper/2020/file/ac4395adcb3da3b2af3d3972d7a10221-Paper.pdf,A Randomized Algorithm to Reduce the Support of Discrete Measures,"Francesco Cosentino, Harald Oberhauser, Alessandro Abate","Given a discrete probability measure supported on
N
N
atoms and a set of
n
n
real-valued functions, there exists a probability measure that is supported on a subset of
n
+
1
n
of the original
N
N
atoms and has the same mean when integrated against each of the
n
n
functions. If
N
≫
n
N
this results in a huge reduction of complexity. We give a simple geometric characterization of barycenters via negative cones and derive a randomized algorithm that computes this new measure by
greedy geometric sampling''. We then study its properties, and benchmark it on synthetic and real-world data to show that it can be very beneficial in the
N
≫
n
N
regime. A Python implementation is available at \url{https://github.com/FraCose/Recombination_Random_Algos}."
neurips,https://proceedings.neurips.cc/paper/2020/file/ac450d10e166657ec8f93a1b65ca1b14-Paper.pdf,Distributionally Robust Federated Averaging,"Yuyang Deng, Mohammad Mahdi Kamani, Mehrdad Mahdavi",
neurips,https://proceedings.neurips.cc/paper/2020/file/ac457ba972fb63b7994befc83f774746-Paper.pdf,Sharp uniform convergence bounds through empirical centralization,"Cyrus Cousins, Matteo Riondato",
neurips,https://proceedings.neurips.cc/paper/2020/file/acaa23f71f963e96c8847585e71352d6-Paper.pdf,COBE: Contextualized Object Embeddings from Narrated Instructional Video,"Gedas Bertasius, Lorenzo Torresani",
neurips,https://proceedings.neurips.cc/paper/2020/file/acab0116c354964a558e65bdd07ff047-Paper.pdf,Knowledge Transfer in Multi-Task Deep Reinforcement Learning for Continuous Control,"Zhiyuan Xu, Kun Wu, Zhengping Che, Jian Tang, Jieping Ye",
neurips,https://proceedings.neurips.cc/paper/2020/file/ad086f59924fffe0773f8d0ca22ea712-Paper.pdf,Finite Versus Infinite Neural Networks: an Empirical Study,"Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, Jascha Sohl-Dickstein",
neurips,https://proceedings.neurips.cc/paper/2020/file/ad1f8bb9b51f023cdc80cf94bb615aa9-Paper.pdf,Supermasks in Superposition,"Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, Ali Farhadi",
neurips,https://proceedings.neurips.cc/paper/2020/file/ad62cfd33e3870262d6bf5331c1f13b0-Paper.pdf,Nonasymptotic Guarantees for Spiked Matrix Recovery with Generative Priors,"Jorio Cocola, Paul Hand, Vlad Voroninski",
neurips,https://proceedings.neurips.cc/paper/2020/file/ad71c82b22f4f65b9398f76d8be4c615-Paper.pdf,Almost Optimal Model-Free Reinforcement Learningvia Reference-Advantage Decomposition,"Zihan Zhang, Yuan Zhou, Xiangyang Ji",
neurips,https://proceedings.neurips.cc/paper/2020/file/ad7ed5d47b9baceb12045a929e7e2f66-Paper.pdf,Learning to Incentivize Other Learning Agents,"Jiachen Yang, Ang Li, Mehrdad Farajtabar, Peter Sunehag, Edward Hughes, Hongyuan Zha",
neurips,https://proceedings.neurips.cc/paper/2020/file/add5aebfcb33a2206b6497d53bc4f309-Paper.pdf,Displacement-Invariant Matching Cost Learning for Accurate Optical Flow Estimation,"Jianyuan Wang, Yiran Zhong, Yuchao Dai, Kaihao Zhang, Pan Ji, Hongdong Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/adf854f418fc96fb01ad92a2ed2fc35c-Paper.pdf,Distributionally Robust Local Non-parametric Conditional Estimation,"Viet Anh Nguyen, Fan Zhang, Jose Blanchet, Erick Delage, Yinyu Ye",
neurips,https://proceedings.neurips.cc/paper/2020/file/ae06fbdc519bddaa88aa1b24bace4500-Paper.pdf,Robust Multi-Object Matching via Iterative Reweighting of the Graph Connection Laplacian,"Yunpeng Shi, Shaohan Li, Gilad Lerman",
neurips,https://proceedings.neurips.cc/paper/2020/file/ae3d525daf92cee0003a7f2d92c34ea3-Paper.pdf,Meta-Gradient Reinforcement Learning with an Objective Discovered Online,"Zhongwen Xu, Hado P. van Hasselt, Matteo Hessel, Junhyuk Oh, Satinder Singh, David Silver",
neurips,https://proceedings.neurips.cc/paper/2020/file/ae87a54e183c075c494c4d397d126a66-Paper.pdf,Learning Strategy-Aware Linear Classifiers,"Yiling Chen, Yang Liu, Chara Podimata",
neurips,https://proceedings.neurips.cc/paper/2020/file/ae95296e27d7f695f891cd26b4f37078-Paper.pdf,Upper Confidence Primal-Dual Reinforcement Learning for CMDP with Adversarial Loss,"Shuang Qiu, Xiaohan Wei, Zhuoran Yang, Jieping Ye, Zhaoran Wang","We consider online learning for episodic stochastically constrained Markov decision processes (CMDP), which plays a central role in ensuring the safety of reinforcement learning. Here the loss function can vary arbitrarily across the episodes, whereas both the loss received and the budget consumption are revealed at the end of each episode. Previous works solve this problem under the restrictive assumption that the transition model of the MDP is known a priori and establish regret bounds that depend polynomially on the cardinalities of the state space
S
S
and the action space
A
A
. In this work, we propose a new \emph{upper confidence primal-dual} algorithm, which only requires the trajectories sampled from the transition model. In particular, we prove that the proposed algorithm achieves
˜
O
(
L
|
S
|
√
|
A
|
T
)
O
upper bounds of both the regret and the constraint violation, where
L
L
is the length of each episode. Our analysis incorporates a new high-probability drift analysis of Lagrange multiplier processes into the celebrated regret analysis of upper confidence reinforcement learning, which demonstrates the power of
optimism in the face of uncertainty'' in constrained online learning."
neurips,https://proceedings.neurips.cc/paper/2020/file/aeb7b30ef1d024a76f21a1d40e30c302-Paper.pdf,Calibrating Deep Neural Networks using Focal Loss,"Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr, Puneet Dokania",
neurips,https://proceedings.neurips.cc/paper/2020/file/aecad42329922dfc97eee948606e1f8e-Paper.pdf,Optimizing Mode Connectivity via Neuron Alignment,"Norman Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk, Prasanna Sattigeri, Rongjie Lai",
neurips,https://proceedings.neurips.cc/paper/2020/file/aee5620fa0432e528275b8668581d9a8-Paper.pdf,Information Theoretic Regret Bounds for Online Nonlinear Control,"Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, Wen Sun","This work studies the problem of sequential control in an unknown, nonlinear dynamical system, where we model the underlying system dynamics as an unknown function in a known Reproducing Kernel Hilbert Space. This framework yields a general setting that permits discrete and continuous control inputs as well as non-smooth, non-differentiable dynamics. Our main result, the Lower Confidence-based Continuous Control (LC3) algorithm, enjoys a near-optimal
O
(
√
T
)
O
regret bound against the optimal controller in episodic settings, where
T
T
is the number of episodes. The bound has no explicit dependence on dimension of the system dynamics, which could be infinite, but instead only depends on information theoretic quantities. We empirically show its application to a number of nonlinear control tasks and demonstrate the benefit of exploration for learning model dynamics."
neurips,https://proceedings.neurips.cc/paper/2020/file/aeefb050911334869a7a5d9e4d0e1689-Paper.pdf,A kernel test for quasi-independence,"Tamara Fernandez, Wenkai Xu, Marc Ditzhaus, Arthur Gretton",
neurips,https://proceedings.neurips.cc/paper/2020/file/af5d5ef24881f3c3049a7b9bfe74d58b-Paper.pdf,First Order Constrained Optimization in Policy Space,"Yiming Zhang, Quan Vuong, Keith Ross",
neurips,https://proceedings.neurips.cc/paper/2020/file/af94ed0d6f5acc95f97170e3685f16c0-Paper.pdf,Learning Augmented Energy Minimization via Speed Scaling,"Etienne Bamas, Andreas Maggiori, Lars Rohwedder, Ola Svensson",
neurips,https://proceedings.neurips.cc/paper/2020/file/af9c0e0c1dee63e5acad8b7ed1a5be96-Paper.pdf,Exploiting MMD and Sinkhorn Divergences for Fair and Transferable Representation Learning,"Luca Oneto, Michele Donini, Giulia Luise, Carlo Ciliberto, Andreas Maurer, Massimiliano Pontil",
neurips,https://proceedings.neurips.cc/paper/2020/file/afb0b97df87090596ae7c503f60bb23f-Paper.pdf,Deep Rao-Blackwellised Particle Filters for Time Series Forecasting,"Richard Kurle, Syama Sundar Rangapuram, Emmanuel de Bézenac, Stephan Günnemann, Jan Gasthaus",
neurips,https://proceedings.neurips.cc/paper/2020/file/b05b57f6add810d3b7490866d74c0053-Paper.pdf,Why are Adaptive Methods Good for Attention Models?,"Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, Suvrit Sra",
neurips,https://proceedings.neurips.cc/paper/2020/file/b090409688550f3cc93f4ed88ec6cafb-Paper.pdf,Neural Sparse Representation for Image Restoration,"Yuchen Fan, Jiahui Yu, Yiqun Mei, Yulun Zhang, Yun Fu, Ding Liu, Thomas S. Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/b096577e264d1ebd6b41041f392eec23-Paper.pdf,Boosting First-Order Methods by Shifting Objective: New Schemes with Faster Worst-Case Rates,"Kaiwen Zhou, Anthony Man-Cho So, James Cheng",
neurips,https://proceedings.neurips.cc/paper/2020/file/b0c7ae2316c7e8214fd659e4bc8a0dea-Paper.pdf,Robust Sequence Submodular Maximization,"Gamal Sallam, Zizhan Zheng, Jie Wu, Bo Ji",
neurips,https://proceedings.neurips.cc/paper/2020/file/b139aeda1c2914e3b579aafd3ceeb1bd-Paper.pdf,Certified Monotonic Neural Networks,"Xingchao Liu, Xing Han, Na Zhang, Qiang Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/b139e104214a08ae3f2ebcce149cdf6e-Paper.pdf,System Identification with Biophysical Constraints: A Circuit Model of the Inner Retina,"Cornelius Schröder, David Klindt, Sarah Strauss, Katrin Franke, Matthias Bethge, Thomas Euler, Philipp Berens",
neurips,https://proceedings.neurips.cc/paper/2020/file/b14680dec683e744ada1f2fe08614086-Paper.pdf,Efficient Algorithms for Device Placement of DNN Graph Operators,"Jakub M. Tarnawski, Amar Phanishayee, Nikhil Devanur, Divya Mahajan, Fanny Nina Paravecino","In this paper, we identify and isolate the structured optimization problem at the core of device placement of DNN operators, for both inference and training, especially in modern pipelined settings. We then provide algorithms that solve this problem to optimality. We demonstrate the applicability and efficiency of our approaches using several contemporary DNN computation graphs."
neurips,https://proceedings.neurips.cc/paper/2020/file/b197ffdef2ddc3308584dce7afa3661b-Paper.pdf,Active Invariant Causal Prediction: Experiment Selection through Stability,"Juan L. Gamella, Christina Heinze-Deml",
neurips,https://proceedings.neurips.cc/paper/2020/file/b19aa25ff58940d974234b48391b9549-Paper.pdf,BOSS: Bayesian Optimization over String Spaces,"Henry Moss, David Leslie, Daniel Beck, Javier González, Paul Rayson",
neurips,https://proceedings.neurips.cc/paper/2020/file/b1adda14824f50ef24ff1c05bb66faf3-Paper.pdf,Model Interpretability through the lens of Computational Complexity,"Pablo Barceló, Mikaël Monet, Jorge Pérez, Bernardo Subercaseaux",
neurips,https://proceedings.neurips.cc/paper/2020/file/b20706935de35bbe643733f856d9e5d6-Paper.pdf,Markovian Score Climbing: Variational Inference with KL(p||q),"Christian Naesseth, Fredrik Lindsten, David Blei",
neurips,https://proceedings.neurips.cc/paper/2020/file/b282d1735283e8eea45bce393cefe265-Paper.pdf,Improved Analysis of Clipping Algorithms for Non-convex Optimization,"Bohang Zhang, Jikai Jin, Cong Fang, Liwei Wang","Gradient clipping is commonly used in training deep neural networks partly due to its practicability in relieving the exploding gradient problem. Recently, \citet{zhang2019gradient} show that clipped (stochastic) Gradient Descent (GD) converges faster than vanilla GD via introducing a new assumption called
(
L
0
,
L
1
)
(
-smoothness, which characterizes the violent fluctuation of gradients typically encountered in deep neural networks. However, their iteration complexities on the problem-dependent parameters are rather pessimistic, and theoretical justification of clipping combined with other crucial techniques, e.g. momentum acceleration, are still lacking. In this paper, we bridge the gap by presenting a general framework to study the clipping algorithms, which also takes momentum methods into consideration.We provide convergence analysis of the framework in both deterministic and stochastic setting, and demonstrate the tightness of our results by comparing them with existing lower bounds. Our results imply that the efficiency of clipping methods will not degenerate even in highly non-smooth regions of the landscape. Experiments confirm the superiority of clipping-based methods in deep learning tasks."
neurips,https://proceedings.neurips.cc/paper/2020/file/b2ea5e977c5fc1ccfa74171a9723dd61-Paper.pdf,Bias no more: high-probability data-dependent regret bounds for adversarial bandits and MDPs,"Chung-Wei Lee, Haipeng Luo, Chen-Yu Wei, Mengxiao Zhang","Besides its simplicity, our approach enjoys several advantages. First, the obtained high-probability regret bounds are data-dependent and could be much smaller than the worst-case bounds, which resolves an open problem asked by Neu (2015). Second, resolving another open problem of Bartlett et al. (2008) and Abernethy and Rakhlin (2009), our approach leads to the first general and efficient algorithm with a high-probability regret bound for adversarial linear bandits, while previous methods are either inefficient or only applicable to specific action sets. Finally, our approach can also be applied to learning adversarial Markov Decision Processes and provides the first algorithm with a high-probability small-loss bound for this problem."
neurips,https://proceedings.neurips.cc/paper/2020/file/b2eeb7362ef83deff5c7813a67e14f0a-Paper.pdf,"A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection","Kemal Oksuz, Baris Can Cam, Emre Akbas, Sinan Kalkan",
neurips,https://proceedings.neurips.cc/paper/2020/file/b2f627fff19fda463cb386442eac2b3d-Paper.pdf,StratLearner: Learning a Strategy for Misinformation Prevention in Social Networks,Guangmo Tong,
neurips,https://proceedings.neurips.cc/paper/2020/file/b30958093daeed059670b35173654dc9-Paper.pdf,A Unified Switching System Perspective and Convergence Analysis of Q-Learning Algorithms,"Donghwan Lee, Niao He",
neurips,https://proceedings.neurips.cc/paper/2020/file/b367e525a7e574817c19ad24b7b35607-Paper.pdf,Kernel Alignment Risk Estimator: Risk Prediction from Training Data,"Arthur Jacot, Berfin Simsek, Francesco Spadaro, Clement Hongler, Franck Gabriel","We study the risk (i.e. generalization error) of Kernel Ridge Regression (KRR) for a kernel
K
K
with ridge
λ
>
0
λ
and i.i.d. observations. For this, we introduce two objects: the Signal Capture Threshold (SCT) and the Kernel Alignment Risk Estimator (KARE). The SCT
ϑ
K
,
λ
ϑ
is a function of the data distribution: it can be used to identify the components of the data that the KRR predictor captures, and to approximate the (expected) KRR risk. This then leads to a KRR risk approximation by the KARE
ρ
K
,
λ
ρ
, an explicit function of the training data, agnostic of the true data distribution. We phrase the regression problem in a functional setting. The key results then follow from a finite-size adaptation of the resolvent method for general Wishart random matrices. Under a natural universality assumption (that the KRR moments depend asymptotically on the first two moments of the observations) we capture the mean and variance of the KRR predictor. We numerically investigate our findings on the Higgs and MNIST datasets for various classical kernels: the KARE gives an excellent approximation of the risk. This supports our universality hypothesis. Using the KARE, one can compare choices of Kernels and hyperparameters directly from the training set. The KARE thus provides a promising data-dependent procedure to select Kernels that generalize well."
neurips,https://proceedings.neurips.cc/paper/2020/file/b3b43aeeacb258365cc69cdaf42a68af-Paper.pdf,Calibrating CNNs for Lifelong Learning,"Pravendra Singh, Vinay Kumar Verma, Pratik Mazumder, Lawrence Carin, Piyush Rai",
neurips,https://proceedings.neurips.cc/paper/2020/file/b3d6e130a30b176f2ca5af7d1e73953f-Paper.pdf,Online Convex Optimization Over Erdos-Renyi Random Networks,"Jinlong Lei, Peng Yi, Yiguang Hong, Jie Chen, Guodong Shi","The work studies how node-to-node communications over an Erd\H{o}s-R\'enyi random network influence distributed online convex optimization, which is vital in solving large-scale machine learning in antagonistic or changing environments. At per step, each node (computing unit) makes a local decision, experiences a loss evaluated with a convex function, and communicates the decision with other nodes over a network. The node-to-node communications are described by the Erd\H{o}s-R\'enyi rule, where independently each link takes place with a probability
p
p
over a prescribed connected graph. The objective is to minimize the system-wide loss accumulated over a finite time horizon. We consider standard distributed gradient descents with full gradients, one-point bandits and two-points bandits for convex and strongly convex losses, respectively. We establish how the regret bounds scale with respect to time horizon
T
T
, network size
N
N
, decision dimension
d
d
, and an algebraic network connectivity. The regret bounds scaling with respect to
T
T
match those obtained by state-of-the-art algorithms and fundamental limits in the corresponding centralized online optimization problems, e.g.,
O
(
√
T
)
O
and
O
(
ln
(
T
)
)
O
regrets are established for convex and strongly convex losses with full gradient feedback and two-points information, respectively. For classical Erd\H{o}s-R\'enyi networks over all-to-all possible node communications, the regret scalings with respect to the probability
p
p
are analytically established, based on which the tradeoff between the communication overhead and computation accuracy is clearly demonstrated. Numerical studies have validated the theoretical findings."
neurips,https://proceedings.neurips.cc/paper/2020/file/b3f61131b6eceeb2b14835fa648a48ff-Paper.pdf,Robustness of Bayesian Neural Networks to Gradient-Based Attacks,"Ginevra Carbone, Matthew Wicker, Luca Laurenti, Andrea Patane', Luca Bortolussi, Guido Sanguinetti",
neurips,https://proceedings.neurips.cc/paper/2020/file/b427426b8acd2c2e53827970f2c2f526-Paper.pdf,Parametric Instance Classification for Unsupervised Visual Feature learning,"Yue Cao, Zhenda Xie, Bin Liu, Yutong Lin, Zheng Zhang, Han Hu",
neurips,https://proceedings.neurips.cc/paper/2020/file/b44182379bf9fae976e6ae5996e13cd8-Paper.pdf,Sparse Weight Activation Training,"Md Aamir Raihan, Tor Aamodt",
neurips,https://proceedings.neurips.cc/paper/2020/file/b460cf6b09878b00a3e1ad4c72344ccd-Paper.pdf,Collapsing Bandits and Their Application to Public Health Intervention,"Aditya Mate, Jackson Killian, Haifeng Xu, Andrew Perrault, Milind Tambe",
neurips,https://proceedings.neurips.cc/paper/2020/file/b4b758962f17808746e9bb832a6fa4b8-Paper.pdf,Neural Sparse Voxel Fields,"Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt",
neurips,https://proceedings.neurips.cc/paper/2020/file/b4edda67f0f57e218a8e766927e3e5c5-Paper.pdf,A Flexible Framework for Designing Trainable Priors with Adaptive Smoothing and Game Encoding,"Bruno Lecouat, Jean Ponce, Julien Mairal",
neurips,https://proceedings.neurips.cc/paper/2020/file/b53b3a3d6ab90ce0268229151c9bde11-Paper.pdf,The Discrete Gaussian for Differential Privacy,"Clément L. Canonne, Gautam Kamath, Thomas Steinke","With these shortcomings in mind, we introduce and analyze the discrete Gaussian in the context of differential privacy. Specifically, we theoretically and experimentally show that adding discrete Gaussian noise provides essentially the same privacy and accuracy guarantees as the addition of continuous Gaussian noise. We also present an simple and efficient algorithm for exact sampling from this distribution. This demonstrates its applicability for privately answering counting queries, or more generally, low-sensitivity integer-valued queries."
neurips,https://proceedings.neurips.cc/paper/2020/file/b58144d7e90b5a43edcce1ca9e642882-Paper.pdf,Robust Sub-Gaussian Principal Component Analysis and Width-Independent Schatten Packing,"Arun Jambulapati, Jerry Li, Kevin Tian","We develop two methods for the following fundamental statistical task: given an
\eps
\eps
-corrupted set of
n
n
samples from a
d
d
-dimensional sub-Gaussian distribution, return an approximate top eigenvector of the covariance matrix. Our first robust PCA algorithm runs in polynomial time, returns a
1
−
O
(
\eps
log
\eps
−
1
)
1
-approximate top eigenvector, and is based on a simple iterative filtering approach. Our second, which attains a slightly worse approximation factor, runs in nearly-linear time and sample complexity under a mild spectral gap assumption. These are the first polynomial-time algorithms yielding non-trivial information about the covariance of a corrupted sub-Gaussian distribution without requiring additional algebraic structure of moments. As a key technical tool, we develop the first width-independent solvers for Schatten-
p
p
norm packing semidefinite programs, giving a
(
1
+
\eps
)
(
-approximate solution in
O
(
p
log
(
n
d
\eps
)
\eps
−
1
)
O
input-sparsity time iterations (where
n
n
,
d
d
are problem dimensions)."
neurips,https://proceedings.neurips.cc/paper/2020/file/b58f7d184743106a8a66028b7a28937c-Paper.pdf,Adaptive Importance Sampling for Finite-Sum Optimization and Sampling with Decreasing Step-Sizes,"Ayoub El Hanchi, David Stephens",
neurips,https://proceedings.neurips.cc/paper/2020/file/b599e8250e4481aaa405a715419c8179-Paper.pdf,Learning efficient task-dependent representations with synaptic plasticity,"Colin Bredenberg, Eero Simoncelli, Cristina Savin",
neurips,https://proceedings.neurips.cc/paper/2020/file/b5b8c484824d8a06f4f3d570bc420313-Paper.pdf,A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions,"Wei Deng, Guang Lin, Faming Liang",
neurips,https://proceedings.neurips.cc/paper/2020/file/b5c01503041b70d41d80e3dbe31bbd8c-Paper.pdf,Error Bounds of Imitating Policies and Environments,"Tian Xu, Ziniu Li, Yang Yu",
neurips,https://proceedings.neurips.cc/paper/2020/file/b5d17ed2b502da15aa727af0d51508d6-Paper.pdf,Disentangling Human Error from Ground Truth in Segmentation of Medical Images,"Le Zhang, Ryutaro Tanno, Mou-Cheng Xu, Chen Jin, Joseph Jacob, Olga Cicarrelli, Frederik Barkhof, Daniel Alexander",
neurips,https://proceedings.neurips.cc/paper/2020/file/b607ba543ad05417b8507ee86c54fcb7-Paper.pdf,Consequences of Misaligned AI,"Simon Zhuang, Dylan Hadfield-Menell",
neurips,https://proceedings.neurips.cc/paper/2020/file/b628386c9b92481fab68fbf284bd6a64-Paper.pdf,Promoting Coordination through Policy Regularization in Multi-Agent Deep Reinforcement Learning,"Julien Roy, Paul Barde, Félix Harvey, Derek Nowrouzezahrai, Chris Pal",
neurips,https://proceedings.neurips.cc/paper/2020/file/b63c87b0a41016ad29313f0d7393cee8-Paper.pdf,Emergent Reciprocity and Team Formation from Randomized Uncertain Social Preferences,Bowen Baker,
neurips,https://proceedings.neurips.cc/paper/2020/file/b6417f112bd27848533e54885b66c288-Paper.pdf,Hitting the High Notes: Subset Selection for Maximizing Expected Order Statistics,"Aranyak Mehta, Uri Nadav, Alexandros Psomas, Aviad Rubinstein","We consider the fundamental problem of selecting
k
k
out of
n
n
random variables in a way that the expected highest or second-highest value is maximized. This question captures several applications where we have uncertainty about the quality of candidates (e.g. auction bids, search results) and have the capacity to explore only a small subset due to an exogenous constraint. For example, consider a second price auction where system constraints (e.g., costly retrieval or model computation) allow the participation of only
k
k
out of
n
n
bidders, and the goal is to optimize the expected efficiency (highest bid) or expected revenue (second highest bid). We study the case where we are given an explicit description of each random variable. We give a PTAS for the problem of maximizing the expected highest value. For the second-highest value, we prove a hardness result: assuming the Planted Clique Hypothesis, there is no constant factor approximation algorithm that runs in polynomial time. Surprisingly, under the assumption that each random variable has monotone hazard rate (MHR), a simple score-based algorithm, namely picking the
k
k
random variables with the largest
1
/
√
k
1
top quantile value, is a constant approximation to the expected highest and second highest value, \emph{simultaneously}."
neurips,https://proceedings.neurips.cc/paper/2020/file/b64a70760bb75e3ecfd1ad86d8f10c88-Paper.pdf,Towards Scale-Invariant Graph-related Problem Solving by Iterative Homogeneous GNNs,"Hao Tang, Zhiao Huang, Jiayuan Gu, Bao-Liang Lu, Hao Su",
neurips,https://proceedings.neurips.cc/paper/2020/file/b67fb3360ae5597d85a005153451dd4e-Paper.pdf,Regret Bounds without Lipschitz Continuity: Online Learning with Relative-Lipschitz Losses,"Yihan Zhou, Victor Sanches Portella, Mark Schmidt, Nicholas Harvey","In this work, we consider OCO for relative Lipschitz and relative strongly convex functions. We extend the known regret bounds for classical OCO algorithms to the relative setting. Specifically, we show regret bounds for the follow the regularized leader algorithms and a variant of online mirror descent. Due to the generality of these methods, these results yield regret bounds for a wide variety of OCO algorithms. Furthermore, we further extend the results to algorithms with extra regularization such as regularized dual averaging."
neurips,https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf,The Lottery Ticket Hypothesis for Pre-trained BERT Networks,"Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, Michael Carbin",
neurips,https://proceedings.neurips.cc/paper/2020/file/b6b90237b3ebd1e462a5d11dbc5c4dae-Paper.pdf,Label-Aware Neural Tangent Kernel: Toward Better Generalization and Local Elasticity,"Shuxiao Chen, Hangfeng He, Weijie Su",
neurips,https://proceedings.neurips.cc/paper/2020/file/b6c8cf4c587f2ead0c08955ee6e2502b-Paper.pdf,Beyond Perturbations: Learning Guarantees with Arbitrary Adversarial Test Examples,"Shafi Goldwasser, Adam Tauman Kalai, Yael Kalai, Omar Montasser",
neurips,https://proceedings.neurips.cc/paper/2020/file/b6cf334c22c8f4ce8eb920bb7b512ed0-Paper.pdf,AdvFlow: Inconspicuous Black-box Adversarial Attacks using Normalizing Flows,"Hadi Mohaghegh Dolatabadi, Sarah Erfani, Christopher Leckie",
neurips,https://proceedings.neurips.cc/paper/2020/file/b6d767d2f8ed5d21a44b0e5886680cb9-Paper.pdf,Few-shot Image Generation with Elastic Weight Consolidation,"Yijun Li, Richard Zhang, Jingwan (Cynthia) Lu, Eli Shechtman",
neurips,https://proceedings.neurips.cc/paper/2020/file/b6dfd41875bc090bd31d0b1740eb5b1b-Paper.pdf,On the Expressiveness of Approximate Inference in Bayesian Neural Networks,"Andrew Foong, David Burt, Yingzhen Li, Richard Turner",
neurips,https://proceedings.neurips.cc/paper/2020/file/b6f8dc086b2d60c5856e4ff517060392-Paper.pdf,Non-Crossing Quantile Regression for Distributional Reinforcement Learning,"Fan Zhou, Jianing Wang, Xingdong Feng",
neurips,https://proceedings.neurips.cc/paper/2020/file/b704ea2c39778f07c617f6b7ce480e9e-Paper.pdf,"Dark Experience for General Continual Learning: a Strong, Simple Baseline","Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, SIMONE CALDERARA",
neurips,https://proceedings.neurips.cc/paper/2020/file/b710915795b9e9c02cf10d6d2bdb688c-Paper.pdf,Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping,"Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, Changjie Fan",
neurips,https://proceedings.neurips.cc/paper/2020/file/b71f5aaf3371c2cdfb7a7c0497f569d4-Paper.pdf,Neural encoding with visual attention,"Meenakshi Khosla, Gia Ngo, Keith Jamison, Amy Kuceyeski, Mert Sabuncu",
neurips,https://proceedings.neurips.cc/paper/2020/file/b7ae8fecf15b8b6c3c69eceae636d203-Paper.pdf,On the linearity of large non-linear models: when and why the tangent kernel is constant,"Chaoyue Liu, Libin Zhu, Misha Belkin",
neurips,https://proceedings.neurips.cc/paper/2020/file/b803a9254688e259cde2ec0361c8abe4-Paper.pdf,PLLay: Efficient Topological Layer based on Persistent Landscapes,"Kwangho Kim, Jisu Kim, Manzil Zaheer, Joon Kim, Frederic Chazal, Larry Wasserman",
neurips,https://proceedings.neurips.cc/paper/2020/file/b8043b9b976639acb17b035ab8963f18-Paper.pdf,Decentralized Langevin Dynamics for Bayesian Learning,"Anjaly Parayil, He Bai, Jemin George, Prudhvi Gurram",
neurips,https://proceedings.neurips.cc/paper/2020/file/b837305e43f7e535a1506fc263eee3ed-Paper.pdf,Shared Space Transfer Learning for analyzing multi-site fMRI data,"Tony Muhammad Yousefnezhad, Alessandro Selvitella, Daoqiang Zhang, Andrew Greenshaw, Russell Greiner",
neurips,https://proceedings.neurips.cc/paper/2020/file/b86e8d03fe992d1b0e19656875ee557c-Paper.pdf,The Diversified Ensemble Neural Network,"Shaofeng Zhang, Meng Liu, Junchi Yan",
neurips,https://proceedings.neurips.cc/paper/2020/file/b87039703fe79778e9f140b78621d7fb-Paper.pdf,Inductive Quantum Embedding,"Santosh Kumar Srivastava, Dinesh Khandelwal, Dhiraj Madan, Dinesh Garg, Hima Karanam, L Venkata Subramaniam",
neurips,https://proceedings.neurips.cc/paper/2020/file/b8a6550662b363eb34145965d64d0cfb-Paper.pdf,Variational Bayesian Unlearning,"Quoc Phong Nguyen, Bryan Kian Hsiang Low, Patrick Jaillet",
neurips,https://proceedings.neurips.cc/paper/2020/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf,Batched Coarse Ranking in Multi-Armed Bandits,"Nikolai Karpov, Qin Zhang","We study the problem of coarse ranking in the multi-armed bandits (MAB) setting, where we have a set of arms each of which is associated with an unknown distribution. The task is to partition the arms into clusters of predefined sizes, such that the mean of any arm in the
i
i
-th cluster is larger than that of any arm in the
j
j
-th cluster for any
j
>
i
j
. Coarse ranking generalizes a number of basic problems in MAB (e.g., best arm identification) and has many real-world applications. We initiate the study of the problem in the batched model where we can only have a small number of policy changes. We study both the fixed budget and fixed confidence variants in MAB, and propose algorithms and prove impossibility results which together give almost tight tradeoffs between the total number of arms pulls and the number of policy changes. We have tested our algorithms in both real and synthetic data; our experimental results have demonstrated the efficiency of the proposed methods."
neurips,https://proceedings.neurips.cc/paper/2020/file/b8ce47761ed7b3b6f48b583350b7f9e4-Paper.pdf,Understanding and Improving Fast Adversarial Training,"Maksym Andriushchenko, Nicolas Flammarion","A recent line of work focused on making adversarial training computationally efficient for deep learning models. In particular, Wong et al. (2020) showed that
ℓ
∞
ℓ
-adversarial training with fast gradient sign method (FGSM) can fail due to a phenomenon called catastrophic overfitting, when the model quickly loses its robustness over a single epoch of training. We show that adding a random step to FGSM, as proposed in Wong et al. (2020), does not prevent catastrophic overfitting, and that randomness is not important per se --- its main role being simply to reduce the magnitude of the perturbation. Moreover, we show that catastrophic overfitting is not inherent to deep and overparametrized networks, but can occur in a single-layer convolutional network with a few filters. In an extreme case, even a single filter can make the network highly non-linear locally, which is the main reason why FGSM training fails. Based on this observation, we propose a new regularization method, GradAlign, that prevents catastrophic overfitting by explicitly maximizing the gradient alignment inside the perturbation set and improves the quality of the FGSM solution. As a result, GradAlign allows to successfully apply FGSM training also for larger
ℓ
∞
ℓ
-perturbations and reduce the gap to multi-step adversarial training. The code of our experiments is available at https://github.com/tml-epfl/understanding-fast-adv-training."
neurips,https://proceedings.neurips.cc/paper/2020/file/b8fd7211e5247891e4d4f0562418868a-Paper.pdf,Coded Sequential Matrix Multiplication For Straggler Mitigation,"Nikhil Krishnan Muralee Krishnan, Seyederfan Hosseini, Ashish Khisti","In this work, we consider a sequence of
J
J
matrix multiplication jobs which needs to be distributed by a master across multiple worker nodes. For
i
∈
{
1
,
2
,
…
,
J
}
i
, job-
i
i
begins in round-
i
i
and has to be completed by round-
(
i
+
T
)
(
. Previous works consider only the special case of
T
=
0
T
and focus on coding across workers. We propose here two schemes with
T
>
0
T
, which feature coding across workers as well as the dimension of time. Our first scheme is a modification of the polynomial coding scheme introduced by Yu et al. and places no assumptions on the straggler model. Exploitation of the temporal dimension helps the scheme handle a larger set of straggler patterns than the polynomial coding scheme, for a given computational load per worker per round. The second scheme assumes a particular straggler model to further improve performance (in terms of encoding/decoding complexity). We develop theoretical results establishing (i) optimality of our proposed schemes for a certain class of straggler patterns and (ii) improved performance for the case of i.i.d. stragglers. These are further validated by experiments, where we implement our schemes to train neural networks."
neurips,https://proceedings.neurips.cc/paper/2020/file/b8ffa41d4e492f0fad2f13e29e1762eb-Paper.pdf,"Attack of the Tails: Yes, You Really Can Backdoor Federated Learning","Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, Dimitris Papailiopoulos",
neurips,https://proceedings.neurips.cc/paper/2020/file/b90c46963248e6d7aab1e0f429743ca0-Paper.pdf,Certifiably Adversarially Robust Detection of Out-of-Distribution Data,"Julian Bitterwolf, Alexander Meinke, Matthias Hein","Deep neural networks are known to be overconfident when applied to out-of-distribution (OOD) inputs which clearly do not belong to any class. This is a problem in safety-critical applications since a reliable assessment of the uncertainty of a classifier is a key property, allowing to trigger human intervention or to transfer into a safe state. In this paper, we are aiming for certifiable worst case guarantees for OOD detection by enforcing not only low confidence at the OOD point but also in an
l
∞
l
-ball around it. For this purpose, we use interval bound propagation (IBP) to upper bound the maximal confidence in the
l
∞
l
-ball and minimize this upper bound during training time. We show that non-trivial bounds on the confidence for OOD data generalizing beyond the OOD dataset seen at training time are possible. Moreover, in contrast to certified adversarial robustness which typically comes with significant loss in prediction performance, certified guarantees for worst case OOD detection are possible without much loss in accuracy."
neurips,https://proceedings.neurips.cc/paper/2020/file/b98249b38337c5088bbc660d8f872d6a-Paper.pdf,Domain Generalization via Entropy Regularization,"Shanshan Zhao, Mingming Gong, Tongliang Liu, Huan Fu, Dacheng Tao",
neurips,https://proceedings.neurips.cc/paper/2020/file/b9cfe8b6042cf759dc4c0cccb27a6737-Paper.pdf,Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels,"Massimiliano Patacchiola, Jack Turner, Elliot J. Crowley, Michael O'Boyle, Amos J. Storkey",
neurips,https://proceedings.neurips.cc/paper/2020/file/ba036d228858d76fb89189853a5503bd-Paper.pdf,Skeleton-bridged Point Completion: From Global Inference to Local Adjustment,"Yinyu Nie, Yiqun Lin, Xiaoguang Han, Shihui Guo, Jian Chang, Shuguang Cui, Jian.J Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/ba053350fe56ed93e64b3e769062b680-Paper.pdf,Compressing Images by Encoding Their Latent Representations with Relative Entropy Coding,"Gergely Flamich, Marton Havasi, José Miguel Hernández-Lobato",
neurips,https://proceedings.neurips.cc/paper/2020/file/ba304f3809ed31d0ad97b5a2b5df2a39-Paper.pdf,Improved Guarantees for k-means++ and k-means++ Parallel,"Konstantin Makarychev, Aravind Reddy, Liren Shan",
neurips,https://proceedings.neurips.cc/paper/2020/file/ba3c95c2962d3aab2f6e667932daa3c5-Paper.pdf,Sparse Spectrum Warped Input Measures for Nonstationary Kernel Learning,"Anthony Tompkins, Rafael Oliveira, Fabio T. Ramos",
neurips,https://proceedings.neurips.cc/paper/2020/file/ba3e9b6a519cfddc560b5d53210df1bd-Paper.pdf,An Efficient Adversarial Attack for Tree Ensembles,"Chong Zhang, Huan Zhang, Cho-Jui Hsieh","We study the problem of efficient adversarial attacks on tree based ensembles such as gradient boosting decision trees (GBDTs) and random forests (RFs). Since these models are non-continuous step functions and gradient does not exist, most existing efficient adversarial attacks are not applicable. Although decision-based black-box attacks can be applied, they cannot utilize the special structure of trees. In our work, we transform the attack problem into a discrete search problem specially designed for tree ensembles, where the goal is to find a valid
leaf tuple'' that leads to mis-classification while having the shortest distance to the original input. With this formulation, we show that a simple yet effective greedy algorithm can be applied to iteratively optimize the adversarial example by moving the leaf tuple to its neighborhood within hamming distance 1. Experimental results on several large GBDT and RF models with up to hundreds of trees demonstrate that our method can be thousands of times faster than the previous mixed-integer linear programming (MILP) based approach, while also providing smaller (better) adversarial examples than decision-based black-box attacks on general
ℓ
p
ℓ
(
p
=
1
,
2
,
∞
p
) norm perturbations."
neurips,https://proceedings.neurips.cc/paper/2020/file/ba4849411c8bbdd386150e5e32204198-Paper.pdf,Learning Continuous System Dynamics from Irregularly-Sampled Partial Observations,"Zijie Huang, Yizhou Sun, Wei Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/ba5451d3c91a0f982f103cdbe249bc78-Paper.pdf,Online Bayesian Persuasion,"Matteo Castiglioni, Andrea Celli, Alberto Marchesi, Nicola Gatti",
neurips,https://proceedings.neurips.cc/paper/2020/file/ba7e36c43aff315c00ec2b8625e3b719-Paper.pdf,Robust Pre-Training by Adversarial Contrastive Learning,"Ziyu Jiang, Tianlong Chen, Ting Chen, Zhangyang Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/ba95d78a7c942571185308775a97a3a0-Paper.pdf,Random Walk Graph Neural Networks,"Giannis Nikolentzos, Michalis Vazirgiannis",
neurips,https://proceedings.neurips.cc/paper/2020/file/ba9a56ce0a9bfa26e8ed9e10b2cc8f46-Paper.pdf,"Explore Aggressively, Update Conservatively: Stochastic Extragradient Methods with Variable Stepsize Scaling","Yu-Guan Hsieh, Franck Iutzeler, Jérôme Malick, Panayotis Mertikopoulos",
neurips,https://proceedings.neurips.cc/paper/2020/file/babcff88f8be8c4795bd6f0f8cccca61-Paper.pdf,"Fast and Accurate
k
k
-means++ via Rejection Sampling","Vincent Cohen-Addad, Silvio Lattanzi, Ashkan Norouzi-Fard, Christian Sohler, Ola Svensson","k
k
-means++ \cite{arthur2007k} is a widely used clustering algorithm that is easy to implement, has nice theoretical guarantees and strong empirical performance. Despite its wide adoption,
k
k
-means++ sometimes suffers from being slow on large data-sets so a natural question has been to obtain more efficient algorithms with similar guarantees. In this paper, we present such a near linear time algorithm for
k
k
-means++ seeding. Interestingly our algorithm obtains the same theoretical guarantees as
k
k
-means++ and significantly improves earlier results on fast
k
k
-means++ seeding. Moreover, we show empirically that our algorithm is significantly faster than
k
k
-means++ and obtains solutions of equivalent quality."
neurips,https://proceedings.neurips.cc/paper/2020/file/bacadc62d6e67d7897cef027fa2d416c-Paper.pdf,Variational Amodal Object Completion,"Huan Ling, David Acuna, Karsten Kreis, Seung Wook Kim, Sanja Fidler",
neurips,https://proceedings.neurips.cc/paper/2020/file/bae876e53dab654a3d9d9768b1b7b91a-Paper.pdf,When Counterpoint Meets Chinese Folk Melodies,"Nan Jiang, Sheng Jin, Zhiyao Duan, Changshui Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/bb073f2855d769be5bf191f6378f7150-Paper.pdf,Sub-linear Regret Bounds for Bayesian Optimisation in Unknown Search Spaces,"Hung Tran-The, Sunil Gupta, Santu Rana, Huong Ha, Svetha Venkatesh",
neurips,https://proceedings.neurips.cc/paper/2020/file/bb7946e7d85c81a9e69fee1cea4a087c-Paper.pdf,Universal Domain Adaptation through Self Supervision,"Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Kate Saenko",
neurips,https://proceedings.neurips.cc/paper/2020/file/bc047286b224b7bfa73d4cb02de1238d-Paper.pdf,Patch2Self: Denoising Diffusion MRI with Self-Supervised Learning,"Shreyas Fadnavis, Joshua Batson, Eleftherios Garyfallidis",
neurips,https://proceedings.neurips.cc/paper/2020/file/bc573864331a9e42e4511de6f678aa83-Paper.pdf,Stochastic Normalization,"Zhi Kou, Kaichao You, Mingsheng Long, Jianmin Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/bc6d753857fe3dd4275dff707dedf329-Paper.pdf,Constrained episodic reinforcement learning in concave-convex and knapsack settings,"Kianté Brantley, Miro Dudik, Thodoris Lykouris, Sobhan Miryoosefi, Max Simchowitz, Aleksandrs Slivkins, Wen Sun",
neurips,https://proceedings.neurips.cc/paper/2020/file/bca382c81484983f2d437f97d1e141f3-Paper.pdf,On Learning Ising Models under Huber's Contamination Model,"Adarsh Prasad, Vishwak Srinivasan, Sivaraman Balakrishnan, Pradeep Ravikumar",
neurips,https://proceedings.neurips.cc/paper/2020/file/bce9abf229ffd7e570818476ee5d7dde-Paper.pdf,Cross-validation Confidence Intervals for Test Error,"Pierre Bayle, Alexandre Bayle, Lucas Janson, Lester Mackey",
neurips,https://proceedings.neurips.cc/paper/2020/file/bcf9d6bd14a2095866ce8c950b702341-Paper.pdf,DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation,"Alexandre Carlier, Martin Danelljan, Alexandre Alahi, Radu Timofte",
neurips,https://proceedings.neurips.cc/paper/2020/file/bcff3f632fd16ff099a49c2f0932b47a-Paper.pdf,Bayesian Attention Modules,"Xinjie Fan, Shujian Zhang, Bo Chen, Mingyuan Zhou",
neurips,https://proceedings.neurips.cc/paper/2020/file/bd4d08cd70f4be1982372107b3b448ef-Paper.pdf,Robustness Analysis of Non-Convex Stochastic Gradient Descent using Biased Expectations,"Kevin Scaman, Cedric Malherbe","This work proposes a novel analysis of stochastic gradient descent (SGD) for non-convex and smooth optimization. Our analysis sheds light on the impact of the probability distribution of the gradient noise on the convergence rate of the norm of the gradient. In the case of sub-Gaussian and centered noise, we prove that, with probability
1
−
δ
1
, the number of iterations to reach a precision
ε
ε
for the squared gradient norm is
O
(
ε
−
2
ln
(
1
/
δ
)
)
O
. In the case of centered and integrable heavy-tailed noise, we show that, while the expectation of the iterates may be infinite, the squared gradient norm still converges with probability
1
−
δ
1
in
O
(
ε
−
p
δ
−
q
)
O
iterations, where
p
,
q
>
2
p
. This result shows that heavy-tailed noise on the gradient slows down the convergence of SGD without preventing it, proving that SGD is robust to gradient noise with unbounded variance, a setting of interest for Deep Learning. In addition, it indicates that choosing a step size proportional to
T
−
1
/
b
T
where
b
b
is the tail-parameter of the noise and
T
T
is the number of iterations leads to the best convergence rates. Both results are simple corollaries of a unified analysis using the novel concept of biased expectations, a simple and intuitive mathematical tool to obtain concentration inequalities. Using this concept, we propose a new quantity to measure the amount of noise added to the gradient, and discuss its value in multiple scenarios."
neurips,https://proceedings.neurips.cc/paper/2020/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf,SoftFlow: Probabilistic Framework for Normalizing Flow on Manifolds,"Hyeongju Kim, Hyeonseung Lee, Woo Hyun Kang, Joun Yeop Lee, Nam Soo Kim",
neurips,https://proceedings.neurips.cc/paper/2020/file/bdbd5ebfde4934142c8a88e7a3796cd5-Paper.pdf,A meta-learning approach to (re)discover plasticity rules that carve a desired function into a neural network,"Basile Confavreux, Friedemann Zenke, Everton Agnes, Timothy Lillicrap, Tim Vogels",
neurips,https://proceedings.neurips.cc/paper/2020/file/be23c41621390a448779ee72409e5f49-Paper.pdf,Greedy Optimization Provably Wins the Lottery: Logarithmic Number of Winning Tickets is Enough,"Mao Ye, Lemeng Wu, Qiang Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/be53d253d6bc3258a8160556dda3e9b2-Paper.pdf,Path Integral Based Convolution and Pooling for Graph Neural Networks,"Zheng Ma, Junyu Xuan, Yu Guang Wang, Ming Li, Pietro Liò",
neurips,https://proceedings.neurips.cc/paper/2020/file/bea5955b308361a1b07bc55042e25e54-Paper.pdf,Estimating the Effects of Continuous-valued Interventions using Generative Adversarial Networks,"Ioana Bica, James Jordon, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2020/file/beb04c41b45927cf7e9f8fd4bb519e86-Paper.pdf,Latent Dynamic Factor Analysis of High-Dimensional Neural Recordings,"Heejong Bong, Zongge Liu, Zhao Ren, Matthew Smith, Valerie Ventura, Kass E. Robert",
neurips,https://proceedings.neurips.cc/paper/2020/file/befe5b0172188ad14d48c3ebe9cf76bf-Paper.pdf,Conditioning and Processing: Techniques to Improve Information-Theoretic Generalization Bounds,"Hassan Hafez-Kolahi, Zeinab Golgooni, Shohreh Kasaei, Mahdieh Soleymani",
neurips,https://proceedings.neurips.cc/paper/2020/file/bf15e9bbff22c7719020f9df4badc20a-Paper.pdf,Bongard-LOGO: A New Benchmark for Human-Level Concept Learning and Reasoning,"Weili Nie, Zhiding Yu, Lei Mao, Ankit B. Patel, Yuke Zhu, Anima Anandkumar",
neurips,https://proceedings.neurips.cc/paper/2020/file/bf201d5407a6509fa536afc4b380577e-Paper.pdf,GAN Memory with No Forgetting,"Yulai Cong, Miaoyun Zhao, Jianqiao Li, Sijia Wang, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2020/file/bf65417dcecc7f2b0006e1f5793b7143-Paper.pdf,Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games,"Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, Chengqi Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/c0356641f421b381e475776b602a5da8-Paper.pdf,Gaussian Gated Linear Networks,"David Budden, Adam Marblestone, Eren Sezener, Tor Lattimore, Gregory Wayne, Joel Veness",
neurips,https://proceedings.neurips.cc/paper/2020/file/c055dcc749c2632fd4dd806301f05ba6-Paper.pdf,Node Classification on Graphs with Few-Shot Novel Labels via Meta Transformed Network Embedding,"Lin Lan, Pinghui Wang, Xuefeng Du, Kaikai Song, Jing Tao, Xiaohong Guan",
neurips,https://proceedings.neurips.cc/paper/2020/file/c0a271bc0ecb776a094786474322cb82-Paper.pdf,Online Fast Adaptation and Knowledge Accumulation (OSAKA): a New Approach to Continual Learning,"Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Page-Caccia, Issam Hadj Laradji, Irina Rish, Alexandre Lacoste, David Vázquez, Laurent Charlin",
neurips,https://proceedings.neurips.cc/paper/2020/file/c0c3a9fb8385d8e03a46adadde9af3bf-Paper.pdf,Convex optimization based on global lower second-order models,"Nikita Doikov, Yurii Nesterov","In this work, we present new second-order algorithms for composite convex optimization, called Contracting-domain Newton methods. These algorithms are affine-invariant and based on global second-order lower approximation for the smooth component of the objective. Our approach has an interpretation both as a second-order generalization of the conditional gradient method, or as a variant of trust-region scheme. Under the assumption, that the problem domain is bounded, we prove
O
(
1
/
k
2
)
O
global rate of convergence in functional residual, where
k
k
is the iteration counter, minimizing convex functions with Lipschitz continuous Hessian. This significantly improves the previously known bound
O
(
1
/
k
)
O
for this type of algorithms. Additionally, we propose a stochastic extension of our method, and present computational results for solving empirical risk minimization problem."
neurips,https://proceedings.neurips.cc/paper/2020/file/c0f971d8cd24364f2029fcb9ac7b71f5-Paper.pdf,Simultaneously Learning Stochastic and Adversarial Episodic MDPs with Known Transition,"Tiancheng Jin, Haipeng Luo",
neurips,https://proceedings.neurips.cc/paper/2020/file/c10f48884c9c7fdbd9a7959c59eebea8-Paper.pdf,Relative gradient optimization of the Jacobian term in unsupervised deep learning,"Luigi Gresele, Giancarlo Fissore, Adrián Javaloy, Bernhard Schölkopf, Aapo Hyvarinen",
neurips,https://proceedings.neurips.cc/paper/2020/file/c1502ae5a4d514baec129f72948c266e-Paper.pdf,Self-Supervised Visual Representation Learning from Hierarchical Grouping,"Xiao Zhang, Michael Maire",
neurips,https://proceedings.neurips.cc/paper/2020/file/c15203a83f778ce8934d0efaf2d5c6f3-Paper.pdf,Optimal Variance Control of the Score-Function Gradient Estimator for Importance-Weighted Bounds,"Valentin Liévin, Andrea Dittadi, Anders Christensen, Ole Winther","This paper introduces novel results for the score-function gradient estimator of the importance-weighted variational bound (IWAE). We prove that in the limit of large
K
K
(number of importance samples) one can choose the control variate such that the Signal-to-Noise ratio (SNR) of the estimator grows as
√
K
K
. This is in contrast to the standard pathwise gradient estimator where the SNR decreases as
1
/
√
K
1
. Based on our theoretical findings we develop a novel control variate that extends on VIMCO. Empirically, for the training of both continuous and discrete generative models, the proposed method yields superior variance reduction, resulting in an SNR for IWAE that increases with
K
K
without relying on the reparameterization trick. The novel estimator is competitive with state-of-the-art reparameterization-free gradient estimators such as Reweighted Wake-Sleep (RWS) and the thermodynamic variational objective (TVO) when training generative models."
neurips,https://proceedings.neurips.cc/paper/2020/file/c16a5320fa475530d9583c34fd356ef5-Paper.pdf,Explicit Regularisation in Gaussian Noise Injections,"Alexander Camuto, Matthew Willetts, Umut Simsekli, Stephen J. Roberts, Chris C. Holmes",
neurips,https://proceedings.neurips.cc/paper/2020/file/c1714160652ca6408774473810765950-Paper.pdf,Numerically Solving Parametric Families of High-Dimensional Kolmogorov Partial Differential Equations via Deep Learning,"Julius Berner, Markus Dablander, Philipp Grohs",
neurips,https://proceedings.neurips.cc/paper/2020/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf,Finite-Time Analysis for Double Q-learning,"Huaqing Xiong, Lin Zhao, Yingbin Liang, Wei Zhang","Although Q-learning is one of the most successful algorithms for finding the best action-value function (and thus the optimal policy) in reinforcement learning, its implementation often suffers from large overestimation of Q-function values incurred by random sampling. The double Q-learning algorithm proposed in~\citet{hasselt2010double} overcomes such an overestimation issue by randomly switching the update between two Q-estimators, and has thus gained significant popularity in practice. However, the theoretical understanding of double Q-learning is rather limited. So far only the asymptotic convergence has been established, which does not characterize how fast the algorithm converges. In this paper, we provide the first non-asymptotic (i.e., finite-time) analysis for double Q-learning. We show that both synchronous and asynchronous double Q-learning are guaranteed to converge to an
ϵ
ϵ
-accurate neighborhood of the global optimum by taking
~
Ω
(
(
1
(
1
−
γ
)
6
ϵ
2
)
1
ω
+
(
1
1
−
γ
)
1
1
−
ω
)
Ω
iterations, where
ω
∈
(
0
,
1
)
ω
is the decay parameter of the learning rate, and
γ
γ
is the discount factor. Our analysis develops novel techniques to derive finite-time bounds on the difference between two inter-connected stochastic processes, which is new to the literature of stochastic approximation."
neurips,https://proceedings.neurips.cc/paper/2020/file/c213877427b46fa96cff6c39e837ccee-Paper.pdf,Learning to Detect Objects with a 1 Megapixel Event Camera,"Etienne Perot, Pierre de Tournemire, Davide Nitti, Jonathan Masci, Amos Sironi",
neurips,https://proceedings.neurips.cc/paper/2020/file/c21f4ce780c5c9d774f79841b81fdc6d-Paper.pdf,End-to-End Learning and Intervention in Games,"Jiayang Li, Jing Yu, Yu Nie, Zhaoran Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/c22abfa379f38b5b0411bc11fa9bf92f-Paper.pdf,Least Squares Regression with Markovian Data: Fundamental Limits and Algorithms,"Dheeraj Nagaraj, Xian Wu, Guy Bresler, Prateek Jain, Praneeth Netrapalli","We study the problem of least squares linear regression where the datapoints are dependent and are sampled from a Markov chain. We establish sharp information theoretic minimax lower bounds for this problem in terms of
\tmix
\tmix
, the mixing time of the underlying Markov chain, under different noise settings. Our results establish that in general, optimization with Markovian data is strictly harder than optimization with independent data and a trivial algorithm (SGD-DD) that works with only one in every
\tmix
\tmix
samples, which are approximately independent, is minimax optimal. In fact, it is strictly better than the popular Stochastic Gradient Descent (SGD) method with constant step-size which is otherwise minimax optimal in the regression with independent data setting. Beyond a worst case analysis, we investigate whether structured datasets seen in practice such as Gaussian auto-regressive dynamics can admit more efficient optimization schemes. Surprisingly, even in this specific and natural setting, Stochastic Gradient Descent (SGD) with constant step-size is still no better than SGD-DD. Instead, we propose an algorithm based on experience replay--a popular reinforcement learning technique--that achieves a significantly better error rate. Our improved rate serves as one of the first results where an algorithm outperforms SGD-DD on an interesting Markov chain and also provides one of the first theoretical analyses to support the use of experience replay in practice."
neurips,https://proceedings.neurips.cc/paper/2020/file/c236337b043acf93c7df397fdb9082b3-Paper.pdf,"Predictive coding in balanced neural networks with noise, chaos and delays","Jonathan Kadmon, Jonathan Timcheck, Surya Ganguli",
neurips,https://proceedings.neurips.cc/paper/2020/file/c24c65259d90ed4a19ab37b6fd6fe716-Paper.pdf,Interpolation Technique to Speed Up Gradients Propagation in Neural ODEs,"Talgat Daulbaev, Alexandr Katrutsa, Larisa Markeeva, Julia Gusak, Andrzej Cichocki, Ivan Oseledets",
neurips,https://proceedings.neurips.cc/paper/2020/file/c24fe9f765a44048868b5a620f05678e-Paper.pdf,On the Equivalence between Online and Private Learnability beyond Binary Classification,"Young Jung, Baekjin Kim, Ambuj Tewari",
neurips,https://proceedings.neurips.cc/paper/2020/file/c28e5b0c9841b5ef396f9f519bf6c217-Paper.pdf,AViD Dataset: Anonymized Videos from Diverse Countries,"AJ Piergiovanni, Michael Ryoo",
neurips,https://proceedings.neurips.cc/paper/2020/file/c291b01517f3e6797c774c306591cc32-Paper.pdf,Probably Approximately Correct Constrained Learning,"Luiz Chamon, Alejandro Ribeiro",
neurips,https://proceedings.neurips.cc/paper/2020/file/c2964caac096f26db222cb325aa267cb-Paper.pdf,RATT: Recurrent Attention to Transient Tasks for Continual Image Captioning,"Riccardo Del Chiaro, Bartłomiej Twardowski, Andrew Bagdanov, Joost van de Weijer",
neurips,https://proceedings.neurips.cc/paper/2020/file/c2ba1bc54b239208cb37b901c0d3b363-Paper.pdf,"Decisions, Counterfactual Explanations and Strategic Behavior","Stratis Tsirtsis, Manuel Gomez Rodriguez",
neurips,https://proceedings.neurips.cc/paper/2020/file/c2f32522a84d5e6357e6abac087f1b0b-Paper.pdf,Hierarchical Patch VAE-GAN: Generating Diverse Videos from a Single Sample,"Shir Gur, Sagie Benaim, Lior Wolf",
neurips,https://proceedings.neurips.cc/paper/2020/file/c336346c777707e09cab2a3c79174d90-Paper.pdf,A Feasible Level Proximal Point Method for Nonconvex Sparse Constrained Optimization,"Digvijay Boob, Qi Deng, Guanghui Lan, Yilin Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/c348616cd8a86ee661c7c98800678fad-Paper.pdf,Reservoir Computing meets Recurrent Kernels and Structured Transforms,"Jonathan Dong, Ruben Ohana, Mushegh Rafayelyan, Florent Krzakala",
neurips,https://proceedings.neurips.cc/paper/2020/file/c3535febaff29fcb7c0d20cbe94391c7-Paper.pdf,Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection,"Zeyi Huang, Yang Zou, B. V. K. Vijaya Kumar, Dong Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/c3581d2150ff68f3b33b22634b8adaea-Paper.pdf,Linear Dynamical Systems as a Core Computational Primitive,Shiva Kaul,
neurips,https://proceedings.neurips.cc/paper/2020/file/c37f9e1283cbd4a6edfd778fc8b1c652-Paper.pdf,Ratio Trace Formulation of Wasserstein Discriminant Analysis,"Hexuan Liu, Yunfeng Cai, You-Lin Chen, Ping Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/c3992e9a68c5ae12bd18488bc579b30d-Paper.pdf,PAC-Bayes Analysis Beyond the Usual Bounds,"Omar Rivasplata, Ilja Kuzborskij, Csaba Szepesvari, John Shawe-Taylor","Specifically, we present a basic PAC-Bayes inequality for stochastic kernels, from which one may derive extensions of various known PAC-Bayes bounds as well as novel bounds. We clarify the role of the requirements of fixed ‘data-free’ priors, bounded losses, and i.i.d. data. We highlight that those requirements were used to upper-bound an exponential moment term, while the basic PAC-Bayes theorem remains valid without those restrictions. We present three bounds that illustrate the use of data-dependent priors, including one for the unbounded square loss."
neurips,https://proceedings.neurips.cc/paper/2020/file/c39e1a03859f9ee215bc49131d0caf33-Paper.pdf,Few-shot Visual Reasoning with Meta-Analogical Contrastive Learning,"Youngsung Kim, Jinwoo Shin, Eunho Yang, Sung Ju Hwang",
neurips,https://proceedings.neurips.cc/paper/2020/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf,MPNet: Masked and Permuted Pre-training for Language Understanding,"Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/c41dd99a69df04044aa4e33ece9c9249-Paper.pdf,Reinforcement Learning with Feedback Graphs,"Christoph Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari, Karthik Sridharan",
neurips,https://proceedings.neurips.cc/paper/2020/file/c42f891cebbc81aa59f8f183243ac2b9-Paper.pdf,Zap Q-Learning With Nonlinear Function Approximation,"Shuhang Chen, Adithya M Devraj, Fan Lu, Ana Busic, Sean Meyn",
neurips,https://proceedings.neurips.cc/paper/2020/file/c46482dd5d39742f0bfd417b492d0e8e-Paper.pdf,Lipschitz-Certifiable Training with a Tight Outer Bound,"Sungyoon Lee, Jaewook Lee, Saerom Park","Verifiable training is a promising research direction for training a robust network. However, most verifiable training methods are slow or lack scalability. In this study, we propose a fast and scalable certifiable training algorithm based on Lipschitz analysis and interval arithmetic. Our certifiable training algorithm provides a tight propagated outer bound by introducing the box constraint propagation (BCP), and it efficiently computes the worst logit over the outer bound. In the experiments, we show that BCP achieves a tighter outer bound than the global Lipschitz-based outer bound. Moreover, our certifiable training algorithm is over 12 times faster than the state-of-the-art dual relaxation-based method; however, it achieves comparable or better verification performance, improving natural accuracy. Our fast certifiable training algorithm with the tight outer bound can scale to Tiny ImageNet with verification accuracy of 20.1\% (
ℓ
2
ℓ
-perturbation of
ϵ
=
36
/
255
ϵ
). Our code is available at \url{https://github.com/sungyoon-lee/bcp}."
neurips,https://proceedings.neurips.cc/paper/2020/file/c49e446a46fa27a6e18ffb6119461c3f-Paper.pdf,Fast Adaptive Non-Monotone Submodular Maximization Subject to a Knapsack Constraint,"Georgios Amanatidis, Federico Fusco, Philip Lazos, Stefano Leonardi, Rebecca Reiffenhäuser","Constrained submodular maximization problems encompass a wide variety of applications, including personalized recommendation, team formation, and revenue maximization via viral marketing. The massive instances occurring in modern-day applications can render existing algorithms prohibitively slow. Moreover, frequently those instances are also inherently stochastic. Focusing on these challenges, we revisit the classic problem of maximizing a (possibly non-monotone) submodular function subject to a knapsack constraint. We present a simple randomized greedy algorithm that achieves a
5.83
5.83
approximation and runs in
O
(
n
log
n
)
O
time, i.e., at least a factor
n
n
faster than other state-of-the-art algorithms. The robustness of our approach allows us to further transfer it to a stochastic version of the problem. There, we obtain a 9-approximation to the best adaptive policy, which is the first constant approximation for non-monotone objectives. Experimental evaluation of our algorithms showcases their improved performance on real and synthetic data."
neurips,https://proceedings.neurips.cc/paper/2020/file/c4b108f53550f1d5967305a9a8140ddd-Paper.pdf,Conformal Symplectic and Relativistic Optimization,"Guilherme Franca, Jeremias Sulam, Daniel Robinson, Rene Vidal",
neurips,https://proceedings.neurips.cc/paper/2020/file/c4c28b367e14df88993ad475dedf6b77-Paper.pdf,Bayes Consistency vs. H-Consistency: The Interplay between Surrogate Loss Functions and the Scoring Function Class,"Mingyuan Zhang, Shivani Agarwal",
neurips,https://proceedings.neurips.cc/paper/2020/file/c4ede56bbd98819ae6112b20ac6bf145-Paper.pdf,Inverting Gradients - How easy is it to break privacy in federated learning?,"Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, Michael Moeller",
neurips,https://proceedings.neurips.cc/paper/2020/file/c4fac8fb3c9e17a2f4553a001f631975-Paper.pdf,Dynamic allocation of limited memory resources in reinforcement learning,"Nisheet Patel, Luigi Acerbi, Alexandre Pouget",
neurips,https://proceedings.neurips.cc/paper/2020/file/c519d47c329c79537fbb2b6f1c551ff0-Paper.pdf,CryptoNAS: Private Inference on a ReLU Budget,"Zahra Ghodsi, Akshaj Kumar Veldanda, Brandon Reagen, Siddharth Garg",
neurips,https://proceedings.neurips.cc/paper/2020/file/c589c3a8f99401b24b9380e86d939842-Paper.pdf,A Stochastic Path Integral Differential EstimatoR Expectation Maximization Algorithm,"Gersende Fort, Eric Moulines, Hoi-To Wai","The Expectation Maximization (EM) algorithm is of key importance for inference in latent variable models including mixture of regressors and experts, missing observations. This paper introduces a novel EM algorithm, called {\tt SPIDER-EM}, for inference from a training set of size
n
n
,
n
≫
1
n
. At the core of our algorithm is an estimator of the full conditional expectation in the {\sf E}-step, adapted from the stochastic path integral differential estimator ({\tt SPIDER}) technique. We derive finite-time complexity bounds for smooth non-convex likelihood: we show that for convergence to an
ϵ
ϵ
-approximate stationary point, the complexity scales as
K
O
p
t
(
n
,
ϵ
)
=
O
(
ϵ
−
1
)
K
and
K
C
E
(
n
,
ϵ
)
=
n
+
√
n
O
(
ϵ
−
1
)
K
, where
K
O
p
t
(
n
,
ϵ
)
K
and
K
C
E
(
n
,
ϵ
)
K
are respectively the number of {\sf M}-steps and the number of per-sample conditional expectations evaluations. This improves over the state-of-the-art algorithms. Numerical results support our findings."
neurips,https://proceedings.neurips.cc/paper/2020/file/c5a0ac0e2f48af1a4e619e7036fe5977-Paper.pdf,CHIP: A Hawkes Process Model for Continuous-time Networks with Scalable and Consistent Estimation,"Makan Arastuie, Subhadeep Paul, Kevin Xu",
neurips,https://proceedings.neurips.cc/paper/2020/file/c5c1bda1194f9423d744e0ef67df94ee-Paper.pdf,SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection,"Xiaoya Li, Yuxian Meng, Mingxin Zhou, Qinghong Han, Fei Wu, Jiwei Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/c5c3d4fe6b2cc463c7d7ecba17cc9de7-Paper.pdf,Design Space for Graph Neural Networks,"Jiaxuan You, Zhitao Ying, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2020/file/c5d736809766d46260d816d8dbc9eb44-Paper.pdf,HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis,"Jungil Kong, Jaehyeon Kim, Jaekyoung Bae",
neurips,https://proceedings.neurips.cc/paper/2020/file/c5f5c23be1b71adb51ea9dc8e9d444a8-Paper.pdf,Unbalanced Sobolev Descent,"Youssef Mroueh, Mattia Rigotti",
neurips,https://proceedings.neurips.cc/paper/2020/file/c6102b3727b2a7d8b1bb6981147081ef-Paper.pdf,Identifying Mislabeled Data using the Area Under the Margin Ranking,"Geoff Pleiss, Tianyi Zhang, Ethan Elenberg, Kilian Q. Weinberger",
neurips,https://proceedings.neurips.cc/paper/2020/file/c61f571dbd2fb949d3fe5ae1608dd48b-Paper.pdf,Combining Deep Reinforcement Learning and Search for Imperfect-Information Games,"Noam Brown, Anton Bakhtin, Adam Lerer, Qucheng Gong",
neurips,https://proceedings.neurips.cc/paper/2020/file/c6447300d99fdbf4f3f7966295b8b5be-Paper.pdf,High-Throughput Synchronous Deep RL,"Iou-Jen Liu, Raymond Yeh, Alexander Schwing",
neurips,https://proceedings.neurips.cc/paper/2020/file/c68c9c8258ea7d85472dd6fd0015f047-Paper.pdf,Contrastive Learning with Adversarial Examples,"Chih-Hui Ho, Nuno Nvasconcelos",
neurips,https://proceedings.neurips.cc/paper/2020/file/c6a01432c8138d46ba39957a8250e027-Paper.pdf,Mixed Hamiltonian Monte Carlo for Mixed Discrete and Continuous Variables,Guangyao Zhou,
neurips,https://proceedings.neurips.cc/paper/2020/file/c6b8c8d762da15fa8dbbdfb6baf9e260-Paper.pdf,Adversarial Sparse Transformer for Time Series Forecasting,"Sifan Wu, Xi Xiao, Qianggang Ding, Peilin Zhao, Ying Wei, Junzhou Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/c6dfc6b7c601ac2978357b7a81e2d7ae-Paper.pdf,The Surprising Simplicity of the Early-Time Learning Dynamics of Neural Networks,"Wei Hu, Lechao Xiao, Ben Adlam, Jeffrey Pennington",
neurips,https://proceedings.neurips.cc/paper/2020/file/c6e81542b125c36346d9167691b8bd09-Paper.pdf,CLEARER: Multi-Scale Neural Architecture Search for Image Restoration,"Yuanbiao Gou, Boyun Li, Zitao Liu, Songfan Yang, Xi Peng",
neurips,https://proceedings.neurips.cc/paper/2020/file/c70341de2c112a6b3496aec1f631dddd-Paper.pdf,Hierarchical Gaussian Process Priors for Bayesian Neural Network Weights,"Theofanis Karaletsos, Thang D. Bui",
neurips,https://proceedings.neurips.cc/paper/2020/file/c74956ffb38ba48ed6ce977af6727275-Paper.pdf,Compositional Explanations of Neurons,"Jesse Mu, Jacob Andreas",
neurips,https://proceedings.neurips.cc/paper/2020/file/c74c4bf0dad9cbae3d80faa054b7d8ca-Paper.pdf,Calibrated Reliable Regression using Maximum Mean Discrepancy,"Peng Cui, Wenbo Hu, Jun Zhu",
neurips,https://proceedings.neurips.cc/paper/2020/file/c76e4b2fa54f8506719a5c0dc14c2eb9-Paper.pdf,Directional convergence and alignment in deep learning,"Ziwei Ji, Matus Telgarsky",
neurips,https://proceedings.neurips.cc/paper/2020/file/c793b3be8f18731f2a4c627fb3c6c63d-Paper.pdf,Functional Regularization for Representation Learning: A Unified Theoretical Perspective,"Siddhant Garg, Yingyu Liang",
neurips,https://proceedings.neurips.cc/paper/2020/file/c7af0926b294e47e52e46cfebe173f20-Paper.pdf,Provably Efficient Online Hyperparameter Optimization with Population-Based Bandits,"Jack Parker-Holder, Vu Nguyen, Stephen J. Roberts",
neurips,https://proceedings.neurips.cc/paper/2020/file/c7bf0b7c1a86d5eb3be2c722cf2cf746-Paper.pdf,Understanding Global Feature Contributions With Additive Importance Measures,"Ian Covert, Scott M. Lundberg, Su-In Lee",
neurips,https://proceedings.neurips.cc/paper/2020/file/c7c46d4baf816bfb07c7f3bf96d88544-Paper.pdf,Online Non-Convex Optimization with Imperfect Feedback,"Amélie Héliou, Matthieu Martin, Panayotis Mertikopoulos, Thibaud Rahier",
neurips,https://proceedings.neurips.cc/paper/2020/file/c8067ad1937f728f51288b3eb986afaa-Paper.pdf,Co-Tuning for Transfer Learning,"Kaichao You, Zhi Kou, Mingsheng Long, Jianmin Wang","Fine-tuning pre-trained deep neural networks (DNNs) to a target dataset, also known as transfer learning, is widely used in computer vision and NLP. Because task-specific layers mainly contain categorical information and categories vary with datasets, practitioners only \textit{partially} transfer pre-trained models by discarding task-specific layers and fine-tuning bottom layers. However, it is a reckless loss to simply discard task-specific parameters who take up as many as
20
%
20
of the total parameters in pre-trained models. To \textit{fully} transfer pre-trained models, we propose a two-step framework named \textbf{Co-Tuning}: (i) learn the relationship between source categories and target categories from the pre-trained model and calibrated predictions; (ii) target labels (one-hot labels), as well as source labels (probabilistic labels) translated by the category relationship, collaboratively supervise the fine-tuning process. A simple instantiation of the framework shows strong empirical results in four visual classification tasks and one NLP classification task, bringing up to
20
%
20
relative improvement. While state-of-the-art fine-tuning techniques mainly focus on how to impose regularization when data are not abundant, Co-Tuning works not only in medium-scale datasets (100 samples per class) but also in large-scale datasets (1000 samples per class) where regularization-based methods bring no gains over the vanilla fine-tuning. Co-Tuning relies on a typically valid assumption that the pre-trained dataset is diverse enough, implying its broad application area."
neurips,https://proceedings.neurips.cc/paper/2020/file/c80d9ba4852b67046bee487bcd9802c0-Paper.pdf,Multifaceted Uncertainty Estimation for Label-Efficient Deep Learning,"Weishi Shi, Xujiang Zhao, Feng Chen, Qi Yu",
neurips,https://proceedings.neurips.cc/paper/2020/file/c81e728d9d4c2f636f067f89cc14862c-Paper.pdf,Continuous Surface Embeddings,"Natalia Neverova, David Novotny, Marc Szafraniec, Vasil Khalidov, Patrick Labatut, Andrea Vedaldi",
neurips,https://proceedings.neurips.cc/paper/2020/file/c82b013313066e0702d58dc70db033ca-Paper.pdf,Succinct and Robust Multi-Agent Communication With Temporal Message Control,"Sai Qian Zhang, Qi Zhang, Jieyu Lin",
neurips,https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf,Big Bird: Transformers for Longer Sequences,"Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed","Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having
O
(
1
)
O
global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."
neurips,https://proceedings.neurips.cc/paper/2020/file/c8b9abffb45bf79a630fb613dcd23449-Paper.pdf,Neural Execution Engines: Learning to Execute Subroutines,"Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, Milad Hashemi",
neurips,https://proceedings.neurips.cc/paper/2020/file/c8cc6e90ccbff44c9cee23611711cdc4-Paper.pdf,Random Reshuffling: Simple Analysis with Vast Improvements,"Konstantin Mishchenko, Ahmed Khaled, Peter Richtarik","Random Reshuffling (RR) is an algorithm for minimizing finite-sum functions that utilizes iterative gradient descent steps in conjunction with data reshuffling. Often contrasted with its sibling Stochastic Gradient Descent (SGD), RR is usually faster in practice and enjoys significant popularity in convex and non-convex optimization. The convergence rate of RR has attracted substantial attention recently and, for strongly convex and smooth functions, it was shown to converge faster than SGD if 1) the stepsize is small, 2) the gradients are bounded, and 3) the number of epochs is large. We remove these 3 assumptions, improve the dependence on the condition number from
κ
2
κ
to
κ
κ
(resp.\ from
κ
κ
to
√
κ
κ
) and, in addition, show that RR has a different type of variance. We argue through theory and experiments that the new variance type gives an additional justification of the superior performance of RR. To go beyond strong convexity, we present several results for non-strongly convex and non-convex objectives. We show that in all cases, our theory improves upon existing literature. Finally, we prove fast convergence of the Shuffle-Once (SO) algorithm, which shuffles the data only once, at the beginning of the optimization process. Our theory for strongly convex objectives tightly matches the known lower bounds for both RR and SO and substantiates the common practical heuristic of shuffling once or only a few times. As a byproduct of our analysis, we also get new results for the Incremental Gradient algorithm (IG), which does not shuffle the data at all."
neurips,https://proceedings.neurips.cc/paper/2020/file/c8d3a760ebab631565f8509d84b3b3f1-Paper.pdf,Long-Horizon Visual Planning with Goal-Conditioned Hierarchical Predictors,"Karl Pertsch, Oleh Rybkin, Frederik Ebert, Shenghao Zhou, Dinesh Jayaraman, Chelsea Finn, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2020/file/c8ecfaea0b7e3aa83b017a786d53b9e8-Paper.pdf,Statistical Optimal Transport posed as Learning Kernel Embedding,"Saketha Nath Jagarlapudi, Pratik Kumar Jawanpuria","The objective in statistical Optimal Transport (OT) is to consistently estimate the optimal transport plan/map solely using samples from the given source and target marginal distributions. This work takes the novel approach of posing statistical OT as that of learning the transport plan's kernel mean embedding from sample based estimates of marginal embeddings. The proposed estimator controls overfitting by employing maximum mean discrepancy based regularization, which is complementary to
ϕ
ϕ
-divergence (entropy) based regularization popularly employed in existing estimators. A key result is that, under very mild conditions,
ϵ
ϵ
-optimal recovery of the transport plan as well as the Barycentric-projection based transport map is possible with a sample complexity that is completely dimension-free. Moreover, the implicit smoothing in the kernel mean embeddings enables out-of-sample estimation. An appropriate representer theorem is proved leading to a kernelized convex formulation for the estimator, which can then be potentially used to perform OT even in non-standard domains. Empirical results illustrate the efficacy of the proposed approach."
neurips,https://proceedings.neurips.cc/paper/2020/file/c91591a8d461c2869b9f535ded3e213e-Paper.pdf,Dual-Resolution Correspondence Networks,"Xinghui Li, Kai Han, Shuda Li, Victor Prisacariu",
neurips,https://proceedings.neurips.cc/paper/2020/file/c91e3483cf4f90057d02aa492d2b25b1-Paper.pdf,"Advances in Black-Box VI: Normalizing Flows, Importance Weighting, and Optimization","Abhinav Agrawal, Daniel R. Sheldon, Justin Domke",
neurips,https://proceedings.neurips.cc/paper/2020/file/c928d86ff00aeb89a39bd4a80e652a38-Paper.pdf,f-Divergence Variational Inference,"Neng Wan, Dapeng Li, NAIRA HOVAKIMYAN",
neurips,https://proceedings.neurips.cc/paper/2020/file/c94a589bdd47870b1d74b258d1ce3b33-Paper.pdf,Unfolding recurrence by Green’s functions for optimized reservoir computing,"Sandra Nestler, Christian Keup, David Dahmen, Matthieu Gilson, Holger Rauhut, Moritz Helias",
neurips,https://proceedings.neurips.cc/paper/2020/file/c96c08f8bb7960e11a1239352a479053-Paper.pdf,The Dilemma of TriHard Loss and an Element-Weighted TriHard Loss for Person Re-Identification,"Yihao Lv, Youzhi Gu, Liu Xinggao",
neurips,https://proceedings.neurips.cc/paper/2020/file/c9f029a6a1b20a8408f372351b321dd8-Paper.pdf,Disentangling by Subspace Diffusion,"David Pfau, Irina Higgins, Alex Botev, Sébastien Racanière",
neurips,https://proceedings.neurips.cc/paper/2020/file/c9f06bc7b46d0247a91c8fc665c13d0e-Paper.pdf,Towards Neural Programming Interfaces,"Zachary Brown, Nathaniel Robinson, David Wingate, Nancy Fulda",
neurips,https://proceedings.neurips.cc/paper/2020/file/c9f2f917078bd2db12f23c3b413d9cba-Paper.pdf,Discovering Symbolic Models from Deep Learning with Inductive Biases,"Miles Cranmer, Alvaro Sanchez Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, Shirley Ho",
neurips,https://proceedings.neurips.cc/paper/2020/file/ca172e964907a97d5ebd876bfdd4adbd-Paper.pdf,Real World Games Look Like Spinning Tops,"Wojciech M. Czarnecki, Gauthier Gidel, Brendan Tracey, Karl Tuyls, Shayegan Omidshafiei, David Balduzzi, Max Jaderberg",
neurips,https://proceedings.neurips.cc/paper/2020/file/ca3a9be77f7e88708afb20c8cdf44b60-Paper.pdf,Cooperative Heterogeneous Deep Reinforcement Learning,"Han Zheng, Pengfei Wei, Jing Jiang, Guodong Long, Qinghua Lu, Chengqi Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/ca4b5656b7e193e6bb9064c672ac8dce-Paper.pdf,Mitigating Forgetting in Online Continual Learning via Instance-Aware Parameterization,"Hung-Jen Chen, An-Chieh Cheng, Da-Cheng Juan, Wei Wei, Min Sun",
neurips,https://proceedings.neurips.cc/paper/2020/file/ca5520b5672ea120b23bde75c46e76c6-Paper.pdf,ImpatientCapsAndRuns: Approximately Optimal Algorithm Configuration from an Infinite Pool,"Gellert Weisz, András György, Wei-I Lin, Devon Graham, Kevin Leyton-Brown, Csaba Szepesvari, Brendan Lucier",
neurips,https://proceedings.neurips.cc/paper/2020/file/ca7be8306ecc3f5fa30ff2c41e64fa7b-Paper.pdf,Dense Correspondences between Human Bodies via Learning Transformation Synchronization on Graphs,"Xiangru Huang, Haitao Yang, Etienne Vouga, Qixing Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/ca886eb9edb61a42256192745c72cd79-Paper.pdf,Reasoning about Uncertainties in Discrete-Time Dynamical Systems using Polynomial Forms.,"Sriram Sankaranarayanan, Yi Chou, Eric Goubault, Sylvie Putot",
neurips,https://proceedings.neurips.cc/paper/2020/file/cae7115f44837c806c9b23ed00a1a28a-Paper.pdf,Applications of Common Entropy for Causal Inference,"Murat Kocaoglu, Sanjay Shakkottai, Alexandros G. Dimakis, Constantine Caramanis, Sriram Vishwanath",
neurips,https://proceedings.neurips.cc/paper/2020/file/cb8acb1dc9821bf74e6ca9068032d623-Paper.pdf,SGD with shuffling: optimal rates without component convexity and large epoch requirements,"Kwangjun Ahn, Chulhee Yun, Suvrit Sra",
neurips,https://proceedings.neurips.cc/paper/2020/file/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Paper.pdf,Unsupervised Joint k-node Graph Representations with Compositional Energy-Based Models,"Leonardo Cotta, Carlos H. C. Teixeira, Ananthram Swami, Bruno Ribeiro",
neurips,https://proceedings.neurips.cc/paper/2020/file/cbf8710b43df3f2c1553e649403426df-Paper.pdf,Neural Manifold Ordinary Differential Equations,"Aaron Lou, Derek Lim, Isay Katsman, Leo Huang, Qingxuan Jiang, Ser Nam Lim, Christopher M. De Sa",
neurips,https://proceedings.neurips.cc/paper/2020/file/cc384c68ad503482fb24e6d1e3b512ae-Paper.pdf,CO-Optimal Transport,"Vayer Titouan, Ievgen Redko, Rémi Flamary, Nicolas Courty",
neurips,https://proceedings.neurips.cc/paper/2020/file/cc3f5463bc4d26bc38eadc8bcffbc654-Paper.pdf,Continuous Meta-Learning without Tasks,"James Harrison, Apoorva Sharma, Chelsea Finn, Marco Pavone",
neurips,https://proceedings.neurips.cc/paper/2020/file/cc58f7abf0b0cf2d5ac95ab60e4f14e9-Paper.pdf,A mathematical theory of cooperative communication,"Pei Wang, Junqi Wang, Pushpi Paranamana, Patrick Shafto",
neurips,https://proceedings.neurips.cc/paper/2020/file/cc75c256acc04ce25a291c4b7a9856c0-Paper.pdf,Penalized Langevin dynamics with vanishing penalty for smooth and log-concave targets,"Avetik Karagulyan, Arnak Dalalyan","We study the problem of sampling from a probability distribution on
R
p
R
defined via a convex and smooth potential function. We first consider a continuous-time diffusion-type process, termed Penalized Langevin dynamics (PLD), the drift of which is the negative gradient of the potential plus a linear penalty that vanishes when time goes to infinity. An upper bound on the Wasserstein-2 distance between the distribution of the PLD at time
t
t
and the target is established. This upper bound highlights the influence of the speed of decay of the penalty on the accuracy of approximation. As a consequence, in the case of low-temperature limit we infer a new result on the convergence of the penalized gradient flow for the optimization problem."
neurips,https://proceedings.neurips.cc/paper/2020/file/cc8090c4d2791cdd9cd2cb3c24296190-Paper.pdf,Learning Invariances in Neural Networks from Training Data,"Gregory Benton, Marc Finzi, Pavel Izmailov, Andrew G. Wilson",
neurips,https://proceedings.neurips.cc/paper/2020/file/cc9b3c69b56df284846bf2432f1cba90-Paper.pdf,A Finite-Time Analysis of Two Time-Scale Actor-Critic Methods,"Yue Frank Wu, Weitong ZHANG, Pan Xu, Quanquan Gu","Actor-critic (AC) methods have exhibited great empirical success compared with other reinforcement learning algorithms, where the actor uses the policy gradient to improve the learning policy and the critic uses temporal difference learning to estimate the policy gradient. Under the two time-scale learning rate schedule, the asymptotic convergence of AC has been well studied in the literature. However, the non-asymptotic convergence and finite sample complexity of actor-critic methods are largely open. In this work, we provide a non-asymptotic analysis for two time-scale actor-critic methods under non-i.i.d. setting. We prove that the actor-critic method is guaranteed to find a first-order stationary point (i.e.,
∥
∇
J
(
\bm
θ
)
∥
2
2
≤
ϵ
‖
) of the non-concave performance function
J
(
\bm
θ
)
J
, with
~
O
(
ϵ
−
2.5
)
O
sample complexity. To the best of our knowledge, this is the first work providing finite-time analysis and sample complexity bound for two time-scale actor-critic methods."
neurips,https://proceedings.neurips.cc/paper/2020/file/ccb1d45fb76f7c5a0bf619f979c6cf36-Paper.pdf,Pruning Filter in Filter,"Fanxu Meng, Hao Cheng, Ke Li, Huixiang Luo, Xiaowei Guo, Guangming Lu, Xing Sun",
neurips,https://proceedings.neurips.cc/paper/2020/file/ccb421d5f36c5a412816d494b15ca9f6-Paper.pdf,Learning to Mutate with Hypergradient Guided Population,"Zhiqiang Tao, Yaliang Li, Bolin Ding, Ce Zhang, Jingren Zhou, Yun Fu",
neurips,https://proceedings.neurips.cc/paper/2020/file/ccd2d123f4ec4d777fc6ef757d0fb642-Paper.pdf,A convex optimization formulation for multivariate regression,Yunzhang Zhu,"Multivariate regression (or multi-task learning) concerns the task of predicting the value of multiple responses from a set of covariates. In this article, we propose a convex optimization formulation for high-dimensional multivariate linear regression under a general error covariance structure. The main difficulty with simultaneous estimation of the regression coefficients and the error covariance matrix lies in the fact that the negative log-likelihood function is not convex. To overcome this difficulty, a new parameterization is proposed, under which the negative log-likelihood function is proved to be convex. For faster computation, two other alternative loss functions are also considered, and proved to be convex under the proposed parameterization. This new parameterization is also useful for covariate-adjusted Gaussian graphical modeling in which the inverse of the error covariance matrix is of interest. A joint non-asymptotic analysis of the regression coefficients and the error covariance matrix is carried out under the new parameterization. In particular, we show that the proposed method recovers the oracle estimator under sharp scaling conditions, and rates of convergence in terms of vector
ℓ
∞
ℓ
norm are also established. Empirically, the proposed methods outperform existing high-dimensional multivariate linear regression methods that are based on either minimizing certain non-convex criteria or certain two-step procedures."
neurips,https://proceedings.neurips.cc/paper/2020/file/cceff8faa855336ad53b3325914caea2-Paper.pdf,Online Meta-Critic Learning for Off-Policy Actor-Critic Methods,"Wei Zhou, Yiying Li, Yongxin Yang, Huaimin Wang, Timothy Hospedales",
neurips,https://proceedings.neurips.cc/paper/2020/file/cd0b43eac0392accf3624b7372dec36e-Paper.pdf,The All-or-Nothing Phenomenon in Sparse Tensor PCA,"Jonathan Niles-Weed, Ilias Zadik","We study the statistical problem of estimating a rank-one sparse tensor corrupted by additive gaussian noise, a Gaussian additive model also known as sparse tensor PCA. We show that for Bernoulli and Bernoulli-Rademacher distributed signals and \emph{for all} sparsity levels which are sublinear in the dimension of the signal, the sparse tensor PCA model exhibits a phase transition called the \emph{all-or-nothing phenomenon}. This is the property that for some signal-to-noise ratio (SNR)
S
N
R
c
S
and any fixed
ϵ
>
0
ϵ
, if the SNR of the model is below
(
1
−
ϵ
)
S
N
R
c
(
, then it is impossible to achieve any arbitrarily small constant correlation with the hidden signal, while if the SNR is above
(
1
+
ϵ
)
S
N
R
c
(
, then it is possible to achieve almost perfect correlation with the hidden signal. The all-or-nothing phenomenon was initially established in the context of sparse linear regression, and over the last year also in the context of sparse 2-tensor (matrix) PCA and Bernoulli group testing. Our results follow from a more general result showing that for any Gaussian additive model with a discrete uniform prior, the all-or-nothing phenomenon follows as a direct outcome of an appropriately defined
near-orthogonality"" property of the support of the prior distribution."
neurips,https://proceedings.neurips.cc/paper/2020/file/cd0f74b5955dc87fd0605745c4b49ee8-Paper.pdf,"Synthesize, Execute and Debug: Learning to Repair for Neural Program Synthesis","Kavi Gupta, Peter Ebert Christensen, Xinyun Chen, Dawn Song",
neurips,https://proceedings.neurips.cc/paper/2020/file/cd10c7f376188a4a2ca3e8fea2c03aeb-Paper.pdf,ARMA Nets: Expanding Receptive Field for Dense Prediction,"Jiahao Su, Shiqi Wang, Furong Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/cd3109c63bf4323e6b987a5923becb96-Paper.pdf,Diversity-Guided Multi-Objective Bayesian Optimization With Batch Evaluations,"Mina Konakovic Lukovic, Yunsheng Tian, Wojciech Matusik",
neurips,https://proceedings.neurips.cc/paper/2020/file/cd3afef9b8b89558cd56638c3631868a-Paper.pdf,SOLOv2: Dynamic and Fast Instance Segmentation,"Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, Chunhua Shen",
neurips,https://proceedings.neurips.cc/paper/2020/file/cd42c963390a9cd025d007dacfa99351-Paper.pdf,Robust Recovery via Implicit Bias of Discrepant Learning Rates for Double Over-parameterization,"Chong You, Zhihui Zhu, Qing Qu, Yi Ma",
neurips,https://proceedings.neurips.cc/paper/2020/file/cdaa9b682e10c291d3bbadca4c96f5de-Paper.pdf,Axioms for Learning from Pairwise Comparisons,"Ritesh Noothigattu, Dominik Peters, Ariel D. Procaccia",
neurips,https://proceedings.neurips.cc/paper/2020/file/cdf1035c34ec380218a8cc9a43d438f9-Paper.pdf,Continuous Regularized Wasserstein Barycenters,"Lingxiao Li, Aude Genevay, Mikhail Yurochkin, Justin M. Solomon",
neurips,https://proceedings.neurips.cc/paper/2020/file/cdf6581cb7aca4b7e19ef136c6e601a5-Paper.pdf,Spectral Temporal Graph Neural Network for Multivariate Time-series Forecasting,"Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, Qi Zhang","In this paper, we propose Spectral Temporal Graph Neural Network (StemGNN) to further improve the accuracy of multivariate time-series forecasting. StemGNN captures inter-series correlations and temporal dependencies jointly in the spectral domain. It combines Graph Fourier Transform (GFT) which models inter-series correlations and Discrete Fourier Transform (DFT) which models temporal dependencies in an end-to-end framework. After passing through GFT and DFT, the spectral representations hold clear patterns and can be predicted effectively by convolution and sequential learning modules. Moreover, StemGNN learns inter-series correlations automatically from the data without using pre-defined priors. We conduct extensive experiments on ten real-world datasets to demonstrate the effectiveness of StemGNN."
neurips,https://proceedings.neurips.cc/paper/2020/file/cdfa4c42f465a5a66871587c69fcfa34-Paper.pdf,Online Multitask Learning with Long-Term Memory,"Mark Herbster, Stephen Pasteris, Lisa Tse",
neurips,https://proceedings.neurips.cc/paper/2020/file/ce016f59ecc2366a43e1c96a4774d167-Paper.pdf,Fewer is More: A Deep Graph Metric Learning Perspective Using Fewer Proxies,"Yuehua Zhu, Muli Yang, Cheng Deng, Wei Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/ce1aad92b939420fc17005e5461e6f48-Paper.pdf,Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting,"LEI BAI, Lina Yao, Can Li, Xianzhi Wang, Can Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/ce4449660c6523b377b22a1dc2da5556-Paper.pdf,On Reward-Free Reinforcement Learning with Linear Function Approximation,"Ruosong Wang, Simon S. Du, Lin Yang, Russ R. Salakhutdinov","Reward-free reinforcement learning (RL) is a framework which is suitable for both the batch RL setting and the setting where there are many reward functions of interest. During the exploration phase, an agent collects samples without using a pre-specified reward function. After the exploration phase, a reward function is given, and the agent uses samples collected during the exploration phase to compute a near-optimal policy. Jin et al. [2020] showed that in the tabular setting, the agent only needs to collect polynomial number of samples (in terms of the number states, the number of actions, and the planning horizon) for reward-free RL. However, in practice, the number of states and actions can be large, and thus function approximation schemes are required for generalization. In this work, we give both positive and negative results for reward-free RL with linear function approximation. We give an algorithm for reward-free RL in the linear Markov decision process setting where both the transition and the reward admit linear representations. The sample complexity of our algorithm is polynomial in the feature dimension and the planning horizon, and is completely independent of the number of states and actions. We further give an exponential lower bound for reward-free RL in the setting where only the optimal
Q
Q
-function admits a linear representation. Our results imply several interesting exponential separations on the sample complexity of reward-free RL."
neurips,https://proceedings.neurips.cc/paper/2020/file/ce46f09027b218b46063eb2b858f622d-Paper.pdf,Robustness of Community Detection to Random Geometric Perturbations,"Sandrine Peche, Vianney Perchet",
neurips,https://proceedings.neurips.cc/paper/2020/file/ce758408f6ef98d7c7a7b786eca7b3a8-Paper.pdf,Learning outside the Black-Box: The pursuit of interpretable models,"Jonathan Crabbe, Yao Zhang, William Zame, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2020/file/cebd648f9146a6345d604ab093b02c73-Paper.pdf,Breaking Reversibility Accelerates Langevin Dynamics for Non-Convex Optimization,"Xuefeng GAO, Mert Gurbuzbalaban, Lingjiong Zhu",
neurips,https://proceedings.neurips.cc/paper/2020/file/cec6f62cfb44b1be110b7bf70c8362d8-Paper.pdf,Robust large-margin learning in hyperbolic space,"Melanie Weber, Manzil Zaheer, Ankit Singh Rawat, Aditya K. Menon, Sanjiv Kumar",
neurips,https://proceedings.neurips.cc/paper/2020/file/cfd382c5eb817d52c7faf45a96f20b81-Paper.pdf,Replica-Exchange Nos\'e-Hoover Dynamics for Bayesian Learning on Large Datasets,"Rui Luo, Qiang Zhang, Yaodong Yang, Jun Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/cfee398643cbc3dc5eefc89334cacdc1-Paper.pdf,Adversarially Robust Few-Shot Learning: A Meta-Learning Approach,"Micah Goldblum, Liam Fowl, Tom Goldstein",
neurips,https://proceedings.neurips.cc/paper/2020/file/cff02a74da64d145a4aed3a577a106ab-Paper.pdf,Neural Anisotropy Directions,"Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi, Pascal Frossard",
neurips,https://proceedings.neurips.cc/paper/2020/file/cffb6e2288a630c2a787a64ccc67097c-Paper.pdf,Digraph Inception Convolutional Networks,"Zekun Tong, Yuxuan Liang, Changsheng Sun, Xinke Li, David Rosenblum, Andrew Lim",
neurips,https://proceedings.neurips.cc/paper/2020/file/d02e9bdc27a894e882fa0c9055c99722-Paper.pdf,PAC-Bayesian Bound for the Conditional Value at Risk,"Zakaria Mhammedi, Benjamin Guedj, Robert C. Williamson","Conditional Value at Risk (
\textsc
C
V
a
R
\textsc
) is a
coherent risk measure'' which generalizes expectation (reduced to a boundary parameter setting). Widely used in mathematical finance, it is garnering increasing interest in machine learning as an alternate approach to regularization, and as a means for ensuring fairness. This paper presents a generalization bound for learning algorithms that minimize the
\textsc
C
V
a
R
\textsc
of the empirical loss. The bound is of PAC-Bayesian type and is guaranteed to be small when the empirical
\textsc
C
V
a
R
\textsc
is small. We achieve this by reducing the problem of estimating
\textsc
C
V
a
R
\textsc
to that of merely estimating an expectation. This then enables us, as a by-product, to obtain concentration inequalities for
\textsc
C
V
a
R
\textsc
even when the random variable in question is unbounded."
neurips,https://proceedings.neurips.cc/paper/2020/file/d03a857a23b5285736c4d55e0bb067c8-Paper.pdf,Stochastic Stein Discrepancies,"Jackson Gorham, Anant Raj, Lester Mackey",
neurips,https://proceedings.neurips.cc/paper/2020/file/d04d42cdf14579cd294e5079e0745411-Paper.pdf,On the Role of Sparsity and DAG Constraints for Learning Linear DAGs,"Ignavier Ng, AmirEmad Ghassami, Kun Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/d072677d210ac4c03ba046120f0802ec-Paper.pdf,Cream of the Crop: Distilling Prioritized Paths For One-Shot Neural Architecture Search,"Houwen Peng, Hao Du, Hongyuan Yu, QI LI, Jing Liao, Jianlong Fu",
neurips,https://proceedings.neurips.cc/paper/2020/file/d0921d442ee91b896ad95059d13df618-Paper.pdf,Fair Multiple Decision Making Through Soft Interventions,"Yaowei Hu, Yongkai Wu, Lu Zhang, Xintao Wu",
neurips,https://proceedings.neurips.cc/paper/2020/file/d0bb8259d8fe3c7df4554dab9d7da3c9-Paper.pdf,Representation Learning for Integrating Multi-domain Outcomes to Optimize Individualized Treatment,"Yuan Chen, Donglin Zeng, Tianchen Xu, Yuanjia Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/d1419302db9c022ab1d48681b13d5f8b-Paper.pdf,Learning to Play No-Press Diplomacy with Best Response Policy Iteration,"Thomas Anthony, Tom Eccles, Andrea Tacchetti, János Kramár, Ian Gemp, Thomas Hudson, Nicolas Porcel, Marc Lanctot, Julien Perolat, Richard Everett, Satinder Singh, Thore Graepel, Yoram Bachrach",
neurips,https://proceedings.neurips.cc/paper/2020/file/d15426b9c324676610fbb01360473ed8-Paper.pdf,Inverse Learning of Symmetries,"Mario Wieser, Sonali Parbhoo, Aleksander Wieczorek, Volker Roth",
neurips,https://proceedings.neurips.cc/paper/2020/file/d16a974d4d6d0d71b29bfbfe045f1da7-Paper.pdf,DiffGCN: Graph Convolutional Networks via Differential Operators and Algebraic Multigrid Pooling,"Moshe Eliasof, Eran Treister",
neurips,https://proceedings.neurips.cc/paper/2020/file/d17e6bcbcef8de3f7a00195cfa5706f1-Paper.pdf,Distributed Newton Can Communicate Less and Resist Byzantine Workers,"Avishek Ghosh, Raj Kumar Maity, Arya Mazumdar",
neurips,https://proceedings.neurips.cc/paper/2020/file/d1d5923fc822531bbfd9d87d4760914b-Paper.pdf,Efficient Nonmyopic Bayesian Optimization via One-Shot Multi-Step Trees,"Shali Jiang, Daniel Jiang, Maximilian Balandat, Brian Karrer, Jacob Gardner, Roman Garnett",
neurips,https://proceedings.neurips.cc/paper/2020/file/d1dc3a8270a6f9394f88847d7f0050cf-Paper.pdf,Effective Diversity in Population Based Reinforcement Learning,"Jack Parker-Holder, Aldo Pacchiano, Krzysztof M. Choromanski, Stephen J. Roberts",
neurips,https://proceedings.neurips.cc/paper/2020/file/d1e39c9bda5c80ac3d8ea9d658163967-Paper.pdf,Elastic-InfoGAN: Unsupervised Disentangled Representation Learning in Class-Imbalanced Data,"Utkarsh Ojha, Krishna Kumar Singh, Cho-Jui Hsieh, Yong Jae Lee",
neurips,https://proceedings.neurips.cc/paper/2020/file/d1e7b08bdb7783ed4fb10abe92c22ffd-Paper.pdf,Direct Policy Gradients: Direct Optimization of Policies in Discrete Action Spaces,"Guy Lorberbom, Chris J. Maddison, Nicolas Heess, Tamir Hazan, Daniel Tarlow",
neurips,https://proceedings.neurips.cc/paper/2020/file/d1e946f4e67db4b362ad23818a6fb78a-Paper.pdf,Hybrid Models for Learning to Branch,"Prateek Gupta, Maxime Gasse, Elias Khalil, Pawan Mudigonda, Andrea Lodi, Yoshua Bengio",
neurips,https://proceedings.neurips.cc/paper/2020/file/d1ff1ec86b62cd5f3903ff19c3a326b2-Paper.pdf,WoodFisher: Efficient Second-Order Approximation for Neural Network Compression,"Sidak Pal Singh, Dan Alistarh","Our main application is to neural network compression, where we build on the classic Optimal Brain Damage/Surgeon framework. We demonstrate that WoodFisher significantly outperforms popular state-of-the-art methods for one-shot pruning. Further, even when iterative, gradual pruning is allowed, our method results in a gain in test accuracy over the state-of-the-art approaches for popular image classification datasets such as ImageNet ILSVRC. Further, we show how our method can be extended to take into account first-order information, and illustrate its ability to automatically set layer-wise pruning thresholds, or perform compression in the limited-data regime."
neurips,https://proceedings.neurips.cc/paper/2020/file/d25a34b9c2a87db380ecd7f7115882ec-Paper.pdf,Bi-level Score Matching for Learning Energy-based Latent Variable Models,"Fan Bao, Chongxuan LI, Kun Xu, Hang Su, Jun Zhu, Bo Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/d27b95cac4c27feb850aaa4070cc4675-Paper.pdf,Counterfactual Contrastive Learning for Weakly-Supervised Vision-Language Grounding,"Zhu Zhang, Zhou Zhao, Zhijie Lin, jieming zhu, Xiuqiang He",
neurips,https://proceedings.neurips.cc/paper/2020/file/d2a10b0bd670e442b1d3caa3fbf9e695-Paper.pdf,Decision trees as partitioning machines to characterize their generalization properties,"Jean-Samuel Leboeuf, Frédéric LeBlanc, Mario Marchand","Decision trees are popular machine learning models that are simple to build and easy to interpret. Even though algorithms to learn decision trees date back to almost 50 years, key properties affecting their generalization error are still weakly bounded. Hence, we revisit binary decision trees on real-valued features from the perspective of partitions of the data. We introduce the notion of partitioning function, and we relate it to the growth function and to the VC dimension. Using this new concept, we are able to find the exact VC dimension of decision stumps, which is given by the largest integer
d
d
such that
2
ℓ
≥
(
d
\floor
d
2
)
2
, where
ℓ
ℓ
is the number of real-valued features. We provide a recursive expression to bound the partitioning functions, resulting in a upper bound on the growth function of any decision tree structure. This allows us to show that the VC dimension of a binary tree structure with
N
N
internal nodes is of order
N
log
(
N
ℓ
)
N
. Finally, we elaborate a pruning algorithm based on these results that performs better than the CART algorithm on a number of datasets, with the advantage that no cross-validation is required."
neurips,https://proceedings.neurips.cc/paper/2020/file/d2a27e83d429f0dcae6b937cf440aeb1-Paper.pdf,Learning to Prove Theorems by Learning to Generate Theorems,"Mingzhe Wang, Jia Deng",
neurips,https://proceedings.neurips.cc/paper/2020/file/d2dc6368837861b42020ee72b0896182-Paper.pdf,3D Self-Supervised Methods for Medical Imaging,"Aiham Taleb, Winfried Loetzsch, Noel Danz, Julius Severin, Thomas Gaertner, Benjamin Bergner, Christoph Lippert",
neurips,https://proceedings.neurips.cc/paper/2020/file/d33174c464c877fb03e77efdab4ae804-Paper.pdf,Bayesian filtering unifies adaptive and non-adaptive neural network optimization methods,Laurence Aitchison,
neurips,https://proceedings.neurips.cc/paper/2020/file/d34a281acc62c6bec66425f0ad6dd645-Paper.pdf,Worst-Case Analysis for Randomly Collected Data,"Justin Chen, Gregory Valiant, Paul Valiant",
neurips,https://proceedings.neurips.cc/paper/2020/file/d35b05a832e2bb91f110d54e34e2da79-Paper.pdf,Truthful Data Acquisition via Peer Prediction,"Yiling Chen, Yiheng Shen, Shuran Zheng",
neurips,https://proceedings.neurips.cc/paper/2020/file/d3696cfb815ab692407d9362e6f06c28-Paper.pdf,Learning Robust Decision Policies from Observational Data,"Muhammad Osama, Dave Zachariah, Peter Stoica",
neurips,https://proceedings.neurips.cc/paper/2020/file/d37eb50d868361ea729bb4147eb3c1d8-Paper.pdf,Byzantine Resilient Distributed Multi-Task Learning,"Jiani Li, Waseem Abbas, Xenofon Koutsoukos",
neurips,https://proceedings.neurips.cc/paper/2020/file/d3b1fb02964aa64e257f9f26a31f72cf-Paper.pdf,Reinforcement Learning in Factored MDPs: Oracle-Efficient Algorithms and Tighter Regret Bounds for the Non-Episodic Setting,"Ziping Xu, Ambuj Tewari","We study reinforcement learning in non-episodic factored Markov decision processes (FMDPs). We propose two near-optimal and oracle-efficient algorithms for FMDPs. Assuming oracle access to an FMDP planner, they enjoy a Bayesian and a frequentist regret bound respectively, both of which reduce to the near-optimal bound
O
(
D
S
√
A
T
)
O
for standard non-factored MDPs. We propose a tighter connectivity measure, factored span, for FMDPs and prove a lower bound that depends on the factored span rather than the diameter
D
D
. In order to decrease the gap between lower and upper bounds, we propose an adaptation of the REGAL.C algorithm whose regret bound depends on the factored span. Our oracle-efficient algorithms outperform previously proposed near-optimal algorithms on computer network administration simulations."
neurips,https://proceedings.neurips.cc/paper/2020/file/d3d9446802a44259755d38e6d163e820-Paper.pdf,Improving model calibration with accuracy versus uncertainty optimization,"Ranganath Krishnan, Omesh Tickoo",
neurips,https://proceedings.neurips.cc/paper/2020/file/d3f06eef2ffac7faadbe3055a70682ac-Paper.pdf,The Convolution Exponential and Generalized Sylvester Flows,"Emiel Hoogeboom, Victor Garcia Satorras, Jakub Tomczak, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2020/file/d3f5d4de09ea19461dab00590df91e4f-Paper.pdf,An Improved Analysis of Stochastic Gradient Descent with Momentum,"Yanli Liu, Yuan Gao, Wotao Yin",
neurips,https://proceedings.neurips.cc/paper/2020/file/d40d35b3063c11244fbf38e9b55074be-Paper.pdf,Precise expressions for random projections: Low-rank approximation and randomized Newton,"Michal Derezinski, Feynman T. Liang, Zhenyu Liao, Michael W. Mahoney",
neurips,https://proceedings.neurips.cc/paper/2020/file/d464b5ac99e74462f321c06ccacc4bff-Paper.pdf,The MAGICAL Benchmark for Robust Imitation,"Sam Toyer, Rohin Shah, Andrew Critch, Stuart Russell",
neurips,https://proceedings.neurips.cc/paper/2020/file/d4a93297083a23cc099f7bd6a8621131-Paper.pdf,X-CAL: Explicit Calibration for Survival Analysis,"Mark Goldstein, Xintian Han, Aahlad Puli, Adler Perotte, Rajesh Ranganath",
neurips,https://proceedings.neurips.cc/paper/2020/file/d4b5b5c16df28e61124e13181db7774c-Paper.pdf,Decentralized Accelerated Proximal Gradient Descent,"Haishan Ye, Ziang Zhou, Luo Luo, Tong Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/d4ca950da1d6fd954520c45ab19fef1c-Paper.pdf,Making Non-Stochastic Control (Almost) as Easy as Stochastic,Max Simchowitz,"Recent literature has made much progress in understanding \emph{online LQR}: a modern learning-theoretic take on the classical control problem where a learner attempts to optimally control an unknown linear dynamical system with fully observed state, perturbed by i.i.d. Gaussian noise. \iftoggle{nips}{The}{It is now understood that the} optimal regret over time horizon
T
T
against the optimal control law scales as
˜
Θ
(
√
T
)
Θ
. In this paper, we show that the same regret rate (against a suitable benchmark) is attainable even in the considerably more general non-stochastic control model, where the system is driven by \emph{arbitrary adversarial} noise \citep{agarwal2019online}. We attain the optimal
˜
O
(
√
T
)
O
regret when the dynamics are unknown to the learner, and
p
o
l
y
(
log
T
)
p
regret when known, provided that the cost functions are strongly convex (as in LQR). Our algorithm is based on a novel variant of online Newton step \citep{hazan2007logarithmic}, which adapts to the geometry induced by adversarial disturbances, and our analysis hinges on generic regret bounds for certain structured losses in the OCO-with-memory framework \citep{anava2015online}."
neurips,https://proceedings.neurips.cc/paper/2020/file/d4dd111a4fd973394238aca5c05bebe3-Paper.pdf,BERT Loses Patience: Fast and Robust Inference with Early Exit,"Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, Furu Wei",
neurips,https://proceedings.neurips.cc/paper/2020/file/d530d454337fb09964237fecb4bea6ce-Paper.pdf,Optimal and Practical Algorithms for Smooth and Strongly Convex Decentralized Optimization,"Dmitry Kovalev, Adil Salim, Peter Richtarik","We consider the task of decentralized minimization of the sum of smooth strongly convex functions stored across the nodes of a network. For this problem, lower bounds on the number of gradient computations and the number of communication rounds required to achieve
ε
ε
accuracy have recently been proven. We propose two new algorithms for this decentralized optimization problem and equip them with complexity guarantees. We show that our first method is optimal both in terms of the number of communication rounds and in terms of the number of gradient computations. Unlike existing optimal algorithms, our algorithm does not rely on the expensive evaluation of dual gradients. Our second algorithm is optimal in terms of the number of communication rounds, without a logarithmic factor. Our approach relies on viewing the two proposed algorithms as accelerated variants of the Forward Backward algorithm to solve monotone inclusions associated with the decentralized optimization problem. We also verify the efficacy of our methods against state-of-the-art algorithms through numerical experiments."
neurips,https://proceedings.neurips.cc/paper/2020/file/d55cbf210f175f4a37916eafe6c04f0d-Paper.pdf,BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement Learning,"Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, Keith Ross",
neurips,https://proceedings.neurips.cc/paper/2020/file/d58f36f7679f85784d8b010ff248f898-Paper.pdf,Regularizing Towards Permutation Invariance In Recurrent Models,"Edo Cohen-Karlik, Avichai Ben David, Amir Globerson",
neurips,https://proceedings.neurips.cc/paper/2020/file/d5ab8dc7ef67ca92e41d730982c5c602-Paper.pdf,What Did You Think Would Happen? Explaining Agent Behaviour through Intended Outcomes,"Herman Yau, Chris Russell, Simon Hadfield",
neurips,https://proceedings.neurips.cc/paper/2020/file/d5ade38a2c9f6f073d69e1bc6b6e64c1-Paper.pdf,Batch normalization provably avoids ranks collapse for randomly initialised deep networks,"Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, Aurelien Lucchi",
neurips,https://proceedings.neurips.cc/paper/2020/file/d5fcc35c94879a4afad61cacca56192c-Paper.pdf,Choice Bandits,"Arpit Agarwal, Nicholas Johnson, Shivani Agarwal","There has been much interest in recent years in the problem of dueling bandits, where on each round the learner plays a pair of arms and receives as feedback the outcome of a relative pairwise comparison between them. Here we study a natural generalization, that we term \emph{choice bandits}, where the learner plays a set of up to
k
≥
2
k
arms and receives limited relative feedback in the form of a single multiway choice among the pulled arms, drawn from an underlying multiway choice model. We study choice bandits under a very general class of choice models that is characterized by the existence of a unique `best' arm (which we term generalized Condorcet winner), and includes as special cases the well-studied multinomial logit (MNL) and multinomial probit (MNP) choice models, and more generally, the class of random utility models with i.i.d. noise (IID-RUMs). We propose an algorithm for choice bandits, termed Winner Beats All (WBA), with distribution dependent
O
(
log
T
)
O
regret bound under all these choice models. The challenge in our setting is that the decision space is
Θ
(
n
k
)
Θ
, which is large for even moderate
k
k
. Our algorithm addresses this challenge by extracting just
O
(
n
2
)
O
statistics from multiway choices and exploiting the existence of a unique `best' arm to find arms that are competitive to this arm in order to construct sets with low regret. Since these statistics are extracted from the same choice observations, one needs a careful martingale analysis in order to show that these statistics are concentrated. We complement our upper bound result with a lower bound result, which shows that our upper bound is order-wise optimal. Our experiments demonstrate that for the special case of
k
=
2
k
, our algorithm is competitive when compared to previous dueling bandit algorithms, and for the more general case
k
>
2
k
, outperforms the recently proposed MaxMinUCB algorithm designed for the MNL model."
neurips,https://proceedings.neurips.cc/paper/2020/file/d61e4bbd6393c9111e6526ea173a7c8b-Paper.pdf,What if Neural Networks had SVDs?,"Alexander Mathiasen, Frederik Hvilshøj, Jakob Rødsgaard Jørgensen, Anshul Nasery, Davide Mottin",
neurips,https://proceedings.neurips.cc/paper/2020/file/d63fbf8c3173730f82b150c5ef38b8ff-Paper.pdf,A Matrix Chernoff Bound for Markov Chains and Its Application to Co-occurrence Matrices,"Jiezhong Qiu, Chi Wang, Ben Liao, Richard Peng, Jie Tang","Our matrix Chernoff bound for Markov chains can be applied to analyze the behavior of co-occurrence statistics for sequential data, which have been common and important data signals in machine learning. We show that given a regular Markov chain with n states and mixing time t, we need a trajectory of length O(t(log(n) + log(t))/e^2) to achieve an estimator of the co-occurrence matrix with error bound e. We conduct several experiments and the experimental results are consistent with the exponentially fast convergence rate from theoretical analysis. Our result gives the first bound on the convergence rate of the co-occurrence matrix and the first sample complexity analysis in graph representation learning."
neurips,https://proceedings.neurips.cc/paper/2020/file/d6428eecbe0f7dff83fc607c5044b2b9-Paper.pdf,CoMIR: Contrastive Multimodal Image Representation for Registration,"Nicolas Pielawski, Elisabeth Wetzer, Johan Öfverstedt, Jiahao Lu, Carolina Wählby, Joakim Lindblad, Natasa Sladoje",
neurips,https://proceedings.neurips.cc/paper/2020/file/d6539d3b57159babf6a72e106beb45bd-Paper.pdf,Ensuring Fairness Beyond the Training Data,"Debmalya Mandal, Samuel Deng, Suman Jana, Jeannette Wing, Daniel J. Hsu",
neurips,https://proceedings.neurips.cc/paper/2020/file/d6d231705f96d5a35aeb3a76402e49a3-Paper.pdf,How do fair decisions fare in long-term qualification?,"Xueru Zhang, Ruibo Tu, Yang Liu, Mingyan Liu, Hedvig Kjellstrom, Kun Zhang, Cheng Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/d6f1dd034aabde7657e6680444ceff62-Paper.pdf,Pre-training via Paraphrasing,"Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, Luke Zettlemoyer",
neurips,https://proceedings.neurips.cc/paper/2020/file/d714d2c5a796d5814c565d78dd16188d-Paper.pdf,GCN meets GPU: Decoupling “When to Sample” from “How to Sample”,"Morteza Ramezani, Weilin Cong, Mehrdad Mahdavi, Anand Sivasubramaniam, Mahmut Kandemir",
neurips,https://proceedings.neurips.cc/paper/2020/file/d7488039246a405baf6a7cbc3613a56f-Paper.pdf,Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks,"Zixuan Ke, Bing Liu, Xingchang Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/d75320797f266ba9ed6dd6dc218cb1b5-Paper.pdf,All your loss are belong to Bayes,"Christian Walder, Richard Nock","In this paper, we rely on a broader view of proper composite losses and a recent construct from information geometry, source functions, whose fitting alleviates constraints faced by canonical links. We introduce a trick on squared Gaussian Processes to obtain a random process whose paths are compliant source functions with many desirable properties in the context of link estimation. Experimental results demonstrate substantial improvements over the state of the art."
neurips,https://proceedings.neurips.cc/paper/2020/file/d77c703536718b95308130ff2e5cf9ee-Paper.pdf,HAWQ-V2: Hessian Aware trace-Weighted Quantization of Neural Networks,"Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W. Mahoney, Kurt Keutzer",
neurips,https://proceedings.neurips.cc/paper/2020/file/d783823cc6284b929c2cd8df2167d212-Paper.pdf,Sample-Efficient Reinforcement Learning of Undercomplete POMDPs,"Chi Jin, Sham Kakade, Akshay Krishnamurthy, Qinghua Liu","Partial observability is a common challenge in many reinforcement learning applications, which requires an agent to maintain memory, infer latent states, and integrate this past information into exploration. This challenge leads to a number of computational and statistical hardness results for learning general Partially Observable Markov Decision Processes (POMDPs). This work shows that these hardness barriers do not preclude efficient reinforcement learning for rich and interesting subclasses of POMDPs. In particular, we present a sample-efficient algorithm, OOM-UCB, for episodic finite undercomplete POMDPs, where the number of observations is larger than the number of latent states and where exploration is essential for learning, thus distinguishing our results from prior works. OOM-UCB achieves an optimal sample complexity of
~
O
(
1
/
ε
2
)
O
for finding an
ε
ε
-optimal policy, along with being polynomial in all other relevant quantities. As an interesting special case, we also provide a computationally and statistically efficient algorithm for POMDPs with deterministic state transitions."
neurips,https://proceedings.neurips.cc/paper/2020/file/d785bf9067f8af9e078b93cf26de2b54-Paper.pdf,Non-Convex SGD Learns Halfspaces with Adversarial Label Noise,"Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis","We study the problem of agnostically learning homogeneous halfspaces in the distribution-specific PAC model. For a broad family of structured distributions, including log-concave distributions, we show that non-convex SGD efficiently converges to a solution with misclassification error
O
(
\opt
)
+
\eps
O
, where
\opt
\opt
is the misclassification error of the best-fitting halfspace. In sharp contrast, we show that optimizing any convex surrogate inherently leads to misclassification error of
ω
(
\opt
)
ω
, even under Gaussian marginals."
neurips,https://proceedings.neurips.cc/paper/2020/file/d79c8788088c2193f0244d8f1f36d2db-Paper.pdf,A Tight Lower Bound and Efficient Reduction for Swap Regret,Shinji Ito,"Swap regret, a generic performance measure of online decision-making algorithms, plays an important role in the theory of repeated games, along with a close connection to correlated equilibria in strategic games. This paper shows an
Ω
(
√
T
N
log
N
)
Ω
-lower bound for swap regret, where
T
T
and
N
N
denote the numbers of time steps and available actions, respectively. Our lower bound is tight up to a constant, and resolves an open problem mentioned, e.g., in the book by Nisan et al. (2007). Besides, we present a computationally efficient reduction method that converts no-external-regret algorithms to no-swap-regret algorithms. This method can be applied not only to the full-information setting but also to the bandit setting and provides a better regret bound than previous results."
neurips,https://proceedings.neurips.cc/paper/2020/file/d7f426ccbc6db7e235c57958c21c5dfa-Paper.pdf,DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction,"Aviral Kumar, Abhishek Gupta, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2020/file/d800149d2f947ad4d64f34668f8b20f6-Paper.pdf,OTLDA: A Geometry-aware Optimal Transport Approach for Topic Modeling,"Viet Huynh, He Zhao, Dinh Phung",
neurips,https://proceedings.neurips.cc/paper/2020/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf,Measuring Robustness to Natural Distribution Shifts in Image Classification,"Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, Ludwig Schmidt",
neurips,https://proceedings.neurips.cc/paper/2020/file/d83de59e10227072a9c034ce10029c39-Paper.pdf,Can I Trust My Fairness Metric? Assessing Fairness with Unlabeled Data and Bayesian Inference,"Disi Ji, Padhraic Smyth, Mark Steyvers",
neurips,https://proceedings.neurips.cc/paper/2020/file/d85b63ef0ccb114d0a3bb7b7d808028f-Paper.pdf,RandAugment: Practical Automated Data Augmentation with a Reduced Search Space,"Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, Quoc Le",
neurips,https://proceedings.neurips.cc/paper/2020/file/d87ca511e2a8593c8039ef732f5bffed-Paper.pdf,Asymptotic normality and confidence intervals for derivatives of 2-layers neural network in the random features model,"Yiwei Shen, Pierre C Bellec","This paper studies two-layers Neural Networks (NN), where the first layer contains random weights, and the second layer is trained using Ridge regularization. This model has been the focus of numerous recent works, showing that despite its simplicity, it captures some of the empirically observed behaviors of NN in the overparametrized regime, such as the double-descent curve where the generalization error decreases as the number of weights increases to
+
∞
+
. This paper establishes asymptotic distribution results for this 2-layers NN model in the regime where the ratios
p
n
p
and
d
n
d
have finite limits, where
n
n
is the sample size,
p
p
the ambient dimension and
d
d
is the width of the first layer. We show that a weighted average of the derivatives of the trained NN at the observed data is asymptotically normal, in a setting with Lipschitz activation functions in a linear regression response with Gaussian features under possibly non-linear perturbations. We then leverage this asymptotic normality result to construct confidence intervals (CIs) for single components of the unknown regression vector. The novelty of our results are threefold: (1) Despite the nonlinearity induced by the activation function, we characterize the asymptotic distribution of a weighted average of the gradients of the network after training; (2) It provides the first frequentist uncertainty quantification guarantees, in the form of valid (
1
-
α
1
)-CIs, based on NN estimates; (3) It shows that the double-descent phenomenon occurs in terms of the length of the CIs, with the length increasing and then decreasing as
d
n
↗
+
∞
d
for certain fixed values of
p
n
p
. We also provide a toolbox to predict the length of CIs numerically, which lets us compare activation functions and other parameters in terms of CI length."
neurips,https://proceedings.neurips.cc/paper/2020/file/d880e783834172e5ebd1868d84463d93-Paper.pdf,DisARM: An Antithetic Gradient Estimator for Binary Latent Variables,"Zhe Dong, Andriy Mnih, George Tucker",
neurips,https://proceedings.neurips.cc/paper/2020/file/d882050bb9eeba930974f596931be527-Paper.pdf,Variational Inference for Graph Convolutional Networks in the Absence of Graph Data and Adversarial Settings,"Pantelis Elinas, Edwin V. Bonilla, Louis Tiao",
neurips,https://proceedings.neurips.cc/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf,Supervised Contrastive Learning,"Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan",
neurips,https://proceedings.neurips.cc/paper/2020/file/d8ea5f53c1b1eb087ac2e356253395d8-Paper.pdf,Learning Optimal Representations with the Decodable Information Bottleneck,"Yann Dubois, Douwe Kiela, David J. Schwab, Ramakrishna Vedantam",
neurips,https://proceedings.neurips.cc/paper/2020/file/d902c3ce47124c66ce615d5ad9ba304f-Paper.pdf,Meta-trained agents implement Bayes-optimal agents,"Vladimir Mikulik, Grégoire Delétang, Tom McGrath, Tim Genewein, Miljan Martic, Shane Legg, Pedro Ortega",
neurips,https://proceedings.neurips.cc/paper/2020/file/d90e5b6628b4291225cba0bdc643c295-Paper.pdf,Learning Agent Representations for Ice Hockey,"Guiliang Liu, Oliver Schulte, Pascal Poupart, Mike Rudd, Mehrsan Javan",
neurips,https://proceedings.neurips.cc/paper/2020/file/d93c96e6a23fff65b91b900aaa541998-Paper.pdf,Weak Form Generalized Hamiltonian Learning,"Kevin Course, Trefor Evans, Prasanth Nair",
neurips,https://proceedings.neurips.cc/paper/2020/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf,Neural Non-Rigid Tracking,"Aljaz Bozic, Pablo Palafox, Michael Zollhöfer, Angela Dai, Justus Thies, Matthias Niessner",
neurips,https://proceedings.neurips.cc/paper/2020/file/d958628e70134d9e1e17499a9d815a71-Paper.pdf,Collegial Ensembles,"Etai Littwin, Ben Myara, Sima Sabah, Joshua Susskind, Shuangfei Zhai, Oren Golan",
neurips,https://proceedings.neurips.cc/paper/2020/file/d961e9f236177d65d21100592edb0769-Paper.pdf,ICNet: Intra-saliency Correlation Network for Co-Saliency Detection,"Wen-Da Jin, Jun Xu, Ming-Ming Cheng, Yi Zhang, Wei Guo",
neurips,https://proceedings.neurips.cc/paper/2020/file/d96409bf894217686ba124d7356686c9-Paper.pdf,Improved Variational Bayesian Phylogenetic Inference with Normalizing Flows,Cheng Zhang,
neurips,https://proceedings.neurips.cc/paper/2020/file/d9812f756d0df06c7381945d2e2c7d4b-Paper.pdf,Deep Metric Learning with Spherical Embedding,"Dingyi Zhang, Yingming Li, Zhongfei Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/d9d3837ee7981e8c064774da6cdd98bf-Paper.pdf,Preference-based Reinforcement Learning with Finite-Time Guarantees,"Yichong Xu, Ruosong Wang, Lin Yang, Aarti Singh, Artur Dubrawski","Preference-based Reinforcement Learning (PbRL) replaces reward values in traditional reinforcement learning by preferences to better elicit human opinion on the target objective, especially when numerical reward values are hard to design or interpret. Despite promising results in applications, the theoretical understanding of PbRL is still in its infancy. In this paper, we present the first finite-time analysis for general PbRL problems. We first show that a unique optimal policy may not exist if preferences over trajectories are deterministic for PbRL. If preferences are stochastic, and the preference probability relates to the hidden reward values, we present algorithms for PbRL, both with and without a simulator, that are able to identify the best policy up to accuracy
ε
ε
with high probability. Our method explores the state space by navigating to under-explored states, and solves PbRL using a combination of dueling bandits and policy search. Experiments show the efficacy of our method when it is applied to real-world problems."
neurips,https://proceedings.neurips.cc/paper/2020/file/d9d4f495e875a2e075a1a4a6e1b9770f-Paper.pdf,AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients,"Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C. Tatikonda, Nicha Dvornek, Xenophon Papademetris, James Duncan",
neurips,https://proceedings.neurips.cc/paper/2020/file/d9dbc51dc534921589adf460c85cd824-Paper.pdf,Interpretable Sequence Learning for Covid-19 Forecasting,"Sercan Arik, Chun-Liang Li, Jinsung Yoon, Rajarishi Sinha, Arkady Epshteyn, Long Le, Vikas Menon, Shashank Singh, Leyou Zhang, Martin Nikoltchev, Yash Sonthalia, Hootan Nakhost, Elli Kanal, Tomas Pfister",
neurips,https://proceedings.neurips.cc/paper/2020/file/da21bae82c02d1e2b8168d57cd3fbab7-Paper.pdf,Off-policy Policy Evaluation For Sequential Decisions Under Unobserved Confounding,"Hongseok Namkoong, Ramtin Keramati, Steve Yadlowsky, Emma Brunskill",
neurips,https://proceedings.neurips.cc/paper/2020/file/da4902cb0bc38210839714ebdcf0efc3-Paper.pdf,Modern Hopfield Networks and Attention for Immune Repertoire Classification,"Michael Widrich, Bernhard Schäfl, Milena Pavlović, Hubert Ramsauer, Lukas Gruber, Markus Holzleitner, Johannes Brandstetter, Geir Kjetil Sandve, Victor Greiff, Sepp Hochreiter, Günter Klambauer",
neurips,https://proceedings.neurips.cc/paper/2020/file/da6ea77475918a3d83c7e49223d453cc-Paper.pdf,One Ring to Rule Them All: Certifiably Robust Geometric Perception with Outliers,"Heng Yang, Luca Carlone",
neurips,https://proceedings.neurips.cc/paper/2020/file/da8ce53cf0240070ce6c69c48cd588ee-Paper.pdf,Task-Robust Model-Agnostic Meta-Learning,"Liam Collins, Aryan Mokhtari, Sanjay Shakkottai","Meta-learning methods have shown an impressive ability to train models that rapidly learn new tasks. However, these methods only aim to perform well in expectation over tasks coming from some particular distribution that is typically equivalent across meta-training and meta-testing, rather than considering worst-case task performance. In this work we introduce the notion of
task-robustness'' by reformulating the popular Model-Agnostic Meta-Learning (MAML) objective \citep{finn2017model} such that the goal is to minimize the maximum loss over the observed meta-training tasks. The solution to this novel formulation is task-robust in the sense that it places equal importance on even the most difficult and/or rare tasks. This also means that it performs well over all distributions of the observed tasks, making it robust to shifts in the task distribution between meta-training and meta-testing. We present an algorithm to solve the proposed min-max problem, and show that it converges to an
ϵ
ϵ
-accurate point at the optimal rate of
O
(
1
/
ϵ
2
)
O
in the convex setting and to an
(
ϵ
,
δ
)
(
-stationary point at the rate of
O
(
max
{
1
/
ϵ
5
,
1
/
δ
5
}
)
O
in nonconvex settings. We also provide an upper bound on the new task generalization error that captures the advantage of minimizing the worst-case task loss, and demonstrate this advantage in sinusoid regression and image classification experiments."
neurips,https://proceedings.neurips.cc/paper/2020/file/da97f65bd113e490a5fab20c4a69f586-Paper.pdf,R-learning in actor-critic model offers a biologically relevant mechanism for sequential decision-making,"Sergey Shuvaev, Sarah Starosta, Duda Kvitsiani, Adam Kepecs, Alexei Koulakov",
neurips,https://proceedings.neurips.cc/paper/2020/file/da9e6a4a4aeca98588e4dd77ceb37695-Paper.pdf,Revisiting Frank-Wolfe for Polytopes: Strict Complementarity and Sparsity,Dan Garber,
neurips,https://proceedings.neurips.cc/paper/2020/file/dab10c50dc668cd8560df444ff3a4227-Paper.pdf,Fast Convergence of Langevin Dynamics on Manifold: Geodesics meet Log-Sobolev,"Xiao Wang, Qi Lei, Ioannis Panageas","Sampling is a fundamental and arguably very important task with numerous applications in Machine Learning. One approach to sample from a high dimensional distribution
e
−
f
e
for some function
f
f
is the Langevin Algorithm (LA). Recently, there has been a lot of progress in showing fast convergence of LA even in cases where
f
f
is non-convex, notably \cite{VW19}, \cite{MoritaRisteski} in which the former paper focuses on functions
f
f
defined in
R
n
R
and the latter paper focuses on functions with symmetries (like matrix completion type objectives) with manifold structure. Our work generalizes the results of \cite{VW19} where
f
f
is defined on a manifold
M
M
rather than
R
n
R
. From technical point of view, we show that KL decreases in a geometric rate whenever the distribution
e
−
f
e
satisfies a log-Sobolev inequality on
M
M
."
neurips,https://proceedings.neurips.cc/paper/2020/file/dab1263d1e6a88c9ba5e7e294def5e8b-Paper.pdf,Tensor Completion Made Practical,"Allen Liu, Ankur Moitra",
neurips,https://proceedings.neurips.cc/paper/2020/file/dab49080d80c724aad5ebf158d63df41-Paper.pdf,Optimization and Generalization Analysis of Transduction through Gradient Boosting and Application to Multi-scale Graph Neural Networks,"Kenta Oono, Taiji Suzuki",
neurips,https://proceedings.neurips.cc/paper/2020/file/dabd8d2ce74e782c65a973ef76fd540b-Paper.pdf,Content Provider Dynamics and Coordination in Recommendation Ecosystems,"Omer Ben-Porat, Itay Rosenberg, Moshe Tennenholtz",
neurips,https://proceedings.neurips.cc/paper/2020/file/daecf755df5b1d637033bb29b319c39a-Paper.pdf,Almost Surely Stable Deep Dynamics,"Nathan Lawrence, Philip Loewen, Michael Forbes, Johan Backstrom, Bhushan Gopaluni",
neurips,https://proceedings.neurips.cc/paper/2020/file/daed210307f1dbc6f1dd9551408d999f-Paper.pdf,Experimental design for MRI by greedy policy search,"Tim Bakker, Herke van Hoof, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2020/file/daf642455364613e2120c636b5a1f9c7-Paper.pdf,Expert-Supervised Reinforcement Learning for Offline Policy Learning and Evaluation,"Aaron Sonabend, Junwei Lu, Leo Anthony Celi, Tianxi Cai, Peter Szolovits",
neurips,https://proceedings.neurips.cc/paper/2020/file/db261d4f615f0e982983be499e57ccda-Paper.pdf,ColdGANs: Taming Language GANs with Cautious Sampling Strategies,"Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano","Generative Adversarial Networks (GANs) could mitigate those limitations. Nonetheless, the discrete nature of text has hindered their application to language generation: the approaches proposed so far, based on Reinforcement Learning, have been shown to under-perform MLE. In this context, the exploration is known to be critical, while surprisingly being under-studied. In this work, we show how the most popular sampling method results in unstable training for language GANs. We propose alternative exploration strategies that we named Cold-GANs. By forcing the sampling to be close to the distribution mode, the learning dynamic becomes smoother."
neurips,https://proceedings.neurips.cc/paper/2020/file/db346ccb62d491029b590bbbf0f5c412-Paper.pdf,Hedging in games: Faster convergence of external and swap regrets,"Xi Chen, Binghui Peng",
neurips,https://proceedings.neurips.cc/paper/2020/file/db5f9f42a7157abe65bb145000b5871a-Paper.pdf,The Origins and Prevalence of Texture Bias in Convolutional Neural Networks,"Katherine Hermann, Ting Chen, Simon Kornblith",
neurips,https://proceedings.neurips.cc/paper/2020/file/db8419f41d890df802dca330e6284952-Paper.pdf,Time-Reversal Symmetric ODE Network,"In Huh, Eunho Yang, Sung Ju Hwang, Jinwoo Shin",
neurips,https://proceedings.neurips.cc/paper/2020/file/db957c626a8cd7a27231adfbf51e20eb-Paper.pdf,Provable Overlapping Community Detection in Weighted Graphs,"Jimit Majmudar, Stephen Vavasis",
neurips,https://proceedings.neurips.cc/paper/2020/file/dba31bb5c75992690f20c2d3b370ec7c-Paper.pdf,Fast Unbalanced Optimal Transport on a Tree,"Ryoma Sato, Makoto Yamada, Hisashi Kashima",
neurips,https://proceedings.neurips.cc/paper/2020/file/dba4c1a117472f6aca95211285d0587e-Paper.pdf,Acceleration with a Ball Optimization Oracle,"Yair Carmon, Arun Jambulapati, Qijia Jiang, Yujia Jin, Yin Tat Lee, Aaron Sidford, Kevin Tian",
neurips,https://proceedings.neurips.cc/paper/2020/file/dc1913d422398c25c5f0b81cab94cc87-Paper.pdf,Avoiding Side Effects By Considering Future Tasks,"Victoria Krakovna, Laurent Orseau, Richard Ngo, Miljan Martic, Shane Legg",
neurips,https://proceedings.neurips.cc/paper/2020/file/dc36f18a9a0a776671d4879cae69b551-Paper.pdf,Handling Missing Data with Graph Representation Learning,"Jiaxuan You, Xiaobai Ma, Yi Ding, Mykel J. Kochenderfer, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2020/file/dc49dfebb0b00fd44aeff5c60cc1f825-Paper.pdf,Improving Auto-Augment via Augmentation-Wise Weight Sharing,"Keyu Tian, Chen Lin, Ming Sun, Luping Zhou, Junjie Yan, Wanli Ouyang",
neurips,https://proceedings.neurips.cc/paper/2020/file/dcd2f3f312b6705fb06f4f9f1b55b55c-Paper.pdf,MMA Regularization: Decorrelating Weights of Neural Networks by Maximizing the Minimal Angles,"Zhennan Wang, Canqun Xiang, Wenbin Zou, Chen Xu",
neurips,https://proceedings.neurips.cc/paper/2020/file/dd1970fb03877a235d530476eb727dab-Paper.pdf,HRN: A Holistic Approach to One Class Learning,"Wenpeng Hu, Mengyu Wang, Qi Qin, Jinwen Ma, Bing Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/dd45045f8c68db9f54e70c67048d32e8-Paper.pdf,The Generalized Lasso with Nonlinear Observations and Generative Priors,"Zhaoqiang Liu, Jonathan Scarlett","In this paper, we study the problem of signal estimation from noisy non-linear measurements when the unknown
n
n
-dimensional signal is in the range of an
L
L
-Lipschitz continuous generative model with bounded
k
k
-dimensional inputs. We make the assumption of sub-Gaussian measurements, which is satisfied by a wide range of measurement models, such as linear, logistic, 1-bit, and other quantized models. In addition, we consider the impact of adversarial corruptions on these measurements. Our analysis is based on a generalized Lasso approach (Plan and Vershynin, 2016). We first provide a non-uniform recovery guarantee, which states that under i.i.d.~Gaussian measurements, roughly
O
(
k
ϵ
2
log
L
)
O
samples suffice for recovery with an
ℓ
2
ℓ
-error of
ϵ
ϵ
, and that this scheme is robust to adversarial noise. Then, we apply this result to neural network generative models, and discuss various extensions to other models and non-i.i.d.~measurements. Moreover, we show that our result can be extended to the uniform recovery guarantee under the assumption of a so-called local embedding property, which is satisfied by the 1-bit and censored Tobit models."
neurips,https://proceedings.neurips.cc/paper/2020/file/ddd808772c035aed516d42ad3559be5f-Paper.pdf,Fair regression via plug-in estimator and recalibration with statistical guarantees,"Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, Massimiliano Pontil",
neurips,https://proceedings.neurips.cc/paper/2020/file/de03beffeed9da5f3639a621bcab5dd4-Paper.pdf,Modeling Shared responses in Neuroimaging Studies through MultiView ICA,"Hugo Richard, Luigi Gresele, Aapo Hyvarinen, Bertrand Thirion, Alexandre Gramfort, Pierre Ablin",
neurips,https://proceedings.neurips.cc/paper/2020/file/de07edeeba9f475c9395959494cd8f64-Paper.pdf,Efficient Planning in Large MDPs with Weak Linear Function Approximation,"Roshan Shariff, Csaba Szepesvari",
neurips,https://proceedings.neurips.cc/paper/2020/file/de6b1cf3fb0a3aa1244d30f7b8c29c41-Paper.pdf,Efficient Learning of Generative Models via Finite-Difference Score Matching,"Tianyu Pang, Kun Xu, Chongxuan LI, Yang Song, Stefano Ermon, Jun Zhu",
neurips,https://proceedings.neurips.cc/paper/2020/file/dea9ddb25cbf2352cf4dec30222a02a5-Paper.pdf,Semialgebraic Optimization for Lipschitz Constants of ReLU Networks,"Tong Chen, Jean B. Lasserre, Victor Magron, Edouard Pauwels",
neurips,https://proceedings.neurips.cc/paper/2020/file/df0b8fb21c53254b7afa62e020447c81-Paper.pdf,Linear-Sample Learning of Low-Rank Distributions,"Ayush Jain, Alon Orlitsky","Many latent-variable applications, including community detection, collaborative filtering, genomic analysis, and NLP, model data as generated by low-rank matrices. Yet despite considerable research, except for very special cases, the number of samples required to efficiently recover the underlying matrices has not been known. We determine the onset of learning in several common latent-variable settings. For all of them, we show that learning
k
×
k
k
, rank-
r
r
, matrices to normalized
L
1
L
distance
ϵ
ϵ
requires
Ω
(
k
r
ϵ
2
)
Ω
samples, and propose an algorithm that uses
O
(
k
r
ϵ
2
log
2
r
ϵ
)
O
samples, a number linear in the high dimension, and nearly linear in the, typically low, rank. The algorithm improves on existing spectral techniques and runs in polynomial time. The proofs establish new results on the rapid convergence of the spectral distance between the model and observation matrices, and may be of independent interest."
neurips,https://proceedings.neurips.cc/paper/2020/file/df12ecd077efc8c23881028604dbb8cc-Paper.pdf,Transferable Calibration with Lower Bias and Variance in Domain Adaptation,"Ximei Wang, Mingsheng Long, Jianmin Wang, Michael Jordan",
neurips,https://proceedings.neurips.cc/paper/2020/file/df1a336b7e0b0cb186de6e66800c43a9-Paper.pdf,Generalization bound of globally optimal non-convex neural network training: Transportation map estimation by infinite dimensional Langevin dynamics,Taiji Suzuki,
neurips,https://proceedings.neurips.cc/paper/2020/file/df3aebc649f9e3b674eeb790a4da224e-Paper.pdf,Online Bayesian Goal Inference for Boundedly Rational Planning Agents,"Tan Zhi-Xuan, Jordyn Mann, Tom Silver, Josh Tenenbaum, Vikash Mansinghka",
neurips,https://proceedings.neurips.cc/paper/2020/file/df5511886da327a5e2877c3cd733d9d7-Paper.pdf,BayReL: Bayesian Relational Learning for Multi-omics Data Integration,"Ehsan Hajiramezanali, Arman Hasanzadeh, Nick Duffield, Krishna Narayanan, Xiaoning Qian",
neurips,https://proceedings.neurips.cc/paper/2020/file/dfb84a11f431c62436cfb760e30a34fe-Paper.pdf,Weakly Supervised Deep Functional Maps for Shape Matching,"Abhishek Sharma, Maks Ovsjanikov",
neurips,https://proceedings.neurips.cc/paper/2020/file/dfbfa7ddcfffeb581f50edcf9a0204bb-Paper.pdf,Domain Adaptation with Conditional Distribution Matching and Generalized Label Shift,"Remi Tachet des Combes, Han Zhao, Yu-Xiang Wang, Geoffrey J. Gordon","Adversarial learning has demonstrated good performance in the unsupervised domain adaptation setting, by learning domain-invariant representations. However, recent work has shown limitations of this approach when label distributions differ between the source and target domains. In this paper, we propose a new assumption, \textit{generalized label shift} (
\glsa
\glsa
), to improve robustness against mismatched label distributions.
\glsa
\glsa
states that, conditioned on the label, there exists a representation of the input that is invariant between the source and target domains. Under
\glsa
\glsa
, we provide theoretical guarantees on the transfer performance of any classifier. We also devise necessary and sufficient conditions for
\glsa
\glsa
to hold, by using an estimation of the relative class weights between domains and an appropriate reweighting of samples. Our weight estimation method could be straightforwardly and generically applied in existing domain adaptation (DA) algorithms that learn domain-invariant representations, with small computational overhead. In particular, we modify three DA algorithms, JAN, DANN and CDAN, and evaluate their performance on standard and artificial DA tasks. Our algorithms outperform the base versions, with vast improvements for large label distribution mismatches. Our code is available at \url{https://tinyurl.com/y585xt6j}."
neurips,https://proceedings.neurips.cc/paper/2020/file/e025b6279c1b88d3ec0eca6fcb6e6280-Paper.pdf,Rethinking the Value of Labels for Improving Class-Imbalanced Learning,"Yuzhe Yang, Zhi Xu",
neurips,https://proceedings.neurips.cc/paper/2020/file/e038453073d221a4f32d0bab94ca7cee-Paper.pdf,Provably Robust Metric Learning,"Lu Wang, Xuanqing Liu, Jinfeng Yi, Yuan Jiang, Cho-Jui Hsieh",
neurips,https://proceedings.neurips.cc/paper/2020/file/e05c7ba4e087beea9410929698dc41a6-Paper.pdf,Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings,"Yu Chen, Lingfei Wu, Mohammed Zaki",
neurips,https://proceedings.neurips.cc/paper/2020/file/e0640c93b05097a9380870aa06aa0df4-Paper.pdf,COPT: Coordinated Optimal Transport on Graphs,"Yihe Dong, Will Sawin",
neurips,https://proceedings.neurips.cc/paper/2020/file/e0688d13958a19e087e123148555e4b4-Paper.pdf,No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems,"Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, Christopher Ré",
neurips,https://proceedings.neurips.cc/paper/2020/file/e069ea4c9c233d36ff9c7f329bc08ff1-Paper.pdf,"Model Rubik’s Cube: Twisting Resolution, Depth and Width for TinyNets","Kai Han, Yunhe Wang, Qiulin Zhang, Wei Zhang, Chunjing XU, Tong Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/e0ab531ec312161511493b002f9be2ee-Paper.pdf,Self-Adaptive Training: beyond Empirical Risk Minimization,"Lang Huang, Chao Zhang, Hongyang Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/e105b88b3e1ac23ec811a708cd7edebf-Paper.pdf,Effective Dimension Adaptive Sketching Methods for Faster Regularized Least-Squares Optimization,"Jonathan Lacotte, Mert Pilanci",
neurips,https://proceedings.neurips.cc/paper/2020/file/e11943a6031a0e6114ae69c257617980-Paper.pdf,Near-Optimal Comparison Based Clustering,"Michaël Perrot, Pascal Esser, Debarghya Ghoshdastidar",
neurips,https://proceedings.neurips.cc/paper/2020/file/e1228be46de6a0234ac22ded31417bc7-Paper.pdf,Multi-Task Temporal Shift Attention Networks for On-Device Contactless Vitals Measurement,"Xin Liu, Josh Fromm, Shwetak Patel, Daniel McDuff",
neurips,https://proceedings.neurips.cc/paper/2020/file/e1696007be4eefb81b1a1d39ce48681b-Paper.pdf,A new convergent variant of Q-learning with linear function approximation,"Diogo Carvalho, Francisco S. Melo, Pedro Santos",
neurips,https://proceedings.neurips.cc/paper/2020/file/e1fc9c082df6cfff8cbcfff2b5a722ef-Paper.pdf,TaylorGAN: Neighbor-Augmented Policy Update Towards Sample-Efficient Natural Language Generation,"Chun-Hsing Lin, Siang-Ruei Wu, Hung-yi Lee, Yun-Nung Chen",
neurips,https://proceedings.neurips.cc/paper/2020/file/e1fe6165cad3f7f3f57d409f78e4415f-Paper.pdf,Neural Networks with Small Weights and Depth-Separation Barriers,"Gal Vardi, Ohad Shamir","In studying the expressiveness of neural networks, an important question is whether there are functions which can only be approximated by sufficiently deep networks, assuming their size is bounded. However, for constant depths, existing results are limited to depths
2
2
and
3
3
, and achieving results for higher depths has been an important open question. In this paper, we focus on feedforward ReLU networks, and prove fundamental barriers to proving such results beyond depth
4
4
, by reduction to open problems and natural-proof barriers in circuit complexity. To show this, we study a seemingly unrelated problem of independent interest: Namely, whether there are polynomially-bounded functions which require super-polynomial weights in order to approximate with constant-depth neural networks. We provide a negative and constructive answer to that question, by showing that if a function can be approximated by a polynomially-sized, constant depth
k
k
network with arbitrarily large weights, it can also be approximated by a polynomially-sized, depth
3
k
+
3
3
network, whose weights are polynomially bounded."
neurips,https://proceedings.neurips.cc/paper/2020/file/e2065cb56f5533494522c46a72f1dfb0-Paper.pdf,Untangling tradeoffs between recurrence and self-attention in artificial neural networks,"Giancarlo Kerg, Bhargav Kanuparthi, Anirudh Goyal ALIAS PARTH GOYAL, Kyle Goyette, Yoshua Bengio, Guillaume Lajoie",
neurips,https://proceedings.neurips.cc/paper/2020/file/e22312179bf43e61576081a2f250f845-Paper.pdf,Dual-Free Stochastic Decentralized Optimization with Variance Reduction,"Hadrien Hendrikx, Francis Bach, Laurent Massoulié","We consider the problem of training machine learning models on distributed data in a decentralized way. For finite-sum problems, fast single-machine algorithms for large datasets rely on stochastic updates combined with variance reduction. Yet, existing decentralized stochastic algorithms either do not obtain the full speedup allowed by stochastic updates, or require oracles that are more expensive than regular gradients. In this work, we introduce a Decentralized stochastic algorithm with Variance Reduction called DVR. DVR only requires computing stochastic gradients of the local functions, and is computationally as fast as a standard stochastic variance-reduced algorithms run on a
1
/
n
1
fraction of the dataset, where
n
n
is the number of nodes. To derive DVR, we use Bregman coordinate descent on a well-chosen dual problem, and obtain a dual-free algorithm using a specific Bregman divergence. We give an accelerated version of DVR based on the Catalyst framework, and illustrate its effectiveness with simulations on real data."
neurips,https://proceedings.neurips.cc/paper/2020/file/e287f0b2e730059c55d97fa92649f4f2-Paper.pdf,Online Learning in Contextual Bandits using Gated Linear Networks,"Eren Sezener, Marcus Hutter, David Budden, Jianan Wang, Joel Veness",
neurips,https://proceedings.neurips.cc/paper/2020/file/e29b722e35040b88678e25a1ec032a21-Paper.pdf,Throughput-Optimal Topology Design for Cross-Silo Federated Learning,"Othmane MARFOQ, CHUAN XU, Giovanni Neglia, Richard Vidal",
neurips,https://proceedings.neurips.cc/paper/2020/file/e2a23af417a2344fe3a23e652924091f-Paper.pdf,Quantized Variational Inference,Amir Dib,
neurips,https://proceedings.neurips.cc/paper/2020/file/e2a7555f7cabd6e31aef45cb8cda4999-Paper.pdf,Asymptotically Optimal Exact Minibatch Metropolis-Hastings,"Ruqi Zhang, A. Feder Cooper, Christopher M. De Sa",
neurips,https://proceedings.neurips.cc/paper/2020/file/e2ce14e81dba66dbff9cbc35ecfdb704-Paper.pdf,Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search,"Linnan Wang, Rodrigo Fonseca, Yuandong Tian",
neurips,https://proceedings.neurips.cc/paper/2020/file/e2d52448d36918c575fa79d88647ba66-Paper.pdf,Feature Shift Detection: Localizing Which Features Have Shifted via Conditional Distribution Tests,"Sean Kulinski, Saurabh Bagchi, David I. Inouye",
neurips,https://proceedings.neurips.cc/paper/2020/file/e2e5096d574976e8f115a8f1e0ffb52b-Paper.pdf,Unifying Activation- and Timing-based Learning Rules for Spiking Neural Networks,"Jinseok Kim, Kyungsu Kim, Jae-Joon Kim",
neurips,https://proceedings.neurips.cc/paper/2020/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Paper.pdf,Space-Time Correspondence as a Contrastive Random Walk,"Allan Jabri, Andrew Owens, Alexei Efros",
neurips,https://proceedings.neurips.cc/paper/2020/file/e3019767b1b23f82883c9850356b71d6-Paper.pdf,The Flajolet-Martin Sketch Itself Preserves Differential Privacy: Private Counting with Minimal Space,"Adam Smith, Shuang Song, Abhradeep Guha Thakurta","We revisit the problem of counting the number of distinct elements
\dist
\dist
in a data stream
D
D
, over a domain
[
u
]
[
. We propose an
(
ϵ
,
δ
)
(
-differentially private algorithm that approximates
\dist
\dist
within a factor of
(
1
±
γ
)
(
, and with additive error of
O
(
√
ln
(
1
/
δ
)
/
ϵ
)
O
, using space
O
(
ln
(
ln
(
u
)
/
γ
)
/
γ
2
)
O
. We improve on the prior work at least quadratically and up to exponentially, in terms of both space and additive error. Our additive error guarantee is optimal up to a factor of
O
(
√
ln
(
1
/
δ
)
)
O
, and the space bound is optimal up to a factor of
O
(
min
{
ln
(
ln
(
u
)
γ
)
,
1
γ
2
}
)
O
. We assume the existence of an ideal uniform random hash function, and ignore the space required to store it. We later relax this requirement by assuming pseudorandom functions and appealing to a computational variant of differential privacy, SIM-CDP. Our algorithm is built on top of the celebrated Flajolet-Martin (FM) sketch. We show that FM-sketch is differentially private as is, as long as there are
≈
√
ln
(
1
/
δ
)
/
(
ϵ
γ
)
≈
distinct elements in the data set. Along the way, we prove a structural result showing that the maximum of
k
k
i.i.d. random variables is statistically close (in the sense of
ϵ
ϵ
-differential privacy) to the maximum of
(
k
+
1
)
(
i.i.d. samples from the same distribution, as long as
k
=
Ω
(
1
ϵ
)
k
. Finally, experiments show that our algorithms introduces error within an order of magnitude of the non-private analogues for streams with thousands of distinct elements, even while providing strong privacy guarantee (
\eps
≤
1
\eps
)."
neurips,https://proceedings.neurips.cc/paper/2020/file/e3251075554389fe91d17a794861d47b-Paper.pdf,Exponential ergodicity of mirror-Langevin diffusions,"Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, Philippe Rigollet, Austin Stromme",
neurips,https://proceedings.neurips.cc/paper/2020/file/e32cc80bf07915058ce90722ee17bb71-Paper.pdf,An Efficient Framework for Clustered Federated Learning,"Avishek Ghosh, Jichan Chung, Dong Yin, Kannan Ramchandran",
neurips,https://proceedings.neurips.cc/paper/2020/file/e33d974aae13e4d877477d51d8bafdc4-Paper.pdf,Autoencoders that don't overfit towards the Identity,Harald Steck,
neurips,https://proceedings.neurips.cc/paper/2020/file/e366d105cfd734677897aaccf51e97a3-Paper.pdf,Polynomial-Time Computation of Optimal Correlated Equilibria in Two-Player Extensive-Form Games with Public Chance Moves and Beyond,"Gabriele Farina, Tuomas Sandholm",
neurips,https://proceedings.neurips.cc/paper/2020/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf,Parameterized Explainer for Graph Neural Network,"Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, Xiang Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/e3844e186e6eb8736e9f53c0c5889527-Paper.pdf,Recursive Inference for Variational Autoencoders,"Minyoung Kim, Vladimir Pavlovic",
neurips,https://proceedings.neurips.cc/paper/2020/file/e3a54649aeec04cf1c13907bc6c5c8aa-Paper.pdf,Flexible mean field variational inference using mixtures of non-overlapping exponential families,Jeffrey Spence,
neurips,https://proceedings.neurips.cc/paper/2020/file/e3a72c791a69f87b05ea7742e04430ed-Paper.pdf,HYDRA: Pruning Adversarially Robust Neural Networks,"Vikash Sehwag, Shiqi Wang, Prateek Mittal, Suman Jana",
neurips,https://proceedings.neurips.cc/paper/2020/file/e3b21256183cf7c2c7a66be163579d37-Paper.pdf,NVAE: A Deep Hierarchical Variational Autoencoder,"Arash Vahdat, Jan Kautz",
neurips,https://proceedings.neurips.cc/paper/2020/file/e3bc4e7f243ebc05d66a0568a3331966-Paper.pdf,Can Temporal-Diﬀerence and Q-Learning Learn Representation? A Mean-Field Theory,"Yufeng Zhang, Qi Cai, Zhuoran Yang, Yongxin Chen, Zhaoran Wang","In particular, temporal-diﬀerence learning converges when the function approximator is linear in a feature representation, which is ﬁxed throughout learning, and possibly diverges otherwise. We aim to answer the following questions: When the function approximator is a neural network, how does the associated feature representation evolve? If it converges, does it converge to the optimal one?"
neurips,https://proceedings.neurips.cc/paper/2020/file/e4191d610537305de1d294adb121b513-Paper.pdf,What Do Neural Networks Learn When Trained With Random Labels?,"Hartmut Maennel, Ibrahim M. Alabdulmohsin, Ilya O. Tolstikhin, Robert Baldock, Olivier Bousquet, Sylvain Gelly, Daniel Keysers",
neurips,https://proceedings.neurips.cc/paper/2020/file/e430ad64df3de73e6be33bcb7f6d0dac-Paper.pdf,Counterfactual Prediction for Bundle Treatment,"Hao Zou, Peng Cui, Bo Li, Zheyan Shen, Jianxin Ma, Hongxia Yang, Yue He",
neurips,https://proceedings.neurips.cc/paper/2020/file/e43739bba7cdb577e9e3e4e42447f5a5-Paper.pdf,Beta Embeddings for Multi-Hop Logical Reasoning in Knowledge Graphs,"Hongyu Ren, Jure Leskovec","One of the fundamental problems in Artificial Intelligence is to perform complex multi-hop logical reasoning over the facts captured by a knowledge graph (KG). This problem is challenging, because KGs can be massive and incomplete. Recent approaches embed KG entities in a low dimensional space and then use these embeddings to find the answer entities. However, it has been an outstanding challenge of how to handle arbitrary first-order logic (FOL) queries as present methods are limited to only a subset of FOL operators. In particular, the negation operator is not supported. An additional limitation of present methods is also that they cannot naturally model uncertainty. Here, we present BetaE, a probabilistic embedding framework for answering arbitrary FOL queries over KGs. BetaE is the first method that can handle a complete set of first-order logical operations: conjunction (
∧
∧
), disjunction (
∨
∨
), and negation (
¬
¬
). A key insight of BetaE is to use probabilistic distributions with bounded support, specifically the Beta distribution, and embed queries/entities as distributions, which as a consequence allows us to also faithfully model uncertainty. Logical operations are performed in the embedding space by neural operators over the probabilistic embeddings. We demonstrate the performance of BetaE on answering arbitrary FOL queries on three large, incomplete KGs. While being more general, BetaE also increases relative performance by up to 25.4% over the current state-of-the-art KG reasoning methods that can only handle conjunctive queries without negation."
neurips,https://proceedings.neurips.cc/paper/2020/file/e449b9317dad920c0dd5ad0a2a2d5e49-Paper.pdf,Learning Disentangled Representations and Group Structure of Dynamical Environments,"Robin Quessard, Thomas Barrett, William Clements",
neurips,https://proceedings.neurips.cc/paper/2020/file/e44e875c12109e4fa3716c05008048b2-Paper.pdf,Learning Linear Programs from Optimal Decisions,"Yingcong Tan, Daria Terekhov, Andrew Delong",
neurips,https://proceedings.neurips.cc/paper/2020/file/e464656edca5e58850f8cec98cbb979b-Paper.pdf,Wisdom of the Ensemble: Improving Consistency of Deep Learning Models,"Lijing Wang, Dipanjan Ghosh, Maria Gonzalez Diaz, Ahmed Farahat, Mahbubul Alam, Chetan Gupta, Jiangzhuo Chen, Madhav Marathe",
neurips,https://proceedings.neurips.cc/paper/2020/file/e4acb4c86de9d2d9a41364f93951028d-Paper.pdf,Universal Function Approximation on Graphs,Rickard Brüel Gabrielsson,
neurips,https://proceedings.neurips.cc/paper/2020/file/e4d78a6b4d93e1d79241f7b282fa3413-Paper.pdf,Accelerating Reinforcement Learning through GPU Atari Emulation,"Steven Dalton, iuri frosio",
neurips,https://proceedings.neurips.cc/paper/2020/file/e4d8163c7a068b65a64c89bd745ec360-Paper.pdf,EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning,"Jiachen Li, Fan Yang, Masayoshi Tomizuka, Chiho Choi",
neurips,https://proceedings.neurips.cc/paper/2020/file/e4f37b9ed429c1fe5ce61860d9902521-Paper.pdf,Comparator-Adaptive Convex Bandits,"Dirk van der Hoeven, Ashok Cutkosky, Haipeng Luo",
neurips,https://proceedings.neurips.cc/paper/2020/file/e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf,Model-based Reinforcement Learning for Semi-Markov Decision Processes with Neural ODEs,"Jianzhun Du, Joseph Futoma, Finale Doshi-Velez",
neurips,https://proceedings.neurips.cc/paper/2020/file/e56954b4f6347e897f954495eab16a88-Paper.pdf,The Adaptive Complexity of Maximizing a Gross Substitutes Valuation,"Ron Kupfer, Sharon Qian, Eric Balkanski, Yaron Singer",
neurips,https://proceedings.neurips.cc/paper/2020/file/e56eea9a45b153de634b23780365f976-Paper.pdf,A Robust Functional EM Algorithm for Incomplete Panel Count Data,"Alexander Moreno, Zhenke Wu, Jamie Roslyn Yap, Cho Lam, David Wetter, Inbal Nahum-Shani, Walter Dempsey, James M. Rehg",
neurips,https://proceedings.neurips.cc/paper/2020/file/e586a4f55fb43a540c2e9dab45e00f53-Paper.pdf,Graph Stochastic Neural Networks for Semi-supervised Learning,"Haibo Wang, Chuan Zhou, Xin Chen, Jia Wu, Shirui Pan, Jilong Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf,Compositional Zero-Shot Learning via Fine-Grained Dense Feature Composition,"Dat Huynh, Ehsan Elhamifar",
neurips,https://proceedings.neurips.cc/paper/2020/file/e5a90182cc81e12ab5e72d66e0b46fe3-Paper.pdf,A Benchmark for Systematic Generalization in Grounded Language Understanding,"Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, Brenden M. Lake",
neurips,https://proceedings.neurips.cc/paper/2020/file/e5e6851e7f7ffd3530e7389e183aa468-Paper.pdf,Weston-Watkins Hinge Loss and Ordered Partitions,"Yutong Wang, Clayton Scott",
neurips,https://proceedings.neurips.cc/paper/2020/file/e615c82aba461681ade82da2da38004a-Paper.pdf,Reinforcement Learning with Augmented Data,"Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, Aravind Srinivas",
neurips,https://proceedings.neurips.cc/paper/2020/file/e61eaa38aed621dd776d0e67cfeee366-Paper.pdf,Towards Minimax Optimal Reinforcement Learning in Factored Markov Decision Processes,"Yi Tian, Jian Qian, Suvrit Sra",
neurips,https://proceedings.neurips.cc/paper/2020/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf,Graduated Assignment for Joint Multi-Graph Matching and Clustering with Application to Unsupervised Graph Matching Network Learning,"Runzhong Wang, Junchi Yan, Xiaokang Yang",
neurips,https://proceedings.neurips.cc/paper/2020/file/e6385d39ec9394f2f3a354d9d2b88eec-Paper.pdf,Estimating Training Data Influence by Tracing Gradient Descent,"Garima Pruthi, Frederick Liu, Satyen Kale, Mukund Sundararajan",
neurips,https://proceedings.neurips.cc/paper/2020/file/e64f346817ce0c93d7166546ac8ce683-Paper.pdf,Joint Policy Search for Multi-agent Collaboration with Imperfect Information,"Yuandong Tian, Qucheng Gong, Yu Jiang","To learn good joint policies for multi-agent collaboration with incomplete information remains a fundamental challenge. While for two-player zero-sum games, coordinate-ascent approaches (optimizing one agent's policy at a time, e.g., self-play) work with guarantees, in multi-agent cooperative setting they often converge to sub-optimal Nash equilibrium. On the other hand, directly modeling joint policy changes in incomplete information game is nontrivial due to complicated interplay of policies (e.g., upstream updates affect downstream state reachability). In this paper, we show global changes of game values can be decomposed to policy changes localized at each information set, with a novel term named \emph{policy-change density}. Based on this, we propose \emph{Joint Policy Search} (JPS) that iteratively improves joint policies of collaborative agents in incomplete information games, without re-evaluating the entire game. On multiple collaborative tabular games, JPS is proven to never worsen performance and can improve solutions provided by unilateral approaches (e.g, CFR), outperforming algorithms designed for collaborative policy learning (e.g. BAD). Furthermore, for real-world game whose states are too many to enumerate, \ours{} has an online form that naturally links with gradient updates. We test it to Contract Bridge, a 4-player imperfect-information game where a team of
2
2
collaborates to compete against the other. In its bidding phase, players bid in turn to find a good contract through a limited information channel. Based on a strong baseline agent that bids competitive bridge purely through domain-agnostic self-play, JPS improves collaboration of team players and outperforms WBridge5, a championship-winning software, by
+
0.63
+
IMPs (International Matching Points) per board over
1000
1000
games, substantially better than previous SoTA (
+
0.41
+
IMPs/b against WBridge5). Note that
+
0.1
+
IMPs/b is regarded as a nontrivial improvement in Computer Bridge."
neurips,https://proceedings.neurips.cc/paper/2020/file/e655c7716a4b3ea67f48c6322fc42ed6-Paper.pdf,Adversarial Bandits with Corruptions: Regret Lower Bound and No-regret Algorithm,"lin yang, Mohammad Hajiesmaili, Mohammad Sadegh Talebi, John C. S. Lui, Wing Shing Wong",
neurips,https://proceedings.neurips.cc/paper/2020/file/e6b4b2a746ed40e1af829d1fa82daa10-Paper.pdf,Beta R-CNN: Looking into Pedestrian Detection from Another Perspective,"Zixuan Xu, Banghuai Li, Ye Yuan, Anhong Dang",
neurips,https://proceedings.neurips.cc/paper/2020/file/e6b738eca0e6792ba8a9cbcba6c1881d-Paper.pdf,Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks,"Soham De, Sam Smith",
neurips,https://proceedings.neurips.cc/paper/2020/file/e6cbc650cd5798a05dfd0f51d14cde5c-Paper.pdf,Learning Retrospective Knowledge with Reverse Reinforcement Learning,"Shangtong Zhang, Vivek Veeriah, Shimon Whiteson",
neurips,https://proceedings.neurips.cc/paper/2020/file/e7023ba77a45f7e84c5ee8a28dd63585-Paper.pdf,Dialog without Dialog Data: Learning Visual Dialog Agents from VQA Data,"Michael Cogswell, Jiasen Lu, Rishabh Jain, Stefan Lee, Devi Parikh, Dhruv Batra",
neurips,https://proceedings.neurips.cc/paper/2020/file/e7532dbeff7ef901f2e70daacb3f452d-Paper.pdf,GCOMB: Learning Budget-constrained Combinatorial Algorithms over Billion-sized Graphs,"Sahil Manchanda, AKASH MITTAL, Anuj Dhawan, Sourav Medya, Sayan Ranu, Ambuj Singh",
neurips,https://proceedings.neurips.cc/paper/2020/file/e769e03a9d329b2e864b4bf4ff54ff39-Paper.pdf,A General Large Neighborhood Search Framework for Solving Integer Linear Programs,"Jialin Song, ravi lanka, Yisong Yue, Bistra Dilkina",
neurips,https://proceedings.neurips.cc/paper/2020/file/e7a425c6ece20cbc9056f98699b53c6f-Paper.pdf,A Theoretical Framework for Target Propagation,"Alexander Meulemans, Francesco Carzaniga, Johan Suykens, João Sacramento, Benjamin F. Grewe",
neurips,https://proceedings.neurips.cc/paper/2020/file/e7c573c14a09b84f6b7782ce3965f335-Paper.pdf,OrganITE: Optimal transplant donor organ offering using an individual treatment effect,"Jeroen Berrevoets, James Jordon, Ioana Bica, alexander gimson, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2020/file/e7db14e12fb49c1d78a573e6e5f542c2-Paper.pdf,The Complete Lasso Tradeoff Diagram,"Hua Wang, Yachong Yang, Zhiqi Bu, Weijie Su",
neurips,https://proceedings.neurips.cc/paper/2020/file/e7e8f8e5982b3298c8addedf6811d500-Paper.pdf,On the universality of deep learning,"Emmanuel Abbe, Colin Sandon",
neurips,https://proceedings.neurips.cc/paper/2020/file/e8219d4c93f6c55c6b10fe6bfe997c6c-Paper.pdf,Regression with reject option and application to kNN,"Ahmed Zaoui, Christophe Denis, Mohamed Hebiri",
neurips,https://proceedings.neurips.cc/paper/2020/file/e834cb114d33f729dbc9c7fb0c6bb607-Paper.pdf,The Primal-Dual method for Learning Augmented Algorithms,"Etienne Bamas, Andreas Maggiori, Ola Svensson",
neurips,https://proceedings.neurips.cc/paper/2020/file/e894d787e2fd6c133af47140aa156f00-Paper.pdf,FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs,"Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, Wen Sun",
neurips,https://proceedings.neurips.cc/paper/2020/file/e8b1cbd05f6e6a358a81dee52493dd06-Paper.pdf,A Class of Algorithms for General Instrumental Variable Models,"Niki Kilbertus, Matt J. Kusner, Ricardo Silva",
neurips,https://proceedings.neurips.cc/paper/2020/file/e8d66338fab3727e34a9179ed8804f64-Paper.pdf,Black-Box Ripper: Copying black-box models using generative evolutionary algorithms,"Antonio Barbalau, Adrian Cosma, Radu Tudor Ionescu, Marius Popescu",
neurips,https://proceedings.neurips.cc/paper/2020/file/e8f2779682fd11fa2067beffc27a9192-Paper.pdf,Bayesian Optimization of Risk Measures,"Sait Cakmak, Raul Astudillo Marban, Peter Frazier, Enlu Zhou","We consider Bayesian optimization of objective functions of the form
ρ
[
F
(
x
,
W
)
]
ρ
, where
F
F
is a black-box expensive-to-evaluate function and
ρ
ρ
denotes either the VaR or CVaR risk measure, computed with respect to the randomness induced by the environmental random variable
W
W
. Such problems arise in decision making under uncertainty, such as in portfolio optimization and robust systems design. We propose a family of novel Bayesian optimization algorithms that exploit the structure of the objective function to substantially improve sampling efficiency. Instead of modeling the objective function directly as is typical in Bayesian optimization, these algorithms model
F
F
as a Gaussian process, and use the implied posterior on the objective function to decide which points to evaluate. We demonstrate the effectiveness of our approach in a variety of numerical experiments."
neurips,https://proceedings.neurips.cc/paper/2020/file/e904831f48e729f9ad8355a894334700-Paper.pdf,TorsionNet: A Reinforcement Learning Approach to Sequential Conformer Search,"Tarun Gogineni, Ziping Xu, Exequiel Punzalan, Runxuan Jiang, Joshua Kammeraad, Ambuj Tewari, Paul Zimmerman",
neurips,https://proceedings.neurips.cc/paper/2020/file/e92e1b476bb5262d793fd40931e0ed53-Paper.pdf,GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis,"Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger",
neurips,https://proceedings.neurips.cc/paper/2020/file/e94550c93cd70fe748e6982b3439ad3b-Paper.pdf,PIE-NET: Parametric Inference of Point Cloud Edges,"Xiaogang Wang, Yuelang Xu, Kai Xu, Andrea Tagliasacchi, Bin Zhou, Ali Mahdavi-Amiri, Hao Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/e946209592563be0f01c844ab2170f0c-Paper.pdf,A Simple Language Model for Task-Oriented Dialogue,"Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, Richard Socher",
neurips,https://proceedings.neurips.cc/paper/2020/file/e9470886ecab9743fb7ea59420c245d2-Paper.pdf,A Continuous-Time Mirror Descent Approach to Sparse Phase Retrieval,"Fan Wu, Patrick Rebeschini","We analyze continuous-time mirror descent applied to sparse phase retrieval, which is the problem of recovering sparse signals from a set of magnitude-only measurements. We apply mirror descent to the unconstrained empirical risk minimization problem (batch setting), using the square loss and square measurements. We provide a full convergence analysis of the algorithm in this non-convex setting and prove that, with the hypentropy mirror map, mirror descent recovers any
k
k
-sparse vector
x
⋆
∈
R
n
x
with minimum (in modulus) non-zero entry on the order of
∥
x
⋆
∥
2
/
√
k
‖
from
k
2
k
Gaussian measurements, modulo logarithmic terms. This yields a simple algorithm which, unlike most existing approaches to sparse phase retrieval, adapts to the sparsity level, without including thresholding steps or adding regularization terms. Our results also provide a principled theoretical understanding for Hadamard Wirtinger flow [54], as Euclidean gradient descent applied to the empirical risk problem with Hadamard parametrization can be recovered as a first-order approximation to mirror descent in discrete time."
neurips,https://proceedings.neurips.cc/paper/2020/file/e96c7de8f6390b1e6c71556e4e0a4959-Paper.pdf,Confidence sequences for sampling without replacement,"Ian Waudby-Smith, Aaditya Ramdas","Many practical tasks involve sampling sequentially without replacement (WoR) from a finite population of size
N
N
, in an attempt to estimate some parameter
θ
⋆
θ
. Accurately quantifying uncertainty throughout this process is a nontrivial task, but is necessary because it often determines when we stop collecting samples and confidently report a result. We present a suite of tools for designing \textit{confidence sequences} (CS) for
θ
⋆
θ
. A CS is a sequence of confidence sets
(
C
n
)
N
n
=
1
(
, that shrink in size, and all contain
θ
⋆
θ
simultaneously with high probability. We first exploit a relationship between Bayesian posteriors and martingales to construct a (frequentist) CS for the parameters of a hypergeometric distribution. We then present Hoeffding- and empirical-Bernstein-type time-uniform CSs and fixed-time confidence intervals for sampling WoR which improve on previous bounds in the literature."
neurips,https://proceedings.neurips.cc/paper/2020/file/e97c864e8ac67f7aed5ce53ec28638f5-Paper.pdf,A mean-field analysis of two-player zero-sum games,"Carles Domingo-Enrich, Samy Jelassi, Arthur Mensch, Grant Rotskoff, Joan Bruna",
neurips,https://proceedings.neurips.cc/paper/2020/file/e992111e4ab9985366e806733383bd8c-Paper.pdf,Leap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge,"Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant",
neurips,https://proceedings.neurips.cc/paper/2020/file/e9bcd1b063077573285ae1a41025f5dc-Paper.pdf,Pipeline PSRO: A Scalable Approach for Finding Approximate Nash Equilibria in Large Games,"Stephen Mcaleer, JB Lanier, Roy Fox, Pierre Baldi",
neurips,https://proceedings.neurips.cc/paper/2020/file/e9bf14a419d77534105016f5ec122d62-Paper.pdf,Improving Sparse Vector Technique with Renyi Differential Privacy,"Yuqing Zhu, Yu-Xiang Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/ea119a40c1592979f51819b0bd38d39d-Paper.pdf,Latent Template Induction with Gumbel-CRFs,"Yao Fu, Chuanqi Tan, Bin Bi, Mosha Chen, Yansong Feng, Alexander Rush",
neurips,https://proceedings.neurips.cc/paper/2020/file/ea33b4fd0fc1ea0a40344be8a8641123-Paper.pdf,Instance Based Approximations to Profile Maximum Likelihood,"Nima Anari, Moses Charikar, Kirankumar Shiragur, Aaron Sidford",
neurips,https://proceedings.neurips.cc/paper/2020/file/ea3502c3594588f0e9d5142f99c66627-Paper.pdf,Factorizable Graph Convolutional Networks,"Yiding Yang, Zunlei Feng, Mingli Song, Xinchao Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/ea3ed20b6b101a09085ef09c97da1597-Paper.pdf,Guided Adversarial Attack for Evaluating and Enhancing Adversarial Defenses,"Gaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, Venkatesh Babu R",
neurips,https://proceedings.neurips.cc/paper/2020/file/ea4eb49329550caaa1d2044105223721-Paper.pdf,A Study on Encodings for Neural Architecture Search,"Colin White, Willie Neiswanger, Sam Nolen, Yash Savani","In this work, we present the first formal study on the effect of architecture encodings for NAS, including a theoretical grounding and an empirical study. First we formally define architecture encodings and give a theoretical characterization on the scalability of the encodings we study. Then we identify the main encoding-dependent subroutines which NAS algorithms employ, running experiments to show which encodings work best with each subroutine for many popular algorithms. The experiments act as an ablation study for prior work, disentangling the algorithmic and encoding-based contributions, as well as a guideline for future work. Our results demonstrate that NAS encodings are an important design decision which can have a significant impact on overall performance. Our code is available at https://github.com/naszilla/naszilla."
neurips,https://proceedings.neurips.cc/paper/2020/file/ea6b2efbdd4255a9f1b3bbc6399b58f4-Paper.pdf,Noise2Same: Optimizing A Self-Supervised Bound for Image Denoising,"Yaochen Xie, Zhengyang Wang, Shuiwang Ji",
neurips,https://proceedings.neurips.cc/paper/2020/file/ea89621bee7c88b2c5be6681c8ef4906-Paper.pdf,Early-Learning Regularization Prevents Memorization of Noisy Labels,"Sheng Liu, Jonathan Niles-Weed, Narges Razavian, Carlos Fernandez-Granda",
neurips,https://proceedings.neurips.cc/paper/2020/file/eaae339c4d89fc102edd9dbdb6a28915-Paper.pdf,LAPAR: Linearly-Assembled Pixel-Adaptive Regression Network for Single Image Super-resolution and Beyond,"Wenbo Li, Kun Zhou, Lu Qi, Nianjuan Jiang, Jiangbo Lu, Jiaya Jia",
neurips,https://proceedings.neurips.cc/paper/2020/file/eaae5e04a259d09af85c108fe4d7dd0c-Paper.pdf,Learning Parities with Neural Networks,"Amit Daniely, Eran Malach",
neurips,https://proceedings.neurips.cc/paper/2020/file/eab1bceaa6c5823d7ed86cfc7a8bd824-Paper.pdf,Consistent Plug-in Classifiers for Complex Objectives and Constraints,"Shiv Kumar Tavker, Harish Guruprasad Ramaswamy, Harikrishna Narasimhan","We present a statistically consistent algorithm for constrained classification problems where the objective (e.g. F-measure, G-mean) and the constraints (e.g. demographic parity, coverage) are defined by general functions of the confusion matrix. The key idea is to reduce the problem into a sequence of plug-in classifier learning problems, which is done by formulating an optimization problem over the intersection of the set of achievable confusion matrices and the set of feasible matrices. For objective and constraints that are convex functions of the confusion matrix, our algorithm requires
O
(
1
/
ϵ
2
)
O
calls to the plug-in routine, which improves on the
O
(
1
/
ϵ
3
)
O
rate achieved by Narasimhan (2018). We demonstrate empirically that our algorithm performs at least as well as the state-of-the-art methods for these problems."
neurips,https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf,Movement Pruning: Adaptive Sparsity by Fine-Tuning,"Victor Sanh, Thomas Wolf, Alexander Rush",
neurips,https://proceedings.neurips.cc/paper/2020/file/eae27d77ca20db309e056e3d2dcd7d69-Paper.pdf,Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot,"Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, Jason D. Lee",
neurips,https://proceedings.neurips.cc/paper/2020/file/eb06b9db06012a7a4179b8f3cb5384d3-Paper.pdf,Online Matrix Completion with Side Information,"Mark Herbster, Stephen Pasteris, Lisa Tse",
neurips,https://proceedings.neurips.cc/paper/2020/file/eb1e78328c46506b46a4ac4a1e378b91-Paper.pdf,Position-based Scaled Gradient for Model Quantization and Pruning,"Jangho Kim, KiYoon Yoo, Nojun Kwak",
neurips,https://proceedings.neurips.cc/paper/2020/file/eb2e9dffe58d635b7d72e99c8e61b5f2-Paper.pdf,Online Learning with Primary and Secondary Losses,"Avrim Blum, Han Shao","We study the problem of online learning with primary and secondary losses. For example, a recruiter making decisions of which job applicants to hire might weigh false positives and false negatives equally (the primary loss) but the applicants might weigh false negatives much higher (the secondary loss). We consider the following question: Can we combine
expert advice'' to achieve low regret with respect to the primary loss, while at the same time performing {\em not much worse than the worst expert} with respect to the secondary loss? Unfortunately, we show that this goal is unachievable without any bounded variance assumption on the secondary loss. More generally, we consider the goal of minimizing the regret with respect to the primary loss and bounding the secondary loss by a linear threshold. On the positive side, we show that running any switching-limited algorithm can achieve this goal if all experts satisfy the assumption that the secondary loss does not exceed the linear threshold by
o
(
T
)
o
for any time interval. If not all experts satisfy this assumption, our algorithms can achieve this goal given access to some external oracles which determine when to deactivate and reactivate experts."
neurips,https://proceedings.neurips.cc/paper/2020/file/ebc2aa04e75e3caabda543a1317160c0-Paper.pdf,Graph Information Bottleneck,"Tailin Wu, Hongyu Ren, Pan Li, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2020/file/ebd64e2bf193fc8c658af2b91952ce8d-Paper.pdf,The Complexity of Adversarially Robust Proper Learning of Halfspaces with Agnostic Noise,"Ilias Diakonikolas, Daniel M. Kane, Pasin Manurangsi","We study the computational complexity of adversarially robust proper learning of halfspaces in the distribution-independent agnostic PAC model, with a focus on
L
p
L
perturbations. We give a computationally efficient learning algorithm and a nearly matching computational hardness result for this problem. An interesting implication of our findings is that the
L
∞
L
perturbations case is provably computationally harder than the case
2
≤
p
<
∞
2
."
neurips,https://proceedings.neurips.cc/paper/2020/file/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Paper.pdf,Adaptive Online Estimation of Piecewise Polynomial Trends,"Dheeraj Baby, Yu-Xiang Wang","We consider the framework of non-stationary stochastic optimization [Besbes et.al. 2015] with squared error losses and noisy gradient feedback where the dynamic regret of an online learner against a time varying comparator sequence is studied. Motivated from the theory of non-parametric regression, we introduce a \emph{new variational constraint} that enforces the comparator sequence to belong to a discrete
k
t
h
k
order Total Variation ball of radius
C
n
C
. This variational constraint models comparators that have piece-wise polynomial structure which has many relevant practical applications [Tibshirani2015]. By establishing connections to the theory of wavelet based non-parametric regression, we design a \emph{polynomial time} algorithm that achieves the nearly \emph{optimal dynamic regret} of
~
O
(
n
1
2
k
+
3
C
2
2
k
+
3
n
)
O
. The proposed policy is \emph{adaptive to the unknown radius}
C
n
C
. Further, we show that the same policy is minimax optimal for several other non-parametric families of interest."
neurips,https://proceedings.neurips.cc/paper/2020/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Paper.pdf,RNNPool: Efficient Non-linear Pooling for RAM Constrained Inference,"Oindrila Saha, Aditya Kusupati, Harsha Vardhan Simhadri, Manik Varma, Prateek Jain",
neurips,https://proceedings.neurips.cc/paper/2020/file/ebea2325dc670423afe9a1f4d9d1aef5-Paper.pdf,Agnostic Learning with Multiple Objectives,"Corinna Cortes, Mehryar Mohri, Javier Gonzalvo, Dmitry Storcheus","Most machine learning tasks are inherently multi-objective. This means that the learner has to come up with a model that performs well across a number of base objectives
\cL
1
,
…
,
\cL
p
\cL
, as opposed to a single one. Since optimizing with respect to multiple objectives at the same time is often computationally expensive, the base objectives are often combined in an ensemble
∑
p
k
=
1
λ
k
\cL
k
∑
, thereby reducing the problem to scalar optimization. The mixture weights
λ
k
λ
are set to uniform or some other fixed distribution, based on the learner's preferences. We argue that learning with a fixed distribution on the mixture weights runs the risk of overfitting to some individual objectives and significantly harming others, despite performing well on an entire ensemble. Moreover, in reality, the true preferences of a learner across multiple objectives are often unknown or hard to express as a specific distribution. Instead, we propose a new framework of \emph{Agnostic Learning with Multiple Objectives} (
\almo
\almo
), where a model is optimized for \emph{any} weights in the mixture of base objectives. We present data-dependent Rademacher complexity guarantees for learning in the
\almo
\almo
framework, which are used to guide a scalable optimization algorithm and the corresponding regularization. We present convergence guarantees for this algorithm, assuming convexity of the loss functions and the underlying hypothesis space. We further implement the algorithm in a popular symbolic gradient computation framework and empirically demonstrate on a number of datasets the benefits of
\almo
\almo
framework versus learning with a fixed mixture weights distribution."
neurips,https://proceedings.neurips.cc/paper/2020/file/ebf99bb5df6533b6dd9180a59034698d-Paper.pdf,3D Multi-bodies: Fitting Sets of Plausible 3D Human Models to Ambiguous Image Data,"Benjamin Biggs, David Novotny, Sebastien Ehrhardt, Hanbyul Joo, Ben Graham, Andrea Vedaldi",
neurips,https://proceedings.neurips.cc/paper/2020/file/ec1f764517b7ffb52057af6df18142b7-Paper.pdf,Auto-Panoptic: Cooperative Multi-Component Architecture Search for Panoptic Segmentation,"Yangxin Wu, Gengwei Zhang, Hang Xu, Xiaodan Liang, Liang Lin",
neurips,https://proceedings.neurips.cc/paper/2020/file/ec24a54d62ce57ba93a531b460fa8d18-Paper.pdf,Differentiable Top-k with Optimal Transport,"Yujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo Zhao, Hongyuan Zha, Wei Wei, Tomas Pfister",
neurips,https://proceedings.neurips.cc/paper/2020/file/ec3183a7f107d1b8dbb90cb3c01ea7d5-Paper.pdf,Information-theoretic Task Selection for Meta-Reinforcement Learning,"Ricardo Luna Gutierrez, Matteo Leonetti",
neurips,https://proceedings.neurips.cc/paper/2020/file/ec79d4bed810ed64267d169b0d37373e-Paper.pdf,A Limitation of the PAC-Bayes Framework,"Roi Livni, Shay Moran","PAC-Bayes is a useful framework for deriving generalization bounds which was introduced by McAllester ('98). This framework has the flexibility of deriving distribution- and algorithm-dependent bounds, which are often tighter than VC-related uniform convergence bounds. In this manuscript we present a limitation for the PAC-Bayes framework. We demonstrate an easy learning task which is not amenable to a PAC-Bayes analysis. Specifically, we consider the task of linear classification in 1D; it is well-known that this task is learnable using just
O
(
log
(
1
/
δ
)
/
ϵ
)
O
examples. On the other hand, we show that this fact can not be proved using a PAC-Bayes analysis: for any algorithm that learns 1-dimensional linear classifiers there exists a (realizable) distribution for which the PAC-Bayes bound is arbitrarily large."
neurips,https://proceedings.neurips.cc/paper/2020/file/ecb287ff763c169694f682af52c1f309-Paper.pdf,On Completeness-aware Concept-Based Explanations in Deep Neural Networks,"Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, Pradeep Ravikumar",
neurips,https://proceedings.neurips.cc/paper/2020/file/ecb47fbb07a752413640f82a945530f8-Paper.pdf,Stochastic Recursive Gradient Descent Ascent for Stochastic Nonconvex-Strongly-Concave Minimax Problems,"Luo Luo, Haishan Ye, Zhichao Huang, Tong Zhang","We consider nonconvex-concave minimax optimization problems of the form
min
x
max
y
∈
Y
f
(
x
,
y
)
min
, where
f
f
is strongly-concave in
y
y
but possibly nonconvex in
x
x
and
Y
Y
is a convex and compact set. We focus on the stochastic setting, where we can only access an unbiased stochastic gradient estimate of
f
f
at each iteration. This formulation includes many machine learning applications as special cases such as robust optimization and adversary training. We are interested in finding an
O
(
ε
)
O
-stationary point of the function
Φ
(
⋅
)
=
max
y
∈
Y
f
(
⋅
,
y
)
Φ
. The most popular algorithm to solve this problem is stochastic gradient decent ascent, which requires
O
(
κ
3
ε
−
4
)
O
stochastic gradient evaluations, where
κ
κ
is the condition number. In this paper, we propose a novel method called Stochastic Recursive gradiEnt Descent Ascent (SREDA), which estimates gradients more efficiently using variance reduction. This method achieves the best known stochastic gradient complexity of
O
(
κ
3
ε
−
3
)
O
, and its dependency on
ε
ε
is optimal for this problem."
neurips,https://proceedings.neurips.cc/paper/2020/file/ecb9fe2fbb99c31f567e9823e884dbec-Paper.pdf,Why Normalizing Flows Fail to Detect Out-of-Distribution Data,"Polina Kirichenko, Pavel Izmailov, Andrew G. Wilson",
neurips,https://proceedings.neurips.cc/paper/2020/file/eccd2a86bae4728b38627162ba297828-Paper.pdf,Explaining Naive Bayes and Other Linear Classifiers with Polynomial Time and Delay,"Joao Marques-Silva, Thomas Gerspacher, Martin Cooper, Alexey Ignatiev, Nina Narodytska",
neurips,https://proceedings.neurips.cc/paper/2020/file/ed23fbf18c2cd35f8c7f8de44f85c08d-Paper.pdf,Unsupervised Translation of Programming Languages,"Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, Guillaume Lample",
neurips,https://proceedings.neurips.cc/paper/2020/file/ed265bc903a5a097f61d3ec064d96d2e-Paper.pdf,Adversarial Style Mining for One-Shot Unsupervised Domain Adaptation,"Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, Yi Yang",
neurips,https://proceedings.neurips.cc/paper/2020/file/ed383ec94720d62a939bfb6bdd98f50c-Paper.pdf,Optimally Deceiving a Learning Leader in Stackelberg Games,"Georgios Birmpas, Jiarui Gan, Alexandros Hollender, Francisco Marmolejo, Ninad Rajgopal, Alexandros Voudouris",
neurips,https://proceedings.neurips.cc/paper/2020/file/ed46558a56a4a26b96a68738a0d28273-Paper.pdf,Online Optimization with Memory and Competitive Control,"Guanya Shi, Yiheng Lin, Soon-Jo Chung, Yisong Yue, Adam Wierman","This paper presents competitive algorithms for a novel class of online optimization problems with memory. We consider a setting where the learner seeks to minimize the sum of a hitting cost and a switching cost that depends on the previous
p
p
decisions. This setting generalizes Smoothed Online Convex Optimization. The proposed approach, Optimistic Regularized Online Balanced Descent, achieves a constant, dimension-free competitive ratio. Further, we show a connection between online optimization with memory and online control with adversarial disturbances. This connection, in turn, leads to a new constant-competitive policy for a rich class of online control problems."
neurips,https://proceedings.neurips.cc/paper/2020/file/ed77eab0b8ff85d0a6a8365df1846978-Paper.pdf,IDEAL: Inexact DEcentralized Accelerated Augmented Lagrangian Method,"Yossi Arjevani, Joan Bruna, Bugra Can, Mert Gurbuzbalaban, Stefanie Jegelka, Hongzhou Lin",
neurips,https://proceedings.neurips.cc/paper/2020/file/eddb904a6db773755d2857aacadb1cb0-Paper.pdf,Evolving Graphical Planner: Contextual Global Planning for Vision-and-Language Navigation,"Zhiwei Deng, Karthik Narasimhan, Olga Russakovsky",
neurips,https://proceedings.neurips.cc/paper/2020/file/eddc3427c5d77843c2253f1e799fe933-Paper.pdf,Learning from Failure: De-biasing Classifier from Biased Classifier,"Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, Jinwoo Shin",
neurips,https://proceedings.neurips.cc/paper/2020/file/eddea82ad2755b24c4e168c5fc2ebd40-Paper.pdf,Likelihood Regret: An Out-of-Distribution Detection Score For Variational Auto-encoder,"Zhisheng Xiao, Qing Yan, Yali Amit",
neurips,https://proceedings.neurips.cc/paper/2020/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf,Deep Diffusion-Invariant Wasserstein Distributional Classification,"Sung Woo Park, Dong Wook Shu, Junseok Kwon",
neurips,https://proceedings.neurips.cc/paper/2020/file/edf0320adc8658b25ca26be5351b6c4a-Paper.pdf,"Finding All
ϵ
ϵ
-Good Arms in Stochastic Bandits","Blake Mason, Lalit Jain, Ardhendu Tripathy, Robert Nowak","The pure-exploration problem in stochastic multi-armed bandits aims to find one or more arms with the largest (or near largest) means. Examples include finding an
ϵ
ϵ
-good arm, best-arm identification, top-
k
k
arm identification, and finding all arms with means above a specified threshold. However, the problem of finding \emph{all}
ϵ
ϵ
-good arms has been overlooked in past work, although arguably this may be the most natural objective in many applications. For example, a virologist may conduct preliminary laboratory experiments on a large candidate set of treatments and move all
ϵ
ϵ
-good treatments into more expensive clinical trials. Since the ultimate clinical efficacy is uncertain, it is important to identify all
ϵ
ϵ
-good candidates. Mathematically, the all-
ϵ
ϵ
-good arm identification problem is presents significant new challenges and surprises that do not arise in the pure-exploration objectives studied in the past. We introduce two algorithms to overcome these and demonstrate their great empirical performance on a large-scale crowd-sourced dataset of
2.2
2.2
M ratings collected by the New Yorker Caption Contest as well as a dataset testing hundreds of possible cancer drugs."
neurips,https://proceedings.neurips.cc/paper/2020/file/ee23e7ad9b473ad072d57aaa9b2a5222-Paper.pdf,Meta-Learning through Hebbian Plasticity in Random Networks,"Elias Najarro, Sebastian Risi",
neurips,https://proceedings.neurips.cc/paper/2020/file/ee715daa76f1b51d80343f45547be570-Paper.pdf,A Computational Separation between Private Learning and Online Learning,Mark Bun,
neurips,https://proceedings.neurips.cc/paper/2020/file/ee76626ee11ada502d5dbf1fb5aae4d2-Paper.pdf,Top-KAST: Top-K Always Sparse Training,"Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon Osindero, Erich Elsen",
neurips,https://proceedings.neurips.cc/paper/2020/file/ee89223a2b625b5152132ed77abbcc79-Paper.pdf,Meta-Learning with Adaptive Hyperparameters,"Sungyong Baik, Myungsub Choi, Janghoon Choi, Heewon Kim, Kyoung Mu Lee",
neurips,https://proceedings.neurips.cc/paper/2020/file/eea5d933e9dce59c7dd0f6532f9ea81b-Paper.pdf,Tight last-iterate convergence rates for no-regret learning in multi-player games,"Noah Golowich, Sarath Pattathil, Constantinos Daskalakis",
neurips,https://proceedings.neurips.cc/paper/2020/file/eeb29740e8e9bcf14dc26c2fff8cca81-Paper.pdf,Curvature Regularization to Prevent Distortion in Graph Embedding,"Hongbin Pei, Bingzhe Wei, Kevin Chang, Chunxu Zhang, Bo Yang",
neurips,https://proceedings.neurips.cc/paper/2020/file/eefc7bfe8fd6e2c8c01aa6ca7b1aab1a-Paper.pdf,Perturbing Across the Feature Hierarchy to Improve Standard and Strict Blackbox Attack Transferability,"Nathan Inkawhich, Kevin Liang, Binghui Wang, Matthew Inkawhich, Lawrence Carin, Yiran Chen","We consider the blackbox transfer-based targeted adversarial attack threat model in the realm of deep neural network (DNN) image classifiers. Rather than focusing on crossing decision boundaries at the output layer of the source model, our method perturbs representations throughout the extracted feature hierarchy to resemble other classes. We design a flexible attack framework that allows for multi-layer perturbations and demonstrates state-of-the-art targeted transfer performance between ImageNet DNNs. We also show the superiority of our feature space methods under a relaxation of the common assumption that the source and target models are trained on the same dataset and label space, in some instances achieving a
10
×
10
increase in targeted success rate relative to other blackbox transfer methods. Finally, we analyze why the proposed methods outperform existing attack strategies and show an extension of the method in the case when limited queries to the blackbox model are allowed."
neurips,https://proceedings.neurips.cc/paper/2020/file/eefc9e10ebdc4a2333b42b2dbb8f27b6-Paper.pdf,Statistical and Topological Properties of Sliced Probability Divergences,"Kimia Nadjahi, Alain Durmus, Lénaïc Chizat, Soheil Kolouri, Shahin Shahrampour, Umut Simsekli",
neurips,https://proceedings.neurips.cc/paper/2020/file/ef0d17b3bdb4ee2aa741ba28c7255c53-Paper.pdf,Probabilistic Active Meta-Learning,"Jean Kaddour, Steindor Saemundsson, Marc Deisenroth (he/him)",
neurips,https://proceedings.neurips.cc/paper/2020/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf,"Knowledge Distillation in Wide Neural Networks: Risk Bound, Data Efficiency and Imperfect Teacher","Guangda Ji, Zhanxing Zhu",
neurips,https://proceedings.neurips.cc/paper/2020/file/ef126722e64e98d1c33933783e52eafc-Paper.pdf,Adversarial Attacks on Deep Graph Matching,"Zijie Zhang, Zeru Zhang, Yang Zhou, Yelong Shen, Ruoming Jin, Dejing Dou",
neurips,https://proceedings.neurips.cc/paper/2020/file/ef2ee09ea9551de88bc11fd7eeea93b0-Paper.pdf,The Generalization-Stability Tradeoff In Neural Network Pruning,"Brian Bartoldson, Ari Morcos, Adrian Barbu, Gordon Erlebacher",
neurips,https://proceedings.neurips.cc/paper/2020/file/ef48e3ef07e359006f7869b04fa07f5e-Paper.pdf,Gradient-EM Bayesian Meta-Learning,"Yayi Zou, Xiaoqi Lu",
neurips,https://proceedings.neurips.cc/paper/2020/file/ef8b5fcc338e003145ac9c134754db71-Paper.pdf,Logarithmic Regret Bound in Partially Observable Linear Dynamical Systems,"Sahin Lale, Kamyar Azizzadenesheli, Babak Hassibi, Anima Anandkumar","We study the problem of system identification and adaptive control in partially observable linear dynamical systems. Adaptive and closed-loop system identification is a challenging problem due to correlations introduced in data collection. In this paper, we present the first model estimation method with finite-time guarantees in both open and closed-loop system identification. Deploying this estimation method, we propose adaptive control online learning (AdapOn), an efficient reinforcement learning algorithm that adaptively learns the system dynamics and continuously updates its controller through online learning steps. AdapOn estimates the model dynamics by occasionally solving a linear regression problem through interactions with the environment. Using policy re-parameterization and the estimated model, AdapOn constructs counterfactual loss functions to be used for updating the controller through online gradient descent. Over time, AdapOn improves its model estimates and obtains more accurate gradient updates to improve the controller. We show that AdapOn achieves a regret upper bound of
polylog
(
T
)
polylog
, after
T
T
time steps of agent-environment interaction. To the best of our knowledge, AdapOn is the first algorithm that achieves
polylog
(
T
)
polylog
regret in adaptive control of \textit{unknown} partially observable linear dynamical systems which includes linear quadratic Gaussian (LQG) control."
neurips,https://proceedings.neurips.cc/paper/2020/file/ef9280fbc5317f17d480e4d4f61b3751-Paper.pdf,Linearly Converging Error Compensated SGD,"Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, Peter Richtarik",
neurips,https://proceedings.neurips.cc/paper/2020/file/efe34c4e2190e97d1adc625902822b13-Paper.pdf,Canonical 3D Deformer Maps: Unifying parametric and non-parametric methods for dense weakly-supervised category reconstruction,"David Novotny, Roman Shapovalov, Andrea Vedaldi",
neurips,https://proceedings.neurips.cc/paper/2020/file/f02208a057804ee16ac72ff4d3cec53b-Paper.pdf,A Self-Tuning Actor-Critic Algorithm,"Tom Zahavy, Zhongwen Xu, Vivek Veeriah, Matteo Hessel, Junhyuk Oh, Hado P. van Hasselt, David Silver, Satinder Singh",
neurips,https://proceedings.neurips.cc/paper/2020/file/f056bfa71038e04a2400266027c169f9-Paper.pdf,The Cone of Silence: Speech Separation by Localization,"Teerapat Jenrungrot, Vivek Jayaram, Steve Seitz, Ira Kemelmacher-Shlizerman","Given a multi-microphone recording of an unknown number of speakers talking concurrently, we simultaneously localize the sources and separate the individual speakers. At the core of our method is a deep network, in the waveform domain, which isolates sources within an angular region
θ
±
w
/
2
θ
, given an angle of interest
θ
θ
and angular window size
w
w
. By exponentially decreasing
w
w
, we can perform a binary search to localize and separate all sources in logarithmic time. Our algorithm also allows for an arbitrary number of potentially moving speakers at test time, including more speakers than seen during training. Experiments demonstrate state of the art performance for both source separation and source localization, particularly in high levels of background noise."
neurips,https://proceedings.neurips.cc/paper/2020/file/f05da679342107f92111ad9d65959cd3-Paper.pdf,High-Dimensional Bayesian Optimization via Nested Riemannian Manifolds,"Noémie Jaquier, Leonel Rozo",
neurips,https://proceedings.neurips.cc/paper/2020/file/f0682320ccbbb1f1fb1e795de5e5639a-Paper.pdf,Train-by-Reconnect: Decoupling Locations of Weights from Their Values,"Yushi Qiu, Reiji Suda",
neurips,https://proceedings.neurips.cc/paper/2020/file/f06edc8ab534b2c7ecbd4c2051d9cb1e-Paper.pdf,Learning discrete distributions: user vs item-level privacy,"Yuhan Liu, Ananda Theertha Suresh, Felix Xinnan X. Yu, Sanjiv Kumar, Michael Riley","Much of the literature on differential privacy focuses on item-level privacy, where loosely speaking, the goal is to provide privacy per item or training example. However, recently many practical applications such as federated learning require preserving privacy for all items of a single user, which is much harder to achieve. Therefore understanding the theoretical limit of user-level privacy becomes crucial. We study the fundamental problem of learning discrete distributions over
k
k
symbols with user-level differential privacy. If each user has
m
m
samples, we show that straightforward applications of Laplace or Gaussian mechanisms require the number of users to be
O
(
k
/
(
m
α
2
)
+
k
/
ϵ
α
)
O
to achieve an
ℓ
1
ℓ
distance of
α
α
between the true and estimated distributions, with the privacy-induced penalty
k
/
ϵ
α
k
independent of the number of samples per user
m
m
. Moreover, we show that any mechanism that only operates on the final aggregate should require a user complexity of the same order. We then propose a mechanism such that the number of users scales as
~
O
(
k
/
(
m
α
2
)
+
k
/
√
m
ϵ
α
)
O
and further show that it is nearly-optimal under certain regimes. Thus the privacy penalty is
~
Θ
(
√
m
)
Θ
times smaller compared to the standard mechanisms. We also propose general techniques for obtaining lower bounds on restricted differentially private estimators and a lower bound on the total variation between binomial distributions, both of which might be of independent interest."
neurips,https://proceedings.neurips.cc/paper/2020/file/f076073b2082f8741a9cd07b789c77a0-Paper.pdf,Matrix Completion with Quantified Uncertainty through Low Rank Gaussian Copula,"Yuxuan Zhao, Madeleine Udell",
neurips,https://proceedings.neurips.cc/paper/2020/file/f0b76267fbe12b936bd65e203dc675c1-Paper.pdf,Sparse and Continuous Attention Mechanisms,"André Martins, António Farinhas, Marcos Treviso, Vlad Niculae, Pedro Aguiar, Mario Figueiredo",
neurips,https://proceedings.neurips.cc/paper/2020/file/f0bda020d2470f2e74990a07a607ebd9-Paper.pdf,Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection,"Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, Jian Yang",
neurips,https://proceedings.neurips.cc/paper/2020/file/f0d7053396e765bf52de12133cf1afe8-Paper.pdf,Learning by Minimizing the Sum of Ranked Range,"Shu Hu, Yiming Ying, xin wang, Siwei Lyu",
neurips,https://proceedings.neurips.cc/paper/2020/file/f0eb6568ea114ba6e293f903c34d7488-Paper.pdf,Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations,"Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, Cho-Jui Hsieh",
neurips,https://proceedings.neurips.cc/paper/2020/file/f106b7f99d2cb30c3db1c3cc0fde9ccb-Paper.pdf,Understanding Anomaly Detection with Deep Invertible Networks through Hierarchies of Distributions and Features,"Robin Schirrmeister, Yuxuan Zhou, Tonio Ball, Dan Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/f10f2da9a238b746d2bac55759915f0d-Paper.pdf,Fair Hierarchical Clustering,"Sara Ahmadian, Alessandro Epasto, Marina Knittel, Ravi Kumar, Mohammad Mahdian, Benjamin Moseley, Philip Pham, Sergei Vassilvitskii, Yuyan Wang","In this paper we extend this notion to hierarchical clustering, where the goal is to recursively partition the data to optimize a specific objective. For various natural objectives, we obtain simple, efficient algorithms to find a provably good fair hierarchical clustering. Empirically, we show that our algorithms can find a fair hierarchical clustering, with only a negligible loss in the objective."
neurips,https://proceedings.neurips.cc/paper/2020/file/f1298750ed09618717f9c10ea8d1d3b0-Paper.pdf,Self-training Avoids Using Spurious Features Under Domain Shift,"Yining Chen, Colin Wei, Ananya Kumar, Tengyu Ma",
neurips,https://proceedings.neurips.cc/paper/2020/file/f12a6a7477077af66212ef0813bcf332-Paper.pdf,Improving Online Rent-or-Buy Algorithms with Sequential Decision Making and ML Predictions,Shom Banerjee,
neurips,https://proceedings.neurips.cc/paper/2020/file/f14bc21be7eaeed046fed206a492e652-Paper.pdf,CircleGAN: Generative Adversarial Learning across Spherical Circles,"Woohyeon Shim, Minsu Cho",
neurips,https://proceedings.neurips.cc/paper/2020/file/f1507aba9fc82ffa7cc7373c58f8a613-Paper.pdf,"WOR and
p
p
's: Sketches for
ℓ
p
ℓ
-Sampling Without Replacement","Edith Cohen, Rasmus Pagh, David Woodruff","Weighted sampling is a fundamental tool in data analysis and machine learning pipelines. Samples are used for efficient estimation of statistics or as sparse representations of the data. When weight distributions are skewed, as is often the case in practice, without-replacement (WOR) sampling is much more effective than with-replacement (WR) sampling: It provides a broader representation and higher accuracy for the same number of samples. We design novel composable sketches for WOR {\em
ℓ
p
ℓ
sampling}, weighted sampling of keys according to a power
p
∈
[
0
,
2
]
p
of their frequency (or for signed data, sum of updates). Our sketches have size that grows only linearly with sample size. Our design is simple and practical, despite intricate analysis, and based on off-the-shelf use of widely implemented heavy hitters sketches such as \texttt{CountSketch}. Our method is the first to provide WOR sampling in the important regime of
p
>
1
p
and the first to handle signed updates for
p
>
0
p
."
neurips,https://proceedings.neurips.cc/paper/2020/file/f1686b4badcf28d33ed632036c7ab0b8-Paper.pdf,Hypersolvers: Toward Fast Continuous-Depth Models,"Michael Poli, Stefano Massaroli, Atsushi Yamashita, Hajime Asama, Jinkyoo Park",
neurips,https://proceedings.neurips.cc/paper/2020/file/f169b1a771215329737c91f70b5bf05c-Paper.pdf,Log-Likelihood Ratio Minimizing Flows: Towards Robust and Quantifiable Neural Distribution Alignment,"Ben Usman, Avneesh Sud, Nick Dufour, Kate Saenko",
neurips,https://proceedings.neurips.cc/paper/2020/file/f1cf2a082126bf02de0b307778ce73a7-Paper.pdf,Escaping the Gravitational Pull of Softmax,"Jincheng Mei, Chenjun Xiao, Bo Dai, Lihong Li, Csaba Szepesvari, Dale Schuurmans",
neurips,https://proceedings.neurips.cc/paper/2020/file/f1daf122cde863010844459363cd31db-Paper.pdf,Regret in Online Recommendation Systems,"Kaito Ariu, Narae Ryu, Se-Young Yun, Alexandre Proutiere","This paper proposes a theoretical analysis of recommendation systems in an online setting, where items are sequentially recommended to users over time. In each round, a user, randomly picked from a population of
m
m
users, arrives. The decision-maker observes the user and selects an item from a catalogue of
n
n
items. Importantly, an item cannot be recommended twice to the same user. The probabilities that a user likes each item are unknown, and the performance of the recommendation algorithm is captured through its regret, considering as a reference an Oracle algorithm aware of these probabilities. We investigate various structural assumptions on these probabilities: we derive for each of them regret lower bounds, and devise algorithms achieving these limits. Interestingly, our analysis reveals the relative weights of the different components of regret: the component due to the constraint of not presenting the same item twice to the same user, that due to learning the chances users like items, and finally that arising when learning the underlying structure."
neurips,https://proceedings.neurips.cc/paper/2020/file/f1de5100906f31712aaa5166689bfdf4-Paper.pdf,On Convergence and Generalization of Dropout Training,"Poorya Mianjy, Raman Arora","We study dropout in two-layer neural networks with rectified linear unit (ReLU) activations. Under mild overparametrization and assuming that the limiting kernel can separate the data distribution with a positive margin, we show that the dropout training with logistic loss achieves
ϵ
ϵ
-suboptimality in the test error in
O
(
1
/
ϵ
)
O
iterations."
neurips,https://proceedings.neurips.cc/paper/2020/file/f1ea154c843f7cf3677db7ce922a2d17-Paper.pdf,Second Order Optimality in Decentralized Non-Convex Optimization via Perturbed Gradient Tracking,"Isidoros Tziotis, Constantine Caramanis, Aryan Mokhtari",
neurips,https://proceedings.neurips.cc/paper/2020/file/f21e255f89e0f258accbe4e984eef486-Paper.pdf,Implicit Regularization in Deep Learning May Not Be Explainable by Norms,"Noam Razin, Nadav Cohen",
neurips,https://proceedings.neurips.cc/paper/2020/file/f231f2107df69eab0a3862d50018a9b2-Paper.pdf,POMO: Policy Optimization with Multiple Optima for Reinforcement Learning,"Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, Seungjai Min",
neurips,https://proceedings.neurips.cc/paper/2020/file/f23d125da1e29e34c552f448610ff25f-Paper.pdf,Uncertainty-aware Self-training for Few-shot Text Classification,"Subhabrata Mukherjee, Ahmed Awadallah",
neurips,https://proceedings.neurips.cc/paper/2020/file/f291e10ec3263bd7724556d62e70e25d-Paper.pdf,Learning to Learn with Feedback and Local Plasticity,"Jack Lindsey, Ashok Litwin-Kumar",
neurips,https://proceedings.neurips.cc/paper/2020/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf,Every View Counts: Cross-View Consistency in 3D Object Detection with Hybrid-Cylindrical-Spherical Voxelization,"Qi Chen, Lin Sun, Ernest Cheung, Alan L. Yuille",
neurips,https://proceedings.neurips.cc/paper/2020/file/f3173935ed8ac4bf073c1bcd63171f8a-Paper.pdf,Sharper Generalization Bounds for Pairwise Learning,"Yunwen Lei, Antoine Ledent, Marius Kloft","Pairwise learning refers to learning tasks with loss functions depending on a pair of training examples, which includes ranking and metric learning as specific examples. Recently, there has been an increasing amount of attention on the generalization analysis of pairwise learning to understand its practical behavior. However, the existing stability analysis provides suboptimal high-probability generalization bounds. In this paper, we provide a refined stability analysis by developing generalization bounds which can be
√
n
n
-times faster than the existing results, where
n
n
is the sample size. This implies excess risk bounds of the order
O
(
n
−
1
/
2
)
O
(up to a logarithmic factor) for both regularized risk minimization and stochastic gradient descent. We also introduce a new on-average stability measure to develop optimistic bounds in a low noise setting. We apply our results to ranking and metric learning, and clearly show the advantage of our generalization bounds over the existing analysis."
neurips,https://proceedings.neurips.cc/paper/2020/file/f340f1b1f65b6df5b5e3f94d95b11daf-Paper.pdf,A Measure-Theoretic Approach to Kernel Conditional Mean Embeddings,"Junhyung Park, Krikamol Muandet",
neurips,https://proceedings.neurips.cc/paper/2020/file/f3507289cfdc8c9ae93f4098111a13f9-Paper.pdf,Quantifying the Empirical Wasserstein Distance to a Set of Measures: Beating the Curse of Dimensionality,"Nian Si, Jose Blanchet, Soumyadip Ghosh, Mark Squillante",
neurips,https://proceedings.neurips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf,Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning,"Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, Michal Valko",
neurips,https://proceedings.neurips.cc/paper/2020/file/f3f27a324736617f20abbf2ffd806f6d-Paper.pdf,Towards Theoretically Understanding Why Sgd Generalizes Better Than Adam in Deep Learning,"Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Chu Hong Hoi, Weinan E",
neurips,https://proceedings.neurips.cc/paper/2020/file/f40723ed94042ea9ea36bfb5ad4157b2-Paper.pdf,RSKDD-Net: Random Sample-based Keypoint Detector and Descriptor,"Fan Lu, Guang Chen, Yinlong Liu, Zhongnan Qu, Alois Knoll",
neurips,https://proceedings.neurips.cc/paper/2020/file/f40ee694989b3e2161be989e7b9907fc-Paper.pdf,Efficient Clustering for Stretched Mixtures: Landscape and Optimality,"Kaizheng Wang, Yuling Yan, Mateo Diaz",
neurips,https://proceedings.neurips.cc/paper/2020/file/f4573fc71c731d5c362f0d7860945b88-Paper.pdf,A Group-Theoretic Framework for Data Augmentation,"Shuxiao Chen, Edgar Dobriban, Jane Lee",
neurips,https://proceedings.neurips.cc/paper/2020/file/f4661398cb1a3abd3ffe58600bf11322-Paper.pdf,The Statistical Cost of Robust Kernel Hyperparameter Turning,"Raphael Meyer, Christopher Musco",
neurips,https://proceedings.neurips.cc/paper/2020/file/f48c04ffab49ff0e5d1176244fdfb65c-Paper.pdf,How does Weight Correlation Affect Generalisation Ability of Deep Neural Networks?,"Gaojie Jin, Xinping Yi, Liang Zhang, Lijun Zhang, Sven Schewe, Xiaowei Huang",
neurips,https://proceedings.neurips.cc/paper/2020/file/f490c742cd8318b8ee6dca10af2a163f-Paper.pdf,ContraGAN: Contrastive Learning for Conditional Image Generation,"Minguk Kang, Jaesik Park",
neurips,https://proceedings.neurips.cc/paper/2020/file/f4b31bee138ff5f7b84ce1575a738f95-Paper.pdf,On the distance between two neural networks and the stability of learning,"Jeremy Bernstein, Arash Vahdat, Yisong Yue, Ming-Yu Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/f4e3ce3e7b581ff32e40968298ba013d-Paper.pdf,A Topological Filter for Learning with Label Noise,"Pengxiang Wu, Songzhu Zheng, Mayank Goswami, Dimitris Metaxas, Chao Chen",
neurips,https://proceedings.neurips.cc/paper/2020/file/f4f1f13c8289ac1b1ee0ff176b56fc60-Paper.pdf,Personalized Federated Learning with Moreau Envelopes,"Canh T. Dinh, Nguyen Tran, Josh Nguyen",
neurips,https://proceedings.neurips.cc/paper/2020/file/f50a6c02a3fc5a3a5d4d9391f05f3efc-Paper.pdf,Avoiding Side Effects in Complex Environments,"Alex Turner, Neale Ratzlaff, Prasad Tadepalli",
neurips,https://proceedings.neurips.cc/paper/2020/file/f51238cd02c93b89d8fbee5667d077fc-Paper.pdf,No-regret Learning in Price Competitions under Consumer Reference Effects,"Negin Golrezaei, Patrick Jaillet, Jason Cheuk Nam Liang",
neurips,https://proceedings.neurips.cc/paper/2020/file/f52a7b2610fb4d3f74b4106fb80b233d-Paper.pdf,Geometric Dataset Distances via Optimal Transport,"David Alvarez-Melis, Nicolo Fusi",
neurips,https://proceedings.neurips.cc/paper/2020/file/f52db9f7c0ae7017ee41f63c2a7353bc-Paper.pdf,Task-Agnostic Amortized Inference of Gaussian Process Hyperparameters,"Sulin Liu, Xingyuan Sun, Peter J. Ramadge, Ryan P. Adams",
neurips,https://proceedings.neurips.cc/paper/2020/file/f53eb4122d5e2ce81a12093f8f9ce922-Paper.pdf,"A novel variational form of the Schatten-
p
p
quasi-norm","Paris Giampouras, Rene Vidal, Athanasios Rontogiannis, Benjamin Haeffele","The Schatten-
p
p
quasi-norm with
p
∈
(
0
,
1
)
p
has recently gained considerable attention in various low-rank matrix estimation problems offering significant benefits over relevant convex heuristics such as the nuclear norm. However, due to the nonconvexity of the Schatten-
p
p
quasi-norm, minimization suffers from two major drawbacks: 1) the lack of theoretical guarantees and 2) the high computational cost which is demanded for the minimization task even for trivial tasks such as finding stationary points. In an attempt to reduce the high computational cost induced by Schatten-
p
p
quasi-norm minimization, variational forms, which are defined over smaller-size matrix factors whose product equals the original matrix, have been proposed. Here, we propose and analyze a novel {\it variational form of Schatten-
p
p
quasi-norm} which, for the first time in the literature, is defined for any continuous value of
p
∈
(
0
,
1
]
p
and decouples along the columns of the factorized matrices. The proposed form can be considered as the natural generalization of the well-known variational form of the nuclear norm to the nonconvex case i.e., for
p
∈
(
0
,
1
)
p
. Notably, low-rankness is now imposed via a group-sparsity promoting regularizer. The resulting formulation gives way to SVD-free algorithms thus offering lower computational complexity than the one that is induced by the original definition of the Schatten-
p
p
quasi-norm. A local optimality analysis is provided which shows~that we can arrive at a local minimum of the original Schatten-
p
p
quasi-norm problem by reaching a local minimum of the matrix factorization based surrogate problem. In addition, for the case of the squared Frobenious loss with linear operators obeying the restricted isometry property (RIP), a rank-one update scheme is proposed, which offers a way to escape poor local minima. Finally, the efficiency of our approach is empirically shown on a matrix completion problem."
neurips,https://proceedings.neurips.cc/paper/2020/file/f5496252609c43eb8a3d147ab9b9c006-Paper.pdf,Energy-based Out-of-distribution Detection,"Weitang Liu, Xiaoyun Wang, John Owens, Yixuan Li",
neurips,https://proceedings.neurips.cc/paper/2020/file/f56d8183992b6c54c92c16a8519a6e2b-Paper.pdf,On the Loss Landscape of Adversarial Training: Identifying Challenges and How to Overcome Them,"Chen Liu, Mathieu Salzmann, Tao Lin, Ryota Tomioka, Sabine Süsstrunk",
neurips,https://proceedings.neurips.cc/paper/2020/file/f56de5ef149cf0aedcc8f4797031e229-Paper.pdf,User-Dependent Neural Sequence Models for Continuous-Time Event Data,"Alex Boyd, Robert Bamler, Stephan Mandt, Padhraic Smyth",
neurips,https://proceedings.neurips.cc/paper/2020/file/f57bd0a58e953e5c43cd4a4e5af46138-Paper.pdf,Active Structure Learning of Causal DAGs via Directed Clique Trees,"Chandler Squires, Sara Magliacane, Kristjan Greenewald, Dmitriy Katz, Murat Kocaoglu, Karthikeyan Shanmugam",
neurips,https://proceedings.neurips.cc/paper/2020/file/f5a14d4963acf488e3a24780a84ac96c-Paper.pdf,Convergence and Stability of Graph Convolutional Networks on Large Random Graphs,"Nicolas Keriven, Alberto Bietti, Samuel Vaiter",
neurips,https://proceedings.neurips.cc/paper/2020/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf,BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization,"Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G. Wilson, Eytan Bakshy",
neurips,https://proceedings.neurips.cc/paper/2020/file/f5cfbc876972bd0d031c8abc37344c28-Paper.pdf,Reconsidering Generative Objectives For Counterfactual Reasoning,"Danni Lu, Chenyang Tao, Junya Chen, Fan Li, Feng Guo, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2020/file/f5e536083a438cec5b64a4954abc17f1-Paper.pdf,Robust Federated Learning: The Case of Affine Distribution Shifts,"Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, Ali Jadbabaie",
neurips,https://proceedings.neurips.cc/paper/2020/file/f5e62af885293cf4d511ceef31e61c80-Paper.pdf,Quantile Propagation for Wasserstein-Approximate Gaussian Processes,"Rui Zhang, Christian Walder, Edwin V. Bonilla, Marian-Andrei Rizoiu, Lexing Xie","Approximate inference techniques are the cornerstone of probabilistic methods based on Gaussian process priors. Despite this, most work approximately optimizes standard divergence measures such as the Kullback-Leibler (KL) divergence, which lack the basic desiderata for the task at hand, while chiefly offering merely technical convenience. We develop a new approximate inference method for Gaussian process models which overcomes the technical challenges arising from abandoning these convenient divergences. Our method---dubbed Quantile Propagation (QP)---is similar to expectation propagation (EP) but minimizes the
L
2
L
Wasserstein distance (WD) instead of the KL divergence. The WD exhibits all the required properties of a distance metric, while respecting the geometry of the underlying sample space. We show that QP matches quantile functions rather than moments as in EP and has the same mean update but a smaller variance update than EP, thereby alleviating EP's tendency to over-estimate posterior variances. Crucially, despite the significant complexity of dealing with the WD, QP has the same favorable locality property as EP, and thereby admits an efficient algorithm. Experiments on classification and Poisson regression show that QP outperforms both EP and variational Bayes."
neurips,https://proceedings.neurips.cc/paper/2020/file/f5f3b8d720f34ebebceb7765e447268b-Paper.pdf,Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning,"Tianren Zhang, Shangqi Guo, Tian Tan, Xiaolin Hu, Feng Chen",
neurips,https://proceedings.neurips.cc/paper/2020/file/f610a13de080fb8df6cf972fc01ad93f-Paper.pdf,High-contrast “gaudy” images improve the training of deep neural network models of visual cortex,"Benjamin Cowley, Jonathan W. Pillow",
neurips,https://proceedings.neurips.cc/paper/2020/file/f6185f0ef02dcaec414a3171cd01c697-Paper.pdf,Duality-Induced Regularizer for Tensor Factorization Based Knowledge Graph Completion,"Zhanqiu Zhang, Jianyu Cai, Jie Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/f629ed9325990b10543ab5946c1362fb-Paper.pdf,Distributed Training with Heterogeneous Data: Bridging Median- and Mean-Based Algorithms,"Xiangyi Chen, Tiancong Chen, Haoran Sun, Steven Z. Wu, Mingyi Hong",
neurips,https://proceedings.neurips.cc/paper/2020/file/f6876a9f998f6472cc26708e27444456-Paper.pdf,H-Mem: Harnessing synaptic plasticity with Hebbian Memory Networks,"Thomas Limbacher, Robert Legenstein",
neurips,https://proceedings.neurips.cc/paper/2020/file/f69e505b08403ad2298b9f262659929a-Paper.pdf,Neural Unsigned Distance Fields for Implicit Function Learning,"Julian Chibane, Mohamad Aymen mir, Gerard Pons-Moll",
neurips,https://proceedings.neurips.cc/paper/2020/file/f6a673f09493afcd8b129a0bcf1cd5bc-Paper.pdf,Curriculum By Smoothing,"Samarth Sinha, Animesh Garg, Hugo Larochelle",
neurips,https://proceedings.neurips.cc/paper/2020/file/f6a8dd1c954c8506aadc764cc32b895e-Paper.pdf,Fast Transformers with Clustered Attention,"Apoorv Vyas, Angelos Katharopoulos, François Fleuret",
neurips,https://proceedings.neurips.cc/paper/2020/file/f6c2a0c4b566bc99d596e58638e342b0-Paper.pdf,"The Convex Relaxation Barrier, Revisited: Tightened Single-Neuron Relaxations for Neural Network Verification","Christian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, KRUNAL KISHOR PATEL, Juan Pablo Vielma",
neurips,https://proceedings.neurips.cc/paper/2020/file/f7177163c833dff4b38fc8d2872f1ec6-Paper.pdf,Strongly Incremental Constituency Parsing with Graph Neural Networks,"Kaiyu Yang, Jia Deng",
neurips,https://proceedings.neurips.cc/paper/2020/file/f718499c1c8cef6730f9fd03c8125cab-Paper.pdf,AOT: Appearance Optimal Transport Based Identity Swapping for Forgery Detection,"Hao Zhu, Chaoyou Fu, Qianyi Wu, Wayne Wu, Chen Qian, Ran He",
neurips,https://proceedings.neurips.cc/paper/2020/file/f73b76ce8949fe29bf2a537cfa420e8f-Paper.pdf,Uncertainty-Aware Learning for Zero-Shot Semantic Segmentation,"Ping Hu, Stan Sclaroff, Kate Saenko",
neurips,https://proceedings.neurips.cc/paper/2020/file/f754186469a933256d7d64095e963594-Paper.pdf,Delta-STN: Efficient Bilevel Optimization for Neural Networks using Structured Response Jacobians,"Juhan Bae, Roger B. Grosse",
neurips,https://proceedings.neurips.cc/paper/2020/file/f75526659f31040afeb61cb7133e4e6d-Paper.pdf,First-Order Methods for Large-Scale Market Equilibrium Computation,"Yuan Gao, Christian Kroer",
neurips,https://proceedings.neurips.cc/paper/2020/file/f75b757d3459c3e93e98ddab7b903938-Paper.pdf,Minimax Optimal Nonparametric Estimation of Heterogeneous Treatment Effects,"Zijun Gao, Yanjun Han",
neurips,https://proceedings.neurips.cc/paper/2020/file/f76a89f0cb91bc419542ce9fa43902dc-Paper.pdf,Residual Force Control for Agile Human Behavior Imitation and Extended Motion Synthesis,"Ye Yuan, Kris Kitani",
neurips,https://proceedings.neurips.cc/paper/2020/file/f7a82ce7e16d9687e7cd9a9feb85d187-Paper.pdf,A General Method for Robust Learning from Batches,"Ayush Jain, Alon Orlitsky","Building on this framework, we derive the first robust agnostic: (1) polynomial-time distribution estimation algorithms for structured distributions, including piecewise-polynomial, monotone, log-concave, and gaussian-mixtures, and also significantly improve their sample complexity; (2) classification algorithms, and also establish their near-optimal sample complexity; (3) computationally-efficient algorithms for the fundamental problem of interval-based classification that underlies nearly all natural-1-dimensional classification problems."
neurips,https://proceedings.neurips.cc/paper/2020/file/f7ac67a9aa8d255282de7d11391e1b69-Paper.pdf,Not All Unlabeled Data are Equal: Learning to Weight Data in Semi-supervised Learning,"Zhongzheng Ren, Raymond Yeh, Alexander Schwing",
neurips,https://proceedings.neurips.cc/paper/2020/file/f7cade80b7cc92b991cf4d2806d6bd78-Paper.pdf,Hard Negative Mixing for Contrastive Learning,"Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, Diane Larlus",
neurips,https://proceedings.neurips.cc/paper/2020/file/f7efa4f864ae9b88d43527f4b14f750f-Paper.pdf,MOReL: Model-Based Offline Reinforcement Learning,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, Thorsten Joachims",
neurips,https://proceedings.neurips.cc/paper/2020/file/f81dee42585b3814de199b2e88757f5c-Paper.pdf,Weisfeiler and Leman go sparse: Towards scalable higher-order graph embeddings,"Christopher Morris, Gaurav Rattan, Petra Mutzel","Graph kernels based on the
1
1
-dimensional Weisfeiler-Leman algorithm and corresponding neural architectures recently emerged as powerful tools for (supervised) learning with graphs. However, due to the purely local nature of the algorithms, they might miss essential patterns in the given data and can only handle binary relations. The
k
k
-dimensional Weisfeiler-Leman algorithm addresses this by considering
k
k
-tuples, defined over the set of vertices, and defines a suitable notion of adjacency between these vertex tuples. Hence, it accounts for the higher-order interactions between vertices. However, it does not scale and may suffer from overfitting when used in a machine learning setting. Hence, it remains an important open problem to design WL-based graph learning methods that are simultaneously expressive, scalable, and non-overfitting. Here, we propose local variants and corresponding neural architectures, which consider a subset of the original neighborhood, making them more scalable, and less prone to overfitting. The expressive power of (one of) our algorithms is strictly higher than the original algorithm, in terms of ability to distinguish non-isomorphic graphs. Our experimental study confirms that the local algorithms, both kernel and neural architectures, lead to vastly reduced computation times, and prevent overfitting. The kernel version establishes a new state-of-the-art for graph classification on a wide range of benchmark datasets, while the neural version shows promising performance on large-scale molecular regression tasks."
neurips,https://proceedings.neurips.cc/paper/2020/file/f86890095c957e9b949d11d15f0d0cd5-Paper.pdf,Adversarial Crowdsourcing Through Robust Rank-One Matrix Completion,"Qianqian Ma, Alex Olshevsky","These results are then applied to the problem of classification from crowdsourced data under the assumption that while the majority of the workers are governed by the standard single-coin David-Skene model (i.e., they output the correct answer with a certain probability), some of the workers can deviate arbitrarily from this model. In particular, the
adversarial'' workers could even make decisions designed to make the algorithm output an incorrect answer. Extensive experimental results show our algorithm for this problem, based on rank-one matrix completion with perturbations, outperforms all other state-of-the-art methods in such an adversarial scenario."
neurips,https://proceedings.neurips.cc/paper/2020/file/f885a14eaf260d7d9f93c750e1174228-Paper.pdf,Learning Semantic-aware Normalization for Generative Adversarial Networks,"Heliang Zheng, Jianlong Fu, Yanhong Zeng, Jiebo Luo, Zheng-Jun Zha",
neurips,https://proceedings.neurips.cc/paper/2020/file/f8b7aa3a0d349d9562b424160ad18612-Paper.pdf,Differentiable Causal Discovery from Interventional Data,"Philippe Brouillard, Sébastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, Alexandre Drouin",
neurips,https://proceedings.neurips.cc/paper/2020/file/f8e59f4b2fe7c5705bf878bbd494ccdf-Paper.pdf,One-sample Guided Object Representation Disassembling,"Zunlei Feng, Yongming He, Xinchao Wang, Xin Gao, Jie Lei, Cheng Jin, Mingli Song",
neurips,https://proceedings.neurips.cc/paper/2020/file/f9028faec74be6ec9b852b0a542e2f39-Paper.pdf,Extrapolation Towards Imaginary 0-Nearest Neighbour and Its Improved Convergence Rate,"Akifumi Okuno, Hidetoshi Shimodaira","k
k
-nearest neighbour (
k
k
-NN) is one of the simplest and most widely-used methods for supervised classification, that predicts a query's label by taking weighted ratio of observed labels of
k
k
objects nearest to the query. The weights and the parameter
k
∈
N
k
regulate its bias-variance trade-off, and the trade-off implicitly affects the convergence rate of the excess risk for the
k
k
-NN classifier; several existing studies considered selecting optimal
k
k
and weights to obtain faster convergence rate. Whereas
k
k
-NN with non-negative weights has been developed widely, it was also proved that negative weights are essential for eradicating the bias terms and attaining optimal convergence rate. In this paper, we propose a novel multiscale
k
k
-NN (MS-
k
k
-NN), that extrapolates unweighted
k
k
-NN estimators from several
k
≥
1
k
values to
k
=
0
k
, thus giving an imaginary 0-NN estimator. Our method implicitly computes optimal real-valued weights that are adaptive to the query and its neighbour points. We theoretically prove that the MS-
k
k
-NN attains the improved rate, which coincides with the existing optimal rate under some conditions."
neurips,https://proceedings.neurips.cc/paper/2020/file/f99499791ad90c9c0ba9852622d0d15f-Paper.pdf,Robust Persistence Diagrams using Reproducing Kernels,"Siddharth Vishwanath, Kenji Fukumizu, Satoshi Kuriki, Bharath K. Sriperumbudur",
neurips,https://proceedings.neurips.cc/paper/2020/file/f9afa97535cf7c8789a1c50a2cd83787-Paper.pdf,Contextual Games: Multi-Agent Learning with Side Information,"Pier Giuseppe Sessa, Ilija Bogunovic, Andreas Krause, Maryam Kamgarpour",
neurips,https://proceedings.neurips.cc/paper/2020/file/f9b9f0fef2274a6b7009b5d52f44a3b6-Paper.pdf,Goal-directed Generation of Discrete Structures with Conditional Generative Models,"Amina Mollaysa, Brooks Paige, Alexandros Kalousis",
neurips,https://proceedings.neurips.cc/paper/2020/file/f9d3a954de63277730a1c66d8b38dee3-Paper.pdf,Beyond Lazy Training for Over-parameterized Tensor Decomposition,"Xiang Wang, Chenwei Wu, Jason D. Lee, Tengyu Ma, Rong Ge","Over-parametrization is an important technique in training neural networks. In both theory and practice, training a larger network allows the optimization algorithm to avoid bad local optimal solutions. In this paper we study a closely related tensor decomposition problem: given an
l
l
-th order tensor in
(
R
d
)
⊗
l
(
of rank
r
r
(where
r
≪
d
r
), can variants of gradient descent find a rank
m
m
decomposition where
m
>
r
m
? We show that in a lazy training regime (similar to the NTK regime for neural networks) one needs at least
m
=
Ω
(
d
l
−
1
)
m
, while a variant of gradient descent can find an approximate tensor when
m
=
O
∗
(
r
2.5
l
log
d
)
m
. Our results show that gradient descent on over-parametrized objective could go beyond the lazy training regime and utilize certain low-rank structure in the data."
neurips,https://proceedings.neurips.cc/paper/2020/file/f9fd2624beefbc7808e4e405d73f57ab-Paper.pdf,Denoised Smoothing: A Provable Defense for Pretrained Classifiers,"Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, J. Zico Kolter","We present a method for provably defending any pretrained image classifier against
ℓ
p
ℓ
adversarial attacks. This method, for instance, allows public vision API providers and users to seamlessly convert pretrained non-robust classification services into provably robust ones. By prepending a custom-trained denoiser to any off-the-shelf image classifier and using randomized smoothing, we effectively create a new classifier that is guaranteed to be
ℓ
p
ℓ
-robust to adversarial examples, without modifying the pretrained classifier. Our approach applies to both the white-box and the black-box settings of the pretrained classifier. We refer to this defense as denoised smoothing, and we demonstrate its effectiveness through extensive experimentation on ImageNet and CIFAR-10. Finally, we use our approach to provably defend the Azure, Google, AWS, and ClarifAI image classification APIs. Our code replicating all the experiments in the paper can be found at: https://github.com/microsoft/denoised-smoothing."
neurips,https://proceedings.neurips.cc/paper/2020/file/fa2246fa0fdf0d3e270c86767b77ba1b-Paper.pdf,Minibatch Stochastic Approximate Proximal Point Methods,"Hilal Asi, Karan Chadha, Gary Cheng, John C. Duchi",
neurips,https://proceedings.neurips.cc/paper/2020/file/fa2431bf9d65058fe34e9713e32d60e6-Paper.pdf,Attribute Prototype Network for Zero-Shot Learning,"Wenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, Zeynep Akata",
neurips,https://proceedings.neurips.cc/paper/2020/file/fa28c6cdf8dd6f41a657c3d7caa5c709-Paper.pdf,CrossTransformers: spatially-aware few-shot transfer,"Carl Doersch, Ankush Gupta, Andrew Zisserman",
neurips,https://proceedings.neurips.cc/paper/2020/file/fa3060edb66e6ff4507886f9912e1ab9-Paper.pdf,Learning Latent Space Energy-Based Prior Model,"Bo Pang, Tian Han, Erik Nijkamp, Song-Chun Zhu, Ying Nian Wu",
neurips,https://proceedings.neurips.cc/paper/2020/file/fa78a16157fed00d7a80515818432169-Paper.pdf,SEVIR : A Storm Event Imagery Dataset for Deep Learning Applications in Radar and Satellite Meteorology,"Mark Veillette, Siddharth Samsi, Chris Mattioli",
neurips,https://proceedings.neurips.cc/paper/2020/file/fae0b27c451c728867a567e8c1bb4e53-Paper.pdf,Lightweight Generative Adversarial Networks for Text-Guided Image Manipulation,"Bowen Li, Xiaojuan Qi, Philip Torr, Thomas Lukasiewicz",
neurips,https://proceedings.neurips.cc/paper/2020/file/faff959d885ec0ecf70741a846c34d1d-Paper.pdf,High-Dimensional Contextual Policy Search with Unknown Context Rewards using Bayesian Optimization,"Qing Feng , Ben Letham, Hongzi Mao, Eytan Bakshy",
neurips,https://proceedings.neurips.cc/paper/2020/file/fb2697869f56484404c8ceee2985b01d-Paper.pdf,Model Fusion via Optimal Transport,"Sidak Pal Singh, Martin Jaggi","We show that this can successfully yield ""one-shot"" knowledge transfer (i.e, without requiring any retraining) between neural networks trained on heterogeneous non-i.i.d. data. In both i.i.d. and non-i.i.d. settings, we illustrate that our approach significantly outperforms vanilla averaging, as well as how it can serve as an efficient replacement for the ensemble with moderate fine-tuning, for standard convolutional networks (like VGG11), residual networks (like ResNet18), and multi-layer perceptrons on CIFAR10, CIFAR100, and MNIST. Finally, our approach also provides a principled way to combine the parameters of neural networks with different widths, and we explore its application for model compression."
neurips,https://proceedings.neurips.cc/paper/2020/file/fb2e203234df6dee15934e448ee88971-Paper.pdf,On the Stability and Convergence of Robust Adversarial Reinforcement Learning: A Case Study on Linear Quadratic Systems,"Kaiqing Zhang, Bin Hu, Tamer Basar",
neurips,https://proceedings.neurips.cc/paper/2020/file/fb2fcd534b0ff3bbed73cc51df620323-Paper.pdf,Learning Individually Inferred Communication for Multi-Agent Cooperation,"gang Ding, Tiejun Huang, Zongqing Lu",
neurips,https://proceedings.neurips.cc/paper/2020/file/fb4ab556bc42d6f0ee0f9e24ec4d1af0-Paper.pdf,Set2Graph: Learning Graphs From Sets,"Hadar Serviansky, Nimrod Segol, Jonathan Shlomi, Kyle Cranmer, Eilam Gross, Haggai Maron, Yaron Lipman","A natural approach for building Set2Graph models is to characterize all linear equivariant set-to-hypergraph layers and stack them with non-linear activations. This posses two challenges: (i) the expressive power of these networks is not well understood; and (ii) these models would suffer from high, often intractable computational and memory complexity, as their dimension grows exponentially."
neurips,https://proceedings.neurips.cc/paper/2020/file/fb4c835feb0a65cc39739320d7a51c02-Paper.pdf,Graph Random Neural Networks for Semi-Supervised Learning on Graphs,"Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang Yang, Evgeny Kharlamov, Jie Tang",
neurips,https://proceedings.neurips.cc/paper/2020/file/fb5d9e209ebda9ab6556a31639190622-Paper.pdf,Gradient Boosted Normalizing Flows,"Robert Giaquinto, Arindam Banerjee",
neurips,https://proceedings.neurips.cc/paper/2020/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf,Open Graph Benchmark: Datasets for Machine Learning on Graphs,"Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2020/file/fb647ca6672b0930e9d00dc384d8b16f-Paper.pdf,Towards Understanding Hierarchical Learning: Benefits of Neural Representations,"Minshuo Chen, Yu Bai, Jason D. Lee, Tuo Zhao, Huan Wang, Caiming Xiong, Richard Socher","Deep neural networks can empirically perform efficient hierarchical learning, in which the layers learn useful representations of the data. However, how they make use of the intermediate representations are not explained by recent theories that relate them to
shallow learners'' such as kernels. In this work, we demonstrate that intermediate \emph{neural representations} add more flexibility to neural networks and can be advantageous over raw inputs. We consider a fixed, randomly initialized neural network as a representation function fed into another trainable network. When the trainable network is the quadratic Taylor model of a wide two-layer network, we show that neural representation can achieve improved sample complexities compared with the raw input: For learning a low-rank degree-
p
p
polynomial (
p
≥
4
p
) in
d
d
dimension, neural representation requires only
˜
O
(
d
\ceil
p
/
2
)
O
samples, while the best-known sample complexity upper bound for the raw input is
˜
O
(
d
p
−
1
)
O
. We contrast our result with a lower bound showing that neural representations do not improve over the raw input (in the infinite width limit), when the trainable network is instead a neural tangent kernel. Our results characterize when neural representations are beneficial, and may provide a new perspective on why depth is important in deep learning."
neurips,https://proceedings.neurips.cc/paper/2020/file/fba9d88164f3e2d9109ee770223212a0-Paper.pdf,Texture Interpolation for Probing Visual Perception,"Jonathan Vacher, Aida Davila, Adam Kohn, Ruben Coen-Cagli",
neurips,https://proceedings.neurips.cc/paper/2020/file/fc146be0b230d7e0a92e66a6114b840d-Paper.pdf,Hierarchical Neural Architecture Search for Deep Stereo Matching,"Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, Yuchao Dai, Xiaojun Chang, Hongdong Li, Tom Drummond, Zongyuan Ge",
neurips,https://proceedings.neurips.cc/paper/2020/file/fc152e73692bc3c934d248f639d9e963-Paper.pdf,MuSCLE: Multi Sweep Compression of LiDAR using Deep Entropy Models,"Sourav Biswas, Jerry Liu, Kelvin Wong, Shenlong Wang, Raquel Urtasun",
neurips,https://proceedings.neurips.cc/paper/2020/file/fc2022c89b61c76bbef978f1370660bf-Paper.pdf,Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy,"Edward Moroshko, Blake E. Woodworth, Suriya Gunasekar, Jason D. Lee, Nati Srebro, Daniel Soudry",
neurips,https://proceedings.neurips.cc/paper/2020/file/fc2dc7d20994a777cfd5e6de734fe254-Paper.pdf,Focus of Attention Improves Information Transfer in Visual Features,"Matteo Tiezzi, Stefano Melacci, Alessandro Betti, Marco Maggini, Marco Gori",
neurips,https://proceedings.neurips.cc/paper/2020/file/fc4ddc15f9f4b4b06ef7844d6bb53abf-Paper.pdf,Auditing Differentially Private Machine Learning: How Private is Private SGD?,"Matthew Jagielski, Jonathan Ullman, Alina Oprea",
neurips,https://proceedings.neurips.cc/paper/2020/file/fc5b3186f1cf0daece964f78259b7ba0-Paper.pdf,A Dynamical Central Limit Theorem for Shallow Neural Networks,"Zhengdao Chen, Grant Rotskoff, Joan Bruna, Eric Vanden-Eijnden",
neurips,https://proceedings.neurips.cc/paper/2020/file/fc84ad56f9f547eb89c72b9bac209312-Paper.pdf,Measuring Systematic Generalization in Neural Proof Generation with Transformers,"Nicolas Gontier, Koustuv Sinha, Siva Reddy, Chris Pal",
neurips,https://proceedings.neurips.cc/paper/2020/file/fcbc95ccdd551da181207c0c1400c655-Paper.pdf,Big Self-Supervised Models are Strong Semi-Supervised Learners,"Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey E. Hinton","One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels (
≤
≤
13 labeled images per class) using ResNet-50, a 10X improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels."
neurips,https://proceedings.neurips.cc/paper/2020/file/fcde14913c766cf307c75059e0e89af5-Paper.pdf,Learning from Label Proportions: A Mutual Contamination Framework,"Clayton Scott, Jianxin Zhang",
neurips,https://proceedings.neurips.cc/paper/2020/file/fcf55a303b71b84d326fb1d06e332a26-Paper.pdf,Fast Matrix Square Roots with Applications to Gaussian Processes and Bayesian Optimization,"Geoff Pleiss, Martin Jankowiak, David Eriksson, Anil Damle, Jacob Gardner",
neurips,https://proceedings.neurips.cc/paper/2020/file/fd348179ec677c5560d4cd9c3ffb6cd9-Paper.pdf,Self-Adaptively Learning to Demoiré from Focused and Defocused Image Pairs,"Lin Liu, Shanxin Yuan, Jianzhuang Liu, Liping Bao, Gregory Slabaugh, Qi Tian",
neurips,https://proceedings.neurips.cc/paper/2020/file/fd4f21f2556dad0ea8b7a5c04eabebda-Paper.pdf,Confounding-Robust Policy Evaluation in Infinite-Horizon Reinforcement Learning,"Nathan Kallus, Angela Zhou",
neurips,https://proceedings.neurips.cc/paper/2020/file/fd512441a1a791770a6fa573d688bff5-Paper.pdf,Model Class Reliance for Random Forests,"Gavin Smith, Roberto Mansilla, James Goulding",
neurips,https://proceedings.neurips.cc/paper/2020/file/fd5ac6ce504b74460b93610f39e481f7-Paper.pdf,Follow the Perturbed Leader: Optimism and Fast Parallel Algorithms for Smooth Minimax Games,"Arun Suggala, Praneeth Netrapalli","We consider the problem of online learning and its application to solving minimax games. For the online learning problem, Follow the Perturbed Leader (FTPL) is a widely studied algorithm which enjoys the optimal
O
(
T
1
/
2
)
O
\emph{worst case} regret guarantee for both convex and nonconvex losses. In this work, we show that when the sequence of loss functions is \emph{predictable}, a simple modification of FTPL which incorporates optimism can achieve better regret guarantees, while retaining the optimal worst-case regret guarantee for unpredictable sequences. A key challenge in obtaining these tighter regret bounds is the stochasticity and optimism in the algorithm, which requires different analysis techniques than those commonly used in the analysis of FTPL. The key ingredient we utilize in our analysis is the dual view of perturbation as regularization. While our algorithm has several applications, we consider the specific application of minimax games. For solving smooth convex-concave games, our algorithm only requires access to a linear optimization oracle. For Lipschitz and smooth nonconvex-nonconcave games, our algorithm requires access to an optimization oracle which computes the perturbed best response. In both these settings, our algorithm solves the game up to an accuracy of
O
(
T
−
1
/
2
)
O
using
T
T
calls to the optimization oracle. An important feature of our algorithm is that it is highly parallelizable and requires only
O
(
T
1
/
2
)
O
iterations, with each iteration making
O
(
T
1
/
2
)
O
parallel calls to the optimization oracle."
neurips,https://proceedings.neurips.cc/paper/2020/file/fd5c905bcd8c3348ad1b35d7231ee2b1-Paper.pdf,"Agnostic
Q
Q
-learning with Function Approximation in Deterministic Systems: Near-Optimal Bounds on Approximation Error and Sample Complexity","Simon S. Du, Jason D. Lee, Gaurav Mahajan, Ruosong Wang","The current paper studies the problem of agnostic
Q
Q
-learning with function approximation in deterministic systems where the optimal
Q
Q
-function is approximable by a function in the class
F
F
with approximation error
δ
≥
0
δ
. We propose a novel recursion-based algorithm and show that if
δ
=
O
(
ρ
/
√
dim
E
)
δ
, then one can find the optimal policy using
O
(
dim
E
)
O
trajectories, where
ρ
ρ
is the gap between the optimal
Q
Q
-value of the best actions and that of the second-best actions and
dim
E
dim
is the Eluder dimension of
F
F
. Our result has two implications:
\begin{enumerate}
\item In conjunction with the lower bound in [Du et al., 2020], our upper bound suggests that the condition $\delta = \widetilde{\Theta}\left(\rho/\sqrt{\dim_E}\right)$ is necessary and sufficient for algorithms with polynomial sample complexity.
\item In conjunction with the obvious lower bound in the tabular case, our upper bound suggests that the sample complexity $\widetilde{\Theta}\left(\dim_E\right)$ is tight in the agnostic setting.
\end{enumerate}
\begin{enumerate}\item In conjunction with the lower bound in [Du et al., 2020], our upper bound suggests that the condition $\delta = \widetilde{\Theta}\left(\rho/\sqrt{\dim_E}\right)$ is necessary and sufficient for algorithms with polynomial sample complexity.\item In conjunction with the obvious lower bound in the tabular case, our upper bound suggests that the sample complexity $\widetilde{\Theta}\left(\dim_E\right)$ is tight in the agnostic setting.\end{enumerate}
Therefore, we help address the open problem on agnostic
Q
Q
-learning proposed in [Wen and Van Roy, 2013]. We further extend our algorithm to the stochastic reward setting and obtain similar results."
neurips,https://proceedings.neurips.cc/paper/2020/file/fd69dbe29f156a7ef876a40a94f65599-Paper.pdf,Learning to Adapt to Evolving Domains,"Hong Liu, Mingsheng Long, Jianmin Wang, Yu Wang",
neurips,https://proceedings.neurips.cc/paper/2020/file/fd9dd764a6f1d73f4340d570804eacc4-Paper.pdf,Synthesizing Tasks for Block-based Programming,"Umair Ahmed, Maria Christakis, Aleksandr Efremov, Nigel Fernandez, Ahana Ghosh, Abhik Roychoudhury, Adish Singla","Block-based visual programming environments play a critical role in introducing computing concepts to K-12 students. One of the key pedagogical challenges in these environments is in designing new practice tasks for a student that match a desired level of difficulty and exercise specific programming concepts. In this paper, we formalize the problem of synthesizing visual programming tasks. In particular, given a reference visual task
\task
i
n
\task
and its solution code
\code
i
n
\code
, we propose a novel methodology to automatically generate a set
{
(
\task
o
u
t
,
\code
o
u
t
)
}
{
of new tasks along with solution codes such that tasks
\task
i
n
\task
and
\task
o
u
t
\task
are conceptually similar but visually dissimilar. Our methodology is based on the realization that the mapping from the space of visual tasks to their solution codes is highly discontinuous; hence, directly mutating reference task
\task
i
n
\task
to generate new tasks is futile. Our task synthesis algorithm operates by first mutating code
\code
i
n
\code
to obtain a set of codes
{
\code
o
u
t
}
{
. Then, the algorithm performs symbolic execution over a code
\code
o
u
t
\code
to obtain a visual task
\task
o
u
t
\task
; this step uses the Monte Carlo Tree Search (MCTS) procedure to guide the search in the symbolic tree. We demonstrate the effectiveness of our algorithm through an extensive empirical evaluation and user study on reference tasks taken from the Hour of Code: Classic Maze challenge by Code.org and the Intro to Programming with Karel course by CodeHS.com."
neurips,https://proceedings.neurips.cc/paper/2020/file/fdb2c3bab9d0701c4a050a4d8d782c7f-Paper.pdf,Scalable Belief Propagation via Relaxed Scheduling,"Vitalii Aksenov, Dan Alistarh, Janne H. Korhonen",
neurips,https://proceedings.neurips.cc/paper/2020/file/fdbe012e2e11314b96402b32c0df26b7-Paper.pdf,Firefly Neural Architecture Descent: a General Approach for Growing Neural Networks,"Lemeng Wu, Bo Liu, Peter Stone, Qiang Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/fdc42b6b0ee16a2f866281508ef56730-Paper.pdf,Risk-Sensitive Reinforcement Learning: Near-Optimal Risk-Sample Tradeoff in Regret,"Yingjie Fei, Zhuoran Yang, Yudong Chen, Zhaoran Wang, Qiaomin Xie","We study risk-sensitive reinforcement learning in episodic Markov decision processes with unknown transition kernels, where the goal is to optimize the total reward under the risk measure of exponential utility. We propose two provably efficient model-free algorithms, Risk-Sensitive Value Iteration (RSVI) and Risk-Sensitive Q-learning (RSQ). These algorithms implement a form of risk-sensitive optimism in the face of uncertainty, which adapts to both risk-seeking and risk-averse modes of exploration. We prove that RSVI attains an \ensuremath{\tilde{O}\big(\lambda(|\beta| H^2) \cdot \sqrt{H^{3} S^{2}AT} \big)}
r
e
g
r
e
t
,
w
h
i
l
e
R
S
Q
a
t
t
a
i
n
s
a
n
r
\ensuremath{\tilde{O}\big(\lambda(|\beta| H^2) \cdot \sqrt{H^{4} SAT} \big)}
r
e
g
r
e
t
,
w
h
e
r
e
r
\lambda(u) = (e^{3u}-1)/u
f
o
r
f
u>0
.
I
n
t
h
e
a
b
o
v
e
,
.
\beta
i
s
t
h
e
r
i
s
k
p
a
r
a
m
e
t
e
r
o
f
t
h
e
e
x
p
o
n
e
n
t
i
a
l
u
t
i
l
i
t
y
f
u
n
c
t
i
o
n
,
i
S
t
h
e
n
u
m
b
e
r
o
f
s
t
a
t
e
s
,
t
A
t
h
e
n
u
m
b
e
r
o
f
a
c
t
i
o
n
s
,
t
T
t
h
e
t
o
t
a
l
n
u
m
b
e
r
o
f
t
i
m
e
s
t
e
p
s
,
a
n
d
t
H
t
h
e
e
p
i
s
o
d
e
l
e
n
g
t
h
.
O
n
t
h
e
f
l
i
p
s
i
d
e
,
w
e
e
s
t
a
b
l
i
s
h
a
r
e
g
r
e
t
l
o
w
e
r
b
o
u
n
d
s
h
o
w
i
n
g
t
h
a
t
t
h
e
e
x
p
o
n
e
n
t
i
a
l
d
e
p
e
n
d
e
n
c
e
o
n
t
|\beta|
a
n
d
a
H
i
s
u
n
a
v
o
i
d
a
b
l
e
f
o
r
a
n
y
a
l
g
o
r
i
t
h
m
w
i
t
h
a
n
i
\tilde{O}(\sqrt{T})
r
e
g
r
e
t
(
e
v
e
n
w
h
e
n
t
h
e
r
i
s
k
o
b
j
e
c
t
i
v
e
i
s
o
n
t
h
e
s
a
m
e
s
c
a
l
e
a
s
t
h
e
o
r
i
g
i
n
a
l
r
e
w
a
r
d
)
,
t
h
u
s
c
e
r
t
i
f
y
i
n
g
t
h
e
n
e
a
r
−
o
p
t
i
m
a
l
i
t
y
o
f
t
h
e
p
r
o
p
o
s
e
d
a
l
g
o
r
i
t
h
m
s
.
O
u
r
r
e
s
u
l
t
s
d
e
m
o
n
s
t
r
a
t
e
t
h
a
t
i
n
c
o
r
p
o
r
a
t
i
n
g
r
i
s
k
a
w
a
r
e
n
e
s
s
i
n
t
o
r
e
i
n
f
o
r
c
e
m
e
n
t
l
e
a
r
n
i
n
g
n
e
c
e
s
s
i
t
a
t
e
s
a
n
e
x
p
o
n
e
n
t
i
a
l
c
o
s
t
i
n
r
|\beta|
a
n
d
a
H$, which quantifies the fundamental tradeoff between risk sensitivity (related to aleatoric uncertainty) and sample efficiency (related to epistemic uncertainty). To the best of our knowledge, this is the first regret analysis of risk-sensitive reinforcement learning with the exponential utility."
neurips,https://proceedings.neurips.cc/paper/2020/file/fdd5b16fc8134339089ef25b3cf0e588-Paper.pdf,Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes,"Salman Habib, Allison Beemer, Joerg Kliewer",
neurips,https://proceedings.neurips.cc/paper/2020/file/fdf1bc5669e8ff5ba45d02fded729feb-Paper.pdf,Faster DBSCAN via subsampled similarity queries,"Heinrich Jiang, Jennifer Jang, Jakub Lacki","DBSCAN is a popular density-based clustering algorithm. It computes the
ϵ
ϵ
-neighborhood graph of a dataset and uses the connected components of the high-degree nodes to decide the clusters. However, the full neighborhood graph may be too costly to compute with a worst-case complexity of
O
(
n
2
)
O
. In this paper, we propose a simple variant called SNG-DBSCAN, which clusters based on a subsampled
ϵ
ϵ
-neighborhood graph, only requires access to similarity queries for pairs of points and in particular avoids any complex data structures which need the embeddings of the data points themselves. The runtime of the procedure is
O
(
s
n
2
)
O
, where
s
s
is the sampling rate. We show under some natural theoretical assumptions that
s
≈
log
n
/
n
s
is sufficient for statistical cluster recovery guarantees leading to an
O
(
n
log
n
)
O
complexity. We provide an extensive experimental analysis showing that on large datasets, one can subsample as little as
0.1
%
0.1
of the neighborhood graph, leading to as much as over 200x speedup and 250x reduction in RAM consumption compared to scikit-learn's implementation of DBSCAN, while still maintaining competitive clustering performance."
neurips,https://proceedings.neurips.cc/paper/2020/file/fdf2aade29d18910051a6c76ae661860-Paper.pdf,De-Anonymizing Text by Fingerprinting Language Generation,"Zhen Sun, Roei Schuster, Vitaly Shmatikov",
neurips,https://proceedings.neurips.cc/paper/2020/file/fdff71fcab656abfbefaabecab1a7f6d-Paper.pdf,Multiparameter Persistence Image for Topological Machine Learning,"Mathieu Carrière, Andrew Blumberg","We introduce a new descriptor for multiparameter persistence, which we call the Multiparameter Persistence Image, that is suitable for machine learning and statistical frameworks, is robust to perturbations in the data, has finer resolution than existing descriptors based on slicing, and can be efficiently computed on data sets of realistic size. Moreover, we demonstrate its efficacy by comparing its performance to other multiparameter descriptors on several classification tasks."
neurips,https://proceedings.neurips.cc/paper/2020/file/fe131d7f5a6b38b23cc967316c13dae2-Paper.pdf,PLANS: Neuro-Symbolic Program Learning from Videos,Raphaël Dang-Nhu,
neurips,https://proceedings.neurips.cc/paper/2020/file/fe2b421b8b5f0e7c355ace66a9fe0206-Paper.pdf,Matrix Inference and Estimation in Multi-Layer Models,"Parthe Pandit, Mojtaba Sahraee Ardakan, Sundeep Rangan, Philip Schniter, Alyson K. Fletcher","We consider the problem of estimating the input and hidden variables of a stochastic multi-layer neural network from an observation of the output. The hidden variables in each layer are represented as matrices with statistical interactions along both rows as well as columns. This problem applies to matrix imputation, signal recovery via deep generative prior models, multi-task and mixed regression, and learning certain classes of two-layer neural networks. We extend a recently-developed algorithm -- Multi-Layer Vector Approximate Message Passing (ML-VAMP), for this matrix-valued inference problem. It is shown that the performance of the proposed Multi-Layer Matrix VAMP (ML-Mat-VAMP) algorithm can be exactly predicted in a certain random large-system limit, where the dimensions
N
×
d
N
of the unknown quantities grow as
N
→
∞
N
with
d
d
fixed. In the two-layer neural-network learning problem, this scaling corresponds to the case where the number of input features, as well as training samples, grow to infinity but the number of hidden nodes stays fixed. The analysis enables a precise prediction of the parameter and test error of the learning."
neurips,https://proceedings.neurips.cc/paper/2020/file/fe40fb944ee700392ed51bfe84dd4e3d-Paper.pdf,MeshSDF: Differentiable Iso-Surface Extraction,"Edoardo Remelli, Artem Lukoianov, Stephan Richter, Benoit Guillard, Timur Bagautdinov, Pierre Baque, Pascal Fua","Unfortunately, these methods are often not suitable for applications that require an explicit mesh-based surface representation because converting an implicit field to such a representation relies on the Marching Cubes algorithm, which cannot be differentiated with respect to the underlying implicit field."
neurips,https://proceedings.neurips.cc/paper/2020/file/fe663a72b27bdc613873fbbb512f6f67-Paper.pdf,Variational Interaction Information Maximization for Cross-domain Disentanglement,"HyeongJoo Hwang, Geon-Hyeong Kim, Seunghoon Hong, Kee-Eung Kim",
neurips,https://proceedings.neurips.cc/paper/2020/file/fe73f687e5bc5280214e0486b273a5f9-Paper.pdf,Provably Efficient Exploration for Reinforcement Learning Using Unsupervised Learning,"Fei Feng, Ruosong Wang, Wotao Yin, Simon S. Du, Lin Yang",
neurips,https://proceedings.neurips.cc/paper/2020/file/fe74074593f21197b7b7be3c08678616-Paper.pdf,Faithful Embeddings for Knowledge Base Queries,"Haitian Sun, Andrew Arnold, Tania Bedrax Weiss, Fernando Pereira, William W. Cohen",
neurips,https://proceedings.neurips.cc/paper/2020/file/fe7ecc4de28b2c83c016b5c6c2acd826-Paper.pdf,Wasserstein Distances for Stereo Disparity Estimation,"Divyansh Garg, Yan Wang, Bharath Hariharan, Mark Campbell, Kilian Q. Weinberger, Wei-Lun Chao",
neurips,https://proceedings.neurips.cc/paper/2020/file/fe87435d12ef7642af67d9bc82a8b3cd-Paper.pdf,Multi-agent Trajectory Prediction with Fuzzy Query Attention,"Nitin Kamra, Hao Zhu, Dweep Kumarbhai Trivedi, Ming Zhang, Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2020/file/fea16e782bc1b1240e4b3c797012e289-Paper.pdf,Multilabel Classification by Hierarchical Partitioning and Data-dependent Grouping,"Shashanka Ubaru, Sanjeeb Dash, Arya Mazumdar, Oktay Gunluk",
neurips,https://proceedings.neurips.cc/paper/2020/file/fec3392b0dc073244d38eba1feb8e6b7-Paper.pdf,An Analysis of SVD for Deep Rotation Estimation,"Jake Levinson, Carlos Esteves, Kefan Chen, Noah Snavely, Angjoo Kanazawa, Afshin Rostamizadeh, Ameesh Makadia",
neurips,https://proceedings.neurips.cc/paper/2020/file/fec87a37cdeec1c6ecf8181c0aa2d3bf-Paper.pdf,Can the Brain Do Backpropagation? --- Exact Implementation of Backpropagation in Predictive Coding Networks,"Yuhang Song, Thomas Lukasiewicz, Zhenghua Xu, Rafal Bogacz",
neurips,https://proceedings.neurips.cc/paper/2020/file/fedc604da8b0f9af74b6cfc0fab2163c-Paper.pdf,Manifold GPLVMs for discovering non-Euclidean latent structure in neural data,"Kristopher Jensen, Ta-Chu Kao, Marco Tripodi, Guillaume Hennequin",
neurips,https://proceedings.neurips.cc/paper/2020/file/fef6f971605336724b5e6c0c12dc2534-Paper.pdf,Distributed Distillation for On-Device Learning,"Ilai Bistritz, Ariana Mann, Nicholas Bambos",
neurips,https://proceedings.neurips.cc/paper/2020/file/ff0abbcc0227c9124a804b084d161a2d-Paper.pdf,COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning,"Simon Ging, Mohammadreza Zolfaghari, Hamed Pirsiavash, Thomas Brox",
neurips,https://proceedings.neurips.cc/paper/2020/file/ff1418e8cc993fe8abcfe3ce2003e5c5-Paper.pdf,Passport-aware Normalization for Deep Model Protection,"Jie Zhang, Dongdong Chen, Jing Liao, Weiming Zhang, Gang Hua, Nenghai Yu",
neurips,https://proceedings.neurips.cc/paper/2020/file/ff42b03a06a1bed4e936f0e04958e168-Paper.pdf,Sampling-Decomposable Generative Adversarial Recommender,"Binbin Jin, Defu Lian, Zheng Liu, Qi Liu, Jianhui Ma, Xing Xie, Enhong Chen",
neurips,https://proceedings.neurips.cc/paper/2020/file/ff4dfdf5904e920ce52b48c1cef97829-Paper.pdf,Limits to Depth Efficiencies of Self-Attention,"Yoav Levine, Noam Wies, Or Sharir, Hofit Bata, Amnon Shashua",
