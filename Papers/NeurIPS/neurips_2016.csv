conference,url,title,authors,abstract
neurips,https://proceedings.neurips.cc/paper/2016/file/018b59ce1fd616d874afad0f44ba338d-Paper.pdf,Eliciting Categorical Data for Optimal Aggregation,"Chien-Ju Ho, Rafael Frongillo, Yiling Chen",
neurips,https://proceedings.neurips.cc/paper/2016/file/01931a6925d3de09e5f87419d9d55055-Paper.pdf,A Locally Adaptive Normal Distribution,"Georgios Arvanitidis, Lars K. Hansen, Søren Hauberg",
neurips,https://proceedings.neurips.cc/paper/2016/file/01eee509ee2f68dc6014898c309e86bf-Paper.pdf,Tagger: Deep Unsupervised Perceptual Grouping,"Klaus Greff, Antti Rasmus, Mathias Berglund, Tele Hao, Harri Valpola, Jürgen Schmidhuber",
neurips,https://proceedings.neurips.cc/paper/2016/file/0233f3bb964cf325a30f8b1c2ed2da93-Paper.pdf,Online Bayesian Moment Matching for Topic Modeling with Unknown Number of Topics,"Wei-Shou Hsu, Pascal Poupart",
neurips,https://proceedings.neurips.cc/paper/2016/file/0245952ecff55018e2a459517fdb40e3-Paper.pdf,Conditional Generative Moment-Matching Networks,"Yong Ren, Jun Zhu, Jialian Li, Yucen Luo",
neurips,https://proceedings.neurips.cc/paper/2016/file/0266e33d3f546cb5436a10798e657d97-Paper.pdf,Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks,"Hao Wang, Xingjian SHI, Dit-Yan Yeung",
neurips,https://proceedings.neurips.cc/paper/2016/file/03255088ed63354a54e0e5ed957e9008-Paper.pdf,Bayesian Intermittent Demand Forecasting for Large Inventories,"Matthias W. Seeger, David Salinas, Valentin Flunkert",
neurips,https://proceedings.neurips.cc/paper/2016/file/03afdbd66e7929b125f8597834fa83a4-Paper.pdf,Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks,"Tianfan Xue, Jiajun Wu, Katherine Bouman, Bill Freeman",
neurips,https://proceedings.neurips.cc/paper/2016/file/03e7ef47cee6fa4ae7567394b99912b7-Paper.pdf,Achieving budget-optimality with adaptive schemes in crowdsourcing,"Ashish Khetan, Sewoong Oh",
neurips,https://proceedings.neurips.cc/paper/2016/file/03f544613917945245041ea1581df0c2-Paper.pdf,Stochastic Gradient Richardson-Romberg Markov Chain Monte Carlo,"Alain Durmus, Umut Simsekli, Eric Moulines, Roland Badeau, Gaël RICHARD",
neurips,https://proceedings.neurips.cc/paper/2016/file/04025959b191f8f9de3f924f0940515f-Paper.pdf,Generating Videos with Scene Dynamics,"Carl Vondrick, Hamed Pirsiavash, Antonio Torralba",
neurips,https://proceedings.neurips.cc/paper/2016/file/046ddf96c233a273fd390c3d0b1a9aa4-Paper.pdf,Approximate maximum entropy principles via Goemans-Williamson with applications to provable variational methods,"Andrej Risteski, Yuanzhi Li",
neurips,https://proceedings.neurips.cc/paper/2016/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf,Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering,"Michaël Defferrard, Xavier Bresson, Pierre Vandergheynst",
neurips,https://proceedings.neurips.cc/paper/2016/file/052335232b11864986bb2fa20fa38748-Paper.pdf,Fast Distributed Submodular Cover: Public-Private Data Summarization,"Baharan Mirzasoleiman, Morteza Zadimoghaddam, Amin Karbasi",
neurips,https://proceedings.neurips.cc/paper/2016/file/06138bc5af6023646ede0e1f7c1eac75-Paper.pdf,Exponential Family Embeddings,"Maja Rudolph, Francisco Ruiz, Stephan Mandt, David Blei",
neurips,https://proceedings.neurips.cc/paper/2016/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf,A Non-parametric Learning Method for Confidently Estimating Patient's Clinical State and Dynamics,"William Hoiles, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2016/file/06409663226af2f3114485aa4e0a23b4-Paper.pdf,Integrated perception with recurrent multi-task neural networks,"Hakan Bilen, Andrea Vedaldi",
neurips,https://proceedings.neurips.cc/paper/2016/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf,Dialog-based Language Learning,Jason E. Weston,
neurips,https://proceedings.neurips.cc/paper/2016/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf,A Theoretically Grounded Application of Dropout in Recurrent Neural Networks,"Yarin Gal, Zoubin Ghahramani",
neurips,https://proceedings.neurips.cc/paper/2016/file/0771fc6f0f4b1d7d1bb73bbbe14e0e31-Paper.pdf,Automatic Neuron Detection in Calcium Imaging Data Using Convolutional Networks,"Noah Apthorpe, Alexander Riordan, Robert Aguilar, Jan Homann, Yi Gu, David Tank, H. Sebastian Seung",
neurips,https://proceedings.neurips.cc/paper/2016/file/07811dc6c422334ce36a09ff5cd6fe71-Paper.pdf,Convolutional Neural Fabrics,"Shreyas Saxena, Jakob Verbeek",
neurips,https://proceedings.neurips.cc/paper/2016/file/07cdfd23373b17c6b337251c22b7ea57-Paper.pdf,Budgeted stream-based active learning via adaptive submodular maximization,"Kaito Fujii, Hisashi Kashima",
neurips,https://proceedings.neurips.cc/paper/2016/file/08e6bea8e90ba87af3c9554d94db6579-Paper.pdf,An equivalence between high dimensional Bayes optimal inference and M-estimation,"Madhu Advani, Surya Ganguli",
neurips,https://proceedings.neurips.cc/paper/2016/file/093b60fd0557804c8ba0cbf1453da22f-Paper.pdf,A Sparse Interactive Model for Matrix Completion with Side Information,"Jin Lu, Guannan Liang, Jiangwen Sun, Jinbo Bi","Matrix completion methods can benefit from side information besides the partially observed matrix. The use of side features describing the row and column entities of a matrix has been shown to reduce the sample complexity for completing the matrix. We propose a novel sparse formulation that explicitly models the interaction between the row and column side features to approximate the matrix entries. Unlike early methods, this model does not require the low-rank condition on the model parameter matrix. We prove that when the side features can span the latent feature space of the matrix to be recovered, the number of observed entries needed for an exact recovery is
O
(
log
N
)
O
where
N
N
is the size of the matrix. When the side features are corrupted latent features of the matrix with a small perturbation, our method can achieve an
ϵ
ϵ
-recovery with
O
(
log
N
)
O
sample complexity, and maintains a
\O
(
N
3
/
2
)
\O
rate similar to classfic methods with no side information. An efficient linearized Lagrangian algorithm is developed with a strong guarantee of convergence. Empirical results show that our approach outperforms three state-of-the-art methods both in simulations and on real world datasets."
neurips,https://proceedings.neurips.cc/paper/2016/file/0966289037ad9846c5e994be2a91bafa-Paper.pdf,Bi-Objective Online Matching and Submodular Allocations,"Hossein Esfandiari, Nitish Korula, Vahab Mirrokni",
neurips,https://proceedings.neurips.cc/paper/2016/file/0a09c8844ba8f0936c20bd791130d6b6-Paper.pdf,Interpretable Distribution Features with Maximum Testing Power,"Wittawat Jitkrittum, Zoltán Szabó, Kacper P. Chwialkowski, Arthur Gretton",
neurips,https://proceedings.neurips.cc/paper/2016/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Paper.pdf,Finding significant combinations of features in the presence of categorical covariates,"Laetitia Papaxanthos, Felipe Llinares-López, Dean Bodenham, Karsten Borgwardt",
neurips,https://proceedings.neurips.cc/paper/2016/file/0a113ef6b61820daa5611c870ed8d5ee-Paper.pdf,A Non-convex One-Pass Framework for Generalized Factorization Machine and Rank-One Matrix Sensing,"Ming Lin, Jieping Ye","We develop an efficient alternating framework for learning a generalized version of Factorization Machine (gFM) on steaming data with provable guarantees. When the instances are sampled from
d
d
dimensional random Gaussian vectors and the target second order coefficient matrix in gFM is of rank
k
k
, our algorithm converges linearly, achieves
O
(
ϵ
)
O
recovery error after retrieving
O
(
k
3
d
log
(
1
/
ϵ
)
)
O
training instances, consumes
O
(
k
d
)
O
memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence endowed with a so-called Conditionally Independent RIP condition (CI-RIP). As special cases of gFM, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval."
neurips,https://proceedings.neurips.cc/paper/2016/file/0a87257e5308197df43230edf4ad1dae-Paper.pdf,Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction,"Jacob Steinhardt, Gregory Valiant, Moses Charikar","We consider a crowdsourcing model in which n workers are asked to rate the quality of n items previously generated by other workers. An unknown set of
α
n
α
workers generate reliable ratings, while the remaining workers may behave arbitrarily and possibly adversarially. The manager of the experiment can also manually evaluate the quality of a small number of items, and wishes to curate together almost all of the high-quality items with at most an fraction of low-quality items. Perhaps surprisingly, we show that this is possible with an amount of work required of the manager, and each worker, that does not scale with n: the dataset can be curated with
~
O
(
1
/
β
α
ϵ
4
)
O
ratings per worker, and
~
O
(
1
/
β
ϵ
2
)
O
ratings by the manager, where
β
β
is the fraction of high-quality items. Our results extend to the more general setting of peer prediction, including peer grading in online classrooms."
neurips,https://proceedings.neurips.cc/paper/2016/file/0bf727e907c5fc9d5356f11e4c45d613-Paper.pdf,"Threshold Bandits, With and Without Censored Feedback","Jacob D. Abernethy, Kareem Amin, Ruihao Zhu","We consider the \emph{Threshold Bandit} setting, a variant of the classical multi-armed bandit problem in which the reward on each round depends on a piece of side information known as a \emph{threshold value}. The learner selects one of
K
K
actions (arms), this action generates a random sample from a fixed distribution, and the action then receives a unit payoff in the event that this sample exceeds the threshold value. We consider two versions of this problem, the \emph{uncensored} and \emph{censored} case, that determine whether the sample is always observed or only when the threshold is not met. Using new tools to understand the popular UCB algorithm, we show that the uncensored case is essentially no more difficult than the classical multi-armed bandit setting. Finally we show that the censored case exhibits more challenges, but we give guarantees in the event that the sequence of threshold values is generated optimistically."
neurips,https://proceedings.neurips.cc/paper/2016/file/0c9ebb2ded806d7ffda75cd0b95eb70c-Paper.pdf,Variational Bayes on Monte Carlo Steroids,"Aditya Grover, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2016/file/0d4f4805c36dc6853edfa4c7e1638b48-Paper.pdf,Finite-Dimensional BFRY Priors and Variational Bayesian Inference for Power Law Models,"Juho Lee, Lancelot F. James, Seungjin Choi",
neurips,https://proceedings.neurips.cc/paper/2016/file/0d73a25092e5c1c9769a9f3255caa65a-Paper.pdf,Maximal Sparsity with Deep Networks?,"Bo Xin, Yizhou Wang, Wen Gao, David Wipf, Baoyuan Wang","The iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity, which collectively resemble a typical neural network layer. Consequently, a lengthy sequence of algorithm iterations can be viewed as a deep network with shared, hand-crafted layer weights. It is therefore quite natural to examine the degree to which a learned network model might act as a viable surrogate for traditional sparse estimation in domains where ample training data is available. While the possibility of a reduced computational budget is readily apparent when a ceiling is imposed on the number of layers, our work primarily focuses on estimation accuracy. In particular, it is well-known that when a signal dictionary has coherent columns, as quantified by a large RIP constant, then most tractable iterative algorithms are unable to find maximally sparse representations. In contrast, we demonstrate both theoretically and empirically the potential for a trained deep network to recover minimal
ℓ
0
ℓ
-norm representations in regimes where existing methods fail. The resulting system, which can effectively learn novel iterative sparse estimation algorithms, is deployed on a practical photometric stereo estimation problem, where the goal is to remove sparse outliers that can disrupt the estimation of surface normals from a 3D scene."
neurips,https://proceedings.neurips.cc/paper/2016/file/0deb1c54814305ca9ad266f53bc82511-Paper.pdf,Single-Image Depth Perception in the Wild,"Weifeng Chen, Zhao Fu, Dawei Yang, Jia Deng",
neurips,https://proceedings.neurips.cc/paper/2016/file/0e55666a4ad822e0e34299df3591d979-Paper.pdf,Single Pass PCA of Matrix Products,"Shanshan Wu, Srinadh Bhojanapalli, Sujay Sanghavi, Alexandros G. Dimakis","In this paper we present a new algorithm for computing a low rank approximation of the product
A
T
B
A
by taking only a single pass of the two matrices
A
A
and
B
B
. The straightforward way to do this is to (a) first sketch
A
A
and
B
B
individually, and then (b) find the top components using PCA on the sketch. Our algorithm in contrast retains additional summary information about
A
,
B
A
(e.g. row and column norms etc.) and uses this additional information to obtain an improved approximation from the sketches. Our main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in addition we also provide results from an Apache Spark implementation that shows better computational and statistical performance on real-world and synthetic evaluation datasets."
neurips,https://proceedings.neurips.cc/paper/2016/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf,Optimal Sparse Linear Encoders and Sparse PCA,"Malik Magdon-Ismail, Christos Boutsidis",
neurips,https://proceedings.neurips.cc/paper/2016/file/0e9fa1f3e9e66792401a6972d477dcc3-Paper.pdf,Measuring the reliability of MCMC inference with bidirectional Monte Carlo,"Roger B. Grosse, Siddharth Ancha, Daniel M. Roy",
neurips,https://proceedings.neurips.cc/paper/2016/file/0ed9422357395a0d4879191c66f4faa2-Paper.pdf,Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections,"Xiaojiao Mao, Chunhua Shen, Yu-Bin Yang",
neurips,https://proceedings.neurips.cc/paper/2016/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf,On Valid Optimal Assignment Kernels and Applications to Graph Classification,"Nils M. Kriege, Pierre-Louis Giscard, Richard Wilson",
neurips,https://proceedings.neurips.cc/paper/2016/file/0f96613235062963ccde717b18f97592-Paper.pdf,Multi-view Anomaly Detection via Robust Probabilistic Latent Variable Models,"Tomoharu Iwata, Makoto Yamada",
neurips,https://proceedings.neurips.cc/paper/2016/file/0fe473396242072e84af286632d3f0ff-Paper.pdf,Optimal Architectures in a Solvable Model of Deep Networks,"Jonathan Kadmon, Haim Sompolinsky",
neurips,https://proceedings.neurips.cc/paper/2016/file/10907813b97e249163587e6246612e21-Paper.pdf,"Efficient state-space modularization for planning: theory, behavioral and neural signatures","Daniel McNamee, Daniel M. Wolpert, Mate Lengyel",
neurips,https://proceedings.neurips.cc/paper/2016/file/10a5ab2db37feedfdeaab192ead4ac0e-Paper.pdf,A Communication-Efficient Parallel Algorithm for Decision Tree,"Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tie-Yan Liu","Decision tree (and its extensions such as Gradient Boosting Decision Trees and Random Forest) is a widely used machine learning algorithm, due to its practical effectiveness and model interpretability. With the emergence of big data, there is an increasing need to parallelize the training process of decision tree. However, most existing attempts along this line suffer from high communication costs. In this paper, we propose a new algorithm, called \emph{Parallel Voting Decision Tree (PV-Tree)}, to tackle this challenge. After partitioning the training data onto a number of (e.g.,
M
M
) machines, this algorithm performs both local voting and global voting in each iteration. For local voting, the top-
k
k
attributes are selected from each machine according to its local data. Then, the indices of these top attributes are aggregated by a server, and the globally top-
2
k
2
attributes are determined by a majority voting among these local candidates. Finally, the full-grained histograms of the globally top-
2
k
2
attributes are collected from local machines in order to identify the best (most informative) attribute and its split point. PV-Tree can achieve a very low communication cost (independent of the total number of attributes) and thus can scale out very well. Furthermore, theoretical analysis shows that this algorithm can learn a near optimal decision tree, since it can find the best attribute with a large probability. Our experiments on real-world datasets show that PV-Tree significantly outperforms the existing parallel decision tree algorithms in the tradeoff between accuracy and efficiency."
neurips,https://proceedings.neurips.cc/paper/2016/file/10c66082c124f8afe3df4886f5e516e0-Paper.pdf,Supervised Word Mover's Distance,"Gao Huang, Chuan Guo, Matt J. Kusner, Yu Sun, Fei Sha, Kilian Q. Weinberger",
neurips,https://proceedings.neurips.cc/paper/2016/file/1145a30ff80745b56fb0cecf65305017-Paper.pdf,Fast and accurate spike sorting of high-channel count probes with KiloSort,"Marius Pachitariu, Nicholas A. Steinmetz, Shabnam N. Kadir, Matteo Carandini, Kenneth D. Harris",
neurips,https://proceedings.neurips.cc/paper/2016/file/130f1a8e9e102707f3f91b010f151b0b-Paper.pdf,Learning brain regions via large-scale online structured sparse dictionary learning,"Elvis DOHMATOB, Arthur Mensch, Gael Varoquaux, Bertrand Thirion","We propose a multivariate online dictionary-learning method for obtaining decompositions of brain images with structured and sparse components (aka atoms). Sparsity is to be understood in the usual sense: the dictionary atoms are constrained to contain mostly zeros. This is imposed via an
ℓ
1
ℓ
-norm constraint. By ""structured"", we mean that the atoms are piece-wise smooth and compact, thus making up blobs, as opposed to scattered patterns of activation. We propose to use a Sobolev (Laplacian) penalty to impose this type of structure. Combining the two penalties, we obtain decompositions that properly delineate brain structures from functional images. This non-trivially extends the online dictionary-learning work of Mairal et al. (2010), at the price of only a factor of 2 or 3 on the overall running time. Just like the Mairal et al. (2010) reference method, the online nature of our proposed algorithm allows it to scale to arbitrarily sized datasets. Experiments on brain data show that our proposed method extracts structured and denoised dictionaries that are more intepretable and better capture inter-subject variability in small medium, and large-scale regimes alike, compared to state-of-the-art models."
neurips,https://proceedings.neurips.cc/paper/2016/file/139f0874f2ded2e41b0393c4ac5644f7-Paper.pdf,Improving PAC Exploration Using the Median Of Means,"Jason Pazis, Ronald E. Parr, Jonathan P. How",
neurips,https://proceedings.neurips.cc/paper/2016/file/13f320e7b5ead1024ac95c3b208610db-Paper.pdf,Active Nearest-Neighbor Learning in Metric Spaces,"Aryeh Kontorovich, Sivan Sabato, Ruth Urner",
neurips,https://proceedings.neurips.cc/paper/2016/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf,Learning from Small Sample Sets by Combining Unsupervised Meta-Training with CNNs,"Yu-Xiong Wang, Martial Hebert",
neurips,https://proceedings.neurips.cc/paper/2016/file/144a3f71a03ab7c4f46f9656608efdb2-Paper.pdf,Learning Bayesian networks with ancestral constraints,"Eunice Yuh-Jie Chen, Yujia Shen, Arthur Choi, Adnan Darwiche",
neurips,https://proceedings.neurips.cc/paper/2016/file/148510031349642de5ca0c544f31b2ef-Paper.pdf,Exponential expressivity in deep neural networks through transient chaos,"Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, Surya Ganguli",
neurips,https://proceedings.neurips.cc/paper/2016/file/14cfdb59b5bda1fc245aadae15b1984a-Paper.pdf,MetaGrad: Multiple Learning Rates in Online Learning,"Tim van Erven, Wouter M. Koolen",
neurips,https://proceedings.neurips.cc/paper/2016/file/14d9e8007c9b41f57891c48e07c23f57-Paper.pdf,Learning under uncertainty: a comparison between R-W and Bayesian approach,"He Huang, Martin Paulus",
neurips,https://proceedings.neurips.cc/paper/2016/file/1579779b98ce9edb98dd85606f2c119d-Paper.pdf,End-to-End Goal-Driven Web Navigation,"Rodrigo Nogueira, Kyunghyun Cho",
neurips,https://proceedings.neurips.cc/paper/2016/file/158fc2ddd52ec2cf54d3c161f2dd6517-Paper.pdf,Higher-Order Factorization Machines,"Mathieu Blondel, Akinori Fujino, Naonori Ueda, Masakazu Ishihata",
neurips,https://proceedings.neurips.cc/paper/2016/file/15de21c670ae7c3f6f3f1f37029303c9-Paper.pdf,Efficient Second Order Online Learning by Sketching,"Haipeng Luo, Alekh Agarwal, Nicolò Cesa-Bianchi, John Langford",
neurips,https://proceedings.neurips.cc/paper/2016/file/16026d60ff9b54410b3435b403afd226-Paper.pdf,Professor Forcing: A New Algorithm for Training Recurrent Networks,"Alex M. Lamb, Anirudh Goyal ALIAS PARTH GOYAL, Ying Zhang, Saizheng Zhang, Aaron C. Courville, Yoshua Bengio",
neurips,https://proceedings.neurips.cc/paper/2016/file/1679091c5a880faf6fb5e6087eb1b2dc-Paper.pdf,Deep ADMM-Net for Compressive Sensing MRI,"yan yang, Jian Sun, Huibin Li, Zongben Xu",
neurips,https://proceedings.neurips.cc/paper/2016/file/1714726c817af50457d810aae9d27a2e-Paper.pdf,Adaptive Averaging in Accelerated Descent Dynamics,"Walid Krichene, Alexandre Bayen, Peter L. Bartlett","We study accelerated descent dynamics for constrained convex optimization. This dynamics can be described naturally as a coupling of a dual variable accumulating gradients at a given rate
η
(
t
)
η
, and a primal variable obtained as the weighted average of the mirrored dual trajectory, with weights
w
(
t
)
w
. Using a Lyapunov argument, we give sufficient conditions on
η
η
and
w
w
to achieve a desired convergence rate. As an example, we show that the replicator dynamics (an example of mirror descent on the simplex) can be accelerated using a simple averaging scheme. We then propose an adaptive averaging heuristic which adaptively computes the weights to speed up the decrease of the Lyapunov function. We provide guarantees on adaptive averaging in continuous-time, prove that it preserves the quadratic convergence rate of accelerated first-order methods in discrete-time, and give numerical experiments to compare it with existing heuristics, such as adaptive restarting. The experiments indicate that adaptive averaging performs at least as well as adaptive restarting, with significant improvements in some cases."
neurips,https://proceedings.neurips.cc/paper/2016/file/1728efbda81692282ba642aafd57be3a-Paper.pdf,Dynamic Mode Decomposition with Reproducing Kernels for Koopman Spectral Analysis,Yoshinobu Kawahara,
neurips,https://proceedings.neurips.cc/paper/2016/file/17ed8abedc255908be746d245e50263a-Paper.pdf,"Total Variation Classes Beyond 1d: Minimax Rates, and the Limitations of Linear Smoothers","Veeranjaneyulu Sadhanala, Yu-Xiang Wang, Ryan J. Tibshirani","We consider the problem of estimating a function defined over
n
n
locations on a
d
d
-dimensional grid (having all side lengths equal to
n
1
/
d
n
). When the function is constrained to have discrete total variation bounded by
C
n
C
, we derive the minimax optimal (squared)
ℓ
2
ℓ
estimation error rate, parametrized by
n
,
C
n
n
. Total variation denoising, also known as the fused lasso, is seen to be rate optimal. Several simpler estimators exist, such as Laplacian smoothing and Laplacian eigenmaps. A natural question is: can these simpler estimators perform just as well? We prove that these estimators, and more broadly all estimators given by linear transformations of the input data, are suboptimal over the class of functions with bounded variation. This extends fundamental findings of Donoho and Johnstone (1998) on 1-dimensional total variation spaces to higher dimensions. The implication is that the computationally simpler methods cannot be used for such sophisticated denoising tasks, without sacrificing statistical accuracy. We also derive minimax rates for discrete Sobolev spaces over
d
d
-dimensional grids, which are, in some sense, smaller than the total variation function spaces. Indeed, these are small enough spaces that linear estimators can be optimal---and a few well-known ones are, such as Laplacian smoothing and Laplacian eigenmaps, as we show. Lastly, we investigate the adaptivity of the total variation denoiser to these smaller Sobolev function spaces."
neurips,https://proceedings.neurips.cc/paper/2016/file/184260348236f9554fe9375772ff966e-Paper.pdf,Hardness of Online Sleeping Combinatorial Optimization Problems,"Satyen Kale, Chansoo Lee, David Pal",
neurips,https://proceedings.neurips.cc/paper/2016/file/185c29dc24325934ee377cfda20e414c-Paper.pdf,Density Estimation via Discrepancy Based Adaptive Sequential Partition,"Dangna Li, Kun Yang, Wing Hung Wong","Given
i
i
d
i
observations from an unknown continuous distribution defined on some domain
Ω
Ω
, we propose a nonparametric method to learn a piecewise constant function to approximate the underlying probability density function. Our density estimate is a piecewise constant function defined on a binary partition of
Ω
Ω
. The key ingredient of the algorithm is to use discrepancy, a concept originates from Quasi Monte Carlo analysis, to control the partition process. The resulting algorithm is simple, efficient, and has provable convergence rate. We demonstrate empirically its efficiency as a density estimation method. We also show how it can be utilized to find good initializations for k-means."
neurips,https://proceedings.neurips.cc/paper/2016/file/186a157b2992e7daed3677ce8e9fe40f-Paper.pdf,Quantized Random Projections and Non-Linear Estimation of Cosine Similarity,"Ping Li, Michael Mitzenmacher, Martin Slawski","Random projections constitute a simple, yet effective technique for dimensionality reduction with applications in learning and search problems. In the present paper, we consider the problem of estimating cosine similarities when the projected data undergo scalar quantization to
b
b
bits. We here argue that the maximum likelihood estimator (MLE) is a principled approach to deal with the non-linearity resulting from quantization, and subsequently study its computational and statistical properties. A specific focus is on the on the trade-off between bit depth and the number of projections given a fixed budget of bits for storage or transmission. Along the way, we also touch upon the existence of a qualitative counterpart to the Johnson-Lindenstrauss lemma in the presence of quantization."
neurips,https://proceedings.neurips.cc/paper/2016/file/186fb23a33995d91ce3c2212189178c8-Paper.pdf,Algorithms and matching lower bounds for approximately-convex optimization,"Andrej Risteski, Yuanzhi Li","In recent years, a rapidly increasing number of applications in practice requires solving non-convex objectives, like training neural networks, learning graphical models, maximum likelihood estimation etc. Though simple heuristics such as gradient descent with very few modifications tend to work well, theoretical understanding is very weak. We consider possibly the most natural class of non-convex functions where one could hope to obtain provable guarantees: functions that are
approximately convex'', i.e. functions
\tf
:
\Real
d
→
\Real
\tf
for which there exists a \emph{convex function}
f
f
such that for all
x
x
,
|
\tf
(
x
)
−
f
(
x
)
|
≤
\errnoise
|
for a fixed value
\errnoise
\errnoise
. We then want to minimize
\tf
\tf
, i.e. output a point
\tx
\tx
such that
\tf
(
\tx
)
≤
min
x
\tf
(
x
)
+
\err
\tf
. It is quite natural to conjecture that for fixed
\err
\err
, the problem gets harder for larger
\errnoise
\errnoise
, however, the exact dependency of
\err
\err
and
\errnoise
\errnoise
is not known. In this paper, we strengthen the known \emph{information theoretic} lower bounds on the trade-off between
\err
\err
and
\errnoise
\errnoise
substantially, and exhibit an algorithm that matches these lower bounds for a large class of convex bodies."
neurips,https://proceedings.neurips.cc/paper/2016/file/18d10dc6e666eab6de9215ae5b3d54df-Paper.pdf,The Parallel Knowledge Gradient Method for Batch Bayesian Optimization,"Jian Wu, Peter Frazier",
neurips,https://proceedings.neurips.cc/paper/2016/file/1a0a283bfe7c549dee6c638a05200e32-Paper.pdf,Edge-exchangeable graphs and sparsity,"Diana Cai, Trevor Campbell, Tamara Broderick",
neurips,https://proceedings.neurips.cc/paper/2016/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Paper.pdf,Stochastic Variance Reduction Methods for Saddle-Point Problems,"Balamurugan Palaniappan, Francis Bach",
neurips,https://proceedings.neurips.cc/paper/2016/file/1abb1e1ea5f481b589da52303b091cbb-Paper.pdf,A Probabilistic Model of Social Decision Making based on Reward Maximization,"Koosha Khalvati, Seongmin A. Park, Jean-Claude Dreher, Rajesh PN Rao",
neurips,https://proceedings.neurips.cc/paper/2016/file/1ce927f875864094e3906a4a0b5ece68-Paper.pdf,Bootstrap Model Aggregation for Distributed Statistical Learning,"JUN HAN, Qiang Liu",
neurips,https://proceedings.neurips.cc/paper/2016/file/1d94108e907bb8311d8802b48fd54b4a-Paper.pdf,Unsupervised Learning of 3D Structure from Images,"Danilo Jimenez Rezende, S. M. Ali Eslami, Shakir Mohamed, Peter Battaglia, Max Jaderberg, Nicolas Heess",
neurips,https://proceedings.neurips.cc/paper/2016/file/1e8c391abfde9abea82d75a2d60278d4-Paper.pdf,beta-risk: a New Surrogate Risk for Learning from Weakly Labeled Data,"Valentina Zantedeschi, Rémi Emonet, Marc Sebban",
neurips,https://proceedings.neurips.cc/paper/2016/file/1f34004ebcb05f9acda6016d5cc52d5e-Paper.pdf,Learning Supervised PageRank with Gradient-Based and Gradient-Free Optimization Methods,"Lev Bogolubsky, Pavel Dvurechenskii, Alexander Gasnikov, Gleb Gusev, Yurii Nesterov, Andrei M. Raigorodskii, Aleksey Tikhonov, Maksim Zhukovskii",
neurips,https://proceedings.neurips.cc/paper/2016/file/1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf,Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods,"Antoine Gautier, Quynh N. Nguyen, Matthias Hein",
neurips,https://proceedings.neurips.cc/paper/2016/file/1f50893f80d6830d62765ffad7721742-Paper.pdf,Optimal Black-Box Reductions Between Optimization Objectives,"Zeyuan Allen-Zhu, Elad Hazan",
neurips,https://proceedings.neurips.cc/paper/2016/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf,Sequential Neural Models with Stochastic Layers,"Marco Fraccaro, Søren Kaae Sønderby, Ulrich Paquet, Ole Winther",
neurips,https://proceedings.neurips.cc/paper/2016/file/20c9f5700da1088260df60fcc5df2b53-Paper.pdf,Iterative Refinement of the Approximate Posterior for Directed Belief Networks,"Devon Hjelm, Russ R. Salakhutdinov, Kyunghyun Cho, Nebojsa Jojic, Vince Calhoun, Junyoung Chung",
neurips,https://proceedings.neurips.cc/paper/2016/file/20d135f0f28185b84a4cf7aa51f29500-Paper.pdf,Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles,"Stefan Lee, Senthil Purushwalkam Shiva Prakash, Michael Cogswell, Viresh Ranjan, David Crandall, Dhruv Batra",
neurips,https://proceedings.neurips.cc/paper/2016/file/228499b55310264a8ea0e27b6e7c6ab6-Paper.pdf,Learning shape correspondence with anisotropic convolutional neural networks,"Davide Boscaini, Jonathan Masci, Emanuele Rodolà, Michael Bronstein",
neurips,https://proceedings.neurips.cc/paper/2016/file/22ac3c5a5bf0b520d281c122d1490650-Paper.pdf,Learning Tree Structured Potential Games,"Vikas Garg, Tommi Jaakkola",
neurips,https://proceedings.neurips.cc/paper/2016/file/231141b34c82aa95e48810a9d1b33a79-Paper.pdf,RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism,"Edward Choi, Mohammad Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy Schuetz, Walter Stewart",
neurips,https://proceedings.neurips.cc/paper/2016/file/2387337ba1e0b0249ba90f55b2ba2521-Paper.pdf,PAC Reinforcement Learning with Rich Observations,"Akshay Krishnamurthy, Alekh Agarwal, John Langford",
neurips,https://proceedings.neurips.cc/paper/2016/file/23ad3e314e2a2b43b4c720507cec0723-Paper.pdf,Generative Shape Models: Joint Text Recognition and Segmentation with Very Little Training Data,"Xinghua Lou, Ken Kansky, Wolfgang Lehrach, CC Laan, Bhaskara Marthi, D. Phoenix, Dileep George",
neurips,https://proceedings.neurips.cc/paper/2016/file/23c97e9cb93576e45d2feaf00d0e8502-Paper.pdf,Probabilistic Linear Multistep Methods,"Onur Teymur, Kostas Zygalakis, Ben Calderhead",
neurips,https://proceedings.neurips.cc/paper/2016/file/2421fcb1263b9530df88f7f002e78ea5-Paper.pdf,Computational and Statistical Tradeoffs in Learning to Rank,"Ashish Khetan, Sewoong Oh",
neurips,https://proceedings.neurips.cc/paper/2016/file/2451041557a22145b3701b0184109cab-Paper.pdf,Split LBI: An Iterative Regularization Path with Structural Sparsity,"Chendi Huang, Xinwei Sun, Jiechao Xiong, Yuan Yao","An iterative regularization path with structural sparsity is proposed in this paper based on variable splitting and the Linearized Bregman Iteration, hence called \emph{Split LBI}. Despite its simplicity, Split LBI outperforms the popular generalized Lasso in both theory and experiments. A theory of path consistency is presented that equipped with a proper early stopping, Split LBI may achieve model selection consistency under a family of Irrepresentable Conditions which can be weaker than the necessary and sufficient condition for generalized Lasso. Furthermore, some
ℓ
2
ℓ
error bounds are also given at the minimax optimal rates. The utility and benefit of the algorithm are illustrated by applications on both traditional image denoising and a novel example on partial order ranking."
neurips,https://proceedings.neurips.cc/paper/2016/file/2596a54cdbb555cfd09cd5d991da0f55-Paper.pdf,Incremental Variational Sparse Gaussian Process Regression,"Ching-An Cheng, Byron Boots",
neurips,https://proceedings.neurips.cc/paper/2016/file/25ddc0f8c9d3e22e03d3076f98d83cb2-Paper.pdf,Sublinear Time Orthogonal Tensor Decomposition,"Zhao Song, David Woodruff, Huan Zhang","A recent work (Wang et. al., NIPS 2015) gives the fastest known algorithms for orthogonal tensor decomposition with provable guarantees. Their algorithm is based on computing sketches of the input tensor, which requires reading the entire input. We show in a number of cases one can achieve the same theoretical guarantees in sublinear time, i.e., even without reading most of the input tensor. Instead of using sketches to estimate inner products in tensor decomposition algorithms, we use importance sampling. To achieve sublinear time, we need to know the norms of tensor slices, and we show how to do this in a number of important cases. For symmetric tensors
T
=
∑
k
i
=
1
λ
i
u
⊗
p
i
T
with
λ
i
>
0
λ
for all i, we estimate such norms in sublinear time whenever p is even. For the important case of p = 3 and small values of k, we can also estimate such norms. For asymmetric tensors sublinear time is not possible in general, but we show if the tensor slice norms are just slightly below
∥
T
∥
F
‖
then sublinear time is again possible. One of the main strengths of our work is empirical - in a number of cases our algorithm is orders of magnitude faster than existing methods with the same accuracy."
neurips,https://proceedings.neurips.cc/paper/2016/file/26f5bd4aa64fdadf96152ca6e6408068-Paper.pdf,Mapping Estimation for Discrete Optimal Transport,"Michaël Perrot, Nicolas Courty, Rémi Flamary, Amaury Habrard","We are interested in the computation of the transport map of an Optimal Transport problem. Most of the computational approaches of Optimal Transport use the Kantorovich relaxation of the problem to learn a probabilistic coupling
\mgamma
\mgamma
but do not address the problem of learning the underlying transport map
\funcT
\funcT
linked to the original Monge problem. Consequently, it lowers the potential usage of such methods in contexts where out-of-samples computations are mandatory. In this paper we propose a new way to jointly learn the coupling and an approximation of the transport map. We use a jointly convex formulation which can be efficiently optimized. Additionally, jointly learning the coupling and the transport map allows to smooth the result of the Optimal Transport and generalize it to out-of-samples examples. Empirically, we show the interest and the relevance of our method in two tasks: domain adaptation and image editing."
neurips,https://proceedings.neurips.cc/paper/2016/file/277a78fc05c8864a170e9a56ceeabc4c-Paper.pdf,Greedy Feature Construction,"Dino Oglic, Thomas Gärtner",
neurips,https://proceedings.neurips.cc/paper/2016/file/2823f4797102ce1a1aec05359cc16dd9-Paper.pdf,Dynamic Network Surgery for Efficient DNNs,"Yiwen Guo, Anbang Yao, Yurong Chen","Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of
\bm
108
×
\bm
and
\bm
17.7
×
\bm
respectively, proving that it outperforms the recent pruning method by considerable margins. Code and some models are available at https://github.com/yiwenguo/Dynamic-Network-Surgery."
neurips,https://proceedings.neurips.cc/paper/2016/file/286674e3082feb7e5afb92777e48821f-Paper.pdf,Graph Clustering: Block-models and model free results,"Yali Wan, Marina Meila",
neurips,https://proceedings.neurips.cc/paper/2016/file/289dff07669d7a23de0ef88d2f7129e7-Paper.pdf,CMA-ES with Optimal Covariance Update and Storage Complexity,"Oswin Krause, Dídac Rodríguez Arbonès, Christian Igel",
neurips,https://proceedings.neurips.cc/paper/2016/file/28b60a16b55fd531047c0c958ce14b95-Paper.pdf,Feature selection in functional data classification with recursive maxima hunting,"José L. Torrecilla, Alberto Suárez",
neurips,https://proceedings.neurips.cc/paper/2016/file/28e209b61a52482a0ae1cb9f5959c792-Paper.pdf,Cyclades: Conflict-free Asynchronous Machine Learning,"Xinghao Pan, Maximilian Lam, Stephen Tu, Dimitris Papailiopoulos, Ce Zhang, Michael I. Jordan, Kannan Ramchandran, Christopher Ré",
neurips,https://proceedings.neurips.cc/paper/2016/file/291597a100aadd814d197af4f4bab3a7-Paper.pdf,Proximal Stochastic Methods for Nonsmooth Nonconvex Finite-Sum Optimization,"Sashank J. Reddi, Suvrit Sra, Barnabas Poczos, Alexander J. Smola",
neurips,https://proceedings.neurips.cc/paper/2016/file/296472c9542ad4d4788d543508116cbc-Paper.pdf,Spectral Learning of Dynamic Systems from Nonequilibrium Data,"Hao Wu, Frank Noe",
neurips,https://proceedings.neurips.cc/paper/2016/file/299570476c6f0309545110c592b6a63b-Paper.pdf,Dimension-Free Iteration Complexity of Finite Sum Optimization Problems,"Yossi Arjevani, Ohad Shamir","Many canonical machine learning problems boil down to a convex optimization problem with a finite sum structure. However, whereas much progress has been made in developing faster algorithms for this setting, the inherent limitations of these problems are not satisfactorily addressed by existing lower bounds. Indeed, current bounds focus on first-order optimization algorithms, and only apply in the often unrealistic regime where the number of iterations is less than
\cO
(
d
/
n
)
\cO
(where
d
d
is the dimension and
n
n
is the number of samples). In this work, we extend the framework of Arjevani et al. \cite{arjevani2015lower,arjevani2016iteration} to provide new lower bounds, which are dimension-free, and go beyond the assumptions of current bounds, thereby covering standard finite sum optimization methods, e.g., SAG, SAGA, SVRG, SDCA without duality, as well as stochastic coordinate-descent methods, such as SDCA and accelerated proximal SDCA."
neurips,https://proceedings.neurips.cc/paper/2016/file/299a23a2291e2126b91d54f3601ec162-Paper.pdf,Hierarchical Object Representation for Open-Ended Object Category Learning and Recognition,"Seyed Hamidreza Kasaei, Ana Maria Tomé, Luís Seabra Lopes",
neurips,https://proceedings.neurips.cc/paper/2016/file/299fb2142d7de959380f91c01c3a293c-Paper.pdf,Active Learning with Oracle Epiphany,"Tzu-Kuo Huang, Lihong Li, Ara Vartanian, Saleema Amershi, Jerry Zhu",
neurips,https://proceedings.neurips.cc/paper/2016/file/2a27b8144ac02f67687f76782a3b5d8f-Paper.pdf,Stochastic Optimization for Large-scale Optimal Transport,"Aude Genevay, Marco Cuturi, Gabriel Peyré, Francis Bach",
neurips,https://proceedings.neurips.cc/paper/2016/file/2a79ea27c279e471f4d180b08d62b00a-Paper.pdf,The Sound of APALM Clapping: Faster Nonsmooth Nonconvex Optimization with Stochastic Asynchronous PALM,"Damek Davis, Brent Edmunds, Madeleine Udell",
neurips,https://proceedings.neurips.cc/paper/2016/file/2b0f658cbffd284984fb11d90254081f-Paper.pdf,Coresets for Scalable Bayesian Logistic Regression,"Jonathan Huggins, Trevor Campbell, Tamara Broderick",
neurips,https://proceedings.neurips.cc/paper/2016/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf,Sorting out typicality with the inverse moment matrix SOS polynomial,"Edouard Pauwels, Jean B. Lasserre",
neurips,https://proceedings.neurips.cc/paper/2016/file/2ba596643cbbbc20318224181fa46b28-Paper.pdf,The Multi-fidelity Multi-armed Bandit,"Kirthevasan Kandasamy, Gautam Dasarathy, Barnabas Poczos, Jeff Schneider","We study a variant of the classical stochastic
K
K
-armed bandit where observing the outcome of each arm is expensive, but cheap approximations to this outcome are available. For example, in online advertising the performance of an ad can be approximated by displaying it for shorter time periods or to narrower audiences. We formalise this task as a \emph{multi-fidelity} bandit, where, at each time step, the forecaster may choose to play an arm at any one of
M
M
fidelities. The highest fidelity (desired outcome) expends cost
\costM
\costM
. The
m
m
\ssth fidelity (an approximation) expends
\costm
<
\costM
\costm
and returns a biased estimate of the highest fidelity. We develop \mfucb, a novel upper confidence bound procedure for this setting and prove that it naturally adapts to the sequence of available approximations and costs thus attaining better regret than naive strategies which ignore the approximations. For instance, in the above online advertising example, \mfucbs would use the lower fidelities to quickly eliminate suboptimal ads and reserve the larger expensive experiments on a small set of promising candidates. We complement this result with a lower bound and show that \mfucbs is nearly optimal under certain conditions."
neurips,https://proceedings.neurips.cc/paper/2016/file/2bb232c0b13c774965ef8558f0fbd615-Paper.pdf,Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition,Theodore Bluche,
neurips,https://proceedings.neurips.cc/paper/2016/file/2c6ae45a3e88aee548c0714fad7f8269-Paper.pdf,k*-Nearest Neighbors: From Global to Local,"Oren Anava, Kfir Levy",
neurips,https://proceedings.neurips.cc/paper/2016/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf,Protein contact prediction from amino acid co-evolution using convolutional networks for graph-valued images,"Vladimir Golkov, Marcin J. Skwark, Antonij Golkov, Alexey Dosovitskiy, Thomas Brox, Jens Meiler, Daniel Cremers",
neurips,https://proceedings.neurips.cc/paper/2016/file/2d405b367158e3f12d7c1e31a96b3af3-Paper.pdf,Learnable Visual Markers,"Oleg Grinchuk, Vadim Lebedev, Victor Lempitsky",
neurips,https://proceedings.neurips.cc/paper/2016/file/2dea61eed4bceec564a00115c4d21334-Paper.pdf,Finite-Sample Analysis of Fixed-k Nearest Neighbor Density Functional Estimators,"Shashank Singh, Barnabas Poczos",
neurips,https://proceedings.neurips.cc/paper/2016/file/2df45244f09369e16ea3f9117ca45157-Paper.pdf,Maximizing Influence in an Ising Network: A Mean-Field Optimal Solution,"Christopher Lynn, Daniel D. Lee",
neurips,https://proceedings.neurips.cc/paper/2016/file/2dffbc474aa176b6dc957938c15d0c8b-Paper.pdf,Regret Bounds for Non-decomposable Metrics with Missing Labels,"Nagarajan Natarajan, Prateek Jain","We consider the problem of recommending relevant labels (items) for a given data point (user). In particular, we are interested in the practically important setting where the evaluation is with respect to non-decomposable (over labels) performance metrics like the
F
1
F
measure, \emph{and} training data has missing labels. To this end, we propose a generic framework that given a performance metric
Ψ
Ψ
, can devise a regularized objective function and a threshold such that all the values in the predicted score vector above and only above the threshold are selected to be positive. We show that the regret or generalization error in the given metric
Ψ
Ψ
is bounded ultimately by estimation error of certain underlying parameters. In particular, we derive regret bounds under three popular settings: a) collaborative filtering, b) multilabel classification, and c) PU (positive-unlabeled) learning. For each of the above problems, we can obtain precise non-asymptotic regret bound which is small even when a large fraction of labels is missing. Our empirical results on synthetic and benchmark datasets demonstrate that by explicitly modeling for missing labels and optimizing the desired performance metric, our algorithm indeed achieves significantly better performance (like
F
1
F
score) when compared to methods that do not model missing label information carefully."
neurips,https://proceedings.neurips.cc/paper/2016/file/2e65f2f2fdaf6c699b223c61b1b5ab89-Paper.pdf,Adaptive Concentration Inequalities for Sequential Decision Problems,"Shengjia Zhao, Enze Zhou, Ashish Sabharwal, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2016/file/2f37d10131f2a483a8dd005b3d14b0d9-Paper.pdf,Refined Lower Bounds for Adversarial Bandits,"Sébastien Gerchinovitz, Tor Lattimore",
neurips,https://proceedings.neurips.cc/paper/2016/file/2f4fe03d77724a7217006e5d16728874-Paper.pdf,Structure-Blind Signal Recovery,"Dmitry Ostrovsky, Zaid Harchaoui, Anatoli Juditsky, Arkadi S. Nemirovski",
neurips,https://proceedings.neurips.cc/paper/2016/file/2f885d0fbe2e131bfc9d98363e55d1d4-Paper.pdf,Reward Augmented Maximum Likelihood for Neural Structured Prediction,"Mohammad Norouzi, Samy Bengio, zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans",
neurips,https://proceedings.neurips.cc/paper/2016/file/30ef30b64204a3088a26bc2e6ecf7602-Paper.pdf,Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning,"Mehdi Sajjadi, Mehran Javanmardi, Tolga Tasdizen",
neurips,https://proceedings.neurips.cc/paper/2016/file/312351bff07989769097660a56395065-Paper.pdf,An Online Sequence-to-Sequence Model Using Partial Conditioning,"Navdeep Jaitly, Quoc V. Le, Oriol Vinyals, Ilya Sutskever, David Sussillo, Samy Bengio",
neurips,https://proceedings.neurips.cc/paper/2016/file/3147da8ab4a0437c15ef51a5cc7f2dc4-Paper.pdf,"Interaction Networks for Learning about Objects, Relations and Physics","Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, koray kavukcuoglu",
neurips,https://proceedings.neurips.cc/paper/2016/file/31839b036f63806cba3f47b93af8ccb5-Paper.pdf,Bayesian optimization under mixed constraints with a slack-variable augmented Lagrangian,"Victor Picheny, Robert B. Gramacy, Stefan Wild, Sebastien Le Digabel",
neurips,https://proceedings.neurips.cc/paper/2016/file/31857b449c407203749ae32dd0e7d64a-Paper.pdf,Combinatorial Energy Learning for Image Segmentation,"Jeremy B. Maitin-Shepard, Viren Jain, Michal Januszewski, Peter Li, Pieter Abbeel",
neurips,https://proceedings.neurips.cc/paper/2016/file/31fefc0e570cb3860f2a6d4b38c6490d-Paper.pdf,Bayesian Optimization for Probabilistic Programs,"Tom Rainforth, Tuan Anh Le, Jan-Willem van de Meent, Michael A. Osborne, Frank Wood",
neurips,https://proceedings.neurips.cc/paper/2016/file/320722549d1751cf3f247855f937b982-Paper.pdf,Coin Betting and Parameter-Free Online Learning,"Francesco Orabona, David Pal",
neurips,https://proceedings.neurips.cc/paper/2016/file/325995af77a0e8b06d1204a171010b3a-Paper.pdf,Learning Deep Embeddings with Histogram Loss,"Evgeniya Ustinova, Victor Lempitsky",
neurips,https://proceedings.neurips.cc/paper/2016/file/329e6581efbc90bd92a1f22c4ba2103d-Paper.pdf,An Efficient Streaming Algorithm for the Submodular Cover Problem,"Ashkan Norouzi-Fard, Abbas Bazzi, Ilija Bogunovic, Marwa El Halabi, Ya-Ping Hsieh, Volkan Cevher",
neurips,https://proceedings.neurips.cc/paper/2016/file/339a18def9898dd60a634b2ad8fbbd58-Paper.pdf,Fundamental Limits of Budget-Fidelity Trade-off in Label Crowdsourcing,"Farshad Lahouti, Babak Hassibi",
neurips,https://proceedings.neurips.cc/paper/2016/file/33bb83720ba9d2b6da87114380314af5-Paper.pdf,Beyond Exchangeability: The Chinese Voting Process,"Moontae Lee, Seok Hyun Jin, David Mimno",
neurips,https://proceedings.neurips.cc/paper/2016/file/34ed066df378efacc9b924ec161e7639-Paper.pdf,Robust Spectral Detection of Global Structures in the Data by Learning a Regularization,Pan Zhang,
neurips,https://proceedings.neurips.cc/paper/2016/file/352fe25daf686bdb4edca223c921acea-Paper.pdf,Optimal spectral transportation with application to music transcription,"Rémi Flamary, Cédric Févotte, Nicolas Courty, Valentin Emiya",
neurips,https://proceedings.neurips.cc/paper/2016/file/35464c848f410e55a13bb9d78e7fddd0-Paper.pdf,MoCap-guided Data Augmentation for 3D Pose Estimation in the Wild,"Gregory Rogez, Cordelia Schmid",
neurips,https://proceedings.neurips.cc/paper/2016/file/357a6fdf7642bf815a88822c447d9dc4-Paper.pdf,A Constant-Factor Bi-Criteria Approximation Guarantee for k-means++,Dennis Wei,"This paper studies the
k
k
-means++ algorithm for clustering as well as the class of
D
ℓ
D
sampling algorithms to which
k
k
-means++ belongs. It is shown that for any constant factor
β
>
1
β
, selecting
β
k
β
cluster centers by
D
ℓ
D
sampling yields a constant-factor approximation to the optimal clustering with
k
k
centers, in expectation and without conditions on the dataset. This result extends the previously known
O
(
log
k
)
O
guarantee for the case
β
=
1
β
to the constant-factor bi-criteria regime. It also improves upon an existing constant-factor bi-criteria result that holds only with constant probability."
neurips,https://proceedings.neurips.cc/paper/2016/file/3636638817772e42b59d74cff571fbb3-Paper.pdf,CNNpack: Packing Convolutional Neural Networks in the Frequency Domain,"Yunhe Wang, Chang Xu, Shan You, Dacheng Tao, Chao Xu",
neurips,https://proceedings.neurips.cc/paper/2016/file/363763e5c3dc3a68b399058c34aecf2c-Paper.pdf,Feature-distributed sparse regression: a screen-and-clean approach,"Jiyan Yang, Michael W. Mahoney, Michael Saunders, Yuekai Sun",
neurips,https://proceedings.neurips.cc/paper/2016/file/371bce7dc83817b7893bcdeed13799b5-Paper.pdf,Generating Images with Perceptual Similarity Metrics based on Deep Networks,"Alexey Dosovitskiy, Thomas Brox",
neurips,https://proceedings.neurips.cc/paper/2016/file/37bc2f75bf1bcfe8450a1a41c200364c-Paper.pdf,Residual Networks Behave Like Ensembles of Relatively Shallow Networks,"Andreas Veit, Michael J. Wilber, Serge Belongie",
neurips,https://proceedings.neurips.cc/paper/2016/file/3806734b256c27e41ec2c6bffa26d9e7-Paper.pdf,Low-Rank Regression with Tensor Responses,"Guillaume Rabusseau, Hachem Kadri",
neurips,https://proceedings.neurips.cc/paper/2016/file/38651c4450f87348fcbe1f992746a954-Paper.pdf,Provable Efficient Online Matrix Completion via Non-convex Stochastic Gradient Descent,"Chi Jin, Sham M. Kakade, Praneeth Netrapalli",
neurips,https://proceedings.neurips.cc/paper/2016/file/3875115bacc48cca24ac51ee4b0e7975-Paper.pdf,Local Maxima in the Likelihood of Gaussian Mixture Models: Structural Results and Algorithmic Consequences,"Chi Jin, Yuchen Zhang, Sivaraman Balakrishnan, Martin J. Wainwright, Michael I. Jordan","We provide two fundamental results on the population (infinite-sample) likelihood function of Gaussian mixture models with
M
≥
3
M
components. Our first main result shows that the population likelihood function has bad local maxima even in the special case of equally-weighted mixtures of well-separated and spherical Gaussians. We prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of Srebro (2007). Our second main result shows that the EM algorithm (or a first-order variant of it) with random initialization will converge to bad critical points with probability at least
1
−
e
−
Ω
(
M
)
1
. We further establish that a first-order variant of EM will not converge to strict saddle points almost surely, indicating that the poor performance of the first-order method can be attributed to the existence of bad local maxima rather than bad saddle points. Overall, our results highlight the necessity of careful initialization when using the EM algorithm in practice, even when applied in highly favorable settings."
neurips,https://proceedings.neurips.cc/paper/2016/file/390e982518a50e280d8e2b535462ec1f-Paper.pdf,Diffusion-Convolutional Neural Networks,"James Atwood, Don Towsley",
neurips,https://proceedings.neurips.cc/paper/2016/file/3937230de3c8041e4da6ac3246a888e8-Paper.pdf,Completely random measures for modelling block-structured sparse networks,"Tue Herlau, Mikkel N. Schmidt, Morten Mørup",
neurips,https://proceedings.neurips.cc/paper/2016/file/3948ead63a9f2944218de038d8934305-Paper.pdf,Pruning Random Forests for Prediction on a Budget,"Feng Nan, Joseph Wang, Venkatesh Saligrama",
neurips,https://proceedings.neurips.cc/paper/2016/file/3b5dca501ee1e6d8cd7b905f4e1bf723-Paper.pdf,Synthesis of MCMC and Belief Propagation,"Sung-Soo Ahn, Michael Chertkov, Jinwoo Shin",
neurips,https://proceedings.neurips.cc/paper/2016/file/3b92d18aa7a6176dd37d372bc2f1eb71-Paper.pdf,Neurons Equipped with Intrinsic Plasticity Learn Stimulus Intensity Statistics,"Travis Monk, Cristina Savin, Jörg Lücke",
neurips,https://proceedings.neurips.cc/paper/2016/file/3baa271bc35fe054c86928f7016e8ae6-Paper.pdf,Disease Trajectory Maps,"Peter Schulam, Raman Arora",
neurips,https://proceedings.neurips.cc/paper/2016/file/3bbfdde8842a5c44a0323518eec97cbe-Paper.pdf,Bayesian optimization for automated model selection,"Gustavo Malkomes, Charles Schaff, Roman Garnett",
neurips,https://proceedings.neurips.cc/paper/2016/file/3c1e4bd67169b8153e0047536c9f541e-Paper.pdf,Designing smoothing functions for improved worst-case competitive ratio in online optimization,"Reza Eghbali, Maryam Fazel","Online optimization covers problems such as online resource allocation, online bipartite matching, adwords (a central problem in e-commerce and advertising), and adwords with separable concave returns. We analyze the worst case competitive ratio of two primal-dual algorithms for a class of online convex (conic) optimization problems that contains the previous examples as special cases defined on the positive orthant. We derive a sufficient condition on the objective function that guarantees a constant worst case competitive ratio (greater than or equal to
1
2
1
) for monotone objective functions. We provide new examples of online problems on the positive orthant % and the positive semidefinite cone that satisfy the sufficient condition. We show how smoothing can improve the competitive ratio of these algorithms, and in particular for separable functions, we show that the optimal smoothing can be derived by solving a convex optimization problem. This result allows us to directly optimize the competitive ratio bound over a class of smoothing functions, and hence design effective smoothing customized for a given cost function."
neurips,https://proceedings.neurips.cc/paper/2016/file/3cef96dcc9b8035d23f69e30bb19218a-Paper.pdf,Towards Unifying Hamiltonian Monte Carlo and Slice Sampling,"Yizhe Zhang, Xiangyu Wang, Changyou Chen, Ricardo Henao, Kai Fan, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2016/file/3cf2559725a9fdfa602ec8c887440f32-Paper.pdf,Multi-step learning and underlying structure in statistical models,Maia Fraser,"In multi-step learning, where a final learning task is accomplished via a sequence of intermediate learning tasks, the intuition is that successive steps or levels transform the initial data into representations more and more
suited"" to the final learning task. A related principle arises in transfer-learning where Baxter (2000) proposed a theoretical framework to study how learning multiple tasks transforms the inductive bias of a learner. The most widespread multi-step learning approach is semi-supervised learning with two steps: unsupervised, then supervised. Several authors (Castelli-Cover, 1996; Balcan-Blum, 2005; Niyogi, 2008; Ben-David et al, 2008; Urner et al, 2011) have analyzed SSL, with Balcan-Blum (2005) proposing a version of the PAC learning framework augmented by a
compatibility function"" to link concept class and unlabeled data distribution. We propose to analyze SSL and other multi-step learning approaches, much in the spirit of Baxter's framework, by defining a learning problem generatively as a joint statistical model on
X
×
Y
X
. This determines in a natural way the class of conditional distributions that are possible with each marginal, and amounts to an abstract form of compatibility function. It also allows to analyze both discrete and non-discrete settings. As tool for our analysis, we define a notion of
γ
γ
-uniform shattering for statistical models. We use this to give conditions on the marginal and conditional models which imply an advantage for multi-step learning approaches. In particular, we recover a more general version of a result of Poggio et al (2012): under mild hypotheses a multi-step approach which learns features invariant under successive factors of a finite group of invariances has sample complexity requirements that are additive rather than multiplicative in the size of the subgroups."
neurips,https://proceedings.neurips.cc/paper/2016/file/3de2334a314a7a72721f1f74a6cb4cee-Paper.pdf,The non-convex Burer-Monteiro approach works on smooth semidefinite programs,"Nicolas Boumal, Vlad Voroninski, Afonso Bandeira",
neurips,https://proceedings.neurips.cc/paper/2016/file/3def184ad8f4755ff269862ea77393dd-Paper.pdf,Minimizing Regret on Reflexive Banach Spaces and Nash Equilibria in Continuous Zero-Sum Games,"Maximilian Balandat, Walid Krichene, Claire Tomlin, Alexandre Bayen",
neurips,https://proceedings.neurips.cc/paper/2016/file/3e7e0224018ab3cf51abb96464d518cd-Paper.pdf,Spatiotemporal Residual Networks for Video Action Recognition,"Christoph Feichtenhofer, Axel Pinz, Richard Wildes",
neurips,https://proceedings.neurips.cc/paper/2016/file/3fab5890d8113d0b5a4178201dc842ad-Paper.pdf,Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes,"Jack Rae, Jonathan J. Hunt, Ivo Danihelka, Timothy Harley, Andrew W. Senior, Gregory Wayne, Alex Graves, Timothy Lillicrap","Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows --- limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs
1
,
000
×
1
faster and with
3
,
000
×
3
less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring
100
,
000
100
s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer."
neurips,https://proceedings.neurips.cc/paper/2016/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf,Neurally-Guided Procedural Models: Amortized Inference for Procedural Graphics Programs using Neural Networks,"Daniel Ritchie, Anna Thomas, Pat Hanrahan, Noah Goodman",
neurips,https://proceedings.neurips.cc/paper/2016/file/404dcc91b2aeaa7caa47487d1483e48a-Paper.pdf,Reconstructing Parameters of Spreading Models from Partial Observations,Andrey Lokhov,
neurips,https://proceedings.neurips.cc/paper/2016/file/405e28906322882c5be9b4b27f4c35fd-Paper.pdf,Tracking the Best Expert in Non-stationary Stochastic Environments,"Chen-Yu Wei, Yi-Te Hong, Chi-Jen Lu","We study the dynamic regret of multi-armed bandit and experts problem in non-stationary stochastic environments. We introduce a new parameter
\W
\W
, which measures the total statistical variance of the loss distributions over
T
T
rounds of the process, and study how this amount affects the regret. We investigate the interaction between
\W
\W
and
Γ
Γ
, which counts the number of times the distributions change, as well as
\W
\W
and
V
V
, which measures how far the distributions deviates over time. One striking result we find is that even when
Γ
Γ
,
V
V
, and
Λ
Λ
are all restricted to constant, the regret lower bound in the bandit setting still grows with
T
T
. The other highlight is that in the full-information setting, a constant regret becomes achievable with constant
Γ
Γ
and
Λ
Λ
, as it can be made independent of
T
T
, while with constant
V
V
and
Λ
Λ
, the regret still has a
T
1
/
3
T
dependency. We not only propose algorithms with upper bound guarantee, but prove their matching lower bounds as well."
neurips,https://proceedings.neurips.cc/paper/2016/file/411ae1bf081d1674ca6091f8c59a266f-Paper.pdf,Statistical Inference for Pairwise Graphical Models Using Score Matching,"Ming Yu, Mladen Kolar, Varun Gupta","Probabilistic graphical models have been widely used to model complex systems and aid scientific discoveries. As a result, there is a large body of literature focused on consistent model selection. However, scientists are often interested in understanding uncertainty associated with the estimated parameters, which current literature has not addressed thoroughly. In this paper, we propose a novel estimator for edge parameters for pairwise graphical models based on Hyv\""arinen scoring rule. Hyv\""arinen scoring rule is especially useful in cases where the normalizing constant cannot be obtained efficiently in a closed form. We prove that the estimator is
√
n
n
-consistent and asymptotically Normal. This result allows us to construct confidence intervals for edge parameters, as well as, hypothesis tests. We establish our results under conditions that are typically assumed in the literature for consistent estimation. However, we do not require that the estimator consistently recovers the graph structure. In particular, we prove that the asymptotic distribution of the estimator is robust to model selection mistakes and uniformly valid for a large number of data-generating processes. We illustrate validity of our estimator through extensive simulation studies."
neurips,https://proceedings.neurips.cc/paper/2016/file/41bfd20a38bb1b0bec75acf0845530a7-Paper.pdf,Learning Structured Sparsity in Deep Neural Networks,"Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li",
neurips,https://proceedings.neurips.cc/paper/2016/file/42998cf32d552343bc8e460416382dca-Paper.pdf,Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis,"Weiran Wang, Jialei Wang, Dan Garber, Dan Garber, Nati Srebro",
neurips,https://proceedings.neurips.cc/paper/2016/file/42e77b63637ab381e8be5f8318cc28a2-Paper.pdf,How Deep is the Feature Analysis underlying Rapid Visual Categorization?,"Sven Eberhardt, Jonah G. Cader, Thomas Serre",
neurips,https://proceedings.neurips.cc/paper/2016/file/430c3626b879b4005d41b8a46172e0c0-Paper.pdf,Regret of Queueing Bandits,"Subhashini Krishnasamy, Rajat Sen, Ramesh Johari, Sanjay Shakkottai",
neurips,https://proceedings.neurips.cc/paper/2016/file/43351f7bf9a215be70c2c2caa7555002-Paper.pdf,Dual Space Gradient Descent for Online Learning,"Trung Le, Tu Nguyen, Vu Nguyen, Dinh Phung",
neurips,https://proceedings.neurips.cc/paper/2016/file/43e4e6a6f341e00671e123714de019a8-Paper.pdf,Asynchronous Parallel Greedy Coordinate Descent,"Yang You, Xiangru Lian, Ji Liu, Hsiang-Fu Yu, Inderjit S. Dhillon, James Demmel, Cho-Jui Hsieh",
neurips,https://proceedings.neurips.cc/paper/2016/file/43fa7f58b7eac7ac872209342e62e8f1-Paper.pdf,Catching heuristics are optimal control policies,"Boris Belousov, Gerhard Neumann, Constantin A. Rothkopf, Jan R. Peters",
neurips,https://proceedings.neurips.cc/paper/2016/file/44968aece94f667e4095002d140b5896-Paper.pdf,Online Pricing with Strategic and Patient Buyers,"Michal Feldman, Tomer Koren, Roi Livni, Yishay Mansour, Aviv Zohar","We consider a seller with an unlimited supply of a single good, who is faced with a stream of
T
T
buyers. Each buyer has a window of time in which she would like to purchase, and would buy at the lowest price in that window, provided that this price is lower than her private value (and otherwise, would not buy at all). In this setting, we give an algorithm that attains
O
(
T
2
/
3
)
O
regret over any sequence of
T
T
buyers with respect to the best fixed price in hindsight, and prove that no algorithm can perform better in the worst case."
neurips,https://proceedings.neurips.cc/paper/2016/file/44f683a84163b3523afe57c2e008bc8c-Paper.pdf,Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling,"Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, Josh Tenenbaum",
neurips,https://proceedings.neurips.cc/paper/2016/file/452bf208bf901322968557227b8f6efe-Paper.pdf,Optimistic Gittins Indices,"Eli Gutin, Vivek Farias",
neurips,https://proceedings.neurips.cc/paper/2016/file/4588e674d3f0faf985047d4c3f13ed0d-Paper.pdf,Stochastic Gradient Methods for Distributionally Robust Optimization with f-divergences,"Hongseok Namkoong, John C. Duchi",
neurips,https://proceedings.neurips.cc/paper/2016/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf,Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation,"Weihao Gao, Sewoong Oh, Pramod Viswanath","Estimators of information theoretic measures such as entropy and mutual information from samples are a basic workhorse for many downstream applications in modern data science. State of the art approaches have been either geometric (nearest neighbor (NN) based) or kernel based (with bandwidth chosen to be data independent and vanishing sub linearly in the sample size). In this paper we combine both these approaches to design new estimators of entropy and mutual information that strongly outperform all state of the art methods. Our estimator uses bandwidth choice of fixed
k
k
-NN distances; such a choice is both data dependent and linearly vanishing in the sample size and necessitates a bias cancellation term that is universal and independent of the underlying distribution. As a byproduct, we obtain a unified way of obtaining both kernel and NN estimators. The corresponding theoretical contribution relating the geometry of NN distances to asymptotic order statistics is of independent mathematical interest."
neurips,https://proceedings.neurips.cc/paper/2016/file/45fbc6d3e05ebd93369ce542e8f2322d-Paper.pdf,Domain Separation Networks,"Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, Dumitru Erhan",
neurips,https://proceedings.neurips.cc/paper/2016/file/46072631582fc240dd2674a7d063b040-Paper.pdf,A Probabilistic Programming Approach To Probabilistic Data Analysis,"Feras Saad, Vikash K. Mansinghka",
neurips,https://proceedings.neurips.cc/paper/2016/file/466accbac9a66b805ba50e42ad715740-Paper.pdf,Assortment Optimization Under the Mallows model,"Antoine Desir, Vineet Goyal, Srikanth Jagabathula, Danny Segev",
neurips,https://proceedings.neurips.cc/paper/2016/file/46922a0880a8f11f8f69cbb52b1396be-Paper.pdf,An algorithm for L1 nearest neighbor search via monotonic embedding,"Xinan Wang, Sanjoy Dasgupta",
neurips,https://proceedings.neurips.cc/paper/2016/file/47d1e990583c9c67424d369f3414728e-Paper.pdf,Multi-armed Bandits: Competing with Optimal Sequences,"Zohar S. Karnin, Oren Anava",
neurips,https://proceedings.neurips.cc/paper/2016/file/495dabfd0ca768a3c3abd672079f48b6-Paper.pdf,NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed and Stochastic Optimization,"Davood Hajinezhad, Mingyi Hong, Tuo Zhao, Zhaoran Wang","We study a stochastic and distributed algorithm for nonconvex problems whose objective consists a sum
N
N
nonconvex
L
i
/
N
L
-smooth functions, plus a nonsmooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT) algorithm splits the problem into
N
N
subproblems, and utilizes an augmented Lagrangian based primal-dual scheme to solve it in a distributed and stochastic manner. With a special non-uniform sampling, a version of NESTT achieves
ϵ
ϵ
-stationary solution using
O
(
(
∑
N
i
=
1
√
L
i
/
N
)
2
/
ϵ
)
O
gradient evaluations, which can be up to
O
(
N
)
O
times better than the (proximal) gradient descent methods. It also achieves Q-linear convergence rate for nonconvex
ℓ
1
ℓ
penalized quadratic problems with polyhedral constraints. Further, we reveal a fundamental connection between {\it primal-dual} based methods and a few {\it primal only} methods such as IAG/SAG/SAGA."
neurips,https://proceedings.neurips.cc/paper/2016/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf,Probing the Compositionality of Intuitive Functions,"Eric Schulz, Josh Tenenbaum, David K. Duvenaud, Maarten Speekenbrink, Samuel J. Gershman",
neurips,https://proceedings.neurips.cc/paper/2016/file/49c9adb18e44be0711a94e827042f630-Paper.pdf,Identification and Overidentification of Linear Structural Equation Models,Bryant Chen,
neurips,https://proceedings.neurips.cc/paper/2016/file/49d4b2faeb4b7b9e745775793141e2b2-Paper.pdf,"An Architecture for Deep, Hierarchical Generative Models",Philip Bachman,
neurips,https://proceedings.neurips.cc/paper/2016/file/4abe17a1c80cbdd2aa241b70840879de-Paper.pdf,Towards Conceptual Compression,"Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, Daan Wierstra",
neurips,https://proceedings.neurips.cc/paper/2016/file/4b0250793549726d5c1ea3906726ebfe-Paper.pdf,Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters,"Zeyuan Allen-Zhu, Yang Yuan, Karthik Sridharan",
neurips,https://proceedings.neurips.cc/paper/2016/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf,Consistent Kernel Mean Estimation for Functions of Random Variables,"Carl-Johann Simon-Gabriel, Adam Scibior, Ilya O. Tolstikhin, Bernhard Schölkopf",
neurips,https://proceedings.neurips.cc/paper/2016/file/4d2e7bd33c475784381a64e43e50922f-Paper.pdf,Hierarchical Clustering via Spreading Metrics,"Aurko Roy, Sebastian Pokutta",
neurips,https://proceedings.neurips.cc/paper/2016/file/4dcf435435894a4d0972046fc566af76-Paper.pdf,Combining Fully Convolutional and Recurrent Neural Networks for 3D Biomedical Image Segmentation,"Jianxu Chen, Lin Yang, Yizhe Zhang, Mark Alber, Danny Z. Chen",
neurips,https://proceedings.neurips.cc/paper/2016/file/4de754248c196c85ee4fbdcee89179bd-Paper.pdf,SDP Relaxation with Randomized Rounding for Energy Disaggregation,"Kiarash Shaloudegi, András György, Csaba Szepesvari, Wilsun Xu",
neurips,https://proceedings.neurips.cc/paper/2016/file/4e0d67e54ad6626e957d15b08ae128a6-Paper.pdf,Finite Sample Prediction and Recovery Bounds for Ordinal Embedding,"Lalit Jain, Kevin G. Jamieson, Rob Nowak","The goal of ordinal embedding is to represent items as points in a low-dimensional Euclidean space given a set of constraints like
item
i
i
is closer to item
j
j
than item
k
k
''. Ordinal constraints like this often come from human judgments. The classic approach to solving this problem is known as non-metric multidimensional scaling. To account for errors and variation in judgments, we consider the noisy situation in which the given constraints are independently corrupted by reversing the correct constraint with some probability. The ordinal embedding problem has been studied for decades, but most past work pays little attention to the question of whether accurate embedding is possible, apart from empirical studies. This paper shows that under a generative data model it is possible to learn the correct embedding from noisy distance comparisons. In establishing this fundamental result, the paper makes several new contributions. First, we derive prediction error bounds for embedding from noisy distance comparisons by exploiting the fact that the rank of a distance matrix of points in
\R
d
\R
is at most
d
+
2
d
. These bounds characterize how well a learned embedding predicts new comparative judgments. Second, we show that the underlying embedding can be recovered by solving a simple convex optimization. This result is highly non-trivial since we show that the linear map corresponding to distance comparisons is non-invertible, but there exists a nonlinear map that is invertible. Third, two new algorithms for ordinal embedding are proposed and evaluated in experiments."
neurips,https://proceedings.neurips.cc/paper/2016/file/4f398cb9d6bc79ae567298335b51ba8a-Paper.pdf,Search Improves Label for Active Learning,"Alina Beygelzimer, Daniel J. Hsu, John Langford, Chicheng Zhang",
neurips,https://proceedings.neurips.cc/paper/2016/file/4f6ffe13a5d75b2d6a3923922b3922e5-Paper.pdf,A Simple Practical Accelerated Method for Finite Sums,Aaron Defazio,
neurips,https://proceedings.neurips.cc/paper/2016/file/502e4a16930e414107ee22b6198c578f-Paper.pdf,Coupled Generative Adversarial Networks,"Ming-Yu Liu, Oncel Tuzel",
neurips,https://proceedings.neurips.cc/paper/2016/file/5055cbf43fac3f7e2336b27310f0b9ef-Paper.pdf,Minimax Estimation of Maximum Mean Discrepancy with Radial Kernels,"Ilya O. Tolstikhin, Bharath K. Sriperumbudur, Bernhard Schölkopf","Maximum Mean Discrepancy (MMD) is a distance on the space of probability measures which has found numerous applications in machine learning and nonparametric testing. This distance is based on the notion of embedding probabilities in a reproducing kernel Hilbert space. In this paper, we present the first known lower bounds for the estimation of MMD based on finite samples. Our lower bounds hold for any radial universal kernel on
\R
d
\R
and match the existing upper bounds up to constants that depend only on the properties of the kernel. Using these lower bounds, we establish the minimax rate optimality of the empirical estimator and its
U
U
-statistic variant, which are usually employed in applications."
neurips,https://proceedings.neurips.cc/paper/2016/file/512c5cad6c37edb98ae91c8a76c3a291-Paper.pdf,Using Social Dynamics to Make Individual Predictions: Variational Inference with a Stochastic Kinetic Model,"Zhen Xu, Wen Dong, Sargur N. Srihari",
neurips,https://proceedings.neurips.cc/paper/2016/file/51ef186e18dc00c2d31982567235c559-Paper.pdf,Multiple-Play Bandits in the Position-Based Model,"Paul Lagrée, Claire Vernade, Olivier Cappe",
neurips,https://proceedings.neurips.cc/paper/2016/file/5227b6aaf294f5f027273aebf16015f2-Paper.pdf,Learning values across many orders of magnitude,"Hado P. van Hasselt, Arthur Guez, Arthur Guez, Matteo Hessel, Volodymyr Mnih, David Silver",
neurips,https://proceedings.neurips.cc/paper/2016/file/52947e0ade57a09e4a1386d08f17b656-Paper.pdf,"Attend, Infer, Repeat: Fast Scene Understanding with Generative Models","S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, koray kavukcuoglu, Geoffrey E. Hinton",
neurips,https://proceedings.neurips.cc/paper/2016/file/5314b9674c86e3f9d1ba25ef9bb32895-Paper.pdf,Supervised Learning with Tensor Networks,"Edwin Stoudenmire, David J. Schwab",
neurips,https://proceedings.neurips.cc/paper/2016/file/535ab76633d94208236a2e829ea6d888-Paper.pdf,Structured Prediction Theory Based on Factor Graph Complexity,"Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, Scott Yang",
neurips,https://proceedings.neurips.cc/paper/2016/file/537de305e941fccdbba5627e3eefbb24-Paper.pdf,The Multiple Quantile Graphical Model,"Alnur Ali, J. Zico Kolter, Ryan J. Tibshirani",
neurips,https://proceedings.neurips.cc/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf,Orthogonal Random Features,"Felix Xinnan X. Yu, Ananda Theertha Suresh, Krzysztof M. Choromanski, Daniel N. Holtmann-Rice, Sanjiv Kumar","We present an intriguing discovery related to Random Fourier Features: replacing multiplication by a random Gaussian matrix with multiplication by a properly scaled random orthogonal matrix significantly decreases kernel approximation error. We call this technique Orthogonal Random Features (ORF), and provide theoretical and empirical justification for its effectiveness. Motivated by the discovery, we further propose Structured Orthogonal Random Features (SORF), which uses a class of structured discrete orthogonal matrices to speed up the computation. The method reduces the time cost from
O
(
d
2
)
O
to
O
(
d
log
d
)
O
, where
d
d
is the data dimensionality, with almost no compromise in kernel approximation quality compared to ORF. Experiments on several datasets verify the effectiveness of ORF and SORF over the existing methods. We also provide discussions on using the same type of discrete orthogonal structure for a broader range of kernels and applications."
neurips,https://proceedings.neurips.cc/paper/2016/file/53ed35c74a2ec275b837374f04396c03-Paper.pdf,Coevolutionary Latent Feature Processes for Continuous-Time User-Item Interactions,"Yichen Wang, Nan Du, Rakshit Trivedi, Le Song",
neurips,https://proceedings.neurips.cc/paper/2016/file/5487315b1286f907165907aa8fc96619-Paper.pdf,Convex Two-Layer Modeling with Latent Structure,"Vignesh Ganapathiraman, Xinhua Zhang, Yaoliang Yu, Junfeng Wen",
neurips,https://proceedings.neurips.cc/paper/2016/file/550a141f12de6341fba65b0ad0433500-Paper.pdf,Online Convex Optimization with Unconstrained Domains and Losses,"Ashok Cutkosky, Kwabena A. Boahen",
neurips,https://proceedings.neurips.cc/paper/2016/file/555d6702c950ecb729a966504af0a635-Paper.pdf,GAP Safe Screening Rules for Sparse-Group Lasso,"Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, Joseph Salmon","For statistical learning in high dimension, sparse regularizations have proven useful to boost both computational and statistical efficiency. In some contexts, it is natural to handle more refined structures than pure sparsity, such as for instance group sparsity. Sparse-Group Lasso has recently been introduced in the context of linear regression to enforce sparsity both at the feature and at the group level. We propose the first (provably) safe screening rules for Sparse-Group Lasso, i.e., rules that allow to discard early in the solver features/groups that are inactive at optimal solution. Thanks to efficient dual gap computations relying on the geometric properties of
ϵ
ϵ
-norm, safe screening rules for Sparse-Group Lasso lead to significant gains in term of computing time for our coordinate descent implementation."
neurips,https://proceedings.neurips.cc/paper/2016/file/556f391937dfd4398cbac35e050a2177-Paper.pdf,Local Similarity-Aware Deep Feature Embedding,"Chen Huang, Chen Change Loy, Xiaoou Tang",
neurips,https://proceedings.neurips.cc/paper/2016/file/55a988dfb00a914717b3000a3374694c-Paper.pdf,Following the Leader and Fast Rates in Linear Prediction: Curved Constraint Sets and Other Regularities,"Ruitong Huang, Tor Lattimore, András György, Csaba Szepesvari",
neurips,https://proceedings.neurips.cc/paper/2016/file/55b1927fdafef39c48e5b73b5d61ea60-Paper.pdf,Learning Multiagent Communication with Backpropagation,"Sainbayar Sukhbaatar, arthur szlam, Rob Fergus",
neurips,https://proceedings.neurips.cc/paper/2016/file/55c567fd4395ecef6d936cf77b8d5b2b-Paper.pdf,Sub-sampled Newton Methods with Non-uniform Sampling,"Peng Xu, Jiyan Yang, Fred Roosta, Christopher Ré, Michael W. Mahoney","We consider the problem of finding the minimizer of a convex function
F
:
R
d
→
R
F
of the form
F
(
w
)
\defeq
∑
n
i
=
1
f
i
(
w
)
+
R
(
w
)
F
where a low-rank factorization of
∇
2
f
i
(
w
)
∇
is readily available.We consider the regime where
n
≫
d
n
. We propose randomized Newton-type algorithms that exploit \textit{non-uniform} sub-sampling of
{
∇
2
f
i
(
w
)
}
n
i
=
1
{
, as well as inexact updates, as means to reduce the computational complexity, and are applicable to a wide range of problems in machine learning. Two non-uniform sampling distributions based on {\it block norm squares} and {\it block partial leverage scores} are considered. Under certain assumptions, we show that our algorithms inherit a linear-quadratic convergence rate in
w
w
and achieve a lower computational complexity compared to similar existing methods. In addition, we show that our algorithms exhibit more robustness and better dependence on problem specific quantities, such as the condition number. We numerically demonstrate the advantages of our algorithms on several real datasets."
neurips,https://proceedings.neurips.cc/paper/2016/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf,"Examples are not enough, learn to criticize! Criticism for Interpretability","Been Kim, Rajiv Khanna, Oluwasanmi O. Koyejo",
neurips,https://proceedings.neurips.cc/paper/2016/file/577ef1154f3240ad5b9b413aa7346a1e-Paper.pdf,R-FCN: Object Detection via Region-based Fully Convolutional Networks,"Jifeng Dai, Yi Li, Kaiming He, Jian Sun",
neurips,https://proceedings.neurips.cc/paper/2016/file/57bafb2c2dfeefba931bb03a835b1fa9-Paper.pdf,Exploiting Tradeoffs for Exact Recovery in Heterogeneous Stochastic Block Models,"Amin Jalali, Qiyang Han, Ioana Dumitriu, Maryam Fazel",
neurips,https://proceedings.neurips.cc/paper/2016/file/58238e9ae2dd305d79c2ebc8c1883422-Paper.pdf,A Powerful Generative Model Using Random Weights for the Deep Image Representation,"Kun He, Yan Wang, John Hopcroft",
neurips,https://proceedings.neurips.cc/paper/2016/file/58c54802a9fb9526cd0923353a34a7ae-Paper.pdf,Privacy Odometers and Filters: Pay-as-you-Go Composition,"Ryan M. Rogers, Aaron Roth, Jonathan Ullman, Salil Vadhan",
neurips,https://proceedings.neurips.cc/paper/2016/file/598920e11d1eb2a49501d59fce5ecbb7-Paper.pdf,"More Supervision, Less Computation: Statistical-Computational Tradeoffs in Weakly Supervised Learning","Xinyang Yi, Zhaoran Wang, Zhuoran Yang, Constantine Caramanis, Han Liu","We consider the weakly supervised binary classification problem where the labels are randomly flipped with probability
1
−
α
1
. Although there exist numerous algorithms for this problem, it remains theoretically unexplored how the statistical accuracies and computational efficiency of these algorithms depend on the degree of supervision, which is quantified by
α
α
. In this paper, we characterize the effect of
α
α
by establishing the information-theoretic and computational boundaries, namely, the minimax-optimal statistical accuracy that can be achieved by all algorithms, and polynomial-time algorithms under an oracle computational model. For small
α
α
, our result shows a gap between these two boundaries, which represents the computational price of achieving the information-theoretic boundary due to the lack of supervision. Interestingly, we also show that this gap narrows as
α
α
increases. In other words, having more supervision, i.e., more correct labels, not only improves the optimal statistical accuracy as expected, but also enhances the computational efficiency for achieving such accuracy."
neurips,https://proceedings.neurips.cc/paper/2016/file/59f51fd6937412b7e56ded1ea2470c25-Paper.pdf,Supervised learning through the lens of compression,"Ofir David, Shay Moran, Amir Yehudayoff",
neurips,https://proceedings.neurips.cc/paper/2016/file/5a1e3a5aede16d438c38862cac1a78db-Paper.pdf,Sparse Support Recovery with Non-smooth Loss Functions,"Kévin Degraux, Gabriel Peyré, Jalal Fadili, Laurent Jacques","In this paper, we study the support recovery guarantees of underdetermined sparse regression using the
ℓ
1
ℓ
-norm as a regularizer and a non-smooth loss function for data fidelity. More precisely, we focus in detail on the cases of
ℓ
1
ℓ
and
ℓ
∞
ℓ
losses, and contrast them with the usual
ℓ
2
ℓ
loss.While these losses are routinely used to account for either sparse (
ℓ
1
ℓ
loss) or uniform (
ℓ
∞
ℓ
loss) noise models, a theoretical analysis of their performance is still lacking. In this article, we extend the existing theory from the smooth
ℓ
2
ℓ
case to these non-smooth cases. We derive a sharp condition which ensures that the support of the vector to recover is stable to small additive noise in the observations, as long as the loss constraint size is tuned proportionally to the noise level. A distinctive feature of our theory is that it also explains what happens when the support is unstable. While the support is not stable anymore, we identify an ""extended support"" and show that this extended support is stable to small additive noise. To exemplify the usefulness of our theory, we give a detailed numerical analysis of the support stability/instability of compressed sensing recovery with these different losses. This highlights different parameter regimes, ranging from total support stability to progressively increasing support instability."
neurips,https://proceedings.neurips.cc/paper/2016/file/5a7f963e5e0504740c3a6b10bb6d4fa5-Paper.pdf,Tractable Operations for Arithmetic Circuits of Probabilistic Models,"Yujia Shen, Arthur Choi, Adnan Darwiche",
neurips,https://proceedings.neurips.cc/paper/2016/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf,Dual Learning for Machine Translation,"Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, Wei-Ying Ma","While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation \emph{dual-NMT}. Experiments show that dual-NMT works very well on English
↔
↔
French translation; especially, by learning from monolingual data (with 10\% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task."
neurips,https://proceedings.neurips.cc/paper/2016/file/5b8add2a5d98b1a652ea7fd72d942dac-Paper.pdf,Solving Random Systems of Quadratic Equations via Truncated Generalized Gradient Flow,"Gang Wang, Georgios Giannakis","This paper puts forth a novel algorithm, termed \emph{truncated generalized gradient flow} (TGGF), to solve for
\bm
x
∈
R
n
/
C
n
\bm
a system of
m
m
quadratic equations
y
i
=
|
⟨
\bm
a
i
,
\bm
x
⟩
|
2
y
,
i
=
1
,
2
,
…
,
m
i
, which even for
{
\bm
a
i
∈
R
n
/
C
n
}
m
i
=
1
{
random is known to be \emph{NP-hard} in general. We prove that as soon as the number of equations
m
m
is on the order of the number of unknowns
n
n
, TGGF recovers the solution exactly (up to a global unimodular constant) with high probability and complexity growing linearly with the time required to read the data
{
(
\bm
a
i
;
y
i
)
}
m
i
=
1
{
. Specifically, TGGF proceeds in two stages: s1) A novel \emph{orthogonality-promoting} initialization that is obtained with simple power iterations; and, s2) a refinement of the initial estimate by successive updates of scalable \emph{truncated generalized gradient iterations}. The former is in sharp contrast to the existing spectral initializations, while the latter handles the rather challenging nonconvex and nonsmooth \emph{amplitude-based} cost function. Numerical tests demonstrate that: i) The novel orthogonality-promoting initialization method returns more accurate and robust estimates relative to its spectral counterparts; and ii) even with the same initialization, our refinement/truncation outperforms Wirtinger-based alternatives, all corroborating the superior performance of TGGF over state-of-the-art algorithms."
neurips,https://proceedings.neurips.cc/paper/2016/file/5bce843dd76db8c939d5323dd3e54ec9-Paper.pdf,Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences,"Daniel Neil, Michael Pfeiffer, Shih-Chii Liu",
neurips,https://proceedings.neurips.cc/paper/2016/file/5d616dd38211ebb5d6ec52986674b6e4-Paper.pdf,Only H is left: Near-tight Episodic PAC RL,,
neurips,https://proceedings.neurips.cc/paper/2016/file/5d6646aad9bcc0be55b2c82f69750387-Paper.pdf,Stochastic Three-Composite Convex Minimization,"Alp Yurtsever, Bang Cong Vu, Volkan Cevher",
neurips,https://proceedings.neurips.cc/paper/2016/file/5d79099fcdf499f12b79770834c0164a-Paper.pdf,Synthesizing the preferred inputs for neurons in neural networks via deep generator networks,"Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, Jeff Clune",
neurips,https://proceedings.neurips.cc/paper/2016/file/5ea1649a31336092c05438df996a3e59-Paper.pdf,Bayesian Optimization with a Finite Budget: An Approximate Dynamic Programming Approach,"Remi Lam, Karen Willcox, David H. Wolpert",
neurips,https://proceedings.neurips.cc/paper/2016/file/605ff764c617d3cd28dbbdd72be8f9a2-Paper.pdf,Gaussian Process Bandit Optimisation with Multi-fidelity Evaluations,"Kirthevasan Kandasamy, Gautam Dasarathy, Junier B. Oliva, Jeff Schneider, Barnabas Poczos","In many scientific and engineering applications, we are tasked with the optimisation of an expensive to evaluate black box function
\func
\func
. Traditional methods for this problem assume just the availability of this single function. However, in many cases, cheap approximations to
\func
\func
may be obtainable. For example, the expensive real world behaviour of a robot can be approximated by a cheap computer simulation. We can use these approximations to eliminate low function value regions cheaply and use the expensive evaluations of
\func
\func
in a small but promising region and speedily identify the optimum. We formalise this task as a \emph{multi-fidelity} bandit problem where the target function and its approximations are sampled from a Gaussian process. We develop \mfgpucb, a novel method based on upper confidence bound techniques. In our theoretical analysis we demonstrate that it exhibits precisely the above behaviour, and achieves better regret than strategies which ignore multi-fidelity information. \mfgpucbs outperforms such naive strategies and other multi-fidelity methods on several synthetic and real experiments."
neurips,https://proceedings.neurips.cc/paper/2016/file/619205da514e83f869515c782a328d3c-Paper.pdf,Learning Parametric Sparse Models for Image Super-Resolution,"Yongbo Li, Weisheng Dong, Xuemei Xie, GUANGMING Shi, Xin Li, Donglai Xu",
neurips,https://proceedings.neurips.cc/paper/2016/file/621bf66ddb7c962aa0d22ac97d69b793-Paper.pdf,Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula,"jean barbier, Mohamad Dia, Nicolas Macris, Florent Krzakala, Thibault Lesieur, Lenka Zdeborová",
neurips,https://proceedings.neurips.cc/paper/2016/file/632cee946db83e7a52ce5e8d6f0fed35-Paper.pdf,Large Margin Discriminant Dimensionality Reduction in Prediction Space,"Mohammad Saberian, Jose Costa Pereira, Can Xu, Jian Yang, Nuno Nvasconcelos",
neurips,https://proceedings.neurips.cc/paper/2016/file/63923f49e5241343aa7acb6a06a751e7-Paper.pdf,Fast learning rates with heavy-tailed losses,"Vu C. Dinh, Lam S. Ho, Binh Nguyen, Duy Nguyen","We study fast learning rates when the losses are not necessarily bounded and may have a distribution with heavy tails. To enable such analyses, we introduce two new conditions: (i) the envelope function
sup
f
∈
F
|
ℓ
∘
f
|
sup
, where
ℓ
ℓ
is the loss function and
F
F
is the hypothesis class, exists and is
L
r
L
-integrable, and (ii)
ℓ
ℓ
satisfies the multi-scale Bernstein's condition on
F
F
. Under these assumptions, we prove that learning rate faster than
O
(
n
−
1
/
2
)
O
can be obtained and, depending on
r
r
and the multi-scale Bernstein's powers, can be arbitrarily close to
O
(
n
−
1
)
O
. We then verify these assumptions and derive fast learning rates for the problem of vector quantization by
k
k
-means clustering with heavy-tailed distributions. The analyses enable us to obtain novel learning rates that extend and complement existing results in the literature from both theoretical and practical viewpoints."
neurips,https://proceedings.neurips.cc/paper/2016/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf,Dynamic matrix recovery from incomplete observations under an exact low-rank constraint,"Liangbei Xu, Mark Davenport",
neurips,https://proceedings.neurips.cc/paper/2016/file/645098b086d2f9e1e0e939c27f9f2d6f-Paper.pdf,Tight Complexity Bounds for Optimizing Composite Objectives,"Blake E. Woodworth, Nati Srebro",
neurips,https://proceedings.neurips.cc/paper/2016/file/65699726a3c601b9f31bf04019c8593c-Paper.pdf,A forward model at Purkinje cell synapses facilitates cerebellar anticipatory control,"Ivan Herreros, Xerxes Arsiwalla, Paul Verschure",
neurips,https://proceedings.neurips.cc/paper/2016/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper.pdf,Verification Based Solution for Structured MAB Problems,Zohar S. Karnin,"We consider the problem of finding the best arm in a stochastic Mutli-armed Bandit (MAB) game and propose a general framework based on verification that applies to multiple well-motivated generalizations of the classic MAB problem. In these generalizations, additional structure is known in advance, causing the task of verifying the optimality of a candidate to be easier than discovering the best arm. Our results are focused on the scenario where the failure probability
δ
δ
must be very low; we essentially show that in this high confidence regime, identifying the best arm is as easy as the task of verification. We demonstrate the effectiveness of our framework by applying it, and improving the state-of-the art results in the problems of: Linear bandits, Dueling bandits with the Condorcet assumption, Copeland dueling bandits, Unimodal bandits and Graphical bandits."
neurips,https://proceedings.neurips.cc/paper/2016/file/65ded5353c5ee48d0b7d48c591b8f430-Paper.pdf,SURGE: Surface Regularized Geometry Estimation from a Single Image,"Peng Wang, Xiaohui Shen, Bryan Russell, Scott Cohen, Brian Price, Alan L. Yuille",
neurips,https://proceedings.neurips.cc/paper/2016/file/65fc52ed8f88c81323a418ca94cec2ed-Paper.pdf,CliqueCNN: Deep Unsupervised Exemplar Learning,"Miguel A. Bautista, Artsiom Sanakoyeu, Ekaterina Tikhoncheva, Bjorn Ommer",
neurips,https://proceedings.neurips.cc/paper/2016/file/66e8ba8216a1e152d72653d99a4f03ab-Paper.pdf,Computing and maximizing influence in linear threshold and triggering models,"Justin T. Khim, Varun Jog, Po-Ling Loh",
neurips,https://proceedings.neurips.cc/paper/2016/file/6709e8d64a5f47269ed5cea9f625f7ab-Paper.pdf,"Data Programming: Creating Large Training Sets, Quickly","Alexander J. Ratner, Christopher M. De Sa, Sen Wu, Daniel Selsam, Christopher Ré",
neurips,https://proceedings.neurips.cc/paper/2016/file/670e8a43b246801ca1eaca97b3e19189-Paper.pdf,Flexible Models for Microclustering with Application to Entity Resolution,"Brenda Betancourt, Giacomo Zanella, Jeffrey W. Miller, Hanna Wallach, Abbas Zaidi, Rebecca C. Steorts",
neurips,https://proceedings.neurips.cc/paper/2016/file/678a1491514b7f1006d605e9161946b1-Paper.pdf,Blind Regression: Nonparametric Regression for Latent Variable Models via Collaborative Filtering,"Dogyoon Song, Christina E. Lee, Yihua Li, Devavrat Shah","We introduce the framework of {\em blind regression} motivated by {\em matrix completion} for recommendation systems: given
m
m
users,
n
n
movies, and a subset of user-movie ratings, the goal is to predict the unobserved user-movie ratings given the data, i.e., to complete the partially observed matrix. Following the framework of non-parametric statistics, we posit that user
u
u
and movie
i
i
have features
x
1
(
u
)
x
and
x
2
(
i
)
x
respectively, and their corresponding rating
y
(
u
,
i
)
y
is a noisy measurement of
f
(
x
1
(
u
)
,
x
2
(
i
)
)
f
for some unknown function
f
f
. In contrast with classical regression, the features
x
=
(
x
1
(
u
)
,
x
2
(
i
)
)
x
are not observed, making it challenging to apply standard regression methods to predict the unobserved ratings. Inspired by the classical Taylor's expansion for differentiable functions, we provide a prediction algorithm that is consistent for all Lipschitz functions. In fact, the analysis through our framework naturally leads to a variant of collaborative filtering, shedding insight into the widespread success of collaborative filtering in practice. Assuming each entry is sampled independently with probability at least
max
(
m
−
1
+
δ
,
n
−
1
/
2
+
δ
)
max
with
δ
>
0
δ
, we prove that the expected fraction of our estimates with error greater than
ϵ
ϵ
is less than
γ
2
/
ϵ
2
γ
plus a polynomially decaying term, where
γ
2
γ
is the variance of the additive entry-wise noise term. Experiments with the MovieLens and Netflix datasets suggest that our algorithm provides principled improvements over basic collaborative filtering and is competitive with matrix factorization methods."
neurips,https://proceedings.neurips.cc/paper/2016/file/67f7fb873eaf29526a11a9b7ac33bfac-Paper.pdf,An ensemble diversity approach to supervised binary hashing,"Miguel A. Carreira-Perpinan, Ramin Raziperchikolaei",
neurips,https://proceedings.neurips.cc/paper/2016/file/68b1fbe7f16e4ae3024973f12f3cb313-Paper.pdf,Learning Influence Functions from Incomplete Observations,"Xinran He, Ke Xu, David Kempe, Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2016/file/697e382cfd25b07a3e62275d3ee132b3-Paper.pdf,Backprop KF: Learning Discriminative Deterministic State Estimators,"Tuomas Haarnoja, Anurag Ajay, Sergey Levine, Pieter Abbeel",
neurips,https://proceedings.neurips.cc/paper/2016/file/69a5b5995110b36a9a347898d97a610e-Paper.pdf,On the Recursive Teaching Dimension of VC Classes,"Xi Chen, Xi Chen, Yu Cheng, Bo Tang","The recursive teaching dimension (RTD) of a concept class
C
⊆
{
0
,
1
}
n
C
, introduced by Zilles et al. [ZLHZ11], is a complexity parameter measured by the worst-case number of labeled examples needed to learn any target concept of
C
C
in the recursive teaching model. In this paper, we study the quantitative relation between RTD and the well-known learning complexity measure VC dimension (VCD), and improve the best known upper and (worst-case) lower bounds on the recursive teaching dimension with respect to the VC dimension. Given a concept class
C
⊆
{
0
,
1
}
n
C
with
V
C
D
(
C
)
=
d
V
, we first show that
R
T
D
(
C
)
R
is at most
d
2
d
+
1
d
. This is the first upper bound for
R
T
D
(
C
)
R
that depends only on
V
C
D
(
C
)
V
, independent of the size of the concept class
|
C
|
|
and its~domain size
n
n
. Before our work, the best known upper bound for
R
T
D
(
C
)
R
is
O
(
d
2
d
log
log
|
C
|
)
O
, obtained by Moran et al. [MSWY15]. We remove the
log
log
|
C
|
log
factor. We also improve the lower bound on the worst-case ratio of
R
T
D
(
C
)
R
to
V
C
D
(
C
)
V
. We present a family of classes
{
C
k
}
k
≥
1
{
with
V
C
D
(
C
k
)
=
3
k
V
and
R
T
D
(
C
k
)
=
5
k
R
, which implies that the ratio of
R
T
D
(
C
)
R
to
V
C
D
(
C
)
V
in the worst case can be as large as
5
/
3
5
. Before our work, the largest ratio known was
3
/
2
3
as obtained by Kuhlmann [Kuh99]. Since then, no finite concept class
C
C
has been known to satisfy
R
T
D
(
C
)
>
(
3
/
2
)
V
C
D
(
C
)
R
."
neurips,https://proceedings.neurips.cc/paper/2016/file/6a10bbd480e4c5573d8f3af73ae0454b-Paper.pdf,Generalized Correspondence-LDA Models (GC-LDA) for Identifying Functional Regions in the Brain,"Timothy Rubin, Oluwasanmi O. Koyejo, Michael N. Jones, Tal Yarkoni",
neurips,https://proceedings.neurips.cc/paper/2016/file/6aca97005c68f1206823815f66102863-Paper.pdf,Fast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation,"George Papamakarios, Iain Murray",
neurips,https://proceedings.neurips.cc/paper/2016/file/6ae07dcb33ec3b7c814df797cbda0f87-Paper.pdf,Ladder Variational Autoencoders,"Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, Ole Winther",
neurips,https://proceedings.neurips.cc/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf,Improved Deep Metric Learning with Multi-class N-pair Loss Objective,Kihyuk Sohn,
neurips,https://proceedings.neurips.cc/paper/2016/file/6be5336db2c119736cf48f475e051bfe-Paper.pdf,Learning Sparse Gaussian Graphical Models with Overlapping Blocks,"Mohammad Javad Hosseini, Su-In Lee",
neurips,https://proceedings.neurips.cc/paper/2016/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf,Probabilistic Inference with Generating Functions for Poisson Latent Variable Models,"Kevin Winner, Daniel R. Sheldon",
neurips,https://proceedings.neurips.cc/paper/2016/file/6c29793a140a811d0c45ce03c1c93a28-Paper.pdf,Achieving the KS threshold in the general stochastic block model with linearized acyclic belief propagation,"Emmanuel Abbe, Colin Sandon","The stochastic block model (SBM) has long been studied in machine learning and network science as a canonical model for clustering and community detection. In the recent years, new developments have demonstrated the presence of threshold phenomena for this model, which have set new challenges for algorithms. For the {\it detection} problem in symmetric SBMs, Decelle et al.\ conjectured that the so-called Kesten-Stigum (KS) threshold can be achieved efficiently. This was proved for two communities, but remained open from three communities. We prove this conjecture here, obtaining a more general result that applies to arbitrary SBMs with linear size communities. The developed algorithm is a linearized acyclic belief propagation (ABP) algorithm, which mitigates the effects of cycles while provably achieving the KS threshold in
O
(
n
ln
n
)
O
time. This extends prior methods by achieving universally the KS threshold while reducing or preserving the computational complexity. ABP is also connected to a power iteration method on a generalized nonbacktracking operator, formalizing the spectral-message passing interplay described in Krzakala et al., and extending results from Bordenave et al."
neurips,https://proceedings.neurips.cc/paper/2016/file/6c9882bbac1c7093bd25041881277658-Paper.pdf,A Unified Approach for Learning the Parameters of Sum-Product Networks,"Han Zhao, Pascal Poupart, Geoffrey J. Gordon",
neurips,https://proceedings.neurips.cc/paper/2016/file/6d3a1e06d6a06349436bc054313b648c-Paper.pdf,The Multiscale Laplacian Graph Kernel,"Risi Kondor, Horace Pan",
neurips,https://proceedings.neurips.cc/paper/2016/file/6e7d2da6d3953058db75714ac400b584-Paper.pdf,Learning the Number of Neurons in Deep Networks,"Jose M. Alvarez, Mathieu Salzmann",
neurips,https://proceedings.neurips.cc/paper/2016/file/6ea2ef7311b482724a9b7b0bc0dd85c6-Paper.pdf,Deep Alternative Neural Network: Exploring Contexts as Early as Possible for Action Recognition,"Jinzhuo Wang, Wenmin Wang, xiongtao Chen, Ronggang Wang, Wen Gao",
neurips,https://proceedings.neurips.cc/paper/2016/file/6ef80bb237adf4b6f77d0700e1255907-Paper.pdf,Online ICA: Understanding Global Dynamics of Nonconvex Optimization via Diffusion Processes,"Chris Junchi Li, Zhaoran Wang, Han Liu",
neurips,https://proceedings.neurips.cc/paper/2016/file/6f2688a5fce7d48c8d19762b88c32c3b-Paper.pdf,Spatio-Temporal Hilbert Maps for Continuous Occupancy Representation in Dynamic Environments,"Ransalu Senanayake, Lionel Ott, Simon O'Callaghan, Fabio T. Ramos",
neurips,https://proceedings.neurips.cc/paper/2016/file/6f3ef77ac0e3619e98159e9b6febf557-Paper.pdf,CRF-CNN: Modeling Structured Information in Human Pose Estimation,"Xiao Chu, Wanli Ouyang, hongsheng Li, Xiaogang Wang",
neurips,https://proceedings.neurips.cc/paper/2016/file/708f3cf8100d5e71834b1db77dfa15d6-Paper.pdf,Bayesian latent structure discovery from multi-neuron recordings,"Scott Linderman, Ryan P. Adams, Jonathan W. Pillow",
neurips,https://proceedings.neurips.cc/paper/2016/file/716e1b8c6cd17b771da77391355749f3-Paper.pdf,Latent Attention For If-Then Program Synthesis,"Chang Liu, Xinyun Chen, Eui Chul Shin, Mingcheng Chen, Dawn Song",
neurips,https://proceedings.neurips.cc/paper/2016/file/7250eb93b3c18cc9daa29cf58af7a004-Paper.pdf,Understanding Probabilistic Sparse Gaussian Process Approximations,"Matthias Bauer, Mark van der Wilk, Carl Edward Rasmussen",
neurips,https://proceedings.neurips.cc/paper/2016/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf,Wasserstein Training of Restricted Boltzmann Machines,"Grégoire Montavon, Klaus-Robert Müller, Marco Cuturi",
neurips,https://proceedings.neurips.cc/paper/2016/file/743c41a921516b04afde48bb48e28ce6-Paper.pdf,A primal-dual method for conic constrained distributed optimization problems,"Necdet Serhat Aybat, Erfan Yazdandoost Hamedani",
neurips,https://proceedings.neurips.cc/paper/2016/file/74563ba21a90da13dacf2a73e3ddefa7-Paper.pdf,Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations,"Behnam Neyshabur, Yuhuai Wu, Russ R. Salakhutdinov, Nati Srebro",
neurips,https://proceedings.neurips.cc/paper/2016/file/7503cfacd12053d309b6bed5c89de212-Paper.pdf,Communication-Optimal Distributed Clustering,"Jiecao Chen, He Sun, David Woodruff, Qin Zhang",
neurips,https://proceedings.neurips.cc/paper/2016/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf,Boosting with Abstention,"Corinna Cortes, Giulia DeSalvo, Mehryar Mohri",
neurips,https://proceedings.neurips.cc/paper/2016/file/76dc611d6ebaafc66cc0879c71b5db5c-Paper.pdf,Linear dynamical neural population models through nonlinear embeddings,"Yuanjun Gao, Evan W. Archer, Liam Paninski, John P. Cunningham",
neurips,https://proceedings.neurips.cc/paper/2016/file/7750ca3559e5b8e1f44210283368fc16-Paper.pdf,Rényi Divergence Variational Inference,"Yingzhen Li, Richard E. Turner",
neurips,https://proceedings.neurips.cc/paper/2016/file/77f959f119f4fb2321e9ce801e2f5163-Paper.pdf,Stochastic Gradient Geodesic MCMC Methods,"Chang Liu, Jun Zhu, Yang Song",
neurips,https://proceedings.neurips.cc/paper/2016/file/7884a9652e94555c70f96b6be63be216-Paper.pdf,A posteriori error bounds for joint matrix decomposition problems,"Nicolo Colombo, Nikos Vlassis",
neurips,https://proceedings.neurips.cc/paper/2016/file/792c7b5aae4a79e78aaeda80516ae2ac-Paper.pdf,Global Analysis of Expectation Maximization for Mixtures of Two Gaussians,"Ji Xu, Daniel J. Hsu, Arian Maleki",
neurips,https://proceedings.neurips.cc/paper/2016/file/795c7a7a5ec6b460ec00c5841019b9e9-Paper.pdf,Stochastic Structured Prediction under Bandit Feedback,"Artem Sokolov, Julia Kreutzer, Stefan Riezler, Christopher Lo",
neurips,https://proceedings.neurips.cc/paper/2016/file/79a49b3e3762632813f9e35f4ba53d6c-Paper.pdf,Estimating the class prior and posterior from noisy positives and unlabeled data,"Shantanu Jain, Martha White, Predrag Radivojac",
neurips,https://proceedings.neurips.cc/paper/2016/file/7b1ce3d73b70f1a7246e7b76a35fb552-Paper.pdf,A Minimax Approach to Supervised Learning,"Farzan Farnia, David Tse",
neurips,https://proceedings.neurips.cc/paper/2016/file/7b4773c039d539af17c883eb9283dd14-Paper.pdf,Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning,"Jean-Bastien Grill, Michal Valko, Remi Munos",
neurips,https://proceedings.neurips.cc/paper/2016/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf,Scaling Factorial Hidden Markov Models: Stochastic Variational Inference without Messages,"Yin Cheng Ng, Pawel M. Chilinski, Ricardo Silva",
neurips,https://proceedings.neurips.cc/paper/2016/file/7bb060764a818184ebb1cc0d43d382aa-Paper.pdf,Improved Dropout for Shallow and Deep Learning,"Zhe Li, Boqing Gong, Tianbao Yang",
neurips,https://proceedings.neurips.cc/paper/2016/file/7bc1ec1d9c3426357e69acd5bf320061-Paper.pdf,Clustering Signed Networks with the Geometric Mean of Laplacians,"Pedro Mercado, Francesco Tudisco, Matthias Hein",
neurips,https://proceedings.neurips.cc/paper/2016/file/7bd28f15a49d5e5848d6ec70e584e625-Paper.pdf,Consistent Estimation of Functions of Data Missing Non-Monotonically and Not at Random,Ilya Shpitser,
neurips,https://proceedings.neurips.cc/paper/2016/file/7c4ede33a62160a19586f6e26eaefacf-Paper.pdf,Discriminative Gaifman Models,Mathias Niepert,
neurips,https://proceedings.neurips.cc/paper/2016/file/7c82fab8c8f89124e2ce92984e04fb40-Paper.pdf,Selective inference for group-sparse linear models,"Fan Yang, Rina Foygel Barber, Prateek Jain, John Lafferty",
neurips,https://proceedings.neurips.cc/paper/2016/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf,InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets,"Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel",
neurips,https://proceedings.neurips.cc/paper/2016/file/7cce53cf90577442771720a370c3c723-Paper.pdf,Automated scalable segmentation of neurons from multispectral images,"Uygar Sümbül, Douglas Roossien, Dawen Cai, Fei Chen, Nicholas Barry, John P. Cunningham, Edward Boyden, Liam Paninski",
neurips,https://proceedings.neurips.cc/paper/2016/file/7ce3284b743aefde80ffd9aec500e085-Paper.pdf,Robustness of classifiers: from adversarial to random noise,"Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard",
neurips,https://proceedings.neurips.cc/paper/2016/file/7d6044e95a16761171b130dcb476a43e-Paper.pdf,Composing graphical models with neural networks for structured representations and fast inference,"Matthew J. Johnson, David K. Duvenaud, Alex Wiltschko, Ryan P. Adams, Sandeep R. Datta",
neurips,https://proceedings.neurips.cc/paper/2016/file/7dcd340d84f762eba80aa538b0c527f7-Paper.pdf,SoundNet: Learning Sound Representations from Unlabeled Video,"Yusuf Aytar, Carl Vondrick, Antonio Torralba",
neurips,https://proceedings.neurips.cc/paper/2016/file/7e83722522e8aeb7512b7075311316b7-Paper.pdf,Dual Decomposed Learning with Factorwise Oracle for Structural SVM of Large Output Domain,"Ian En-Hsu Yen, Xiangru Huang, Kai Zhong, Ruohan Zhang, Pradeep K. Ravikumar, Inderjit S. Dhillon","Many applications of machine learning involve structured output with large domain, where learning of structured predictor is prohibitive due to repetitive calls to expensive inference oracle. In this work, we show that, by decomposing training of Structural Support Vector Machine (SVM) into a series of multiclass SVM problems connected through messages, one can replace expensive structured oracle with Factorwise Maximization Oracle (FMO) that allows efficient implementation of complexity sublinear to the factor domain. A Greedy Direction Method of Multiplier (GDMM) algorithm is proposed to exploit sparsity of messages which guarantees
ϵ
ϵ
sub-optimality after
O
(
l
o
g
(
1
/
ϵ
)
)
O
passes of FMO calls. We conduct experiments on chain-structured problems and fully-connected problems of large output domains. The proposed approach is orders-of-magnitude faster than the state-of-the-art training algorithms for Structural SVM."
neurips,https://proceedings.neurips.cc/paper/2016/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf,Deep Learning for Predicting Human Strategic Behavior,"Jason S. Hartford, James R. Wright, Kevin Leyton-Brown",
neurips,https://proceedings.neurips.cc/paper/2016/file/7eb7eabbe9bd03c2fc99881d04da9cbd-Paper.pdf,Online and Differentially-Private Tensor Decomposition,"Yining Wang, Anima Anandkumar",
neurips,https://proceedings.neurips.cc/paper/2016/file/7ef605fc8dba5425d6965fbd4c8fbe1f-Paper.pdf,Multivariate tests of association based on univariate tests,"Ruth Heller, Yair Heller",
neurips,https://proceedings.neurips.cc/paper/2016/file/7f100b7b36092fb9b06dfb4fac360931-Paper.pdf,Variational Information Maximization for Feature Selection,"Shuyang Gao, Greg Ver Steeg, Aram Galstyan",
neurips,https://proceedings.neurips.cc/paper/2016/file/7f53f8c6c730af6aeb52e66eb74d8507-Paper.pdf,Learning Bound for Parameter Transfer Learning,Wataru Kumagai,
neurips,https://proceedings.neurips.cc/paper/2016/file/7fb8ceb3bd59c7956b1df66729296a4c-Paper.pdf,Matrix Completion has No Spurious Local Minimum,"Rong Ge, Jason D. Lee, Tengyu Ma",
neurips,https://proceedings.neurips.cc/paper/2016/file/7fea637fd6d02b8f0adf6f7dc36aed93-Paper.pdf,Deep Submodular Functions: Definitions and Learning,"Brian W. Dolhansky, Jeff A. Bilmes",
neurips,https://proceedings.neurips.cc/paper/2016/file/7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf,Adaptive optimal training of animal behavior,"Ji Hyun Bak, Jung Yoon Choi, Athena Akrami, Ilana Witten, Jonathan W. Pillow",
neurips,https://proceedings.neurips.cc/paper/2016/file/806beafe154032a5b818e97b4420ad98-Paper.pdf,Structured Matrix Recovery via the Generalized Dantzig Selector,"Sheng Chen, Arindam Banerjee",
neurips,https://proceedings.neurips.cc/paper/2016/file/80a8155eb153025ea1d513d0b2c4b675-Paper.pdf,Robust k-means: a Theoretical Revisit,ALEXANDROS GEORGOGIANNIS,
neurips,https://proceedings.neurips.cc/paper/2016/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf,Tree-Structured Reinforcement Learning for Sequential Object Localization,"Zequn Jie, Xiaodan Liang, Jiashi Feng, Xiaojie Jin, Wen Lu, Shuicheng Yan",
neurips,https://proceedings.neurips.cc/paper/2016/file/814a9c18f5abff398787c9cfcbf3d80c-Paper.pdf,One-vs-Each Approximation to Softmax for Scalable Estimation of Probabilities,Michalis Titsias RC AUEB,
neurips,https://proceedings.neurips.cc/paper/2016/file/8169e05e2a0debcb15458f2cc1eff0ea-Paper.pdf,Poisson-Gamma dynamical systems,"Aaron Schein, Hanna Wallach, Mingyuan Zhou",
neurips,https://proceedings.neurips.cc/paper/2016/file/81c650caac28cdefce4de5ddc18befa0-Paper.pdf,Convergence guarantees for kernel-based quadrature rules in misspecified settings,"Motonobu Kanagawa, Bharath K. Sriperumbudur, Kenji Fukumizu","Kernel-based quadrature rules are becoming important in machine learning and statistics, as they achieve super-
¥
s
q
r
t
n
¥
convergence rates in numerical integration, and thus provide alternatives to Monte Carlo integration in challenging settings where integrands are expensive to evaluate or where integrands are high dimensional. These rules are based on the assumption that the integrand has a certain degree of smoothness, which is expressed as that the integrand belongs to a certain reproducing kernel Hilbert space (RKHS). However, this assumption can be violated in practice (e.g., when the integrand is a black box function), and no general theory has been established for the convergence of kernel quadratures in such misspecified settings. Our contribution is in proving that kernel quadratures can be consistent even when the integrand does not belong to the assumed RKHS, i.e., when the integrand is less smooth than assumed. Specifically, we derive convergence rates that depend on the (unknown) lesser smoothness of the integrand, where the degree of smoothness is expressed via powers of RKHSs or via Sobolev spaces."
neurips,https://proceedings.neurips.cc/paper/2016/file/81c8727c62e800be708dbf37c4695dff-Paper.pdf,Maximization of Approximately Submodular Functions,"Thibaut Horel, Yaron Singer","We study the problem of maximizing a function that is approximately submodular under a cardinality constraint. Approximate submodularity implicitly appears in a wide range of applications as in many cases errors in evaluation of a submodular function break submodularity. Say that
F
F
is
\eps
\eps
-approximately submodular if there exists a submodular function
f
f
such that
(
1
−
\eps
)
f
(
S
)
≤
F
(
S
)
≤
(
1
+
\eps
)
f
(
S
)
(
for all subsets
S
S
. We are interested in characterizing the query-complexity of maximizing
F
F
subject to a cardinality constraint
k
k
as a function of the error level
\eps
>
0
\eps
. We provide both lower and upper bounds: for
\eps
>
n
−
1
/
2
\eps
we show an exponential query-complexity lower bound. In contrast, when
\eps
<
1
/
k
\eps
or under a stronger bounded curvature assumption, we give constant approximation algorithms."
neurips,https://proceedings.neurips.cc/paper/2016/file/81ca0262c82e712e50c580c032d99b60-Paper.pdf,Causal meets Submodular: Subset Selection with Directed Information,"Yuxun Zhou, Costas J. Spanos",
neurips,https://proceedings.neurips.cc/paper/2016/file/8232e119d8f59aa83050a741631803a6-Paper.pdf,Linear Feature Encoding for Reinforcement Learning,"Zhao Song, Ronald E. Parr, Xuejun Liao, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2016/file/8248a99e81e752cb9b41da3fc43fbe7f-Paper.pdf,Mixed Linear Regression with Multiple Components,"Kai Zhong, Prateek Jain, Inderjit S. Dhillon","In this paper, we study the mixed linear regression (MLR) problem, where the goal is to recover multiple underlying linear models from their unlabeled linear measurements. We propose a non-convex objective function which we show is {\em locally strongly convex} in the neighborhood of the ground truth. We use a tensor method for initialization so that the initial models are in the local strong convexity region. We then employ general convex optimization algorithms to minimize the objective function. To the best of our knowledge, our approach provides first exact recovery guarantees for the MLR problem with
K
≥
2
K
components. Moreover, our method has near-optimal computational complexity
~
O
(
N
d
)
O
as well as near-optimal sample complexity
~
O
(
d
)
O
for {\em constant}
K
K
. Furthermore, we show that our non-convex formulation can be extended to solving the {\em subspace clustering} problem as well. In particular, when initialized within a small constant distance to the true subspaces, our method converges to the global optima (and recovers true subspaces) in time {\em linear} in the number of points. Furthermore, our empirical results indicate that even with random initialization, our approach converges to the global optima in linear time, providing speed-up of up to two orders of magnitude."
neurips,https://proceedings.neurips.cc/paper/2016/file/82b8a3434904411a9fdc43ca87cee70c-Paper.pdf,Unsupervised Learning of Spoken Language with Visual Context,"David Harwath, Antonio Torralba, James Glass",
neurips,https://proceedings.neurips.cc/paper/2016/file/82f2b308c3b01637c607ce05f52a2fed-Paper.pdf,Crowdsourced Clustering: Querying Edges vs Triangles,"Ramya Korlakai Vinayak, Babak Hassibi",
neurips,https://proceedings.neurips.cc/paper/2016/file/839ab46820b524afda05122893c2fe8e-Paper.pdf,Learning feed-forward one-shot learners,"Luca Bertinetto, João F. Henriques, Jack Valmadre, Philip Torr, Andrea Vedaldi",
neurips,https://proceedings.neurips.cc/paper/2016/file/83adc9225e4deb67d7ce42d58fe5157c-Paper.pdf,Reshaped Wirtinger Flow for Solving Quadratic System of Equations,"Huishuai Zhang, Yingbin Liang","We study the problem of recovering a vector
\bx
∈
\bbR
n
\bx
from its magnitude measurements
y
i
=
|
⟨
\ba
i
,
\bx
⟩
|
,
i
=
1
,
.
.
.
,
m
y
. Our work is along the line of the Wirtinger flow (WF) approach \citet{candes2015phase}, which solves the problem by minimizing a nonconvex loss function via a gradient algorithm and can be shown to converge to a global optimal point under good initialization. In contrast to the smooth loss function used in WF, we adopt a nonsmooth but lower-order loss function, and design a gradient-like algorithm (referred to as reshaped-WF). We show that for random Gaussian measurements, reshaped-WF enjoys geometric convergence to a global optimal point as long as the number
m
m
of measurements is at the order of
\cO
(
n
)
\cO
, where
n
n
is the dimension of the unknown
\bx
\bx
. This improves the sample complexity of WF, and achieves the same sample complexity as truncated-WF \citet{chen2015solving} but without truncation at gradient step. Furthermore, reshaped-WF costs less computationally than WF, and runs faster numerically than both WF and truncated-WF. Bypassing higher-order variables in the loss function and truncations in the gradient loop, analysis of reshaped-WF is simplified."
neurips,https://proceedings.neurips.cc/paper/2016/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf,Data Poisoning Attacks on Factorization-Based Collaborative Filtering,"Bo Li, Yining Wang, Aarti Singh, Yevgeniy Vorobeychik",
neurips,https://proceedings.neurips.cc/paper/2016/file/84d2004bf28a2095230e8e14993d398d-Paper.pdf,PAC-Bayesian Theory Meets Bayesian Inference,"Pascal Germain, Francis Bach, Alexandre Lacoste, Simon Lacoste-Julien",
neurips,https://proceedings.neurips.cc/paper/2016/file/850af92f8d9903e7a4e0559a98ecc857-Paper.pdf,"Fast Mixing Markov Chains for Strongly Rayleigh Measures, DPPs, and Constrained Sampling","Chengtao Li, Suvrit Sra, Stefanie Jegelka",
neurips,https://proceedings.neurips.cc/paper/2016/file/85422afb467e9456013a2a51d4dff702-Paper.pdf,Temporal Regularized Matrix Factorization for High-dimensional Time Series Prediction,"Hsiang-Fu Yu, Nikhil Rao, Inderjit S. Dhillon",
neurips,https://proceedings.neurips.cc/paper/2016/file/854d6fae5ee42911677c739ee1734486-Paper.pdf,FPNN: Field Probing Neural Networks for 3D Data,"Yangyan Li, Soeren Pirk, Hao Su, Charles R. Qi, Leonidas J. Guibas",
neurips,https://proceedings.neurips.cc/paper/2016/file/856fc81623da2150ba2210ba1b51d241-Paper.pdf,Object based Scene Representations using Fisher Scores of Local Subspace Projections,"Mandar D. Dixit, Nuno Vasconcelos",
neurips,https://proceedings.neurips.cc/paper/2016/file/860320be12a1c050cd7731794e231bd3-Paper.pdf,Architectural Complexity Measures of Recurrent Neural Networks,"Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Russ R. Salakhutdinov, Yoshua Bengio",
neurips,https://proceedings.neurips.cc/paper/2016/file/861dc9bd7f4e7dd3cccd534d0ae2a2e9-Paper.pdf,Interaction Screening: Efficient and Sample-Optimal Learning of Ising Models,"Marc Vuffray, Sidhant Misra, Andrey Lokhov, Michael Chertkov",
neurips,https://proceedings.neurips.cc/paper/2016/file/8698ff92115213ab187d31d4ee5da8ea-Paper.pdf,Community Detection on Evolving Graphs,"Aris Anagnostopoulos, Jakub Łącki, Silvio Lattanzi, Stefano Leonardi, Mohammad Mahdian",
neurips,https://proceedings.neurips.cc/paper/2016/file/872488f88d1b2db54d55bc8bba2fad1b-Paper.pdf,Preference Completion from Partial Rankings,"Suriya Gunasekar, Oluwasanmi O. Koyejo, Joydeep Ghosh","We propose a novel and efficient algorithm for the collaborative preference completion problem, which involves jointly estimating individualized rankings for a set of entities over a shared set of items, based on a limited number of observed affinity values. Our approach exploits the observation that while preferences are often recorded as numerical scores, the predictive quantity of interest is the underlying rankings. Thus, attempts to closely match the recorded scores may lead to overfitting and impair generalization performance. Instead, we propose an estimator that directly fits the underlying preference order, combined with nuclear norm constraints to encourage low--rank parameters. Besides (approximate) correctness of the ranking order, the proposed estimator makes no generative assumption on the numerical scores of the observations. One consequence is that the proposed estimator can fit any consistent partial ranking over a subset of the items represented as a directed acyclic graph (DAG), generalizing standard techniques that can only fit preference scores. Despite this generality, for supervision representing total or blockwise total orders, the computational complexity of our algorithm is within a
log
log
factor of the standard algorithms for nuclear norm regularization based estimates for matrix completion. We further show promising empirical results for a novel and challenging application of collaboratively ranking of the associations between brain--regions and cognitive neuroscience terms."
neurips,https://proceedings.neurips.cc/paper/2016/file/88a199611ac2b85bd3f76e8ee7e55650-Paper.pdf,“Congruent” and “Opposite” Neurons: Sisters for Multisensory Integration and Segregation,"Wen-Hao Zhang, He Wang, K. Y. Michael Wong, Si Wu",
neurips,https://proceedings.neurips.cc/paper/2016/file/88a839f2f6f1427879fc33ee4acf4f66-Paper.pdf,A Consistent Regularization Approach for Structured Prediction,"Carlo Ciliberto, Lorenzo Rosasco, Alessandro Rudi",
neurips,https://proceedings.neurips.cc/paper/2016/file/8929c70f8d710e412d38da624b21c3c8-Paper.pdf,Fast recovery from a union of subspaces,"Chinmay Hegde, Piotr Indyk, Ludwig Schmidt",
neurips,https://proceedings.neurips.cc/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf,Improved Techniques for Training GANs,"Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, Xi Chen",
neurips,https://proceedings.neurips.cc/paper/2016/file/8b4066554730ddfaa0266346bdc1b202-Paper.pdf,Coordinate-wise Power Method,"Qi Lei, Kai Zhong, Inderjit S. Dhillon",
neurips,https://proceedings.neurips.cc/paper/2016/file/8b5700012be65c9da25f49408d959ca0-Paper.pdf,On Mixtures of Markov Chains,"Rishi Gupta, Ravi Kumar, Sergei Vassilvitskii",
neurips,https://proceedings.neurips.cc/paper/2016/file/8bdb5058376143fa358981954e7626b8-Paper.pdf,Near-Optimal Smoothing of Structured Conditional Probability Matrices,"Moein Falahatgar, Mesrob I. Ohannessian, Alon Orlitsky",
neurips,https://proceedings.neurips.cc/paper/2016/file/8bf1211fd4b7b94528899de0a43b9fb3-Paper.pdf,Dynamic Filter Networks,"Xu Jia, Bert De Brabandere, Tinne Tuytelaars, Luc V. Gool",
neurips,https://proceedings.neurips.cc/paper/2016/file/8c00dee24c9878fea090ed070b44f1ab-Paper.pdf,Estimating the Size of a Large Network and its Communities from a Random Sample,"Lin Chen, Amin Karbasi, Forrest W. Crawford",
neurips,https://proceedings.neurips.cc/paper/2016/file/8c01a75941549a705cf7275e41b21f0d-Paper.pdf,Generalization of ERM in Stochastic Convex Optimization: The Dimension Strikes Back,Vitaly Feldman,"In stochastic convex optimization the goal is to minimize a convex function
F
(
x
)
≐
\E
f
∼
D
[
f
(
x
)
]
F
over a convex set
\K
⊂
\R
d
\K
where
D
D
is some unknown distribution and each
f
(
⋅
)
f
in the support of
D
D
is convex over
\K
\K
. The optimization is based on i.i.d.~samples
f
1
,
f
2
,
…
,
f
n
f
from
D
D
. A common approach to such problems is empirical risk minimization (ERM) that optimizes
F
S
(
x
)
≐
1
n
∑
i
≤
n
f
i
(
x
)
F
. Here we consider the question of how many samples are necessary for ERM to succeed and the closely related question of uniform convergence of
F
S
F
to
F
F
over
\K
\K
. We demonstrate that in the standard
ℓ
p
/
ℓ
q
ℓ
setting of Lipschitz-bounded functions over a
\K
\K
of bounded radius, ERM requires sample size that scales linearly with the dimension
d
d
. This nearly matches standard upper bounds and improves on
Ω
(
log
d
)
Ω
dependence proved for
ℓ
2
/
ℓ
2
ℓ
setting in (Shalev-Shwartz et al. 2009). In stark contrast, these problems can be solved using dimension-independent number of samples for
ℓ
2
/
ℓ
2
ℓ
setting and
log
d
log
dependence for
ℓ
1
/
ℓ
∞
ℓ
setting using other approaches. We also demonstrate that for a more general class of range-bounded (but not Lipschitz-bounded) stochastic convex programs an even stronger gap appears already in dimension 2."
neurips,https://proceedings.neurips.cc/paper/2016/file/8d317bdcf4aafcfc22149d77babee96d-Paper.pdf,Nested Mini-Batch K-Means,"James Newling, François Fleuret",
neurips,https://proceedings.neurips.cc/paper/2016/file/8d55a249e6baa5c06772297520da2051-Paper.pdf,Infinite Hidden Semi-Markov Modulated Interaction Point Process,"matt zhang, Peng Lin, Peng Lin, Ting Guo, Yang Wang, Yang Wang, Fang Chen",
neurips,https://proceedings.neurips.cc/paper/2016/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf,A Credit Assignment Compiler for Joint Prediction,"Kai-Wei Chang, He He, Stephane Ross, Hal Daume III, John Langford",
neurips,https://proceedings.neurips.cc/paper/2016/file/8d8818c8e140c64c743113f563cf750f-Paper.pdf,Deep Exploration via Bootstrapped DQN,"Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy",
neurips,https://proceedings.neurips.cc/paper/2016/file/8d9fc2308c8f28d2a7d2f6f48801c705-Paper.pdf,Estimating Nonlinear Neural Response Functions using GP Priors and Kronecker Methods,"Cristina Savin, Gasper Tkacik",
neurips,https://proceedings.neurips.cc/paper/2016/file/8e82ab7243b7c66d768f1b8ce1c967eb-Paper.pdf,Structured Sparse Regression via Greedy Hard Thresholding,"Prateek Jain, Nikhil Rao, Inderjit S. Dhillon","Several learning applications require solving high-dimensional regression problems where the relevant features belong to a small number of (overlapping) groups. For very large datasets and under standard sparsity constraints, hard thresholding methods have proven to be extremely efficient, but such methods require NP hard projections when dealing with overlapping groups. In this paper, we show that such NP-hard projections can not only be avoided by appealing to submodular optimization, but such methods come with strong theoretical guarantees even in the presence of poorly conditioned data (i.e. say when two features have correlation
≥
0.99
≥
), which existing analyses cannot handle. These methods exhibit an interesting computation-accuracy trade-off and can be extended to significantly harder problems such as sparse overlapping groups. Experiments on both real and synthetic data validate our claims and demonstrate that the proposed methods are orders of magnitude faster than other greedy and convex relaxation techniques for learning with group-structured sparsity."
neurips,https://proceedings.neurips.cc/paper/2016/file/8ebda540cbcc4d7336496819a46a1b68-Paper.pdf,A Multi-Batch L-BFGS Method for Machine Learning,"Albert S. Berahas, Jorge Nocedal, Martin Takac",
neurips,https://proceedings.neurips.cc/paper/2016/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf,Cooperative Graphical Models,"Josip Djolonga, Stefanie Jegelka, Sebastian Tschiatschek, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2016/file/8fecb20817b3847419bb3de39a609afe-Paper.pdf,What Makes Objects Similar: A Unified Multi-Metric Learning Approach,"Han-Jia Ye, De-Chuan Zhan, Xue-Min Si, Yuan Jiang, Zhi-Hua Zhou",
neurips,https://proceedings.neurips.cc/paper/2016/file/90e1357833654983612fb05e3ec9148c-Paper.pdf,Matching Networks for One Shot Learning,"Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, Daan Wierstra",
neurips,https://proceedings.neurips.cc/paper/2016/file/9188905e74c28e489b44e954ec0b9bca-Paper.pdf,Gradient-based Sampling: An Adaptive Importance Sampling for Least-squares,Rong Zhu,
neurips,https://proceedings.neurips.cc/paper/2016/file/92262bf907af914b95a0fc33c3f33bf6-Paper.pdf,Accelerating Stochastic Composition Optimization,"Mengdi Wang, Ji Liu, Ethan Fang",
neurips,https://proceedings.neurips.cc/paper/2016/file/9232fe81225bcaef853ae32870a2b0fe-Paper.pdf,Variational Inference in Mixed Probabilistic Submodular Models,"Josip Djolonga, Sebastian Tschiatschek, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2016/file/955a1584af63a546588caae4d23840b3-Paper.pdf,The Limits of Learning with Missing Data,"Brian Bullins, Elad Hazan, Tomer Koren",
neurips,https://proceedings.neurips.cc/paper/2016/file/9597353e41e6957b5e7aa79214fcb256-Paper.pdf,Clustering with Same-Cluster Queries,"Hassan Ashtiani, Shrinu Kushagra, Shai Ben-David","We propose a framework for Semi-Supervised Active Clustering framework (SSAC), where the learner is allowed to interact with a domain expert, asking whether two given instances belong to the same cluster or not. We study the query and computational complexity of clustering in this framework. We consider a setting where the expert conforms to a center-based clustering with a notion of margin. We show that there is a trade off between computational complexity and query complexity; We prove that for the case of
k
k
-means clustering (i.e., when the expert conforms to a solution of
k
k
-means), having access to relatively few such queries allows efficient solutions to otherwise NP hard problems. In particular, we provide a probabilistic polynomial-time (BPP) algorithm for clustering in this setting that asks
O
(
k
2
log
k
+
k
log
n
)
O
same-cluster queries and runs with time complexity
O
(
k
n
log
n
)
O
(where
k
k
is the number of clusters and
n
n
is the number of instances). The success of the algorithm is guaranteed for data satisfying the margin condition under which, without queries, we show that the problem is NP hard. We also prove a lower bound on the number of queries needed to have a computationally efficient clustering algorithm in this setting."
neurips,https://proceedings.neurips.cc/paper/2016/file/962e56a8a0b0420d87272a682bfd1e53-Paper.pdf,Deconvolving Feedback Loops in Recommender Systems,"Ayan Sinha, David F. Gleich, Karthik Ramani",
neurips,https://proceedings.neurips.cc/paper/2016/file/9657c1fffd38824e5ab0472e022e577e-Paper.pdf,Recovery Guarantee of Non-negative Matrix Factorization via Alternating Updates,"Yuanzhi Li, Yingyu Liang, Andrej Risteski",
neurips,https://proceedings.neurips.cc/paper/2016/file/96c5c28becf18e71190460a9955aa4d8-Paper.pdf,Threshold Learning for Optimal Decision Making,Nathan F. Lepora,
neurips,https://proceedings.neurips.cc/paper/2016/file/979d472a84804b9f647bc185a877a8b5-Paper.pdf,Joint M-Best-Diverse Labelings as a Parametric Submodular Minimization,"Alexander Kirillov, Alexander Shekhovtsov, Carsten Rother, Bogdan Savchynskyy","We consider the problem of jointly inferring the
M
M
-best diverse labelings for a binary (high-order) submodular energy of a graphical model. Recently, it was shown that this problem can be solved to a global optimum, for many practically interesting diversity measures. It was noted that the labelings are, so-called, nested. This nestedness property also holds for labelings of a class of parametric submodular minimization problems, where different values of the global parameter
γ
γ
give rise to different solutions. The popular example of the parametric submodular minimization is the monotonic parametric max-flow problem, which is also widely used for computing multiple labelings. As the main contribution of this work we establish a close relationship between diversity with submodular energies and the parametric submodular minimization. In particular, the joint
M
M
-best diverse labelings can be obtained by running a non-parametric submodular minimization (in the special case - max-flow) solver for
M
M
different values of
γ
γ
in parallel, for certain diversity measures. Importantly, the values for~
γ
γ
can be computed in a closed form in advance, prior to any optimization. These theoretical results suggest two simple yet efficient algorithms for the joint
M
M
-best diverse problem, which outperform competitors in terms of runtime and quality of results. In particular, as we show in the paper, the new methods compute the exact
M
M
-best diverse labelings faster than a popular method of Batra et al., which in some sense only obtains approximate solutions."
neurips,https://proceedings.neurips.cc/paper/2016/file/97d0145823aeb8ed80617be62e08bdcc-Paper.pdf,Confusions over Time: An Interpretable Bayesian Model to Characterize Trends in Decision Making,"Himabindu Lakkaraju, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2016/file/980ecd059122ce2e50136bda65c25e07-Paper.pdf,Measuring Neural Net Robustness with Constraints,"Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, Antonio Criminisi",
neurips,https://proceedings.neurips.cc/paper/2016/file/98b297950041a42470269d56260243a1-Paper.pdf,The Power of Adaptivity in Identifying Statistical Alternatives,"Kevin G. Jamieson, Daniel Haas, Benjamin Recht","This paper studies the trade-off between two different kinds of pure exploration: breadth versus depth. We focus on the most biased coin problem, asking how many total coin flips are required to identify a
heavy'' coin from an infinite bag containing both
heavy'' coins with mean
θ
1
∈
(
0
,
1
)
θ
, and
light"" coins with mean
θ
0
∈
(
0
,
θ
1
)
θ
, where heavy coins are drawn from the bag with proportion
α
∈
(
0
,
1
/
2
)
α
. When
α
,
θ
0
,
θ
1
α
are unknown, the key difficulty of this problem lies in distinguishing whether the two kinds of coins have very similar means, or whether heavy coins are just extremely rare. While existing solutions to this problem require some prior knowledge of the parameters
θ
0
,
θ
1
,
α
θ
, we propose an adaptive algorithm that requires no such knowledge yet still obtains near-optimal sample complexity guarantees. In contrast, we provide a lower bound showing that non-adaptive strategies require at least quadratically more samples. In characterizing this gap between adaptive and nonadaptive strategies, we make connections to anomaly detection and prove lower bounds on the sample complexity of differentiating between a single parametric distribution and a mixture of two such distributions."
neurips,https://proceedings.neurips.cc/paper/2016/file/98d6f58ab0dafbb86b083a001561bb34-Paper.pdf,Adaptive Skills Adaptive Partitions (ASAP),"Daniel J. Mankowitz, Timothy A. Mann, Shie Mannor",
neurips,https://proceedings.neurips.cc/paper/2016/file/98e6f17209029f4ae6dc9d88ec8eac2c-Paper.pdf,Riemannian SVRG: Fast Stochastic Optimization on Riemannian Manifolds,"Hongyi Zhang, Sashank J. Reddi, Suvrit Sra",
neurips,https://proceedings.neurips.cc/paper/2016/file/996009f2374006606f4c0b0fda878af1-Paper.pdf,Hypothesis Testing in Unsupervised Domain Adaptation with Applications in Alzheimer's Disease,"Hao Zhou, Vamsi K. Ithapu, Sathya Narayanan Ravi, Vikas Singh, Grace Wahba, Sterling C. Johnson","Consider samples from two different data sources
{
x
i
s
}
∼
P
s
o
u
r
c
e
{
and
{
x
i
t
}
∼
P
t
a
r
g
e
t
{
. We only observe their transformed versions
h
(
x
i
s
)
h
and
g
(
x
i
t
)
g
, for some known function class
h
(
⋅
)
h
and
g
(
⋅
)
g
. Our goal is to perform a statistical test checking if
P
s
o
u
r
c
e
P
=
P
t
a
r
g
e
t
P
while removing the distortions induced by the transformations. This problem is closely related to concepts underlying numerous domain adaptation algorithms, and in our case, is motivated by the need to combine clinical and imaging based biomarkers from multiple sites and/or batches, where this problem is fairly common and an impediment in the conduct of analyses with much larger sample sizes. We develop a framework that addresses this problem using ideas from hypothesis testing on the transformed measurements, where in the distortions need to be estimated {\it in tandem} with the testing. We derive a simple algorithm and study its convergence and consistency properties in detail, and we also provide lower-bound strategies based on recent work in continuous optimization. On a dataset of individuals at risk for neurological disease, our results are competitive with alternative procedures that are twice as expensive and in some cases operationally infeasible to implement."
neurips,https://proceedings.neurips.cc/paper/2016/file/9996535e07258a7bbfd8b132435c5962-Paper.pdf,Review Networks for Caption Generation,"Zhilin Yang, Ye Yuan, Yuexin Wu, William W. Cohen, Russ R. Salakhutdinov",
neurips,https://proceedings.neurips.cc/paper/2016/file/99c5e07b4d5de9d18c350cdf64c5aa3d-Paper.pdf,Distributed Flexible Nonlinear Tensor Factorization,"Shandian Zhe, Kai Zhang, Pengyuan Wang, Kuang-chih Lee, Zenglin Xu, Yuan Qi, Zoubin Ghahramani",
neurips,https://proceedings.neurips.cc/paper/2016/file/9a3d458322d70046f63dfd8b0153ece4-Paper.pdf,Safe Policy Improvement by Minimizing Robust Baseline Regret,"Mohammad Ghavamzadeh, Marek Petrik, Yinlam Chow",
neurips,https://proceedings.neurips.cc/paper/2016/file/9a49a25d845a483fae4be7e341368e36-Paper.pdf,Safe Exploration in Finite Markov Decision Processes with Gaussian Processes,"Matteo Turchetta, Felix Berkenkamp, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2016/file/9b04d152845ec0a378394003c96da594-Paper.pdf,Multimodal Residual Learning for Visual QA,"Jin-Hwa Kim, Sang-Woo Lee, Donghyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang",
neurips,https://proceedings.neurips.cc/paper/2016/file/9b698eb3105bd82528f23d0c92dedfc0-Paper.pdf,Variance Reduction in Stochastic Gradient Langevin Dynamics,"Kumar Avinava Dubey, Sashank J. Reddi, Sinead A. Williamson, Barnabas Poczos, Alexander J. Smola, Eric P. Xing",
neurips,https://proceedings.neurips.cc/paper/2016/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Paper.pdf,On Regularizing Rademacher Observation Losses,Richard Nock,
neurips,https://proceedings.neurips.cc/paper/2016/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf,A scalable end-to-end Gaussian process adapter for irregularly sampled time series classification,"Steven Cheng-Xian Li, Benjamin M. Marlin",
neurips,https://proceedings.neurips.cc/paper/2016/file/9c838d2e45b2ad1094d42f4ef36764f6-Paper.pdf,Learning User Perceived Clusters with Feature-Level Supervision,"Ting-Yu Cheng, Guiguan Lin, xinyang gong, Kang-Jun Liu, Shan-Hung (Brandon) Wu",
neurips,https://proceedings.neurips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf,Equality of Opportunity in Supervised Learning,"Moritz Hardt, Eric Price, Eric Price, Nati Srebro",
neurips,https://proceedings.neurips.cc/paper/2016/file/9d7311ba459f9e45ed746755a32dcd11-Paper.pdf,Kernel Observers: Systems-Theoretic Modeling and Inference of Spatiotemporally Evolving Processes,"Hassan A. Kingravi, Harshal R. Maske, Girish Chowdhary",
neurips,https://proceedings.neurips.cc/paper/2016/file/9dcb88e0137649590b755372b040afad-Paper.pdf,Hierarchical Question-Image Co-Attention for Visual Question Answering,"Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh",
neurips,https://proceedings.neurips.cc/paper/2016/file/9de6d14fff9806d4bcd1ef555be766cd-Paper.pdf,Double Thompson Sampling for Dueling Bandits,"Huasen Wu, Xin Liu","In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for dueling bandit problems. As its name suggests, D-TS selects both the first and the second candidates according to Thompson Sampling. Specifically, D-TS maintains a posterior distribution for the preference matrix, and chooses the pair of arms for comparison according to two sets of samples independently drawn from the posterior distribution. This simple algorithm applies to general Copeland dueling bandits, including Condorcet dueling bandits as its special case. For general Copeland dueling bandits, we show that D-TS achieves
O
(
K
2
log
T
)
O
regret. Moreover, using a back substitution argument, we refine the regret to
O
(
K
log
T
+
K
2
log
log
T
)
O
in Condorcet dueling bandits and many practical Copeland dueling bandits. In addition, we propose an enhancement of D-TS, referred to as D-TS+, that reduces the regret by carefully breaking ties. Experiments based on both synthetic and real-world data demonstrate that D-TS and D-TS
+
+
significantly improve the overall performance, in terms of regret and robustness."
neurips,https://proceedings.neurips.cc/paper/2016/file/9f396fe44e7c05c16873b05ec425cbad-Paper.pdf,A state-space model of cross-region dynamic connectivity in MEG/EEG,"Ying Yang, Elissa Aminoff, Michael Tarr, Kass E. Robert",
neurips,https://proceedings.neurips.cc/paper/2016/file/9f44e956e3a2b7b5598c625fcc802c36-Paper.pdf,Using Fast Weights to Attend to the Recent Past,"Jimmy Ba, Geoffrey E. Hinton, Volodymyr Mnih, Joel Z. Leibo, Catalin Ionescu",
neurips,https://proceedings.neurips.cc/paper/2016/file/9f61408e3afb633e50cdf1b20de6f466-Paper.pdf,High-Rank Matrix Completion and Clustering under Self-Expressive Models,Ehsan Elhamifar,
neurips,https://proceedings.neurips.cc/paper/2016/file/9f62b8625f914a002496335037e9ad97-Paper.pdf,Adaptive Newton Method for Empirical Risk Minimization to Statistical Accuracy,"Aryan Mokhtari, Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann, Alejandro Ribeiro",
neurips,https://proceedings.neurips.cc/paper/2016/file/9fdb62f932adf55af2c0e09e55861964-Paper.pdf,Yggdrasil: An Optimized System for Training Deep Decision Trees at Scale,"Firas Abuzaid, Joseph K. Bradley, Feynman T. Liang, Andrew Feng, Lee Yang, Matei Zaharia, Ameet S. Talwalkar",
neurips,https://proceedings.neurips.cc/paper/2016/file/9fe8593a8a330607d76796b35c64c600-Paper.pdf,Adaptive Maximization of Pointwise Submodular Functions With Budget Constraint,"Nguyen Cuong, Huan Xu",
neurips,https://proceedings.neurips.cc/paper/2016/file/a00e5eb0973d24649a4a920fc53d9564-Paper.pdf,Guided Policy Search via Approximate Mirror Descent,"William H. Montgomery, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2016/file/a01a0380ca3c61428c26a231f0e49a09-Paper.pdf,On Graph Reconstruction via Empirical Risk Minimization: Fast Learning Rates and Scalability,"Guillaume Papa, Aurélien Bellet, Stephan Clémençon",
neurips,https://proceedings.neurips.cc/paper/2016/file/a0872cc5b5ca4cc25076f3d868e1bdf8-Paper.pdf,Geometric Dirichlet Means Algorithm for topic inference,"Mikhail Yurochkin, XuanLong Nguyen",
neurips,https://proceedings.neurips.cc/paper/2016/file/a0e2a2c563d57df27213ede1ac4ac780-Paper.pdf,Learned Region Sparsity and Diversity Also Predicts Visual Attention,"Zijun Wei, Hossein Adeli, Minh Hoai Nguyen, Greg Zelinsky, Dimitris Samaras",
neurips,https://proceedings.neurips.cc/paper/2016/file/a1d33d0dfec820b41b54430b50e96b5c-Paper.pdf,Deep Learning Models of the Retinal Response to Natural Scenes,"Lane McIntosh, Niru Maheswaranathan, Aran Nayebi, Surya Ganguli, Stephen Baccus",
neurips,https://proceedings.neurips.cc/paper/2016/file/a1d7311f2a312426d710e1c617fcbc8c-Paper.pdf,Batched Gaussian Process Bandit Optimization via Determinantal Point Processes,"Tarun Kathuria, Amit Deshpande, Pushmeet Kohli",
neurips,https://proceedings.neurips.cc/paper/2016/file/a26398dca6f47b49876cbaffbc9954f9-Paper.pdf,Inference by Reparameterization in Neural Population Codes,"Rajkumar Vasudeva Raju, Zachary Pitkow",
neurips,https://proceedings.neurips.cc/paper/2016/file/a284df1155ec3e67286080500df36a9a-Paper.pdf,Blind Attacks on Machine Learners,"Alex Beatson, Zhaoran Wang, Han Liu",
neurips,https://proceedings.neurips.cc/paper/2016/file/a376033f78e144f494bfc743c0be3330-Paper.pdf,Learning Deep Parsimonious Representations,"Renjie Liao, Alex Schwing, Richard Zemel, Raquel Urtasun",
neurips,https://proceedings.neurips.cc/paper/2016/file/a3d68b461bd9d3533ee1dd3ce4628ed4-Paper.pdf,Scalable Adaptive Stochastic Optimization Using Random Projections,"Gabriel Krummenacher, Brian McWilliams, Yannic Kilcher, Joachim M. Buhmann, Nicolai Meinshausen",
neurips,https://proceedings.neurips.cc/paper/2016/file/a42a596fc71e17828440030074d15e74-Paper.pdf,"Graphons, mergeons, and so on!","Justin Eldridge, Mikhail Belkin, Yusu Wang",
neurips,https://proceedings.neurips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf,Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings,"Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, Adam T. Kalai",
neurips,https://proceedings.neurips.cc/paper/2016/file/a501bebf79d570651ff601788ea9d16d-Paper.pdf,Memory-Efficient Backpropagation Through Time,"Audrunas Gruslys, Remi Munos, Ivo Danihelka, Marc Lanctot, Alex Graves",
neurips,https://proceedings.neurips.cc/paper/2016/file/a532400ed62e772b9dc0b86f46e583ff-Paper.pdf,Solving Marginal MAP Problems with NP Oracles and Parity Constraints,"Yexiang Xue, Zhiyuan Li, Stefano Ermon, Carla P. Gomes, Bart Selman",
neurips,https://proceedings.neurips.cc/paper/2016/file/a7aeed74714116f3b292a982238f83d2-Paper.pdf,Differential Privacy without Sensitivity,"Kentaro Minami, HItomi Arai, Issei Sato, Hiroshi Nakagawa","The exponential mechanism is a general method to construct a randomized estimator that satisfies
(
ε
,
0
)
(
-differential privacy. Recently, Wang et al. showed that the Gibbs posterior, which is a data-dependent probability distribution that contains the Bayesian posterior, is essentially equivalent to the exponential mechanism under certain boundedness conditions on the loss function. While the exponential mechanism provides a way to build an
(
ε
,
0
)
(
-differential private algorithm, it requires boundedness of the loss function, which is quite stringent for some learning problems. In this paper, we focus on
(
ε
,
δ
)
(
-differential privacy of Gibbs posteriors with convex and Lipschitz loss functions. Our result extends the classical exponential mechanism, allowing the loss functions to have an unbounded sensitivity."
neurips,https://proceedings.neurips.cc/paper/2016/file/a869ccbcbd9568808b8497e28275c7c8-Paper.pdf,Adaptive Smoothed Online Multi-Task Learning,"Keerthiram Murugesan, Hanxiao Liu, Jaime Carbonell, Yiming Yang",
neurips,https://proceedings.neurips.cc/paper/2016/file/a86c450b76fb8c371afead6410d55534-Paper.pdf,Efficient and Robust Spiking Neural Circuit for Navigation Inspired by Echolocating Bats,"Pulkit Tandon, Yash H. Malviya, Bipin Rajendran",
neurips,https://proceedings.neurips.cc/paper/2016/file/a8849b052492b5106526b2331e526138-Paper.pdf,Optimal Cluster Recovery in the Labeled Stochastic Block Model,"Se-Young Yun, Alexandre Proutiere","We consider the problem of community detection or clustering in the labeled Stochastic Block Model (LSBM) with a finite number
K
K
of clusters of sizes linearly growing with the global population of items
n
n
. Every pair of items is labeled independently at random, and label
ℓ
ℓ
appears with probability
p
(
i
,
j
,
ℓ
)
p
between two items in clusters indexed by
i
i
and
j
j
, respectively. The objective is to reconstruct the clusters from the observation of these random labels. Clustering under the SBM and their extensions has attracted much attention recently. Most existing work aimed at characterizing the set of parameters such that it is possible to infer clusters either positively correlated with the true clusters, or with a vanishing proportion of misclassified items, or exactly matching the true clusters. We find the set of parameters such that there exists a clustering algorithm with at most
s
s
misclassified items in average under the general LSBM and for any
s
=
o
(
n
)
s
, which solves one open problem raised in \cite{abbe2015community}. We further develop an algorithm, based on simple spectral methods, that achieves this fundamental performance limit within
O
(
n
polylog
(
n
)
)
O
computations and without the a-priori knowledge of the model parameters."
neurips,https://proceedings.neurips.cc/paper/2016/file/a89cf525e1d9f04d16ce31165e139a4b-Paper.pdf,Relevant sparse codes with variational information bottleneck,"Matthew Chalk, Olivier Marre, Gasper Tkacik",
neurips,https://proceedings.neurips.cc/paper/2016/file/a8f15eda80c50adb0e71943adc8015cf-Paper.pdf,Learning What and Where to Draw,"Scott E. Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, Honglak Lee",
neurips,https://proceedings.neurips.cc/paper/2016/file/a9078e8653368c9c291ae2f8b74012e7-Paper.pdf,A Bio-inspired Redundant Sensing Architecture,"Anh Tuan Nguyen, Jian Xu, Zhi Yang",
neurips,https://proceedings.neurips.cc/paper/2016/file/a96d3afec184766bfeca7a9f989fc7e7-Paper.pdf,Bayesian Optimization with Robust Bayesian Neural Networks,"Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, Frank Hutter",
neurips,https://proceedings.neurips.cc/paper/2016/file/a9b7ba70783b617e9998dc4dd82eb3c5-Paper.pdf,Statistical Inference for Cluster Trees,"Jisu KIM, Yen-Chi Chen, Sivaraman Balakrishnan, Alessandro Rinaldo, Larry Wasserman",
neurips,https://proceedings.neurips.cc/paper/2016/file/aa169b49b583a2b5af89203c2b78c67c-Paper.pdf,Combinatorial Multi-Armed Bandit with General Reward Functions,"Wei Chen, Wei Hu, Fu Li, Jian Li, Yu Liu, Pinyan Lu","In this paper, we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the
max
(
)
max
function and nonlinear utility functions. Existing techniques relying on accurate estimations of the means of random variables, such as the upper confidence bound (UCB) technique, do not work directly on these functions. We propose a new algorithm called stochastically dominant confidence bound (SDCB), which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds. We prove that SDCB can achieve
O
(
log
T
)
O
distribution-dependent regret and
~
O
(
√
T
)
O
distribution-independent regret, where
T
T
is the time horizon. We apply our results to the
K
K
-MAX problem and expected utility maximization problems. In particular, for
K
K
-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first
~
O
(
√
T
)
O
bound on the
(
1
−
ϵ
)
(
-approximation regret of its online problem, for any
ϵ
>
0
ϵ
."
neurips,https://proceedings.neurips.cc/paper/2016/file/aa486f25175cbdc3854151288a645c19-Paper.pdf,Learning Sensor Multiplexing Design through Back-propagation,Ayan Chakrabarti,
neurips,https://proceedings.neurips.cc/paper/2016/file/aace49c7d80767cffec0e513ae886df0-Paper.pdf,Short-Dot: Computing Large Linear Transforms Distributedly Using Coded Short Dot Products,"Sanghamitra Dutta, Viveck Cadambe, Pulkit Grover",
neurips,https://proceedings.neurips.cc/paper/2016/file/ab88b15733f543179858600245108dd8-Paper.pdf,SEBOOST - Boosting Stochastic Learning Using Subspace Optimization Techniques,"Elad Richardson, Rom Herskovitz, Boris Ginsburg, Michael Zibulevsky",
neurips,https://proceedings.neurips.cc/paper/2016/file/abd815286ba1007abfbb8415b83ae2cf-Paper.pdf,VIME: Variational Information Maximizing Exploration,"Rein Houthooft, Xi Chen, Xi Chen, Yan Duan, John Schulman, Filip De Turck, Pieter Abbeel",
neurips,https://proceedings.neurips.cc/paper/2016/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf,Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity,"Amit Daniely, Roy Frostig, Yoram Singer",
neurips,https://proceedings.neurips.cc/paper/2016/file/ac627ab1ccbdb62ec96e702f07f6425b-Paper.pdf,Unsupervised Domain Adaptation with Residual Transfer Networks,"Mingsheng Long, Han Zhu, Jianmin Wang, Michael I. Jordan",
neurips,https://proceedings.neurips.cc/paper/2016/file/ac796a52db3f16bbdb6557d3d89d1c5a-Paper.pdf,Stochastic Gradient MCMC with Stale Gradients,"Changyou Chen, Nan Ding, Chunyuan Li, Yizhe Zhang, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2016/file/acc3e0404646c57502b480dc052c4fe1-Paper.pdf,Efficient Nonparametric Smoothness Estimation,"Shashank Singh, Simon S. Du, Barnabas Poczos",
neurips,https://proceedings.neurips.cc/paper/2016/file/ad13a2a07ca4b7642959dc0c4c740ab6-Paper.pdf,Adversarial Multiclass Classification: A Risk Minimization Perspective,"Rizal Fathony, Anqi Liu, Kaiser Asif, Brian Ziebart",
neurips,https://proceedings.neurips.cc/paper/2016/file/af4732711661056eadbf798ba191272a-Paper.pdf,Long-term Causal Effects via Behavioral Game Theory,"Panagiotis Toulis, David C. Parkes",
neurips,https://proceedings.neurips.cc/paper/2016/file/afd4836712c5e77550897e25711e1d96-Paper.pdf,Sampling for Bayesian Program Learning,"Kevin Ellis, Armando Solar-Lezama, Josh Tenenbaum",
neurips,https://proceedings.neurips.cc/paper/2016/file/afda332245e2af431fb7b672a68b659d-Paper.pdf,Unifying Count-Based Exploration and Intrinsic Motivation,"Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos",
neurips,https://proceedings.neurips.cc/paper/2016/file/afe434653a898da20044041262b3ac74-Paper.pdf,Learning HMMs with Nonparametric Emissions via Spectral Decompositions of Continuous Matrices,"Kirthevasan Kandasamy, Maruan Al-Shedivat, Eric P. Xing","Recently, there has been a surge of interest in using spectral methods for estimating latent variable models. However, it is usually assumed that the distribution of the observations conditioned on the latent variables is either discrete or belongs to a parametric family. In this paper, we study the estimation of an
m
m
-state hidden Markov model (HMM) with only smoothness assumptions, such as H\""olderian conditions, on the emission densities. By leveraging some recent advances in continuous linear algebra and numerical analysis, we develop a computationally efficient spectral algorithm for learning nonparametric HMMs. Our technique is based on computing an SVD on nonparametric estimates of density functions by viewing them as \emph{continuous matrices}. We derive sample complexity bounds via concentration results for nonparametric density estimation and novel perturbation theory results for continuous matrices. We implement our method using Chebyshev polynomial approximations. Our method is competitive with other baselines on synthetic and real problems and is also very computationally efficient."
neurips,https://proceedings.neurips.cc/paper/2016/file/aff1621254f7c1be92f64550478c56e6-Paper.pdf,Observational-Interventional Priors for Dose-Response Learning,Ricardo Silva,
neurips,https://proceedings.neurips.cc/paper/2016/file/b069b3415151fa7217e870017374de7c-Paper.pdf,Improved Error Bounds for Tree Representations of Metric Spaces,"Samir Chowdhury, Facundo Mémoli, Zane T. Smith",
neurips,https://proceedings.neurips.cc/paper/2016/file/b06f50d1f89bd8b2a0fb771c1a69c2b0-Paper.pdf,A Bayesian method for reducing bias in neural representational similarity analysis,"Mingbo Cai, Nicolas W. Schuck, Jonathan W. Pillow, Yael Niv",
neurips,https://proceedings.neurips.cc/paper/2016/file/b090409688550f3cc93f4ed88ec6cafb-Paper.pdf,Multistage Campaigning in Social Networks,"Mehrdad Farajtabar, Xiaojing Ye, Sahar Harati, Le Song, Hongyuan Zha",
neurips,https://proceedings.neurips.cc/paper/2016/file/b1301141feffabac455e1f90a7de2054-Paper.pdf,Conditional Image Generation with PixelCNN Decoders,"Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, koray kavukcuoglu, Oriol Vinyals, Alex Graves",
neurips,https://proceedings.neurips.cc/paper/2016/file/b139e104214a08ae3f2ebcce149cdf6e-Paper.pdf,Global Optimality of Local Search for Low Rank Matrix Recovery,"Srinadh Bhojanapalli, Behnam Neyshabur, Nati Srebro",
neurips,https://proceedings.neurips.cc/paper/2016/file/b1563a78ec59337587f6ab6397699afc-Paper.pdf,Tensor Switching Networks,"Chuan-Yung Tsai, Andrew M. Saxe, Andrew M. Saxe, David Cox",
neurips,https://proceedings.neurips.cc/paper/2016/file/b20bb95ab626d93fd976af958fbc61ba-Paper.pdf,Optimistic Bandit Convex Optimization,"Scott Yang, Mehryar Mohri","We introduce the general and powerful scheme of predicting information re-use in optimization algorithms. This allows us to devise a computationally efficient algorithm for bandit convex optimization with new state-of-the-art guarantees for both Lipschitz loss functions and loss functions with Lipschitz gradients. This is the first algorithm admitting both a polynomial time complexity and a regret that is polynomial in the dimension of the action space that improves upon the original regret bound for Lipschitz loss functions, achieving a regret of
˜
O
(
T
11
/
16
d
3
/
8
)
O
. Our algorithm further improves upon the best existing polynomial-in-dimension bound (both computationally and in terms of regret) for loss functions with Lipschitz gradients, achieving a regret of
˜
O
(
T
8
/
13
d
5
/
3
)
O
."
neurips,https://proceedings.neurips.cc/paper/2016/file/b2531e7bb29bf22e1daae486fae3417a-Paper.pdf,Interpretable Nonlinear Dynamic Modeling of Neural Trajectories,"Yuan Zhao, Il Memming Park",
neurips,https://proceedings.neurips.cc/paper/2016/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf,Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm,"Qiang Liu, Dilin Wang",
neurips,https://proceedings.neurips.cc/paper/2016/file/b3f61131b6eceeb2b14835fa648a48ff-Paper.pdf,Learning in Games: Robustness of Fast Convergence,"Dylan J. Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, Eva Tardos",
neurips,https://proceedings.neurips.cc/paper/2016/file/b4288d9c0ec0a1841b3b3728321e7088-Paper.pdf,Causal Bandits: Learning Good Interventions via Causal Inference,"Finnian Lattimore, Tor Lattimore, Mark D. Reid",
neurips,https://proceedings.neurips.cc/paper/2016/file/b4568df26077653eeadf29596708c94b-Paper.pdf,Minimax Optimal Alternating Minimization for Kernel Nonparametric Tensor Learning,"Taiji Suzuki, Heishiro Kanagawa, Hayato Kobayashi, Nobuyuki Shimizu, Yukihiro Tagami",
neurips,https://proceedings.neurips.cc/paper/2016/file/b495ce63ede0f4efc9eec62cb947c162-Paper.pdf,Universal Correspondence Network,"Christopher B. Choy, JunYoung Gwak, Silvio Savarese, Manmohan Chandraker","We present a deep learning framework for accurate visual correspondences and demonstrate its effectiveness for both geometric and semantic matching, spanning across rigid motions to intra-class shape or appearance variations. In contrast to previous CNN-based approaches that optimize a surrogate patch similarity objective, we use deep metric learning to directly learn a feature space that preserves either geometric or semantic similarity. Our fully convolutional architecture, along with a novel correspondence contrastive loss allows faster training by effective reuse of computations, accurate gradient computation through the use of thousands of examples per image pair and faster testing with
O
(
n
)
O
feedforward passes for n keypoints, instead of
O
(
n
2
)
O
for typical patch similarity methods. We propose a convolutional spatial transformer to mimic patch normalization in traditional features like SIFT, which is shown to dramatically boost accuracy for semantic correspondences across intra-class shape variations. Extensive experiments on KITTI, PASCAL and CUB-2011 datasets demonstrate the significant advantages of our features over prior works that use either hand-constructed or learned features."
neurips,https://proceedings.neurips.cc/paper/2016/file/b51a15f382ac914391a58850ab343b00-Paper.pdf,Phased Exploration with Greedy Exploitation in Stochastic Combinatorial Partial Monitoring Games,"Sougata Chaudhuri, Ambuj Tewari","Partial monitoring games are repeated games where the learner receives feedback that might be different from adversary's move or even the reward gained by the learner. Recently, a general model of combinatorial partial monitoring (CPM) games was proposed \cite{lincombinatorial2014}, where the learner's action space can be exponentially large and adversary samples its moves from a bounded, continuous space, according to a fixed distribution. The paper gave a confidence bound based algorithm (GCB) that achieves
O
(
T
2
/
3
log
T
)
O
distribution independent and
O
(
log
T
)
O
distribution dependent regret bounds. The implementation of their algorithm depends on two separate offline oracles and the distribution dependent regret additionally requires existence of a unique optimal action for the learner. Adopting their CPM model, our first contribution is a Phased Exploration with Greedy Exploitation (PEGE) algorithmic framework for the problem. Different algorithms within the framework achieve
O
(
T
2
/
3
√
log
T
)
O
distribution independent and
O
(
log
2
T
)
O
distribution dependent regret respectively. Crucially, our framework needs only the simpler
argmax'' oracle from GCB and the distribution dependent regret does not require existence of a unique optimal action. Our second contribution is another algorithm, PEGE2, which combines gap estimation with a PEGE algorithm, to achieve an
O
(
log
T
)
O
regret bound, matching the GCB guarantee but removing the dependence on size of the learner's action space. However, like GCB, PEGE2 requires access to both offline oracles and the existence of a unique optimal action. Finally, we discuss how our algorithm can be efficiently applied to a CPM problem of practical interest: namely, online ranking with feedback at the top."
neurips,https://proceedings.neurips.cc/paper/2016/file/b5488aeff42889188d03c9895255cecc-Paper.pdf,Showing versus doing: Teaching by demonstration,"Mark K. Ho, Michael Littman, James MacGlashan, Fiery Cushman, Joseph L. Austerweil",
neurips,https://proceedings.neurips.cc/paper/2016/file/b59c67bf196a4758191e42f76670ceba-Paper.pdf,Learning Transferrable Representations for Unsupervised Domain Adaptation,"Ozan Sener, Hyun Oh Song, Ashutosh Saxena, Silvio Savarese",
neurips,https://proceedings.neurips.cc/paper/2016/file/b5a1fc2085986034e448d2ccc5bb9703-Paper.pdf,On Robustness of Kernel Clustering,"Bowei Yan, Purnamrita Sarkar",
neurips,https://proceedings.neurips.cc/paper/2016/file/b5dc4e5d9b495d0196f61d45b26ef33e-Paper.pdf,"Homotopy Smoothing for Non-Smooth Problems with Lower Complexity than
O
(
1
/
ϵ
)
O","Yi Xu, Yan Yan, Qihang Lin, Tianbao Yang","In this paper, we develop a novel {\bf ho}moto{\bf p}y {\bf s}moothing (HOPS) algorithm for solving a family of non-smooth problems that is composed of a non-smooth term with an explicit max-structure and a smooth term or a simple non-smooth term whose proximal mapping is easy to compute. The best known iteration complexity for solving such non-smooth optimization problems is
O
(
1
/
ϵ
)
O
without any assumption on the strong convexity. In this work, we will show that the proposed HOPS achieved a lower iteration complexity of
~
O
(
1
/
ϵ
1
−
θ
)
O
with
θ
∈
(
0
,
1
]
θ
capturing the local sharpness of the objective function around the optimal solutions. To the best of our knowledge, this is the lowest iteration complexity achieved so far for the considered non-smooth optimization problems without strong convexity assumption. The HOPS algorithm employs Nesterov's smoothing technique and Nesterov's accelerated gradient method and runs in stages, which gradually decreases the smoothing parameter in a stage-wise manner until it yields a sufficiently good approximation of the original function. We show that HOPS enjoys a linear convergence for many well-known non-smooth problems (e.g., empirical risk minimization with a piece-wise linear loss function and
ℓ
1
ℓ
norm regularizer, finding a point in a polyhedron, cone programming, etc). Experimental results verify the effectiveness of HOPS in comparison with Nesterov's smoothing algorithm and the primal-dual style of first-order methods."
neurips,https://proceedings.neurips.cc/paper/2016/file/b5f1e8fb36cd7fbeb7988e8639ac79e9-Paper.pdf,Fast Algorithms for Robust PCA via Gradient Descent,"Xinyang Yi, Dohyung Park, Yudong Chen, Constantine Caramanis","We consider the problem of Robust PCA in the fully and partially observed settings. Without corruptions, this is the well-known matrix completion problem. From a statistical standpoint this problem has been recently well-studied, and conditions on when recovery is possible (how many observations do we need, how many corruptions can we tolerate) via polynomial-time algorithms is by now understood. This paper presents and analyzes a non-convex optimization approach that greatly reduces the computational complexity of the above problems, compared to the best available algorithms. In particular, in the fully observed case, with
r
r
denoting rank and
d
d
dimension, we reduce the complexity from
O
(
r
2
d
2
log
(
1
/
ϵ
)
)
O
to
O
(
r
d
2
log
(
1
/
ϵ
)
)
O
-- a big savings when the rank is big. For the partially observed case, we show the complexity of our algorithm is no more than
O
(
r
4
d
log
(
d
)
log
(
1
/
ϵ
)
)
O
. Not only is this the best-known run-time for a provable algorithm under partial observation, but in the setting where
r
r
is small compared to
d
d
, it also allows for near-linear-in-
d
d
run-time that can be exploited in the fully-observed case as well, by simply running our algorithm on a subset of the observations."
neurips,https://proceedings.neurips.cc/paper/2016/file/b7087c1f4f89e63af8d46f3b20271153-Paper.pdf,Dimensionality Reduction of Massive Sparse Datasets Using Coresets,"Dan Feldman, Mikhail Volkov, Daniela Rus","In this paper we present a practical solution with performance guarantees to the problem of dimensionality reduction for very large scale sparse matrices. We show applications of our approach to computing the Principle Component Analysis (PCA) of any
n
×
d
n
matrix, using one pass over the stream of its rows. Our solution uses coresets: a scaled subset of the
n
n
rows that approximates their sum of squared distances to \emph{every}
k
k
-dimensional \emph{affine} subspace. An open theoretical problem has been to compute such a coreset that is independent of both
n
n
and
d
d
. An open practical problem has been to compute a non-trivial approximation to the PCA of very large but sparse databases such as the Wikipedia document-term matrix in a reasonable time. We answer both of these questions affirmatively. Our main technical result is a new framework for deterministic coreset constructions based on a reduction to the problem of counting items in a stream."
neurips,https://proceedings.neurips.cc/paper/2016/file/b73dfe25b4b8714c029b37a6ad3006fa-Paper.pdf,Doubly Convolutional Neural Networks,"Shuangfei Zhai, Yu Cheng, Zhongfei (Mark) Zhang, Weining Lu",
neurips,https://proceedings.neurips.cc/paper/2016/file/b9d487a30398d42ecff55c228ed5652b-Paper.pdf,Brains on Beats,"Umut Güçlü, Jordy Thielen, Michael Hanke, Marcel van Gerven",
neurips,https://proceedings.neurips.cc/paper/2016/file/b9f94c77652c9a76fc8a442748cd54bd-Paper.pdf,Local Minimax Complexity of Stochastic Convex Optimization,"sabyasachi chatterjee, John C. Duchi, John Lafferty, Yuancheng Zhu",
neurips,https://proceedings.neurips.cc/paper/2016/file/bad5f33780c42f2588878a9d07405083-Paper.pdf,Kronecker Determinantal Point Processes,"Zelda E. Mariet, Suvrit Sra",
neurips,https://proceedings.neurips.cc/paper/2016/file/bb03e43ffe34eeb242a2ee4a4f125e56-Paper.pdf,Normalized Spectral Map Synchronization,"Yanyao Shen, Qixing Huang, Nati Srebro, Sujay Sanghavi",
neurips,https://proceedings.neurips.cc/paper/2016/file/bb7946e7d85c81a9e69fee1cea4a087c-Paper.pdf,Error Analysis of Generalized Nyström Kernel Regression,"Hong Chen, Haifeng Xia, Heng Huang, Weidong Cai","Nystr\""{o}m method has been used successfully to improve the computational efficiency of kernel ridge regression (KRR). Recently, theoretical analysis of Nystr\""{o}m KRR, including generalization bound and convergence rate, has been established based on reproducing kernel Hilbert space (RKHS) associated with the symmetric positive semi-definite kernel. However, in real world applications, RKHS is not always optimal and kernel function is not necessary to be symmetric or positive semi-definite. In this paper, we consider the generalized Nystr\""{o}m kernel regression (GNKR) with
ℓ
2
ℓ
coefficient regularization, where the kernel just requires the continuity and boundedness. Error analysis is provided to characterize its generalization performance and the column norm sampling is introduced to construct the refined hypothesis space. In particular, the fast learning rate with polynomial decay is reached for the GNKR. Experimental analysis demonstrates the satisfactory performance of GNKR with the column norm sampling."
neurips,https://proceedings.neurips.cc/paper/2016/file/bbf94b34eb32268ada57a3be5062fe7d-Paper.pdf,Regularized Nonlinear Acceleration,"Damien Scieur, Alexandre d'Aspremont, Francis Bach",
neurips,https://proceedings.neurips.cc/paper/2016/file/bcb41ccdc4363c6848a1d760f26c28a0-Paper.pdf,Pairwise Choice Markov Chains,"Stephen Ragain, Johan Ugander",
neurips,https://proceedings.neurips.cc/paper/2016/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf,Stochastic Variational Deep Kernel Learning,"Andrew G. Wilson, Zhiting Hu, Russ R. Salakhutdinov, Eric P. Xing",
neurips,https://proceedings.neurips.cc/paper/2016/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf,Theoretical Comparisons of Positive-Unlabeled Learning against Positive-Negative Learning,"Gang Niu, Marthinus Christoffel du Plessis, Tomoya Sakai, Yao Ma, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2016/file/be3e9d3f7d70537357c67bb3f4086846-Paper.pdf,A Non-generative Framework and Convex Relaxations for Unsupervised Learning,"Elad Hazan, Tengyu Ma",
neurips,https://proceedings.neurips.cc/paper/2016/file/c0e190d8267e36708f955d7ab048990d-Paper.pdf,DISCO Nets : DISsimilarity COefficients Networks,"Diane Bouchacourt, Pawan K. Mudigonda, Sebastian Nowozin",
neurips,https://proceedings.neurips.cc/paper/2016/file/c203d8a151612acf12457e4d67635a95-Paper.pdf,Learning to Poke by Poking: Experiential Learning of Intuitive Physics,"Pulkit Agrawal, Ashvin V. Nair, Pieter Abbeel, Jitendra Malik, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2016/file/c21002f464c5fc5bee3b98ced83963b8-Paper.pdf,Value Iteration Networks,"Aviv Tamar, YI WU, Garrett Thomas, Sergey Levine, Pieter Abbeel",
neurips,https://proceedings.neurips.cc/paper/2016/file/c24cd76e1ce41366a4bbe8a49b02a028-Paper.pdf,Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images,"Junhua Mao, Jiajing Xu, Kevin Jing, Alan L. Yuille",
neurips,https://proceedings.neurips.cc/paper/2016/file/c2626d850c80ea07e7511bbae4c76f4b-Paper.pdf,Simple and Efficient Weighted Minwise Hashing,Anshumali Shrivastava,
neurips,https://proceedings.neurips.cc/paper/2016/file/c3395dd46c34fa7fd8d729d8cf88b7a8-Paper.pdf,Cooperative Inverse Reinforcement Learning,"Dylan Hadfield-Menell, Stuart J. Russell, Pieter Abbeel, Anca Dragan",
neurips,https://proceedings.neurips.cc/paper/2016/file/c3992e9a68c5ae12bd18488bc579b30d-Paper.pdf,Safe and Efficient Off-Policy Reinforcement Learning,"Remi Munos, Tom Stepleton, Anna Harutyunyan, Marc Bellemare",
neurips,https://proceedings.neurips.cc/paper/2016/file/c3e4035af2a1cde9f21e1ae1951ac80b-Paper.pdf,LightRNN: Memory and Computation-Efficient Recurrent Neural Networks,"Xiang Li, Tao Qin, Jian Yang, Tie-Yan Liu","Recurrent neural networks (RNNs) have achieved state-of-the-art performances in many natural language processing tasks, such as language modeling and machine translation. However, when the vocabulary is large, the RNN model will become very big (e.g., possibly beyond the memory capacity of a GPU device) and its training will become very inefficient. In this work, we propose a novel technique to tackle this challenge. The key idea is to use 2-Component (2C) shared embedding for word representations. We allocate every word in the vocabulary into a table, each row of which is associated with a vector, and each column associated with another vector. Depending on its position in the table, a word is jointly represented by two components: a row vector and a column vector. Since the words in the same row share the row vector and the words in the same column share the column vector, we only need
2
√
|
V
|
2
vectors to represent a vocabulary of
|
V
|
|
unique words, which are far less than the
|
V
|
|
vectors required by existing approaches. Based on the 2-Component shared embedding, we design a new RNN algorithm and evaluate it using the language modeling task on several benchmark datasets. The results show that our algorithm significantly reduces the model size and speeds up the training process, without sacrifice of accuracy (it achieves similar, if not better, perplexity as compared to state-of-the-art language models). Remarkably, on the One-Billion-Word benchmark Dataset, our algorithm achieves comparable perplexity to previous language models, whilst reducing the model size by a factor of 40-100, and speeding up the training process by a factor of 2. We name our proposed algorithm \emph{LightRNN} to reflect its very small model size and very high training speed."
neurips,https://proceedings.neurips.cc/paper/2016/file/c4015b7f368e6b4871809f49debe0579-Paper.pdf,Deep Learning Games,"Dale Schuurmans, Martin A. Zinkevich",
neurips,https://proceedings.neurips.cc/paper/2016/file/c4492cbe90fbdbf88a5aec486aa81ed5-Paper.pdf,Strategic Attentive Writer for Learning Macro-Actions,"Alexander Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex Graves, Oriol Vinyals, John Agapiou, koray kavukcuoglu",
neurips,https://proceedings.neurips.cc/paper/2016/file/c4851e8e264415c4094e4e85b0baa7cc-Paper.pdf,Clustering with Bregman Divergences: an Asymptotic Analysis,"Chaoyue Liu, Mikhail Belkin","Clustering, in particular
k
k
-means clustering, is a central topic in data analysis. Clustering with Bregman divergences is a recently proposed generalization of
k
k
-means clustering which has already been widely used in applications. In this paper we analyze theoretical properties of Bregman clustering when the number of the clusters
k
k
is large. We establish quantization rates and describe the limiting distribution of the centers as
k
→
∞
k
, extending well-known results for
k
k
-means clustering."
neurips,https://proceedings.neurips.cc/paper/2016/file/c51ce410c124a10e0db5e4b97fc2af39-Paper.pdf,Swapout: Learning an ensemble of deep architectures,"Saurabh Singh, Derek Hoiem, David Forsyth",
neurips,https://proceedings.neurips.cc/paper/2016/file/c52f1bd66cc19d05628bd8bf27af3ad6-Paper.pdf,Stochastic Online AUC Maximization,"Yiming Ying, Longyin Wen, Siwei Lyu",
neurips,https://proceedings.neurips.cc/paper/2016/file/c5ff2543b53f4cc0ad3819a36752467b-Paper.pdf,Optimizing affinity-based binary hashing using auxiliary coordinates,"Ramin Raziperchikolaei, Miguel A. Carreira-Perpinan",
neurips,https://proceedings.neurips.cc/paper/2016/file/c667d53acd899a97a85de0c201ba99be-Paper.pdf,Sample Complexity of Automated Mechanism Design,"Maria-Florina F. Balcan, Tuomas Sandholm, Ellen Vitercik",
neurips,https://proceedings.neurips.cc/paper/2016/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Paper.pdf,LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain,"Zeyuan Allen-Zhu, Yuanzhi Li","We study k-SVD that is to obtain the first k singular vectors of a matrix A. Recently, a few breakthroughs have been discovered on k-SVD: Musco and Musco [1] proved the first gap-free convergence result using the block Krylov method, Shamir [2] discovered the first variance-reduction stochastic method, and Bhojanapalli et al. [3] provided the fastest
O
(
n
n
z
(
A
)
+
p
o
l
y
(
1
/
ε
)
)
O
-time algorithm using alternating minimization. In this paper, we put forward a new and simple LazySVD framework to improve the above breakthroughs. This framework leads to a faster gap-free method outperforming [1], and the first accelerated and stochastic method outperforming [2]. In the
O
(
n
n
z
(
A
)
+
p
o
l
y
(
1
/
ε
)
)
O
running-time regime, LazySVD outperforms [3] in certain parameter regimes without even using alternating minimization."
neurips,https://proceedings.neurips.cc/paper/2016/file/c70daf247944fe3add32218f914c75a6-Paper.pdf,A Probabilistic Framework for Deep Learning,"Ankit B. Patel, Minh Tan Nguyen, Richard Baraniuk",
neurips,https://proceedings.neurips.cc/paper/2016/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf,Without-Replacement Sampling for Stochastic Gradient Methods,Ohad Shamir,
neurips,https://proceedings.neurips.cc/paper/2016/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf,Learning to Communicate with Deep Multi-Agent Reinforcement Learning,"Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, Shimon Whiteson",
neurips,https://proceedings.neurips.cc/paper/2016/file/c8067ad1937f728f51288b3eb986afaa-Paper.pdf,Understanding the Effective Receptive Field in Deep Convolutional Neural Networks,"Wenjie Luo, Yujia Li, Raquel Urtasun, Richard Zemel",
neurips,https://proceedings.neurips.cc/paper/2016/file/c86a7ee3d8ef0b551ed58e354a836f2b-Paper.pdf,Barzilai-Borwein Step Size for Stochastic Gradient Descent,"Conghui Tan, Shiqian Ma, Yu-Hong Dai, Yuqiu Qian",
neurips,https://proceedings.neurips.cc/paper/2016/file/c8758b517083196f05ac29810b924aca-Paper.pdf,The Power of Optimization from Samples,"Eric Balkanski, Aviad Rubinstein, Yaron Singer",
neurips,https://proceedings.neurips.cc/paper/2016/file/c88d8d0a6097754525e02c2246d8d27f-Paper.pdf,New Liftable Classes for First-Order Probabilistic Inference,"Seyed Mehran Kazemi, Angelika Kimmig, Guy Van den Broeck, David Poole",
neurips,https://proceedings.neurips.cc/paper/2016/file/c8ed21db4f678f3b13b9d5ee16489088-Paper.pdf,Optimal Tagging with Markov Chain Optimization,"Nir Rosenfeld, Amir Globerson",
neurips,https://proceedings.neurips.cc/paper/2016/file/c913303f392ffc643f7240b180602652-Paper.pdf,Fast and Flexible Monotonic Functions with Ensembles of Lattices,"Mahdi Milani Fard, Kevin Canini, Andrew Cotter, Jan Pfeifer, Maya Gupta",
neurips,https://proceedings.neurips.cc/paper/2016/file/c9f0f895fb98ab9159f51fd0297e236d-Paper.pdf,A scaled Bregman theorem with applications,"Richard Nock, Aditya Menon, Cheng Soon Ong",
neurips,https://proceedings.neurips.cc/paper/2016/file/ca460332316d6da84b08b9bcf39b687b-Paper.pdf,The Product Cut,"Thomas Laurent, James von Brecht, Xavier Bresson, arthur szlam",
neurips,https://proceedings.neurips.cc/paper/2016/file/ca8155f4d27f205953f9d3d7974bdd70-Paper.pdf,Learning from Rational Behavior: Predicting Solutions to Unknown Linear Programs,"Shahin Jabbari, Ryan M. Rogers, Aaron Roth, Steven Z. Wu",
neurips,https://proceedings.neurips.cc/paper/2016/file/cb2c2041d9763d84d7d655e81178f444-Paper.pdf,Unified Methods for Exploiting Piecewise Linear Structure in Convex Optimization,"Tyler B. Johnson, Carlos Guestrin",
neurips,https://proceedings.neurips.cc/paper/2016/file/cb79f8fa58b91d3af6c9c991f63962d3-Paper.pdf,Large-Scale Price Optimization via Network Flow,"Shinji Ito, Ryohei Fujimaki",
neurips,https://proceedings.neurips.cc/paper/2016/file/cc7e2b878868cbae992d1fb743995d8f-Paper.pdf,Generative Adversarial Imitation Learning,"Jonathan Ho, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2016/file/ce78d1da254c0843eb23951ae077ff5f-Paper.pdf,Truncated Variance Reduction: A Unified Approach to Bayesian Optimization and Level-Set Estimation,"Ilija Bogunovic, Jonathan Scarlett, Andreas Krause, Volkan Cevher",
neurips,https://proceedings.neurips.cc/paper/2016/file/cedebb6e872f539bef8c3f919874e9d7-Paper.pdf,f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization,"Sebastian Nowozin, Botond Cseke, Ryota Tomioka","Generative neural networks are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any
f
f
-divergence can be used for training generative neural networks. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models."
neurips,https://proceedings.neurips.cc/paper/2016/file/cf1f78fe923afe05f7597da2be7a3da8-Paper.pdf,Nearly Isometric Embedding by Relaxation,"James McQueen, Marina Meila, Dominique Joncas",
neurips,https://proceedings.neurips.cc/paper/2016/file/cfee398643cbc3dc5eefc89334cacdc1-Paper.pdf,DECOrrelated feature space partitioning for distributed sparse regression,"Xiangyu Wang, David B. Dunson, Chenlei Leng",
neurips,https://proceedings.neurips.cc/paper/2016/file/d09bf41544a3365a46c9077ebb5e35c3-Paper.pdf,Incremental Boosting Convolutional Neural Network for Facial Action Unit Recognition,"Shizhong Han, Zibo Meng, AHMED-SHEHAB KHAN, Yan Tong",
neurips,https://proceedings.neurips.cc/paper/2016/file/d1a21da7bca4abff8b0b61b87597de73-Paper.pdf,An urn model for majority voting in classification ensembles,"Victor Soto, Alberto Suárez, Gonzalo Martinez-Muñoz",
neurips,https://proceedings.neurips.cc/paper/2016/file/d305281faf947ca7acade9ad5c8c818c-Paper.pdf,Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA,"Aapo Hyvarinen, Hiroshi Morioka",
neurips,https://proceedings.neurips.cc/paper/2016/file/d43ab110ab2489d6b9b2caa394bf920f-Paper.pdf,Leveraging Sparsity for Efficient Submodular Data Summarization,"Erik Lindgren, Shanshan Wu, Alexandros G. Dimakis",
neurips,https://proceedings.neurips.cc/paper/2016/file/d46e1fcf4c07ce4a69ee07e4134bcef1-Paper.pdf,Mistake Bounds for Binary Matrix Completion,"Mark Herbster, Stephen Pasteris, Massimiliano Pontil",
neurips,https://proceedings.neurips.cc/paper/2016/file/d47268e9db2e9aa3827bba3afb7ff94a-Paper.pdf,Quantum Perceptron Models,"Ashish Kapoor, Nathan Wiebe, Krysta Svore","We demonstrate how quantum computation can provide non-trivial improvements in the computational and statistical complexity of the perceptron model. We develop two quantum algorithms for perceptron learning. The first algorithm exploits quantum information processing to determine a separating hyperplane using a number of steps sublinear in the number of data points
N
N
, namely
O
(
√
N
)
O
. The second algorithm illustrates how the classical mistake bound of
O
(
1
γ
2
)
O
can be further improved to
O
(
1
√
γ
)
O
through quantum means, where
γ
γ
denotes the margin. Such improvements are achieved through the application of quantum amplitude amplification to the version space interpretation of the perceptron model."
neurips,https://proceedings.neurips.cc/paper/2016/file/d490d7b4576290fa60eb31b5fc917ad1-Paper.pdf,Direct Feedback Alignment Provides Learning in Deep Neural Networks,Arild Nøkland,
neurips,https://proceedings.neurips.cc/paper/2016/file/d54e99a6c03704e95e6965532dec148b-Paper.pdf,Average-case hardness of RIP certification,"Tengyao Wang, Quentin Berthet, Yaniv Plan",
neurips,https://proceedings.neurips.cc/paper/2016/file/d5e2fbef30a4eb668a203060ec8e5eef-Paper.pdf,Efficient High-Order Interaction-Aware Feature Selection Based on Conditional Mutual Information,"Alexander Shishkin, Anastasia Bezzubtseva, Alexey Drutsa, Ilia Shishkov, Ekaterina Gladkikh, Gleb Gusev, Pavel Serdyukov",
neurips,https://proceedings.neurips.cc/paper/2016/file/d67d8ab4f4c10bf22aa353e27879133c-Paper.pdf,Fast and Provably Good Seedings for k-Means,"Olivier Bachem, Mario Lucic, Hamed Hassani, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2016/file/d707329bece455a462b58ce00d1194c9-Paper.pdf,Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm,"Kejun Huang, Xiao Fu, Nikolaos D. Sidiropoulos",
neurips,https://proceedings.neurips.cc/paper/2016/file/d757719ed7c2b66dd17dcee2a3cb29f4-Paper.pdf,High Dimensional Structured Superposition Models,"Qilong Gu, Arindam Banerjee",
neurips,https://proceedings.neurips.cc/paper/2016/file/d79aac075930c83c2f1e369a511148fe-Paper.pdf,A Bandit Framework for Strategic Regression,"Yang Liu, Yiling Chen","We consider a learner's problem of acquiring data dynamically for training a regression model, where the training data are collected from strategic data sources. A fundamental challenge is to incentivize data holders to exert effort to improve the quality of their reported data, despite that the quality is not directly verifiable by the learner. In this work, we study a dynamic data acquisition process where data holders can contribute multiple times. Using a bandit framework, we leverage on the long-term incentive of future job opportunities to incentivize high-quality contributions. We propose a Strategic Regression-Upper Confidence Bound (SR-UCB) framework, an UCB-style index combined with a simple payment rule, where the index of a worker approximates the quality of his past contributions and is used by the learner to determine whether the worker receives future work. For linear regression and certain family of non-linear regression problems, we show that SR-UCB enables a
O
(
√
log
T
/
T
)
O
-Bayesian Nash Equilibrium (BNE) where each worker exerting a target effort level that the learner has chosen, with
T
T
being the number of data acquisition stages. The SR-UCB framework also has some other desirable properties: (1) The indexes can be updated in an online fashion (hence computationally light). (2) A slight variant, namely Private SR-UCB (PSR-UCB), is able to preserve
(
O
(
log
−
1
T
)
,
O
(
log
−
1
T
)
)
(
-differential privacy for workers' data, with only a small compromise on incentives (achieving
O
(
log
6
T
/
√
T
)
O
-BNE)."
neurips,https://proceedings.neurips.cc/paper/2016/file/d79c6256b9bdac53a55801a066b70da3-Paper.pdf,Linear Relaxations for Finding Diverse Elements in Metric Spaces,"Aditya Bhaskara, Mehrdad Ghadiri, Vahab Mirrokni, Ola Svensson",
neurips,https://proceedings.neurips.cc/paper/2016/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf,Binarized Neural Networks,"Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio",
neurips,https://proceedings.neurips.cc/paper/2016/file/d840cc5d906c3e9c84374c8919d2074e-Paper.pdf,Learning a Metric Embedding for Face Recognition using the Multibatch Method,"Oren Tadmor, Tal Rosenwein, Shai Shalev-Shwartz, Yonatan Wexler, Amnon Shashua","This work is motivated by the engineering task of achieving a near state-of-the-art face recognition on a minimal computing budget running on an embedded system. Our main technical contribution centers around a novel training method, called Multibatch, for similarity learning, i.e., for the task of generating an invariant
face signature'' through training pairs of
same'' and
not-same'' face images. The Multibatch method first generates signatures for a mini-batch of
k
k
face images and then constructs an unbiased estimate of the full gradient by relying on all
k
2
−
k
k
pairs from the mini-batch. We prove that the variance of the Multibatch estimator is bounded by
O
(
1
/
k
2
)
O
, under some mild conditions. In contrast, the standard gradient estimator that relies on random
k
/
2
k
pairs has a variance of order
1
/
k
1
. The smaller variance of the Multibatch estimator significantly speeds up the convergence rate of stochastic gradient descent. Using the Multibatch method we train a deep convolutional neural network that achieves an accuracy of
98.2
%
98.2
on the LFW benchmark, while its prediction runtime takes only
30
30
msec on a single ARM Cortex A9 core. Furthermore, the entire training process took only 12 hours on a single Titan X GPU."
neurips,https://proceedings.neurips.cc/paper/2016/file/d947bf06a885db0d477d707121934ff8-Paper.pdf,Operator Variational Inference,"Rajesh Ranganath, Dustin Tran, Jaan Altosaar, David Blei",
neurips,https://proceedings.neurips.cc/paper/2016/file/d9d4f495e875a2e075a1a4a6e1b9770f-Paper.pdf,Unsupervised Learning for Physical Interaction through Video Prediction,"Chelsea Finn, Ian Goodfellow, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2016/file/d9ff90f4000eacd3a6c9cb27f78994cf-Paper.pdf,Full-Capacity Unitary Recurrent Neural Networks,"Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, Les Atlas",
neurips,https://proceedings.neurips.cc/paper/2016/file/daca41214b39c5dc66674d09081940f0-Paper.pdf,Linear-Memory and Decomposition-Invariant Linearly Convergent Conditional Gradient Algorithm for Structured Polytopes,"Dan Garber, Dan Garber, Ofer Meshi",
neurips,https://proceedings.neurips.cc/paper/2016/file/db116b39f7a3ac5366079b1d9fe249a5-Paper.pdf,Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning,"Wouter M. Koolen, Peter Grünwald, Tim van Erven",
neurips,https://proceedings.neurips.cc/paper/2016/file/db1915052d15f7815c8b88e879465a1e-Paper.pdf,A Comprehensive Linear Speedup Analysis for Asynchronous Stochastic Parallel Optimization from Zeroth-Order to First-Order,"Xiangru Lian, Huan Zhang, Cho-Jui Hsieh, Yijun Huang, Ji Liu",
neurips,https://proceedings.neurips.cc/paper/2016/file/dc09c97fd73d7a324bdbfe7c79525f64-Paper.pdf,Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling,"Maria-Florina F. Balcan, Hongyang Zhang","We study the problem of recovering an incomplete
m
×
n
m
matrix of rank
r
r
with columns arriving online over time. This is known as the problem of life-long matrix completion, and is widely applied to recommendation system, computer vision, system identification, etc. The challenge is to design provable algorithms tolerant to a large amount of noises, with small sample complexity. In this work, we give algorithms achieving strong guarantee under two realistic noise models. In bounded deterministic noise, an adversary can add any bounded yet unstructured noise to each column. For this problem, we present an algorithm that returns a matrix of a small error, with sample complexity almost as small as the best prior results in the noiseless case. For sparse random noise, where the corrupted columns are sparse and drawn randomly, we give an algorithm that exactly recovers an
μ
0
μ
-incoherent matrix by probability at least
1
−
δ
1
with sample complexity as small as
O
(
μ
0
r
n
log
(
r
/
δ
)
)
O
. This result advances the state-of-the-art work and matches the lower bound in a worst case. We also study the scenario where the hidden matrix lies on a mixture of subspaces and show that the sample complexity can be even smaller. Our proposed algorithms perform well experimentally in both synthetic and real-world datasets."
neurips,https://proceedings.neurips.cc/paper/2016/file/dc4c44f624d600aa568390f1f1104aa0-Paper.pdf,Satisfying Real-world Goals with Dataset Constraints,"Gabriel Goh, Andrew Cotter, Maya Gupta, Michael P. Friedlander",
neurips,https://proceedings.neurips.cc/paper/2016/file/dc5c768b5dc76a084531934b34601977-Paper.pdf,Launch and Iterate: Reducing Prediction Churn,"Mahdi Milani Fard, Quentin Cormier, Kevin Canini, Maya Gupta",
neurips,https://proceedings.neurips.cc/paper/2016/file/dc960c46c38bd16e953d97cdeefdbc68-Paper.pdf,Constraints Based Convex Belief Propagation,"Yaniv Tenzer, Alex Schwing, Kevin Gimpel, Tamir Hazan",
neurips,https://proceedings.neurips.cc/paper/2016/file/dd055f53a45702fe05e449c30ac80df9-Paper.pdf,Data driven estimation of Laplace-Beltrami operator,"Frederic Chazal, Ilaria Giulini, Bertrand Michel",
neurips,https://proceedings.neurips.cc/paper/2016/file/dd458505749b2941217ddd59394240e8-Paper.pdf,The Robustness of Estimator Composition,"Pingfan Tang, Jeff M. Phillips",
neurips,https://proceedings.neurips.cc/paper/2016/file/dd77279f7d325eec933f05b1672f6a1f-Paper.pdf,Active Learning from Imperfect Labelers,"Songbai Yan, Kamalika Chaudhuri, Tara Javidi",
neurips,https://proceedings.neurips.cc/paper/2016/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf,Improved Variational Inference with Inverse Autoregressive Flow,"Durk P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2016/file/de03beffeed9da5f3639a621bcab5dd4-Paper.pdf,Select-and-Sample for Spike-and-Slab Sparse Coding,"Abdul-Saboor Sheikh, Jörg Lücke",
neurips,https://proceedings.neurips.cc/paper/2016/file/df12ecd077efc8c23881028604dbb8cc-Paper.pdf,Learning Infinite RBMs with Frank-Wolfe,"Wei Ping, Qiang Liu, Alexander T. Ihler",
neurips,https://proceedings.neurips.cc/paper/2016/file/df877f3865752637daa540ea9cbc474f-Paper.pdf,Faster Projection-free Convex Optimization over the Spectrahedron,"Dan Garber, Dan Garber","Minimizing a convex function over the spectrahedron, i.e., the set of all
d
×
d
d
positive semidefinite matrices with unit trace, is an important optimization task with many applications in optimization, machine learning, and signal processing. It is also notoriously difficult to solve in large-scale since standard techniques require to compute expensive matrix decompositions. An alternative, is the conditional gradient method (aka Frank-Wolfe algorithm) that regained much interest in recent years, mostly due to its application to this specific setting. The key benefit of the CG method is that it avoids expensive matrix decompositions all together, and simply requires a single eigenvector computation per iteration, which is much more efficient. On the downside, the CG method, in general, converges with an inferior rate. The error for minimizing a
β
β
-smooth function after
t
t
iterations scales like
β
/
t
β
. This rate does not improve even if the function is also strongly convex. In this work we present a modification of the CG method tailored for the spectrahedron. The per-iteration complexity of the method is essentially identical to that of the standard CG method: only a single eigenvecor computation is required. For minimizing an
α
α
-strongly convex and
β
β
-smooth function, the \textit{expected} error of the method after
t
t
iterations is:
O
⎛
⎝
min
{
β
t
,
(
β
√
\rank
(
\X
∗
)
α
1
/
4
t
)
4
/
3
,
(
β
√
α
λ
min
(
\X
∗
)
t
)
2
}
⎞
⎠
O
. Beyond the significant improvement in convergence rate, it also follows that when the optimum is low-rank, our method provides better accuracy-rank tradeoff than the standard CG method. To the best of our knowledge, this is the first result that attains provably faster convergence rates for a CG variant for optimization over the spectrahedron. We also present encouraging preliminary empirical results."
neurips,https://proceedings.neurips.cc/paper/2016/file/dfa92d8f817e5b08fcaafb50d03763cf-Paper.pdf,Improved Regret Bounds for Oracle-Based Adversarial Contextual Bandits,"Vasilis Syrgkanis, Haipeng Luo, Akshay Krishnamurthy, Robert E. Schapire","We propose a new oracle-based algorithm, BISTRO+, for the adversarial contextual bandit problem, where either contexts are drawn i.i.d. or the sequence of contexts is known a priori, but where the losses are picked adversarially. Our algorithm is computationally efficient, assuming access to an offline optimization oracle, and enjoys a regret of order
O
(
(
K
T
)
2
3
(
log
N
)
1
3
)
O
, where
K
K
is the number of actions,
T
T
is the number of iterations, and
N
N
is the number of baseline policies. Our result is the first to break the
O
(
T
3
4
)
O
barrier achieved by recent algorithms, which was left as a major open problem. Our analysis employs the recent relaxation framework of (Rakhlin and Sridharan, ICML'16)."
neurips,https://proceedings.neurips.cc/paper/2016/file/dfce06801e1a85d6d06f1fdd4475dacd-Paper.pdf,Joint quantile regression in vector-valued RKHSs,"Maxime Sangnier, Olivier Fercoq, Florence d'Alché-Buc",
neurips,https://proceedings.neurips.cc/paper/2016/file/e139c454239bfde741e893edb46a06cc-Paper.pdf,Kernel Bayesian Inference with Posterior Regularization,"Yang Song, Jun Zhu, Yong Ren",
neurips,https://proceedings.neurips.cc/paper/2016/file/e1696007be4eefb81b1a1d39ce48681b-Paper.pdf,Scaled Least Squares Estimator for GLMs in Large-Scale Problems,"Murat A. Erdogdu, Lee H. Dicker, Mohsen Bayati","We study the problem of efficiently estimating the coefficients of generalized linear models (GLMs) in the large-scale setting where the number of observations
n
n
is much larger than the number of predictors
p
p
, i.e.
n
≫
p
≫
1
n
. We show that in GLMs with random (not necessarily Gaussian) design, the GLM coefficients are approximately proportional to the corresponding ordinary least squares (OLS) coefficients. Using this relation, we design an algorithm that achieves the same accuracy as the maximum likelihood estimator (MLE) through iterations that attain up to a cubic convergence rate, and that are cheaper than any batch optimization algorithm by at least a factor of
O
(
p
)
O
. We provide theoretical guarantees for our algorithm, and analyze the convergence behavior in terms of data dimensions. % Finally, we demonstrate the performance of our algorithm through extensive numerical studies on large-scale real and synthetic datasets, and show that it achieves the highest performance compared to several other widely used optimization algorithms."
neurips,https://proceedings.neurips.cc/paper/2016/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf,Contextual semibandits via supervised learning oracles,"Akshay Krishnamurthy, Alekh Agarwal, Miro Dudik",
neurips,https://proceedings.neurips.cc/paper/2016/file/e2a2dcc36a08a345332c751b2f2e476c-Paper.pdf,Learning Treewidth-Bounded Bayesian Networks with Thousands of Variables,"Mauro Scanagatta, Giorgio Corani, Cassio P. de Campos, Marco Zaffalon",
neurips,https://proceedings.neurips.cc/paper/2016/file/e4873aa9a05cc5ed839561d121516766-Paper.pdf,Unsupervised Learning from Noisy Networks with Applications to Hi-C Data,"Bo Wang, Junjie Zhu, Armin Pourshafeie, Oana Ursu, Serafim Batzoglou, Anshul Kundaje",
neurips,https://proceedings.neurips.cc/paper/2016/file/e4da3b7fbbce2345d7772b0674a318d5-Paper.pdf,Scan Order in Gibbs Sampling: Models in Which it Matters and Bounds on How Much,"Bryan D. He, Christopher M. De Sa, Ioannis Mitliagkas, Christopher Ré",
neurips,https://proceedings.neurips.cc/paper/2016/file/e56b06c51e1049195d7b26d043c478a0-Paper.pdf,Deep Neural Networks with Inexact Matching for Person Re-Identification,"Arulkumar Subramaniam, Moitreya Chatterjee, Anurag Mittal",
neurips,https://proceedings.neurips.cc/paper/2016/file/e6c2dc3dee4a51dcec3a876aa2339a78-Paper.pdf,Efficient Neural Codes under Metabolic Constraints,"Zhuo Wang, Xue-Xin Wei, Alan A. Stocker, Daniel D. Lee",
neurips,https://proceedings.neurips.cc/paper/2016/file/e70611883d2760c8bbafb4acb29e3446-Paper.pdf,Learning Kernels with Random Features,"Aman Sinha, John C. Duchi",
neurips,https://proceedings.neurips.cc/paper/2016/file/e816c635cad85a60fabd6b97b03cbcc9-Paper.pdf,Combinatorial semi-bandit with known covariance,"Rémy Degenne, Vianney Perchet",
neurips,https://proceedings.neurips.cc/paper/2016/file/e820a45f1dfc7b95282d10b6087e11c0-Paper.pdf,Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision,"Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, Honglak Lee",
neurips,https://proceedings.neurips.cc/paper/2016/file/e9b73bccd1762555582b513ff9d02492-Paper.pdf,Exact Recovery of Hard Thresholding Pursuit,"Xiaotong Yuan, Ping Li, Tong Zhang","The Hard Thresholding Pursuit (HTP) is a class of truncated gradient descent methods for finding sparse solutions of
ℓ
0
ℓ
-constrained loss minimization problems. The HTP-style methods have been shown to have strong approximation guarantee and impressive numerical performance in high dimensional statistical learning applications. However, the current theoretical treatment of these methods has traditionally been restricted to the analysis of parameter estimation consistency. It remains an open problem to analyze the support recovery performance (a.k.a., sparsistency) of this type of methods for recovering the global minimizer of the original NP-hard problem. In this paper, we bridge this gap by showing, for the first time, that exact recovery of the global sparse minimizer is possible for HTP-style methods under restricted strong condition number bounding conditions. We further show that HTP-style methods are able to recover the support of certain relaxed sparse solutions without assuming bounded restricted strong condition number. Numerical results on simulated data confirms our theoretical predictions."
neurips,https://proceedings.neurips.cc/paper/2016/file/e9fd7c2c6623306db59b6aef5c0d5cac-Paper.pdf,Parameter Learning for Log-supermodular Distributions,"Tatiana Shpakova, Francis Bach",
neurips,https://proceedings.neurips.cc/paper/2016/file/ea6b2efbdd4255a9f1b3bbc6399b58f4-Paper.pdf,A Multi-step Inertial Forward-Backward Splitting Method for Non-convex Optimization,"Jingwei Liang, Jalal Fadili, Gabriel Peyré",
neurips,https://proceedings.neurips.cc/paper/2016/file/eaa52f3366768bca401dca9ea5b181dd-Paper.pdf,Optimal Binary Classifier Aggregation for General Losses,"Akshay Balsubramani, Yoav S. Freund",
neurips,https://proceedings.neurips.cc/paper/2016/file/eaae339c4d89fc102edd9dbdb6a28915-Paper.pdf,Dense Associative Memory for Pattern Recognition,"Dmitry Krotov, John J. Hopfield",
neurips,https://proceedings.neurips.cc/paper/2016/file/eb163727917cbba1eea208541a643e74-Paper.pdf,Fairness in Learning: Classic and Contextual Bandits,"Matthew Joseph, Michael Kearns, Jamie H. Morgenstern, Aaron Roth",
neurips,https://proceedings.neurips.cc/paper/2016/file/eb86d510361fc23b59f18c1bc9802cc6-Paper.pdf,"Variational Autoencoder for Deep Learning of Images, Labels and Captions","Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2016/file/ed265bc903a5a097f61d3ec064d96d2e-Paper.pdf,Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks,"Tim Salimans, Durk P. Kingma",
neurips,https://proceedings.neurips.cc/paper/2016/file/ee26fc66b1369c7625333bedafbfcaf6-Paper.pdf,"Learning Additive Exponential Family Graphical Models via
ℓ
2
,
1
ℓ
-norm Regularized M-Estimation","Xiaotong Yuan, Ping Li, Tong Zhang, Qingshan Liu, Guangcan Liu","We investigate a subclass of exponential family graphical models of which the sufficient statistics are defined by arbitrary additive forms. We propose two
ℓ
2
,
1
ℓ
-norm regularized maximum likelihood estimators to learn the model parameters from i.i.d. samples. The first one is a joint MLE estimator which estimates all the parameters simultaneously. The second one is a node-wise conditional MLE estimator which estimates the parameters for each node individually. For both estimators, statistical analysis shows that under mild conditions the extra flexibility gained by the additive exponential family models comes at almost no cost of statistical efficiency. A Monte-Carlo approximation method is developed to efficiently optimize the proposed estimators. The advantages of our estimators over Gaussian graphical models and Nonparanormal estimators are demonstrated on synthetic and real data sets."
neurips,https://proceedings.neurips.cc/paper/2016/file/ef0917ea498b1665ad6c701057155abe-Paper.pdf,Disentangling factors of variation in deep representation using adversarial training,"Michael F. Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprechmann, Yann LeCun",
neurips,https://proceedings.neurips.cc/paper/2016/file/ef1e491a766ce3127556063d49bc2f98-Paper.pdf,Gaussian Processes for Survival Analysis,"Tamara Fernandez, Nicolas Rivera, Yee Whye Teh",
neurips,https://proceedings.neurips.cc/paper/2016/file/ef4e3b775c934dada217712d76f3d51f-Paper.pdf,Correlated-PCA: Principal Components' Analysis when Data and Noise are Correlated,"Namrata Vaswani, Han Guo",
neurips,https://proceedings.neurips.cc/paper/2016/file/ef575e8837d065a1683c022d2077d342-Paper.pdf,On Explore-Then-Commit strategies,"Aurelien Garivier, Tor Lattimore, Emilie Kaufmann",
neurips,https://proceedings.neurips.cc/paper/2016/file/f0adc8838f4bdedde4ec2cfad0515589-Paper.pdf,Adaptive Neural Compilation,"Rudy R. Bunel, Alban Desmaison, Pawan K. Mudigonda, Pushmeet Kohli, Philip Torr",
neurips,https://proceedings.neurips.cc/paper/2016/file/f0bbac6fa079f1e00b2c14c1d3c6ccf0-Paper.pdf,Graphical Time Warping for Joint Alignment of Multiple Curves,"Yizhi Wang, David J. Miller, Kira Poskanzer, Yue Wang, Lin Tian, Guoqiang Yu",
neurips,https://proceedings.neurips.cc/paper/2016/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Paper.pdf,PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions,"Mikhail Figurnov, Aizhan Ibraimova, Dmitry P. Vetrov, Pushmeet Kohli",
neurips,https://proceedings.neurips.cc/paper/2016/file/f197002b9a0853eca5e046d9ca4663d5-Paper.pdf,DeepMath - Deep Sequence Models for Premise Selection,"Geoffrey Irving, Christian Szegedy, Alexander A. Alemi, Niklas Een, Francois Chollet, Josef Urban",
neurips,https://proceedings.neurips.cc/paper/2016/file/f2201f5191c4e92cc5af043eebfd0946-Paper.pdf,A Pseudo-Bayesian Algorithm for Robust PCA,"Tae-Hyun Oh, Yasuyuki Matsushita, In Kweon, David Wipf",
neurips,https://proceedings.neurips.cc/paper/2016/file/f26dab9bf6a137c3b6782e562794c2f2-Paper.pdf,The Forget-me-not Process,"Kieran Milan, Joel Veness, James Kirkpatrick, Michael Bowling, Anna Koop, Demis Hassabis",
neurips,https://proceedings.neurips.cc/paper/2016/file/f2d887e01a80e813d9080038decbbabb-Paper.pdf,Unsupervised Risk Estimation Using Only Conditional Independence Structure,"Jacob Steinhardt, Percy S. Liang",
neurips,https://proceedings.neurips.cc/paper/2016/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf,Deep Learning without Poor Local Minima,Kenji Kawaguchi,
neurips,https://proceedings.neurips.cc/paper/2016/file/f3144cefe89a60d6a1afaf7859c5076b-Paper.pdf,Linear Contextual Bandits with Knapsacks,"Shipra Agrawal, Nikhil Devanur",
neurips,https://proceedings.neurips.cc/paper/2016/file/f337d999d9ad116a7b4f3d409fcc6480-Paper.pdf,High resolution neural connectivity from incomplete tracing data using nonnegative spline regression,"Kameron D. Harris, Stefan Mihalas, Eric Shea-Brown",
neurips,https://proceedings.neurips.cc/paper/2016/file/f340f1b1f65b6df5b5e3f94d95b11daf-Paper.pdf,Learning and Forecasting Opinion Dynamics in Social Networks,"Abir De, Isabel Valera, Niloy Ganguly, Sourangshu Bhattacharya, Manuel Gomez Rodriguez",
neurips,https://proceedings.neurips.cc/paper/2016/file/f39ae9ff3a81f499230c4126e01f421b-Paper.pdf,Lifelong Learning with Weighted Majority Votes,"Anastasia Pentina, Ruth Urner",
neurips,https://proceedings.neurips.cc/paper/2016/file/f3bd5ad57c8389a8a1a541a76be463bf-Paper.pdf,Depth from a Single Image by Harmonizing Overcomplete Local Network Predictions,"Ayan Chakrabarti, Jingyu Shao, Greg Shakhnarovich",
neurips,https://proceedings.neurips.cc/paper/2016/file/f3d9de86462c28781cbe5c47ef22c3e5-Paper.pdf,Ancestral Causal Inference,"Sara Magliacane, Tom Claassen, Joris M. Mooij",
neurips,https://proceedings.neurips.cc/paper/2016/file/f442d33fa06832082290ad8544a8da27-Paper.pdf,Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,"Tejas D. Kulkarni, Karthik Narasimhan, Ardavan Saeedi, Josh Tenenbaum",
neurips,https://proceedings.neurips.cc/paper/2016/file/f48c04ffab49ff0e5d1176244fdfb65c-Paper.pdf,Agnostic Estimation for Misspecified Phase Retrieval Models,"Matey Neykov, Zhaoran Wang, Han Liu","The goal of noisy high-dimensional phase retrieval is to estimate an
s
s
-sparse parameter
β
∗
∈
R
d
β
from
n
n
realizations of the model
Y
=
(
X
⊤
β
∗
)
2
+
ε
Y
. Based on this model, we propose a significant semi-parametric generalization called misspecified phase retrieval (MPR), in which
Y
=
f
(
X
⊤
β
∗
,
ε
)
Y
with unknown
f
f
and
Cov
(
Y
,
(
X
⊤
β
∗
)
2
)
>
0
Cov
. For example, MPR encompasses
Y
=
h
(
|
X
⊤
β
∗
|
)
+
ε
Y
with increasing
h
h
as a special case. Despite the generality of the MPR model, it eludes the reach of most existing semi-parametric estimators. In this paper, we propose an estimation procedure, which consists of solving a cascade of two convex programs and provably recovers the direction of
β
∗
β
. Our theory is backed up by thorough numerical results."
neurips,https://proceedings.neurips.cc/paper/2016/file/f4a331b7a22d1b237565d8813a34d8ac-Paper.pdf,Can Peripheral Representations Improve Clutter Metrics on Complex Scenes?,"Arturo Deza, Miguel Eckstein",
neurips,https://proceedings.neurips.cc/paper/2016/file/f4be00279ee2e0a53eafdaa94a151e2c-Paper.pdf,Proximal Deep Structured Models,"Shenlong Wang, Sanja Fidler, Raquel Urtasun",
neurips,https://proceedings.neurips.cc/paper/2016/file/f4f6dce2f3a0f9dada0c2b5b66452017-Paper.pdf,SPALS: Fast Alternating Least Squares via Implicit Leverage Scores Sampling,"Dehua Cheng, Richard Peng, Yan Liu, Ioakeim Perros",
neurips,https://proceedings.neurips.cc/paper/2016/file/f69e505b08403ad2298b9f262659929a-Paper.pdf,On Multiplicative Integration with Recurrent Neural Networks,"Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, Russ R. Salakhutdinov",
neurips,https://proceedings.neurips.cc/paper/2016/file/f718499c1c8cef6730f9fd03c8125cab-Paper.pdf,The Generalized Reparameterization Gradient,"Francisco R. Ruiz, Michalis Titsias RC AUEB, David Blei",
neurips,https://proceedings.neurips.cc/paper/2016/file/f76a89f0cb91bc419542ce9fa43902dc-Paper.pdf,Semiparametric Differential Graph Models,"Pan Xu, Quanquan Gu",
neurips,https://proceedings.neurips.cc/paper/2016/file/f83630579d055dc5843ae693e7cdafe0-Paper.pdf,Neural Universal Discrete Denoiser,"Taesup Moon, Seonwoo Min, Byunghan Lee, Sungroh Yoon",
neurips,https://proceedings.neurips.cc/paper/2016/file/f9b902fc3289af4dd08de5d1de54f68f-Paper.pdf,Testing for Differences in Gaussian Graphical Models: Applications to Brain Connectivity,"Eugene Belilovsky, Gaël Varoquaux, Matthew B. Blaschko",
neurips,https://proceedings.neurips.cc/paper/2016/file/fb87582825f9d28a8d42c5e5e5e8b23d-Paper.pdf,Learning to learn by gradient descent by gradient descent,"Marcin Andrychowicz, Misha Denil, Sergio Gómez, Matthew W. Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, Nando de Freitas",
neurips,https://proceedings.neurips.cc/paper/2016/file/fb89705ae6d743bf1e848c206e16a1d7-Paper.pdf,Mixed vine copulas as joint models of spike counts and local field potentials,"Arno Onken, Stefano Panzeri",
neurips,https://proceedings.neurips.cc/paper/2016/file/fb8feff253bb6c834deb61ec76baa893-Paper.pdf,Can Active Memory Replace Attention?,"Łukasz Kaiser, Samy Bengio",
neurips,https://proceedings.neurips.cc/paper/2016/file/fc2c7c47b918d0c2d792a719dfb602ef-Paper.pdf,Fast Active Set Methods for Online Spike Inference from Calcium Imaging,"Johannes Friedrich, Liam Paninski","Fluorescent calcium indicators are a popular means for observing the spiking activity of large neuronal populations. Unfortunately, extracting the spike train of each neuron from raw fluorescence calcium imaging data is a nontrivial problem. We present a fast online active set method to solve this sparse nonnegative deconvolution problem. Importantly, the algorithm progresses through each time series sequentially from beginning to end, thus enabling real-time online spike inference during the imaging session. Our algorithm is a generalization of the pool adjacent violators algorithm (PAVA) for isotonic regression and inherits its linear-time computational complexity. We gain remarkable increases in processing speed: more than one order of magnitude compared to currently employed state of the art convex solvers relying on interior point methods. Our method can exploit warm starts; therefore optimizing model hyperparameters only requires a handful of passes through the data. The algorithm enables real-time simultaneous deconvolution of
O
(
10
5
)
O
traces of whole-brain zebrafish imaging data on a laptop."
neurips,https://proceedings.neurips.cc/paper/2016/file/fc490ca45c00b1249bbe3554a4fdf6fb-Paper.pdf,Human Decision-Making under Limited Time,"Pedro A. Ortega, Alan A. Stocker",
neurips,https://proceedings.neurips.cc/paper/2016/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf,End-to-End Kernel Learning with Supervised Convolutional Kernel Networks,Julien Mairal,
neurips,https://proceedings.neurips.cc/paper/2016/file/fccb3cdc9acc14a6e70a12f74560c026-Paper.pdf,Dueling Bandits: Beyond Condorcet Winners to General Tournament Solutions,"Siddartha Y. Ramamohan, Arun Rajkumar, Shivani Agarwal, Shivani Agarwal","Recent work on deriving
O
(
log
T
)
O
anytime regret bounds for stochastic dueling bandit problems has considered mostly Condorcet winners, which do not always exist, and more recently, winners defined by the Copeland set, which do always exist. In this work, we consider a broad notion of winners defined by tournament solutions in social choice theory, which include the Copeland set as a special case but also include several other notions of winners such as the top cycle, uncovered set, and Banks set, and which, like the Copeland set, always exist. We develop a family of UCB-style dueling bandit algorithms for such general tournament solutions, and show
O
(
log
T
)
O
anytime regret bounds for them. Experiments confirm the ability of our algorithms to achieve low regret relative to the target winning set of interest."
neurips,https://proceedings.neurips.cc/paper/2016/file/fd69dbe29f156a7ef876a40a94f65599-Paper.pdf,Visual Question Answering with Question Representation Update (QRU),"Ruiyu Li, Jiaya Jia",
neurips,https://proceedings.neurips.cc/paper/2016/file/fe40fb944ee700392ed51bfe84dd4e3d-Paper.pdf,Optimal Learning for Multi-pass Stochastic Gradient Methods,"Junhong Lin, Lorenzo Rosasco",
neurips,https://proceedings.neurips.cc/paper/2016/file/fe51510c80bfd6e5d78a164cd5b1f688-Paper.pdf,General Tensor Spectral Co-clustering for Higher-Order Data,"Tao Wu, Austin R. Benson, David F. Gleich",
neurips,https://proceedings.neurips.cc/paper/2016/file/fe70c36866add1572a8e2b96bfede7bf-Paper.pdf,Balancing Suspense and Surprise: Timely Decision Making with Endogenous Information Acquisition,"Ahmed M. Alaa, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2016/file/fe8c15fed5f808006ce95eddb7366e35-Paper.pdf,Generating Long-term Trajectories Using Deep Hierarchical Networks,"Stephan Zheng, Yisong Yue, Jennifer Hobbs",
neurips,https://proceedings.neurips.cc/paper/2016/file/fe9fc289c3ff0af142b6d3bead98a923-Paper.pdf,Natural-Parameter Networks: A Class of Probabilistic Neural Networks,"Hao Wang, Xingjian SHI, Dit-Yan Yeung",
neurips,https://proceedings.neurips.cc/paper/2016/file/feab05aa91085b7a8012516bc3533958-Paper.pdf,Minimizing Quadratic Functions in Constant Time,"Kohei Hayashi, Yuichi Yoshida","A sampling-based optimization method for quadratic functions is proposed. Our method approximately solves the following
n
n
-dimensional quadratic minimization problem in constant time, which is independent of
n
n
:
z
∗
=
min
\bv
∈
\bbR
n
\bracket
\bv
A
\bv
+
n
\bracket
\bv
\diag
(
\bd
)
\bv
+
n
\bracket
\bb
\bv
z
, where
A
∈
\bbR
n
×
n
A
is a matrix and
\bd
,
\bb
∈
\bbR
n
\bd
are vectors. Our theoretical analysis specifies the number of samples
k
(
δ
,
ϵ
)
k
such that the approximated solution
z
z
satisfies
|
z
−
z
∗
|
=
O
(
ϵ
n
2
)
|
with probability
1
−
δ
1
. The empirical performance (accuracy and runtime) is positively confirmed by numerical experiments."
