conference,url,title,authors,abstract
neurips,https://proceedings.neurips.cc/paper/2017/file/0060ef47b12160b9198302ebdb144dcf-Paper.pdf,Real Time Image Saliency for Black Box Classifiers,"Piotr Dabkowski, Yarin Gal",
neurips,https://proceedings.neurips.cc/paper/2017/file/0070d23b06b1486a538c0eaa45dd167a-Paper.pdf,Joint distribution optimal transportation for domain adaptation,"Nicolas Courty, Rémi Flamary, Amaury Habrard, Alain Rakotomamonjy","This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function
f
f
in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a non-linear transformation between the joint feature/label space distributions of the two domain
\ps
\ps
and
\pt
\pt
. We propose a solution of this problem with optimal transport, that allows to recover an estimated target
\pt
f
=
(
X
,
f
(
X
)
)
\pt
by optimizing simultaneously the optimal coupling and
f
f
. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results."
neurips,https://proceedings.neurips.cc/paper/2017/file/00a03ec6533ca7f5c644d198d815329c-Paper.pdf,Learning A Structured Optimal Bipartite Graph for Co-Clustering,"Feiping Nie, Xiaoqian Wang, Cheng Deng, Heng Huang",
neurips,https://proceedings.neurips.cc/paper/2017/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf,Learning to Inpaint for Image Compression,"Mohammad Haris Baig, Vladlen Koltun, Lorenzo Torresani",
neurips,https://proceedings.neurips.cc/paper/2017/file/01894d6f048493d2cacde3c579c315a3-Paper.pdf,Inverse Filtering for Hidden Markov Models,"Robert Mattila, Cristian Rojas, Vikram Krishnamurthy, Bo Wahlberg",
neurips,https://proceedings.neurips.cc/paper/2017/file/018dd1e07a2de4a08e6612341bf2323e-Paper.pdf,On clustering network-valued data,"Soumendu Sundar Mukherjee, Purnamrita Sarkar, Lizhen Lin",
neurips,https://proceedings.neurips.cc/paper/2017/file/019d385eb67632a7e958e23f24bd07d7-Paper.pdf,Langevin Dynamics with Continuous Tempering for Training Deep Neural Networks,"Nanyang Ye, Zhanxing Zhu, Rafal Mantiuk",
neurips,https://proceedings.neurips.cc/paper/2017/file/01a0683665f38d8e5e567b3b15ca98bf-Paper.pdf,Beyond Worst-case: A Probabilistic Analysis of Affine Policies in Dynamic Optimization,"Omar El Housni, Vineet Goyal","Affine policies (or control) are widely used as a solution approach in dynamic optimization where computing an optimal adjustable solution is usually intractable. While the worst case performance of affine policies can be significantly bad, the empirical performance is observed to be near-optimal for a large class of problem instances. For instance, in the two-stage dynamic robust optimization problem with linear covering constraints and uncertain right hand side, the worst-case approximation bound for affine policies is
O
(
√
m
)
O
that is also tight (see Bertsimas and Goyal (2012)), whereas observed empirical performance is near-optimal. In this paper, we aim to address this stark-contrast between the worst-case and the empirical performance of affine policies. In particular, we show that affine policies give a good approximation for the two-stage adjustable robust optimization problem with high probability on random instances where the constraint coefficients are generated i.i.d. from a large class of distributions; thereby, providing a theoretical justification of the observed empirical performance. On the other hand, we also present a distribution such that the performance bound for affine policies on instances generated according to that distribution is
Ω
(
√
m
)
Ω
with high probability; however, the constraint coefficients are not i.i.d.. This demonstrates that the empirical performance of affine policies can depend on the generative model for instances."
neurips,https://proceedings.neurips.cc/paper/2017/file/01e9565cecc4e989123f9620c1d09c09-Paper.pdf,Few-Shot Learning Through an Information Retrieval Lens,"Eleni Triantafillou, Richard Zemel, Raquel Urtasun",
neurips,https://proceedings.neurips.cc/paper/2017/file/024d7f84fff11dd7e8d9c510137a2381-Paper.pdf,Accelerated consensus via Min-Sum Splitting,"Patrick Rebeschini, Sekhar C. Tatikonda",
neurips,https://proceedings.neurips.cc/paper/2017/file/028ee724157b05d04e7bdcf237d12e60-Paper.pdf,Saliency-based Sequential Image Attention with Multiset Prediction,"Sean Welleck, Jialin Mao, Kyunghyun Cho, Zheng Zhang",
neurips,https://proceedings.neurips.cc/paper/2017/file/02a32ad2669e6fe298e607fe7cc0e1a0-Paper.pdf,Adaptive Bayesian Sampling with Monte Carlo EM,"Anirban Roychowdhury, Srinivasan Parthasarathy",
neurips,https://proceedings.neurips.cc/paper/2017/file/02b1be0d48924c327124732726097157-Paper.pdf,Scalable Levy Process Priors for Spectral Kernel Learning,"Phillip A. Jang, Andrew Loeb, Matthew Davidow, Andrew G. Wilson",
neurips,https://proceedings.neurips.cc/paper/2017/file/02f039058bd48307e6f653a2005c9dd2-Paper.pdf,Model-Powered Conditional Independence Test,"Rajat Sen, Ananda Theertha Suresh, Karthikeyan Shanmugam, Alexandros G. Dimakis, Sanjay Shakkottai","We consider the problem of non-parametric Conditional Independence testing (CI testing) for continuous random variables. Given i.i.d samples from the joint distribution
f
(
x
,
y
,
z
)
f
of continuous random vectors
X
,
Y
X
and
Z
,
Z
we determine whether
X
\independent
Y
|
Z
X
. We approach this by converting the conditional independence test into a classification problem. This allows us to harness very powerful classifiers like gradient-boosted trees and deep neural networks. These models can handle complex probability distributions and allow us to perform significantly better compared to the prior state of the art, for high-dimensional CI testing. The main technical challenge in the classification problem is the need for samples from the conditional product distribution
f
C
I
(
x
,
y
,
z
)
=
f
(
x
|
z
)
f
(
y
|
z
)
f
(
z
)
f
-- the joint distribution if and only if
X
\independent
Y
|
Z
.
X
-- when given access only to i.i.d. samples from the true joint distribution
f
(
x
,
y
,
z
)
f
. To tackle this problem we propose a novel nearest neighbor bootstrap procedure and theoretically show that our generated samples are indeed close to
f
C
I
f
in terms of total variational distance. We then develop theoretical results regarding the generalization bounds for classification for our problem, which translate into error bounds for CI testing. We provide a novel analysis of Rademacher type classification bounds in the presence of non-i.i.d \textit{near-independent} samples. We empirically validate the performance of our algorithm on simulated and real datasets and show performance gains over previous methods."
neurips,https://proceedings.neurips.cc/paper/2017/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf,Learning Multiple Tasks with Multilinear Relationship Networks,"Mingsheng Long, ZHANGJIE CAO, Jianmin Wang, Philip S. Yu",
neurips,https://proceedings.neurips.cc/paper/2017/file/03e7ef47cee6fa4ae7567394b99912b7-Paper.pdf,Query Complexity of Clustering with Side Information,"Arya Mazumdar, Barna Saha","Suppose, we are given a set of
n
n
elements to be clustered into
k
k
(unknown) clusters, and an oracle/expert labeler that can interactively answer pair-wise queries of the form,
do two elements
u
u
and
v
v
belong to the same cluster?''. The goal is to recover the optimum clustering by asking the minimum number of queries. In this paper, we provide a rigorous theoretical study of this basic problem of query complexity of interactive clustering, and give strong information theoretic lower bounds, as well as nearly matching upper bounds. Most clustering problems come with a similarity matrix, which is used by an automated process to cluster similar points together. To improve accuracy of clustering, a fruitful approach in recent years has been to ask a domain expert or crowd to obtain labeled data interactively. Many heuristics have been proposed, and all of these use a similarity function to come up with a querying strategy. Even so, there is a lack systematic theoretical study. Our main contribution in this paper is to show the dramatic power of side information aka similarity matrix on reducing the query complexity of clustering. A similarity matrix represents noisy pair-wise relationships such as one computed by some function on attributes of the elements. A natural noisy model is where similarity values are drawn independently from some arbitrary probability distribution
f
+
f
when the underlying pair of elements belong to the same cluster, and from some
f
−
f
otherwise. We show that given such a similarity matrix, the query complexity reduces drastically from
Θ
(
n
k
)
Θ
(no similarity matrix) to
O
(
k
2
log
n
\cH
2
(
f
+
∥
f
−
)
)
O
where
\cH
2
\cH
denotes the squared Hellinger divergence. Moreover, this is also information-theoretic optimal within an
O
(
log
n
)
O
factor. Our algorithms are all efficient, and parameter free, i.e., they work without any knowledge of
k
,
f
+
k
and
f
−
f
, and only depend logarithmically with
n
n
."
neurips,https://proceedings.neurips.cc/paper/2017/file/04048aeca2c0f5d84639358008ed2ae7-Paper.pdf,Non-parametric Structured Output Networks,"Andreas Lehrmann, Leonid Sigal",
neurips,https://proceedings.neurips.cc/paper/2017/file/044a23cadb567653eb51d4eb40acaa88-Paper.pdf,Robust Imitation of Diverse Behaviors,"Ziyu Wang, Josh S. Merel, Scott E. Reed, Nando de Freitas, Gregory Wayne, Nicolas Heess",
neurips,https://proceedings.neurips.cc/paper/2017/file/051928341be67dcba03f0e04104d9047-Paper.pdf,High-Order Attention Models for Visual Question Answering,"Idan Schwartz, Alexander Schwing, Tamir Hazan",
neurips,https://proceedings.neurips.cc/paper/2017/file/05546b0e38ab9175cd905eebcc6ebb76-Paper.pdf,FALKON: An Optimal Large Scale Kernel Method,"Alessandro Rudi, Luigi Carratino, Lorenzo Rosasco","Kernel methods provide a principled way to perform non linear, nonparametric learning. They rely on solid functional analytic foundations and enjoy optimal statistical properties. However, at least in their basic form, they have limited applicability in large scale scenarios because of stringent computational requirements in terms of time and especially memory. In this paper, we take a substantial step in scaling up kernel methods, proposing FALKON, a novel algorithm that allows to efficiently process millions of points. FALKON is derived combining several algorithmic principles, namely stochastic subsampling, iterative solvers and preconditioning. Our theoretical analysis shows that optimal statistical accuracy is achieved requiring essentially
O
(
n
)
O
memory and
O
(
n
√
n
)
O
time. An extensive experimental analysis on large scale datasets shows that, even with a single machine, FALKON outperforms previous state of the art solutions, which exploit parallel/distributed architectures."
neurips,https://proceedings.neurips.cc/paper/2017/file/061412e4a03c02f9902576ec55ebbe77-Paper.pdf,Generalized Linear Model Regression under Distance-to-set Penalties,"Jason Xu, Eric Chi, Kenneth Lange",
neurips,https://proceedings.neurips.cc/paper/2017/file/07042ac7d03d3b9911a00da43ce0079a-Paper.pdf,Fisher GAN,"Youssef Mroueh, Tom Sercu",
neurips,https://proceedings.neurips.cc/paper/2017/file/070dbb6024b5ef93784428afc71f2146-Paper.pdf,Minimax Estimation of Bandable Precision Matrices,"Addison Hu, Sahand Negahban","The inverse covariance matrix provides considerable insight for understanding statistical models in the multivariate setting. In particular, when the distribution over variables is assumed to be multivariate normal, the sparsity pattern in the inverse covariance matrix, commonly referred to as the precision matrix, corresponds to the adjacency matrix representation of the Gauss-Markov graph, which encodes conditional independence statements between variables. Minimax results under the spectral norm have previously been established for covariance matrices, both sparse and banded, and for sparse precision matrices. We establish minimax estimation bounds for estimating banded precision matrices under the spectral norm. Our results greatly improve upon the existing bounds; in particular, we find that the minimax rate for estimating banded precision matrices matches that of estimating banded covariance matrices. The key insight in our analysis is that we are able to obtain barely-noisy estimates of
k
×
k
k
subblocks of the precision matrix by inverting slightly wider blocks of the empirical covariance matrix along the diagonal. Our theoretical results are complemented by experiments demonstrating the sharpness of our bounds."
neurips,https://proceedings.neurips.cc/paper/2017/file/07211688a0869d995947a8fb11b215d6-Paper.pdf,Kernel functions based on triplet comparisons,"Matthäus Kleindessner, Ulrike von Luxburg",
neurips,https://proceedings.neurips.cc/paper/2017/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf,Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization,"Fabian Pedregosa, Rémi Leblond, Simon Lacoste-Julien",
neurips,https://proceedings.neurips.cc/paper/2017/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf,A New Theory for Matrix Completion,"Guangcan Liu, Qingshan Liu, Xiaotong Yuan",
neurips,https://proceedings.neurips.cc/paper/2017/file/076023edc9187cf1ac1f1163470e479a-Paper.pdf,A Bayesian Data Augmentation Approach for Learning Deep Models,"Toan Tran, Trung Pham, Gustavo Carneiro, Lyle Palmer, Ian Reid",
neurips,https://proceedings.neurips.cc/paper/2017/file/0768281a05da9f27df178b5c39a51263-Paper.pdf,Deep Hyperalignment,"Muhammad Yousefnezhad, Daoqiang Zhang",
neurips,https://proceedings.neurips.cc/paper/2017/file/077e29b11be80ab57e1a2ecabb7da330-Paper.pdf,Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model,"Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, Dhruv Batra",
neurips,https://proceedings.neurips.cc/paper/2017/file/07811dc6c422334ce36a09ff5cd6fe71-Paper.pdf,PASS-GLM: polynomial approximate sufficient statistics for scalable Bayesian GLM inference,"Jonathan Huggins, Ryan P. Adams, Tamara Broderick",
neurips,https://proceedings.neurips.cc/paper/2017/file/08b255a5d42b89b0585260b6f2360bdd-Paper.pdf,Online multiclass boosting,"Young Hun Jung, Jack Goetz, Ambuj Tewari",
neurips,https://proceedings.neurips.cc/paper/2017/file/08e6bea8e90ba87af3c9554d94db6579-Paper.pdf,State Aware Imitation Learning,"Yannick Schroecker, Charles L. Isbell",
neurips,https://proceedings.neurips.cc/paper/2017/file/09fb05dd477d4ae6479985ca56c5a12d-Paper.pdf,Adaptive SVRG Methods under Error Bound Conditions with Unknown Growth Parameter,"Yi Xu, Qihang Lin, Tianbao Yang",
neurips,https://proceedings.neurips.cc/paper/2017/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Paper.pdf,Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data,"Wei-Ning Hsu, Yu Zhang, James Glass",
neurips,https://proceedings.neurips.cc/paper/2017/file/0a5c79b1eaf15445da252ada718857e9-Paper.pdf,Recurrent Ladder Networks,"Isabeau Prémont-Schwarz, Alexander Ilin, Tele Hao, Antti Rasmus, Rinu Boney, Harri Valpola",
neurips,https://proceedings.neurips.cc/paper/2017/file/0abdc563a06105aee3c6136871c9f4d1-Paper.pdf,Distral: Robust multitask reinforcement learning,"Yee Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, Razvan Pascanu",
neurips,https://proceedings.neurips.cc/paper/2017/file/0bed45bd5774ffddc95ffe500024f628-Paper.pdf,Real-Time Bidding with Side Information,"arthur flajolet, Patrick Jaillet","We consider the problem of repeated bidding in online advertising auctions when some side information (e.g. browser cookies) is available ahead of submitting a bid in the form of a
d
d
-dimensional vector. The goal for the advertiser is to maximize the total utility (e.g. the total number of clicks) derived from displaying ads given that a limited budget
B
B
is allocated for a given time horizon
T
T
. Optimizing the bids is modeled as a contextual Multi-Armed Bandit (MAB) problem with a knapsack constraint and a continuum of arms. We develop UCB-type algorithms that combine two streams of literature: the confidence-set approach to linear contextual MABs and the probabilistic bisection search method for stochastic root-finding. Under mild assumptions on the underlying unknown distribution, we establish distribution-independent regret bounds of order
~
O
(
d
⋅
√
T
)
O
when either
B
=
∞
B
or when
B
B
scales linearly with
T
T
."
neurips,https://proceedings.neurips.cc/paper/2017/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf,Learning Spherical Convolution for Fast Features from 360° Imagery,"Yu-Chuan Su, Kristen Grauman",
neurips,https://proceedings.neurips.cc/paper/2017/file/0d9095b0d6bbe98ea0c9c02b11b59ee3-Paper.pdf,Approximate Supermodularity Bounds for Experimental Design,"Luiz Chamon, Alejandro Ribeiro",
neurips,https://proceedings.neurips.cc/paper/2017/file/0e55666a4ad822e0e34299df3591d979-Paper.pdf,Differentiable Learning of Logical Rules for Knowledge Base Reasoning,"Fan Yang, Zhilin Yang, William W. Cohen",
neurips,https://proceedings.neurips.cc/paper/2017/file/0e7c7d6c41c76b9ee6445ae01cc0181d-Paper.pdf,When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent,"Mert Gurbuzbalaban, Asuman Ozdaglar, Pablo A. Parrilo, Nuri Vanli",
neurips,https://proceedings.neurips.cc/paper/2017/file/0ebcc77dc72360d0eb8e9504c78d38bd-Paper.pdf,Principles of Riemannian Geometry in Neural Networks,"Michael Hauser, Asok Ray",
neurips,https://proceedings.neurips.cc/paper/2017/file/0efbe98067c6c73dba1250d2beaa81f9-Paper.pdf,Continual Learning with Deep Generative Replay,"Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim",
neurips,https://proceedings.neurips.cc/paper/2017/file/0f3d014eead934bbdbacb62a01dc4831-Paper.pdf,Nonlinear random matrix theory for deep learning,"Jeffrey Pennington, Pratik Worah","Neural network configurations with random weights play an important role in the analysis of deep learning. They define the initial loss landscape and are closely related to kernel and random feature methods. Despite the fact that these networks are built out of random matrices, the vast and powerful machinery of random matrix theory has so far found limited success in studying them. A main obstacle in this direction is that neural networks are nonlinear, which prevents the straightforward utilization of many of the existing mathematical results. In this work, we open the door for direct applications of random matrix theory to deep learning by demonstrating that the pointwise nonlinearities typically applied in neural networks can be incorporated into a standard method of proof in random matrix theory known as the moments method. The test case for our study is the Gram matrix
Y
T
Y
Y
,
Y
=
f
(
W
X
)
Y
, where
W
W
is a random weight matrix,
X
X
is a random data matrix, and
f
f
is a pointwise nonlinear activation function. We derive an explicit representation for the trace of the resolvent of this matrix, which defines its limiting spectral distribution. We apply these results to the computation of the asymptotic performance of single-layer random feature methods on a memorization task and to the analysis of the eigenvalues of the data covariance matrix as it propagates through a neural network. As a byproduct of our analysis, we identify an intriguing new class of activation functions with favorable properties."
neurips,https://proceedings.neurips.cc/paper/2017/file/1006ff12c465532f8c574aeaa4461b16-Paper.pdf,Identification of Gaussian Process State Space Models,"Stefanos Eleftheriadis, Tom Nicholson, Marc Deisenroth, James Hensman",
neurips,https://proceedings.neurips.cc/paper/2017/file/10c272d06794d3e5785d5e7c5356e9ff-Paper.pdf,Estimation of the covariance structure of heavy-tailed distributions,"Xiaohan Wei, Stanislav Minsker",
neurips,https://proceedings.neurips.cc/paper/2017/file/10c66082c124f8afe3df4886f5e516e0-Paper.pdf,Robust Optimization for Non-Convex Objectives,"Robert S. Chen, Brendan Lucier, Yaron Singer, Vasilis Syrgkanis","We consider robust optimization problems, where the goal is to optimize in the worst case over a class of objective functions. We develop a reduction from robust improper optimization to stochastic optimization: given an oracle that returns
α
α
-approximate solutions for distributions over objectives, we compute a distribution over solutions that is
α
α
-approximate in the worst case. We show that derandomizing this solution is NP-hard in general, but can be done for a broad class of statistical learning tasks. We apply our results to robust neural network training and submodular optimization. We evaluate our approach experimentally on corrupted character classification and robust influence maximization in networks."
neurips,https://proceedings.neurips.cc/paper/2017/file/10ce03a1ed01077e3e289f3e53c72813-Paper.pdf,Exploring Generalization in Deep Learning,"Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, Nati Srebro",
neurips,https://proceedings.neurips.cc/paper/2017/file/1113d7a76ffceca1bb350bfe145467c6-Paper.pdf,Spherical convolutions and their application in molecular modelling,"Wouter Boomsma, Jes Frellsen",
neurips,https://proceedings.neurips.cc/paper/2017/file/1177967c7957072da3dc1db4ceb30e7a-Paper.pdf,Safe Adaptive Importance Sampling,"Sebastian U. Stich, Anant Raj, Martin Jaggi",
neurips,https://proceedings.neurips.cc/paper/2017/file/11b921ef080f7736089c757404650e40-Paper.pdf,Introspective Classification with Convolutional Nets,"Long Jin, Justin Lazarow, Zhuowen Tu",
neurips,https://proceedings.neurips.cc/paper/2017/file/1264a061d82a2edae1574b07249800d6-Paper.pdf,Hybrid Reward Architecture for Reinforcement Learning,"Harm Van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes, Jeffrey Tsang",
neurips,https://proceedings.neurips.cc/paper/2017/file/1271a7029c9df08643b631b02cf9e116-Paper.pdf,When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness,"Chris Russell, Matt J. Kusner, Joshua Loftus, Ricardo Silva",
neurips,https://proceedings.neurips.cc/paper/2017/file/12a1d073d5ed3fa12169c67c4e2ce415-Paper.pdf,Dualing GANs,"Yujia Li, Alexander Schwing, Kuan-Chieh Wang, Richard Zemel",
neurips,https://proceedings.neurips.cc/paper/2017/file/136f951362dab62e64eb8e841183c2a9-Paper.pdf,A Universal Analysis of Large-Scale Regularized Least Squares Solutions,"Ashkan Panahi, Babak Hassibi","A problem that has been of recent interest in statistical inference, machine learning and signal processing is that of understanding the asymptotic behavior of regularized least squares solutions under random measurement matrices (or dictionaries). The Least Absolute Shrinkage and Selection Operator (LASSO or least-squares with
ℓ
1
ℓ
regularization) is perhaps one of the most interesting examples. Precise expressions for the asymptotic performance of LASSO have been obtained for a number of different cases, in particular when the elements of the dictionary matrix are sampled independently from a Gaussian distribution. It has also been empirically observed that the resulting expressions remain valid when the entries of the dictionary matrix are independently sampled from certain non-Gaussian distributions. In this paper, we confirm these observations theoretically when the distribution is sub-Gaussian. We further generalize the previous expressions for a broader family of regularization functions and under milder conditions on the underlying random, possibly non-Gaussian, dictionary matrix. In particular, we establish the universality of the asymptotic statistics (e.g., the average quadratic risk) of LASSO with non-Gaussian dictionaries."
neurips,https://proceedings.neurips.cc/paper/2017/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf,Diffusion Approximations for Online Principal Component Estimation and Global Convergence,"Chris Junchi Li, Mengdi Wang, Han Liu, Tong Zhang",
neurips,https://proceedings.neurips.cc/paper/2017/file/13fe9d84310e77f13a6d184dbf1232f3-Paper.pdf,k-Support and Ordered Weighted Sparsity for Overlapping Groups: Hardness and Algorithms,"Cong Han Lim, Stephen Wright",
neurips,https://proceedings.neurips.cc/paper/2017/file/147ebe637038ca50a1265abac8dea181-Paper.pdf,Learning to Model the Tail,"Yu-Xiong Wang, Deva Ramanan, Martial Hebert",
neurips,https://proceedings.neurips.cc/paper/2017/file/14e422f05b68cc0139988e128ee880df-Paper.pdf,Neural Variational Inference and Learning in Undirected Graphical Models,"Volodymyr Kuleshov, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2017/file/14ea0d5b0cf49525d1866cb1e95ada5d-Paper.pdf,Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification,"Bikash Joshi, Massih R. Amini, Ioannis Partalas, Franck Iutzeler, Yury Maximov",
neurips,https://proceedings.neurips.cc/paper/2017/file/165a59f7cf3b5c4396ba65953d679f17-Paper.pdf,Learning Linear Dynamical Systems via Spectral Filtering,"Elad Hazan, Karan Singh, Cyril Zhang",
neurips,https://proceedings.neurips.cc/paper/2017/file/1680e9fa7b4dd5d62ece800239bb53bd-Paper.pdf,Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes,"Zhenwen Dai, Mauricio Álvarez, Neil Lawrence",
neurips,https://proceedings.neurips.cc/paper/2017/file/16a5cdae362b8d27a1d8f8c7b78b4330-Paper.pdf,Semi-Supervised Learning for Optical Flow with Generative Adversarial Networks,"Wei-Sheng Lai, Jia-Bin Huang, Ming-Hsuan Yang",
neurips,https://proceedings.neurips.cc/paper/2017/file/1700002963a49da13542e0726b7bb758-Paper.pdf,Phase Transitions in the Pooled Data Problem,"Jonathan Scarlett, Volkan Cevher",
neurips,https://proceedings.neurips.cc/paper/2017/file/17d8da815fa21c57af9829fb0a869602-Paper.pdf,Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning,"Christoph Dann, Tor Lattimore, Emma Brunskill",
neurips,https://proceedings.neurips.cc/paper/2017/file/17ed8abedc255908be746d245e50263a-Paper.pdf,Stein Variational Gradient Descent as Gradient Flow,Qiang Liu,
neurips,https://proceedings.neurips.cc/paper/2017/file/17fafe5f6ce2f1904eb09d2e80a4cbf6-Paper.pdf,Expectation Propagation for t-Exponential Family Using q-Algebra,"Futoshi Futami, Issei Sato, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2017/file/186a157b2992e7daed3677ce8e9fe40f-Paper.pdf,Collaborative PAC Learning,"Avrim Blum, Nika Haghtalab, Ariel D. Procaccia, Mingda Qiao",
neurips,https://proceedings.neurips.cc/paper/2017/file/18bb68e2b38e4a8ce7cf4f6b2625768c-Paper.pdf,Polynomial time algorithms for dual volume sampling,"Chengtao Li, Stefanie Jegelka, Suvrit Sra",
neurips,https://proceedings.neurips.cc/paper/2017/file/18d10dc6e666eab6de9215ae5b3d54df-Paper.pdf,Premise Selection for Theorem Proving by Deep Graph Embedding,"Mingzhe Wang, Yihe Tang, Jian Wang, Jia Deng",
neurips,https://proceedings.neurips.cc/paper/2017/file/192fc044e74dffea144f9ac5dc9f3395-Paper.pdf,Differentiable Learning of Submodular Models,"Josip Djolonga, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2017/file/1943102704f8f8f3302c2b730728e023-Paper.pdf,YASS: Yet Another Spike Sorter,"Jin Hyung Lee, David E. Carlson, Hooshmand Shokri Razaghi, Weichi Yao, Georges A. Goetz, Espen Hagen, Eleanor Batty, E.J. Chichilnisky, Gaute T. Einevoll, Liam Paninski",
neurips,https://proceedings.neurips.cc/paper/2017/file/194cf6c2de8e00c05fcf16c498adc7bf-Paper.pdf,Variational Laws of Visual Attention for Dynamic Scenes,"Dario Zanca, Marco Gori",
neurips,https://proceedings.neurips.cc/paper/2017/file/1abb1e1ea5f481b589da52303b091cbb-Paper.pdf,How regularization affects the critical points in linear networks,"Amirhossein Taghvaei, Jin W. Kim, Prashant Mehta",
neurips,https://proceedings.neurips.cc/paper/2017/file/1b5230e3ea6d7123847ad55a1e06fffd-Paper.pdf,On Tensor Train Rank Minimization : Statistical Efficiency and Scalable Algorithm,"Masaaki Imaizumi, Takanori Maehara, Kohei Hayashi",
neurips,https://proceedings.neurips.cc/paper/2017/file/1baff70e2669e8376347efd3a874a341-Paper.pdf,EX2: Exploration with Exemplar Models for Deep Reinforcement Learning,"Justin Fu, John Co-Reyes, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2017/file/1c303b0eed3133200cf715285011b4e4-Paper.pdf,Training Quantized Nets: A Deeper Understanding,"Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, Tom Goldstein",
neurips,https://proceedings.neurips.cc/paper/2017/file/1c54985e4f95b7819ca0357c0cb9a09f-Paper.pdf,Convolutional Gaussian Processes,"Mark van der Wilk, Carl Edward Rasmussen, James Hensman",
neurips,https://proceedings.neurips.cc/paper/2017/file/1ce927f875864094e3906a4a0b5ece68-Paper.pdf,Best Response Regression,"Omer Ben-Porat, Moshe Tennenholtz",
neurips,https://proceedings.neurips.cc/paper/2017/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf,Elementary Symmetric Polynomials for Optimal Experimental Design,"Zelda E. Mariet, Suvrit Sra",
neurips,https://proceedings.neurips.cc/paper/2017/file/1dba5eed8838571e1c80af145184e515-Paper.pdf,Learning from Complementary Labels,"Takashi Ishida, Gang Niu, Weihua Hu, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2017/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf,Dynamic Importance Sampling for Anytime Bounds of the Partition Function,"Qi Lou, Rina Dechter, Alexander T. Ihler",
neurips,https://proceedings.neurips.cc/paper/2017/file/1f71e393b3809197ed66df836fe833e5-Paper.pdf,Process-constrained batch Bayesian optimisation,"Pratibha Vellanki, Santu Rana, Sunil Gupta, David Rubin, Alessandra Sutti, Thomas Dorin, Murray Height, Paul Sanders, Svetha Venkatesh",
neurips,https://proceedings.neurips.cc/paper/2017/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Paper.pdf,Uprooting and Rerooting Higher-Order Graphical Models,"Mark Rowland, Adrian Weller",
neurips,https://proceedings.neurips.cc/paper/2017/file/20c86a628232a67e7bd46f76fba7ce12-Paper.pdf,Learned in Translation: Contextualized Word Vectors,"Bryan McCann, James Bradbury, Caiming Xiong, Richard Socher",
neurips,https://proceedings.neurips.cc/paper/2017/file/2131f8ecf18db66a758f718dc729e00e-Paper.pdf,"Semisupervised Clustering, AND-Queries and Locally Encodable Source Coding","Arya Mazumdar, Soumyabrata Pal","Source coding is the canonical problem of data compression in information theory. In a locally encodable source coding, each compressed bit depends on only few bits of the input. In this paper, we show that a recently popular model of semisupervised clustering is equivalent to locally encodable source coding. In this model, the task is to perform multiclass labeling of unlabeled elements. At the beginning, we can ask in parallel a set of simple queries to an oracle who provides (possibly erroneous) binary answers to the queries. The queries cannot involve more than two (or a fixed constant number
Δ
Δ
of) elements. Now the labeling of all the elements (or clustering) must be performed based on the (noisy) query answers. The goal is to recover all the correct labelings while minimizing the number of such queries. The equivalence to locally encodable source codes leads us to find lower bounds on the number of queries required in variety of scenarios. We are also able to show fundamental limitations of pairwise `same cluster' queries - and propose pairwise AND queries, that provably performs better in many situations."
neurips,https://proceedings.neurips.cc/paper/2017/file/217e342fc01668b10cb1188d40d3370e-Paper.pdf,Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization,"Hyeonwoo Noh, Tackgeun You, Jonghwan Mun, Bohyung Han",
neurips,https://proceedings.neurips.cc/paper/2017/file/21c5bba1dd6aed9ab48c2b34c1a0adde-Paper.pdf,Few-Shot Adversarial Domain Adaptation,"Saeid Motiian, Quinn Jones, Seyed Iranmanesh, Gianfranco Doretto",
neurips,https://proceedings.neurips.cc/paper/2017/file/2227d753dc18505031869d44673728e2-Paper.pdf,Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes,"Taylor W. Killian, Samuel Daulton, George Konidaris, Finale Doshi-Velez",
neurips,https://proceedings.neurips.cc/paper/2017/file/227f6afd3b7f89b96c4bb91f95d50f6d-Paper.pdf,Multi-View Decision Processes: The Helper-AI Problem,"Christos Dimitrakakis, David C. Parkes, Goran Radanovic, Paul Tylkin",
neurips,https://proceedings.neurips.cc/paper/2017/file/2288f691b58edecadcc9a8691762b4fd-Paper.pdf,Maximum Margin Interval Trees,"Alexandre Drouin, Toby Hocking, Francois Laviolette",
neurips,https://proceedings.neurips.cc/paper/2017/file/22b1f2e0983160db6f7bb9f62f4dbb39-Paper.pdf,Online Learning with a Hint,"Ofer Dekel, arthur flajolet, Nika Haghtalab, Patrick Jaillet",
neurips,https://proceedings.neurips.cc/paper/2017/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf,DPSCREEN: Dynamic Personalized Screening,"Kartik Ahuja, William Zame, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2017/file/23af4b45f1e166141a790d1a3126e77a-Paper.pdf,Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity Auctions,"M. Sevi Baltaoglu, Lang Tong, Qing Zhao","We study the online learning problem of a bidder who participates in repeated auctions. With the goal of maximizing his T-period payoff, the bidder determines the optimal allocation of his budget among his bids for
K
K
goods at each period. As a bidding strategy, we propose a polynomial-time algorithm, inspired by the dynamic programming approach to the knapsack problem. The proposed algorithm, referred to as dynamic programming on discrete set (DPDS), achieves a regret order of
O
(
√
T
log
T
)
O
. By showing that the regret is lower bounded by
Ω
(
√
T
)
Ω
for any strategy, we conclude that DPDS is order optimal up to a
√
log
T
log
term. We evaluate the performance of DPDS empirically in the context of virtual trading in wholesale electricity markets by using historical data from the New York market. Empirical results show that DPDS consistently outperforms benchmark heuristic methods that are derived from machine learning and online learning approaches."
neurips,https://proceedings.neurips.cc/paper/2017/file/2417dc8af8570f274e6775d4d60496da-Paper.pdf,A-NICE-MC: Adversarial Training for MCMC,"Jiaming Song, Shengjia Zhao, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2017/file/24681928425f5a9133504de568f5f6df-Paper.pdf,Question Asking as Program Generation,"Anselm Rothe, Brenden M. Lake, Todd Gureckis",
neurips,https://proceedings.neurips.cc/paper/2017/file/24b43fb034a10d78bec71274033b4096-Paper.pdf,Gradient Methods for Submodular Maximization,"Hamed Hassani, Mahdi Soltanolkotabi, Amin Karbasi","In this paper, we study the problem of maximizing continuous submodular functions that naturally arise in many learning applications such as those involving utility functions in active learning and sensing, matrix approximations and network inference. Despite the apparent lack of convexity in such functions, we prove that stochastic projected gradient methods can provide strong approximation guarantees for maximizing continuous submodular functions with convex constraints. More specifically, we prove that for monotone continuous DR-submodular functions, all fixed points of projected gradient ascent provide a factor
1
/
2
1
approximation to the global maxima. We also study stochastic gradient methods and show that after
O
(
1
/
ϵ
2
)
O
iterations these methods reach solutions which achieve in expectation objective values exceeding
(
OPT
2
−
ϵ
)
(
. An immediate application of our results is to maximize submodular functions that are defined stochastically, i.e. the submodular function is defined as an expectation over a family of submodular functions with an unknown distribution. We will show how stochastic gradient methods are naturally well-suited for this setting, leading to a factor
1
/
2
1
approximation when the function is monotone. In particular, it allows us to approximately maximize discrete, monotone submodular optimization problems via projected gradient ascent on a continuous relaxation, directly connecting the discrete and continuous domains. Finally, experiments on real data demonstrate that our projected gradient methods consistently achieve the best utility compared to other continuous baselines while remaining competitive in terms of computational effort."
neurips,https://proceedings.neurips.cc/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Recycling Privileged Learning and Distribution Matching for Fairness,"Novi Quadrianto, Viktoriia Sharmanska",
neurips,https://proceedings.neurips.cc/paper/2017/file/253614bbac999b38b5b60cae531c4969-Paper.pdf,Collecting Telemetry Data Privately,"Bolin Ding, Janardhan Kulkarni, Sergey Yekhanin",
neurips,https://proceedings.neurips.cc/paper/2017/file/253f7b5d921338af34da817c00f42753-Paper.pdf,Parallel Streaming Wasserstein Barycenters,"Matthew Staib, Sebastian Claici, Justin M. Solomon, Stefanie Jegelka",
neurips,https://proceedings.neurips.cc/paper/2017/file/2612aa892d962d6f8056b195ca6e550d-Paper.pdf,"Adaptive Accelerated Gradient Converging Method under H\""{o}lderian Error Bound Condition","Mingrui Liu, Tianbao Yang","Recent studies have shown that proximal gradient (PG) method and accelerated gradient method (APG) with restarting can enjoy a linear convergence under a weaker condition than strong convexity, namely a quadratic growth condition (QGC). However, the faster convergence of restarting APG method relies on the potentially unknown constant in QGC to appropriately restart APG, which restricts its applicability. We address this issue by developing a novel adaptive gradient converging methods, i.e., leveraging the magnitude of proximal gradient as a criterion for restart and termination. Our analysis extends to a much more general condition beyond the QGC, namely the H\""{o}lderian error bound (HEB) condition. {\it The key technique} for our development is a novel synthesis of {\it adaptive regularization and a conditional restarting scheme}, which extends previous work focusing on strongly convex problems to a much broader family of problems. Furthermore, we demonstrate that our results have important implication and applications in machine learning: (i) if the objective function is coercive and semi-algebraic, PG's convergence speed is essentially
o
(
1
t
)
o
, where
t
t
is the total number of iterations; (ii) if the objective function consists of an
ℓ
1
ℓ
,
ℓ
∞
ℓ
,
ℓ
1
,
∞
ℓ
, or huber norm regularization and a convex smooth piecewise quadratic loss (e.g., square loss, squared hinge loss and huber loss), the proposed algorithm is parameter-free and enjoys a {\it faster linear convergence} than PG without any other assumptions (e.g., restricted eigen-value condition). It is notable that our linear convergence results for the aforementioned problems are global instead of local. To the best of our knowledge, these improved results are first shown in this work."
neurips,https://proceedings.neurips.cc/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf,What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?,"Alex Kendall, Yarin Gal",
neurips,https://proceedings.neurips.cc/paper/2017/file/269d837afada308dd4aeab28ca2d57e4-Paper.pdf,Reconstruct & Crush Network,"Erinc Merdivan, Mohammad Reza Loghmani, Matthieu Geist",
neurips,https://proceedings.neurips.cc/paper/2017/file/275d7fb2fd45098ad5c3ece2ed4a2824-Paper.pdf,Permutation-based Causal Inference Algorithms with Interventions,"Yuhao Wang, Liam Solus, Karren Yang, Caroline Uhler",
neurips,https://proceedings.neurips.cc/paper/2017/file/27ed0fb950b856b06e1273989422e7d3-Paper.pdf,Deep Dynamic Poisson Factorization Model,"Chengyue Gong, win-bin huang",
neurips,https://proceedings.neurips.cc/paper/2017/file/28dd2c7955ce926456240b2ff0100bde-Paper.pdf,Scalable Generalized Linear Bandits: Online Computation and Hashing,"Kwang-Sung Jun, Aniruddha Bhargava, Robert Nowak, Rebecca Willett","Generalized Linear Bandits (GLBs), a natural extension of the stochastic linear bandits, has been popular and successful in recent years. However, existing GLBs scale poorly with the number of rounds and the number of arms, limiting their utility in practice. This paper proposes new, scalable solutions to the GLB problem in two respects. First, unlike existing GLBs, whose per-time-step space and time complexity grow at least linearly with time
t
t
, we propose a new algorithm that performs online computations to enjoy a constant space and time complexity. At its heart is a novel Generalized Linear extension of the Online-to-confidence-set Conversion (GLOC method) that takes \emph{any} online learning algorithm and turns it into a GLB algorithm. As a special case, we apply GLOC to the online Newton step algorithm, which results in a low-regret GLB algorithm with much lower time and memory complexity than prior work. Second, for the case where the number
N
N
of arms is very large, we propose new algorithms in which each next arm is selected via an inner product search. Such methods can be implemented via hashing algorithms (i.e.,
hash-amenable'') and result in a time complexity sublinear in
N
N
. While a Thompson sampling extension of GLOC is hash-amenable, its regret bound for
d
d
-dimensional arm sets scales with
d
3
/
2
d
, whereas GLOC's regret bound scales with
d
d
. Towards closing this gap, we propose a new hash-amenable algorithm whose regret bound scales with
d
5
/
4
d
. Finally, we propose a fast approximate hash-key computation (inner product) with a better accuracy than the state-of-the-art, which can be of independent interest. We conclude the paper with preliminary experimental results confirming the merits of our methods."
neurips,https://proceedings.neurips.cc/paper/2017/file/291d43c696d8c3704cdbe0a72ade5f6c-Paper.pdf,Experimental Design for Learning Causal Graphs with Latent Variables,"Murat Kocaoglu, Karthikeyan Shanmugam, Elias Bareinboim",
neurips,https://proceedings.neurips.cc/paper/2017/file/298f95e1bf9136124592c8d4825a06fc-Paper.pdf,Lower bounds on the robustness to adversarial perturbations,"Jonathan Peck, Joris Roels, Bart Goossens, Yvan Saeys",
neurips,https://proceedings.neurips.cc/paper/2017/file/299a23a2291e2126b91d54f3601ec162-Paper.pdf,Reliable Decision Support using Counterfactual Models,"Peter Schulam, Suchi Saria",
neurips,https://proceedings.neurips.cc/paper/2017/file/2aedcba61ca55ceb62d785c6b7f10a83-Paper.pdf,Group Additive Structure Identification for Kernel Nonparametric Regression,"Chao Pan, Michael Zhu","The additive model is one of the most popularly used models for high dimensional nonparametric regression analysis. However, its main drawback is that it neglects possible interactions between predictor variables. In this paper, we reexamine the group additive model proposed in the literature, and rigorously define the intrinsic group additive structure for the relationship between the response variable
Y
Y
and the predictor vector
\vect
X
\vect
, and further develop an effective structure-penalized kernel method for simultaneous identification of the intrinsic group additive structure and nonparametric function estimation. The method utilizes a novel complexity measure we derive for group additive structures. We show that the proposed method is consistent in identifying the intrinsic group additive structure. Simulation study and real data applications demonstrate the effectiveness of the proposed method as a general tool for high dimensional nonparametric regression."
neurips,https://proceedings.neurips.cc/paper/2017/file/2b0f658cbffd284984fb11d90254081f-Paper.pdf,A multi-agent reinforcement learning model of common-pool resource appropriation,"Julien Pérolat, Joel Z. Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, Thore Graepel",
neurips,https://proceedings.neurips.cc/paper/2017/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf,Decoding with Value Networks for Neural Machine Translation,"Di He, Hanqing Lu, Yingce Xia, Tao Qin, Liwei Wang, Tie-Yan Liu","Neural Machine Translation (NMT) has become a popular technology in recent years, and beam search is its de facto decoding method due to the shrunk search space and reduced computational complexity. However, since it only searches for local optima at each time step through one-step forward looking, it usually cannot output the best target sentence. Inspired by the success and methodology of AlphaGo, in this paper we propose using a prediction network to improve beam search, which takes the source sentence
x
x
, the currently available decoding output
y
1
,
⋯
,
y
t
−
1
y
and a candidate word
w
w
at step
t
t
as inputs and predicts the long-term value (e.g., BLEU score) of the partial target sentence if it is completed by the NMT model. Following the practice in reinforcement learning, we call this prediction network \emph{value network}. Specifically, we propose a recurrent structure for the value network, and train its parameters from bilingual data. During the test time, when choosing a word
w
w
for decoding, we consider both its conditional probability given by the NMT model and its long-term value predicted by the value network. Experiments show that such an approach can significantly improve the translation accuracy on several translation tasks."
neurips,https://proceedings.neurips.cc/paper/2017/file/2b45e8d6abf59038a975faeeb6dc0782-Paper.pdf,Population Matching Discrepancy and Applications in Deep Learning,"Jianfei Chen, Chongxuan LI, Yizhong Ru, Jun Zhu",
neurips,https://proceedings.neurips.cc/paper/2017/file/2bb0502c80b7432eee4c5847a5fd077b-Paper.pdf,Predictive State Recurrent Neural Networks,"Carlton Downey, Ahmed Hefny, Byron Boots, Geoffrey J. Gordon, Boyue Li",
neurips,https://proceedings.neurips.cc/paper/2017/file/2bb232c0b13c774965ef8558f0fbd615-Paper.pdf,Robust Hypothesis Test for Nonlinear Effect with Gaussian Processes,"Jeremiah Liu, Brent Coull","This work constructs a hypothesis test for detecting whether an data-generating function
h
:
\real
p
→
\real
h
belongs to a specific reproducing kernel Hilbert space
H
0
H
, where the structure of
H
0
H
is only partially known. Utilizing the theory of reproducing kernels, we reduce this hypothesis to a simple one-sided score test for a scalar parameter, develop a testing procedure that is robust against the mis-specification of kernel functions, and also propose an ensemble-based estimator for the null model to guarantee test performance in small samples. To demonstrate the utility of the proposed method, we apply our test to the problem of detecting nonlinear interaction between groups of continuous features. We evaluate the finite-sample performance of our test under different data-generating functions and estimation strategies for the null model. Our results revealed interesting connection between notions in machine learning (model underfit/overfit) and those in statistical inference (i.e. Type I error/power of hypothesis test), and also highlighted unexpected consequences of common model estimating strategies (e.g. estimating kernel hyperparameters using maximum likelihood estimation) on model inference."
neurips,https://proceedings.neurips.cc/paper/2017/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf,"Sharpness, Restart and Acceleration","Vincent Roulet, Alexandre d'Aspremont",
neurips,https://proceedings.neurips.cc/paper/2017/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf,Dynamic Routing Between Capsules,"Sara Sabour, Nicholas Frosst, Geoffrey E. Hinton",
neurips,https://proceedings.neurips.cc/paper/2017/file/2cd4e8a2ce081c3d7c32c3cde4312ef7-Paper.pdf,InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations,"Yunzhu Li, Jiaming Song, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2017/file/2d1b2a5ff364606ff041650887723470-Paper.pdf,A Regularized Framework for Sparse and Structured Neural Attention,"Vlad Niculae, Mathieu Blondel",
neurips,https://proceedings.neurips.cc/paper/2017/file/2d2c8394e31101a261abf1784302bf75-Paper.pdf,Style Transfer from Non-Parallel Text by Cross-Alignment,"Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola",
neurips,https://proceedings.neurips.cc/paper/2017/file/2d2ca7eedf739ef4c3800713ec482e1a-Paper.pdf,Unsupervised Learning of Disentangled Representations from Video,"Emily L. Denton, vighnesh Birodkar",
neurips,https://proceedings.neurips.cc/paper/2017/file/2e0aca891f2a8aedf265edf533a6d9a8-Paper.pdf,Countering Feedback Delays in Multi-Agent Learning,"Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Peter W. Glynn, Claire Tomlin",
neurips,https://proceedings.neurips.cc/paper/2017/file/2e1b24a664f5e9c18f407b2f9c73e821-Paper.pdf,Affinity Clustering: Hierarchical Clustering at Scale,"Mohammadhossein Bateni, Soheil Behnezhad, Mahsa Derakhshan, MohammadTaghi Hajiaghayi, Raimondas Kiveris, Silvio Lattanzi, Vahab Mirrokni","Graph clustering is a fundamental task in many data-mining and machine-learning pipelines. In particular, identifying a good hierarchical structure is at the same time a fundamental and challenging problem for several applications. The amount of data to analyze is increasing at an astonishing rate each day. Hence there is a need for new solutions to efficiently compute effective hierarchical clusterings on such huge data. The main focus of this paper is on minimum spanning tree (MST) based clusterings. In particular, we propose affinity, a novel hierarchical clustering based on Boruvka's MST algorithm. We prove certain theoretical guarantees for affinity (as well as some other classic algorithms) and show that in practice it is superior to several other state-of-the-art clustering algorithms. Furthermore, we present two MapReduce implementations for affinity. The first one works for the case where the input graph is dense and takes constant rounds. It is based on a Massively Parallel MST algorithm for dense graphs that improves upon the state-of-the-art algorithm of Lattanzi et al. (SPAA 2011). Our second algorithm has no assumption on the density of the input graph and finds the affinity clustering in
O
(
log
n
)
O
rounds using Distributed Hash Tables (DHTs). We show experimentally that our algorithms are scalable for huge data sets, e.g., for graphs with trillions of edges."
neurips,https://proceedings.neurips.cc/paper/2017/file/2eace51d8f796d04991c831a07059758-Paper.pdf,Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks,"Federico Monti, Michael Bronstein, Xavier Bresson",
neurips,https://proceedings.neurips.cc/paper/2017/file/2eb5657d37f474e4c4cf01e4882b8962-Paper.pdf,Maximizing Subset Accuracy with Recurrent Neural Networks in Multi-label Classification,"Jinseok Nam, Eneldo Loza Mencía, Hyunwoo J. Kim, Johannes Fürnkranz",
neurips,https://proceedings.neurips.cc/paper/2017/file/2f2b265625d76a6704b08093c652fd79-Paper.pdf,f-GANs in an Information Geometric Nutshell,"Richard Nock, Zac Cranko, Aditya K. Menon, Lizhen Qu, Robert C. Williamson","Nowozin \textit{et al} showed last year how to extend the GAN \textit{principle} to all
f
f
-divergences. The approach is elegant but falls short of a full description of the supervised game, and says little about the key player, the generator: for example, what does the generator actually converge to if solving the GAN game means convergence in some space of parameters? How does that provide hints on the generator's design and compare to the flourishing but almost exclusively experimental literature on the subject? In this paper, we unveil a broad class of distributions for which such convergence happens --- namely, deformed exponential families, a wide superset of exponential families ---. We show that current deep architectures are able to factorize a very large number of such densities using an especially compact design, hence displaying the power of deep architectures and their concinnity in the
f
f
-GAN game. This result holds given a sufficient condition on \textit{activation functions} --- which turns out to be satisfied by popular choices. The key to our results is a variational generalization of an old theorem that relates the KL divergence between regular exponential families and divergences between their natural parameters. We complete this picture with additional results and experimental insights on how these results may be used to ground further improvements of GAN architectures, via (i) a principled design of the activation functions in the generator and (ii) an explicit integration of proper composite losses' link function in the discriminator."
neurips,https://proceedings.neurips.cc/paper/2017/file/2f37d10131f2a483a8dd005b3d14b0d9-Paper.pdf,Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples,"Haw-Shiuan Chang, Erik Learned-Miller, Andrew McCallum",
neurips,https://proceedings.neurips.cc/paper/2017/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf,SchNet: A continuous-filter convolutional neural network for modeling quantum interactions,"Kristof Schütt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, Klaus-Robert Müller",
neurips,https://proceedings.neurips.cc/paper/2017/file/30f8f6b940d1073d8b6a5eebc46dd6e5-Paper.pdf,GibbsNet: Iterative Adversarial Inference for Deep Graphical Models,"Alex M. Lamb, Devon Hjelm, Yaroslav Ganin, Joseph Paul Cohen, Aaron C. Courville, Yoshua Bengio","Directed latent variable models that formulate the joint distribution as
p
(
x
,
z
)
=
p
(
z
)
p
(
x
∣
z
)
p
have the advantage of fast and exact sampling. However, these models have the weakness of needing to specify
p
(
z
)
p
, often with a simple fixed prior that limits the expressiveness of the model. Undirected latent variable models discard the requirement that
p
(
z
)
p
be specified with a prior, yet sampling from them generally requires an iterative procedure such as blocked Gibbs-sampling that may require many steps to draw samples from the joint distribution
p
(
x
,
z
)
p
. We propose a novel approach to learning the joint distribution between the data and a latent code which uses an adversarially learned iterative procedure to gradually refine the joint distribution,
p
(
x
,
z
)
p
, to better match with the data distribution on each step. GibbsNet is the best of both worlds both in theory and in practice. Achieving the speed and simplicity of a directed latent variable model, it is guaranteed (assuming the adversarial game reaches the virtual training criteria global minimum) to produce samples from
p
(
x
,
z
)
p
with only a few sampling iterations. Achieving the expressiveness and flexibility of an undirected latent variable model, GibbsNet does away with the need for an explicit
p
(
z
)
p
and has the ability to do attribute prediction, class-conditional generation, and joint image-attribute modeling in a single model which is not trained for any of these specific tasks. We show empirically that GibbsNet is able to learn a more complex
p
(
z
)
p
and show that this leads to improved inpainting and iterative refinement of
p
(
x
,
z
)
p
for dozens of steps and stable generation without collapse for thousands of steps, despite being trained on only a few steps."
neurips,https://proceedings.neurips.cc/paper/2017/file/312351bff07989769097660a56395065-Paper.pdf,Bayesian GAN,"Yunus Saatci, Andrew G. Wilson",
neurips,https://proceedings.neurips.cc/paper/2017/file/3210ddbeaa16948a702b6049b8d9a202-Paper.pdf,Alternating minimization for dictionary learning with random initialization,"Niladri Chatterji, Peter L. Bartlett","We present theoretical guarantees for an alternating minimization algorithm for the dictionary learning/sparse coding problem. The dictionary learning problem is to factorize vector samples
y
1
,
y
2
,
…
,
y
n
y
into an appropriate basis (dictionary)
A
∗
A
and sparse vectors
x
1
∗
,
…
,
x
n
∗
x
. Our algorithm is a simple alternating minimization procedure that switches between
ℓ
1
ℓ
minimization and gradient descent in alternate steps. Dictionary learning and specifically alternating minimization algorithms for dictionary learning are well studied both theoretically and empirically. However, in contrast to previous theoretical analyses for this problem, we replace a condition on the operator norm (that is, the largest magnitude singular value) of the true underlying dictionary
A
∗
A
with a condition on the matrix infinity norm (that is, the largest magnitude term). This not only allows us to get convergence rates for the error of the estimated dictionary measured in the matrix infinity norm, but also ensures that a random initialization will provably converge to the global optimum. Our guarantees are under a reasonable generative model that allows for dictionaries with growing operator norms, and can handle an arbitrary level of overcompleteness, while having sparsity that is information theoretically optimal. We also establish upper bounds on the sample complexity of our algorithm."
neurips,https://proceedings.neurips.cc/paper/2017/file/3214a6d842cc69597f9edf26df552e43-Paper.pdf,"Sparse Embedded
k
k
-Means Clustering","Weiwei Liu, Xiaobo Shen, Ivor Tsang","The
k
k
-means clustering algorithm is a ubiquitous tool in data mining and machine learning that shows promising performance. However, its high computational cost has hindered its applications in broad domains. Researchers have successfully addressed these obstacles with dimensionality reduction methods. Recently, [1] develop a state-of-the-art random projection (RP) method for faster
k
k
-means clustering. Their method delivers many improvements over other dimensionality reduction methods. For example, compared to the advanced singular value decomposition based feature extraction approach, [1] reduce the running time by a factor of
min
{
n
,
d
}
ϵ
2
l
o
g
(
d
)
/
k
min
for data matrix
X
∈
R
n
×
d
X
with
n
n
data points and
d
d
features, while losing only a factor of one in approximation accuracy. Unfortunately, they still require
O
(
n
d
k
ϵ
2
l
o
g
(
d
)
)
O
for matrix multiplication and this cost will be prohibitive for large values of
n
n
and
d
d
. To break this bottleneck, we carefully build a sparse embedded
k
k
-means clustering algorithm which requires
O
(
n
n
z
(
X
)
)
O
(
n
n
z
(
X
)
n
denotes the number of non-zeros in
X
X
) for fast matrix multiplication. Moreover, our proposed algorithm improves on [1]'s results for approximation accuracy by a factor of one. Our empirical studies corroborate our theoretical findings, and demonstrate that our approach is able to significantly accelerate
k
k
-means clustering, while achieving satisfactory clustering performance."
neurips,https://proceedings.neurips.cc/paper/2017/file/325995af77a0e8b06d1204a171010b3a-Paper.pdf,Reducing Reparameterization Gradient Variance,"Andrew Miller, Nick Foti, Alexander D'Amour, Ryan P. Adams","Optimization with noisy gradients has become ubiquitous in statistics and machine learning. Reparameterization gradients, or gradient estimates computed via the
reparameterization trick,'' represent a class of noisy gradients often used in Monte Carlo variational inference (MCVI). However, when these gradient estimators are too noisy, the optimization procedure can be slow or fail to converge. One way to reduce noise is to generate more samples for the gradient estimate, but this can be computationally expensive. Instead, we view the noisy gradient as a random variable, and form an inexpensive approximation of the generating procedure for the gradient sample. This approximation has high correlation with the noisy gradient by construction, making it a useful control variate for variance reduction. We demonstrate our approach on a non-conjugate hierarchical model and a Bayesian neural net where our method attained orders of magnitude (20-2{,}000
×
×
) reduction in gradient variance resulting in faster and more stable optimization."
neurips,https://proceedings.neurips.cc/paper/2017/file/327708dd10d68b1361ad3addbaca01f2-Paper.pdf,Min-Max Propagation,"Christopher Srinivasa, Inmar Givoni, Siamak Ravanbakhsh, Brendan J. Frey",
neurips,https://proceedings.neurips.cc/paper/2017/file/32b3ee0272954b956a7d1f86f76afa21-Paper.pdf,Statistical Cost Sharing,"Eric Balkanski, Umar Syed, Sergei Vassilvitskii",
neurips,https://proceedings.neurips.cc/paper/2017/file/32bb90e8976aab5298d5da10fe66f21d-Paper.pdf,Dilated Recurrent Neural Networks,"Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock, Mark A. Hasegawa-Johnson, Thomas S. Huang",
neurips,https://proceedings.neurips.cc/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Paper.pdf,The Expressive Power of Neural Networks: A View from the Width,"Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, Liwei Wang",
neurips,https://proceedings.neurips.cc/paper/2017/file/32fdab6559cdfa4f167f8c31b9199643-Paper.pdf,Inverse Reward Design,"Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J. Russell, Anca Dragan",
neurips,https://proceedings.neurips.cc/paper/2017/file/331316d4efb44682092a006307b9ae3a-Paper.pdf,The power of absolute discounting: all-dimensional distribution estimation,"Moein Falahatgar, Mesrob I. Ohannessian, Alon Orlitsky, Venkatadheeraj Pichapati",
neurips,https://proceedings.neurips.cc/paper/2017/file/3323fe11e9595c09af38fe67567a9394-Paper.pdf,A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning,"Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, Thore Graepel",
neurips,https://proceedings.neurips.cc/paper/2017/file/333cb763facc6ce398ff83845f224d62-Paper.pdf,Spectral Mixture Kernels for Multi-Output Gaussian Processes,"Gabriel Parra, Felipe Tobar",
neurips,https://proceedings.neurips.cc/paper/2017/file/347665597cbfaef834886adbb848011f-Paper.pdf,Affine-Invariant Online Optimization and the Low-rank Experts Problem,"Tomer Koren, Roi Livni","We present a new affine-invariant optimization algorithm called Online Lazy Newton. The regret of Online Lazy Newton is independent of conditioning: the algorithm's performance depends on the best possible preconditioning of the problem in retrospect and on its \emph{intrinsic} dimensionality. As an application, we show how Online Lazy Newton can be used to achieve an optimal regret of order
√
r
T
r
for the low-rank experts problem, improving by a
√
r
r
factor over the previously best known bound and resolving an open problem posed by Hazan et al (2016)."
neurips,https://proceedings.neurips.cc/paper/2017/file/34ed066df378efacc9b924ec161e7639-Paper.pdf,Pose Guided Person Image Generation,"Liqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuytelaars, Luc Van Gool","This paper proposes the novel Pose Guided Person Generation Network (PG
2
2
) that allows to synthesize person images in arbitrary poses, based on an image of that person and a novel pose. Our generation framework PG
2
2
utilizes the pose information explicitly and consists of two key stages: pose integration and image refinement. In the first stage the condition image and the target pose are fed into a U-Net-like network to generate an initial but coarse image of the person with the target pose. The second stage then refines the initial and blurry result by training a U-Net-like generator in an adversarial way. Extensive experimental results on both 128
×
×
64 re-identification images and 256
×
×
256 fashion photos show that our model generates high-quality person images with convincing details."
neurips,https://proceedings.neurips.cc/paper/2017/file/350db081a661525235354dd3e19b8c05-Paper.pdf,Successor Features for Transfer in Reinforcement Learning,"Andre Barreto, Will Dabney, Remi Munos, Jonathan J. Hunt, Tom Schaul, Hado P. van Hasselt, David Silver",
neurips,https://proceedings.neurips.cc/paper/2017/file/351b33587c5fdd93bd42ef7ac9995a28-Paper.pdf,On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex Sparse Learning,"Xingguo Li, Lin Yang, Jason Ge, Jarvis Haupt, Tong Zhang, Tuo Zhao",
neurips,https://proceedings.neurips.cc/paper/2017/file/352fe25daf686bdb4edca223c921acea-Paper.pdf,Hypothesis Transfer Learning via Transformation Functions,"Simon S. Du, Jayanth Koushik, Aarti Singh, Barnabas Poczos",
neurips,https://proceedings.neurips.cc/paper/2017/file/353de26971b93af88da102641069b440-Paper.pdf,Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov Setting,"Yue Wang, Wei Chen, Yuting Liu, Zhi-Ming Ma, Tie-Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2017/file/35464c848f410e55a13bb9d78e7fddd0-Paper.pdf,"Variational Inference via
χ
χ
Upper Bound Minimization","Adji Bousso Dieng, Dustin Tran, Rajesh Ranganath, John Paisley, David Blei","Variational inference (VI) is widely used as an efficient alternative to Markov chain Monte Carlo. It posits a family of approximating distributions
q
q
and finds the closest member to the exact posterior
p
p
. Closeness is usually measured via a divergence
D
(
q
|
|
p
)
D
from
q
q
to
p
p
. While successful, this approach also has problems. Notably, it typically leads to underestimation of the posterior variance. In this paper we propose CHIVI, a black-box variational inference algorithm that minimizes
D
χ
(
p
|
|
q
)
D
, the
χ
χ
-divergence from
p
p
to
q
q
. CHIVI minimizes an upper bound of the model evidence, which we term the
χ
χ
upper bound (CUBO). Minimizing the CUBO leads to improved posterior uncertainty, and it can also be used with the classical VI lower bound (ELBO) to provide a sandwich estimate of the model evidence. We study CHIVI on three models: probit regression, Gaussian process classification, and a Cox process model of basketball plays. When compared to expectation propagation and classical VI, CHIVI produces better error rates and more accurate estimates of posterior variance."
neurips,https://proceedings.neurips.cc/paper/2017/file/35936504a37d53e03abdfbc7318d9ec7-Paper.pdf,A Probabilistic Framework for Nonlinearities in Stochastic Neural Networks,"Qinliang Su, xuejun Liao, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2017/file/361440528766bbaaaa1901845cf4152b-Paper.pdf,Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation,"Yuhuai Wu, Elman Mansimov, Roger B. Grosse, Shun Liao, Jimmy Ba",
neurips,https://proceedings.neurips.cc/paper/2017/file/3621f1454cacf995530ea53652ddf8fb-Paper.pdf,Optimistic posterior sampling for reinforcement learning: worst-case regret bounds,"Shipra Agrawal, Randy Jia","We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of
~
O
(
D
√
S
A
T
)
O
for any communicating MDP with
S
S
states,
A
A
actions and diameter
D
D
, when
T
≥
S
5
A
T
. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon
T
T
. This result improves over the best previously known upper bound of
~
O
(
D
S
√
A
T
)
O
achieved by any algorithm in this setting, and matches the dependence on
S
S
in the established lower bound of
Ω
(
√
D
S
A
T
)
Ω
for this problem. Our techniques involve proving some novel results about the anti-concentration of Dirichlet distribution, which may be of independent interest."
neurips,https://proceedings.neurips.cc/paper/2017/file/366f0bc7bd1d4bf414073cabbadfdfcd-Paper.pdf,Efficient Second-Order Online Kernel Learning with Adaptive Embedding,"Daniele Calandriello, Alessandro Lazaric, Michal Valko","Online kernel learning (OKL) is a flexible framework to approach prediction problems, since the large approximation space provided by reproducing kernel Hilbert spaces can contain an accurate function for the problem. Nonetheless, optimizing over this space is computationally expensive. Not only first order methods accumulate
\O
(
√
T
)
\O
more loss than the optimal function, but the curse of kernelization results in a
\O
(
t
)
\O
per step complexity. Second-order methods get closer to the optimum much faster, suffering only
\O
(
log
(
T
)
)
\O
regret, but second-order updates are even more expensive, with a
\O
(
t
2
)
\O
per-step cost. Existing approximate OKL methods try to reduce this complexity either by limiting the Support Vectors (SV) introduced in the predictor, or by avoiding the kernelization process altogether using embedding. Nonetheless, as long as the size of the approximation space or the number of SV does not grow over time, an adversary can always exploit the approximation process. In this paper, we propose PROS-N-KONS, a method that combines Nystrom sketching to project the input point in a small, accurate embedded space, and performs efficient second-order updates in this space. The embedded space is continuously updated to guarantee that the embedding remains accurate, and we show that the per-step cost only grows with the effective dimension of the problem and not with
T
T
. Moreover, the second-order updated allows us to achieve the logarithmic regret. We empirically compare our algorithm on recent large-scales benchmarks and show it performs favorably."
neurips,https://proceedings.neurips.cc/paper/2017/file/36a1694bce9815b7e38a9dad05ad42e0-Paper.pdf,Solving Most Systems of Random Quadratic Equations,"Gang Wang, Georgios Giannakis, Yousef Saad, Jie Chen","This paper deals with finding an
n
n
-dimensional solution
\bm
x
\bm
to a system of quadratic equations
y
i
=
|
⟨
\bm
a
i
,
\bm
x
⟩
|
2
y
,
1
≤
i
≤
m
1
, which in general is known to be NP-hard. We put forth a novel procedure, that starts with a \emph{weighted maximal correlation initialization} obtainable with a few power iterations, followed by successive refinements based on \emph{iteratively reweighted gradient-type iterations}. The novel techniques distinguish themselves from prior works by the inclusion of a fresh (re)weighting regularization. For certain random measurement models, the proposed procedure returns the true solution
\bm
x
\bm
with high probability in time proportional to reading the data
{
(
\bm
a
i
;
y
i
)
}
1
≤
i
≤
m
{
, provided that the number
m
m
of equations is some constant
c
>
0
c
times the number
n
n
of unknowns, that is,
m
≥
c
n
m
. Empirically, the upshots of this contribution are: i) perfect signal recovery in the high-dimensional regime given only an \emph{information-theoretic limit number} of equations; and, ii) (near-)optimal statistical accuracy in the presence of additive noise. Extensive numerical tests using both synthetic data and real images corroborate its improved signal recovery performance and computational efficiency relative to state-of-the-art approaches."
neurips,https://proceedings.neurips.cc/paper/2017/file/36e729ec173b94133d8fa552e4029f8b-Paper.pdf,Online Reinforcement Learning in Stochastic Games,"Chen-Yu Wei, Yi-Te Hong, Chi-Jen Lu","We study online reinforcement learning in average-reward stochastic games (SGs). An SG models a two-player zero-sum game in a Markov environment, where state transitions and one-step payoffs are determined simultaneously by a learner and an adversary. We propose the \textsc{UCSG} algorithm that achieves a sublinear regret compared to the game value when competing with an arbitrary opponent. This result improves previous ones under the same setting. The regret bound has a dependency on the \textit{diameter}, which is an intrinsic value related to the mixing property of SGs. Slightly extended, \textsc{UCSG} finds an
ε
ε
-maximin stationary policy with a sample complexity of
~
O
(
poly
(
1
/
ε
)
)
O
, where
ε
ε
is the error parameter. To the best of our knowledge, this extended result is the first in the average-reward setting. In the analysis, we develop Markov chain's perturbation bounds for mean first passage times and techniques to deal with non-stationary opponents, which may be of interest in their own right."
neurips,https://proceedings.neurips.cc/paper/2017/file/37d097caf1299d9aa79c2c2b843d2d78-Paper.pdf,Independence clustering (without a matrix),Daniil Ryabko,"The independence clustering problem is considered in the following formulation: given a set
S
S
of random variables, it is required to find the finest partitioning
{
U
1
,
…
,
U
k
}
{
of
S
S
into clusters such that the clusters
U
1
,
…
,
U
k
U
are mutually independent. Since mutual independence is the target, pairwise similarity measurements are of no use, and thus traditional clustering algorithms are inapplicable. The distribution of the random variables in
S
S
is, in general, unknown, but a sample is available. Thus, the problem is cast in terms of time series. Two forms of sampling are considered: i.i.d.\ and stationary time series, with the main emphasis being on the latter, more general, case. A consistent, computationally tractable algorithm for each of the settings is proposed, and a number of fascinating open directions for further research are outlined."
neurips,https://proceedings.neurips.cc/paper/2017/file/38811c5285e34e2e3319ab7d9f2cfa5b-Paper.pdf,Effective Parallelisation for Machine Learning,"Michael Kamp, Mario Boley, Olana Missura, Thomas Gärtner",
neurips,https://proceedings.neurips.cc/paper/2017/file/38913e1d6a7b94cb0f55994f679f5956-Paper.pdf,Deep Mean-Shift Priors for Image Restoration,"Siavash Arjomand Bigdeli, Matthias Zwicker, Paolo Favaro, Meiguang Jin",
neurips,https://proceedings.neurips.cc/paper/2017/file/38db3aed920cf82ab059bfccbd02be6a-Paper.pdf,On Structured Prediction Theory with Calibrated Convex Surrogate Losses,"Anton Osokin, Francis Bach, Simon Lacoste-Julien",
neurips,https://proceedings.neurips.cc/paper/2017/file/38ed162a0dbef7b3fe0f628aa08b90e7-Paper.pdf,Invariance and Stability of Deep Convolutional Representations,"Alberto Bietti, Julien Mairal",
neurips,https://proceedings.neurips.cc/paper/2017/file/3937230de3c8041e4da6ac3246a888e8-Paper.pdf,Variational Memory Addressing in Generative Models,"Jörg Bornschein, Andriy Mnih, Daniel Zoran, Danilo Jimenez Rezende",
neurips,https://proceedings.neurips.cc/paper/2017/file/393c55aea738548df743a186d15f3bef-Paper.pdf,Shallow Updates for Deep Reinforcement Learning,"Nir Levine, Tom Zahavy, Daniel J. Mankowitz, Aviv Tamar, Shie Mannor",
neurips,https://proceedings.neurips.cc/paper/2017/file/39ae2ed11b14a4ccb41d35e9d1ba5d11-Paper.pdf,Learning with Bandit Feedback in Potential Games,"Amélie Heliou, Johanne Cohen, Panayotis Mertikopoulos",
neurips,https://proceedings.neurips.cc/paper/2017/file/39d352b0395ba768e18f042c6e2a8621-Paper.pdf,A Greedy Approach for Budgeted Maximum Inner Product Search,"Hsiang-Fu Yu, Cho-Jui Hsieh, Qi Lei, Inderjit S. Dhillon",
neurips,https://proceedings.neurips.cc/paper/2017/file/3a0844cee4fcf57de0c71e9ad3035478-Paper.pdf,Riemannian approach to batch normalization,"Minhyung Cho, Jaehyung Lee",
neurips,https://proceedings.neurips.cc/paper/2017/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf,Adaptive Clustering through Semidefinite Programming,Martin Royer,
neurips,https://proceedings.neurips.cc/paper/2017/file/3a20f62a0af1aa152670bab3c602feed-Paper.pdf,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning,"Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, Pieter Abbeel",
neurips,https://proceedings.neurips.cc/paper/2017/file/3a835d3215755c435ef4fe9965a3f2a0-Paper.pdf,Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition,"Naoya Takeishi, Yoshinobu Kawahara, Takehisa Yairi",
neurips,https://proceedings.neurips.cc/paper/2017/file/3b3dbaf68507998acd6a5a5254ab2d76-Paper.pdf,Online Prediction with Selfish Experts,"Tim Roughgarden, Okke Schrijvers",
neurips,https://proceedings.neurips.cc/paper/2017/file/3baa271bc35fe054c86928f7016e8ae6-Paper.pdf,Streaming Robust Submodular Maximization: A Partitioned Thresholding Approach,"Slobodan Mitrovic, Ilija Bogunovic, Ashkan Norouzi-Fard, Jakub M. Tarnawski, Volkan Cevher",
neurips,https://proceedings.neurips.cc/paper/2017/file/3bf55bbad370a8fcad1d09b005e278c2-Paper.pdf,Neural Program Meta-Induction,"Jacob Devlin, Rudy R. Bunel, Rishabh Singh, Matthew Hausknecht, Pushmeet Kohli","Most recently proposed methods for Neural Program induction work under the assumption of having a large set of input/output (I/O) examples for learning any given input-output mapping. This paper aims to address the problem of data and computation efficiency of program induction by leveraging information from related tasks. Specifically, we propose two novel approaches for cross-task knowledge transfer to improve program induction in limited-data scenarios. In our first proposal, portfolio adaptation, a set of induction models is pretrained on a set of related tasks, and the best model is adapted towards the new task using transfer learning. In our second approach, meta program induction, a
k
k
-shot learning approach is used to make a model generalize to new tasks without additional training. To test the efficacy of our methods, we constructed a new benchmark of programs written in the Karel programming language. Using an extensive experimental evaluation on the Karel benchmark, we demonstrate that our proposals dramatically outperform the baseline induction method that does not use knowledge transfer. We also analyze the relative performance of the two approaches and study conditions in which they perform best. In particular, meta induction outperforms all existing approaches under extreme data sparsity (when a very small number of examples are available), i.e., fewer than ten. As the number of available I/O examples increase (i.e. a thousand or more), portfolio adapted program induction becomes the best approach. For intermediate data sizes, we demonstrate that the combined method of adapted meta program induction has the strongest performance."
neurips,https://proceedings.neurips.cc/paper/2017/file/3cfbdf468f0a03187f6cee51a25e5e9a-Paper.pdf,The Scaling Limit of High-Dimensional Online Independent Component Analysis,"Chuang Wang, Yue Lu",
neurips,https://proceedings.neurips.cc/paper/2017/file/3d779cae2d46cf6a8a99a35ba4167977-Paper.pdf,Practical Locally Private Heavy Hitters,"Raef Bassily, Kobbi Nissim, Uri Stemmer, Abhradeep Guha Thakurta","We present new practical local differentially private heavy hitters algorithms achieving optimal or near-optimal worst-case error -- TreeHist and Bitstogram. In both algorithms, server running time is
~
O
(
n
)
O
and user running time is
~
O
(
1
)
O
, hence improving on the prior state-of-the-art result of Bassily and Smith [STOC 2015] requiring
~
O
(
n
5
/
2
)
O
server time and
~
O
(
n
3
/
2
)
O
user time. With a typically large number of participants in local algorithms (
n
n
in the millions), this reduction in time complexity, in particular at the user side, is crucial for the use of such algorithms in practice. We implemented Algorithm TreeHist to verify our theoretical analysis and compared its performance with the performance of Google's RAPPOR code."
neurips,https://proceedings.neurips.cc/paper/2017/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Paper.pdf,Mixture-Rank Matrix Approximation for Collaborative Filtering,"Dongsheng Li, Chao Chen, Wei Liu, Tun Lu, Ning Gu, Stephen Chu",
neurips,https://proceedings.neurips.cc/paper/2017/file/3e60e09c222f206c725385f53d7e567c-Paper.pdf,Higher-Order Total Variation Classes on Grids: Minimax Theory and Trend Filtering Methods,"Veeranjaneyulu Sadhanala, Yu-Xiang Wang, James L. Sharpnack, Ryan J. Tibshirani","We consider the problem of estimating the values of a function over
n
n
nodes of a
d
d
-dimensional grid graph (having equal side lengths
n
1
/
d
n
) from noisy observations. The function is assumed to be smooth, but is allowed to exhibit different amounts of smoothness at different regions in the grid. Such heterogeneity eludes classical measures of smoothness from nonparametric statistics, such as Holder smoothness. Meanwhile, total variation (TV) smoothness classes allow for heterogeneity, but are restrictive in another sense: only constant functions count as perfectly smooth (achieve zero TV). To move past this, we define two new higher-order TV classes, based on two ways of compiling the discrete derivatives of a parameter across the nodes. We relate these two new classes to Holder classes, and derive lower bounds on their minimax errors. We also analyze two naturally associated trend filtering methods; when
d
=
2
d
, each is seen to be rate optimal over the appropriate class."
neurips,https://proceedings.neurips.cc/paper/2017/file/3eb414bf1c2a66a09c185d60553417b8-Paper.pdf,Robust Conditional Probabilities,"Yoav Wald, Amir Globerson","Conditional probabilities are a core concept in machine learning. For example, optimal prediction of a label
Y
Y
given an input
X
X
corresponds to maximizing the conditional probability of
Y
Y
given
X
X
. A common approach to inference tasks is learning a model of conditional probabilities. However, these models are often based on strong assumptions (e.g., log-linear models), and hence their estimate of conditional probabilities is not robust and is highly dependent on the validity of their assumptions. Here we propose a framework for reasoning about conditional probabilities without assuming anything about the underlying distributions, except knowledge of their second order marginals, which can be estimated from data. We show how this setting leads to guaranteed bounds on conditional probabilities, which can be calculated efficiently in a variety of settings, including structured-prediction. Finally, we apply them to semi-supervised deep learning, obtaining results competitive with variational autoencoders."
neurips,https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf,Attention is All you Need,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
neurips,https://proceedings.neurips.cc/paper/2017/file/3f647cadf56541fb9513cb63ec370187-Paper.pdf,A General Framework for Robust Interactive Learning,"Ehsan Emamjomeh-Zadeh, David Kempe",
neurips,https://proceedings.neurips.cc/paper/2017/file/3f998e713a6e02287c374fd26835d87e-Paper.pdf,Sample and Computationally Efficient Learning Algorithms under S-Concave Distributions,"Maria-Florina F. Balcan, Hongyang Zhang","We provide new results for noise-tolerant and sample-efficient learning algorithms under
s
s
-concave distributions. The new class of
s
s
-concave distributions is a broad and natural generalization of log-concavity, and includes many important additional distributions, e.g., the Pareto distribution and
t
t
distribution. This class has been studied in the context of efficient sampling, integration, and optimization, but much remains unknown about the geometry of this class of distributions and their applications in the context of learning. The challenge is that unlike the commonly used distributions in learning (uniform or more generally log-concave distributions), this broader class is not closed under the marginalization operator and many such distributions are fat-tailed. In this work, we introduce new convex geometry tools to study the properties of
s
s
-concave distributions and use these properties to provide bounds on quantities of interest to learning including the probability of disagreement between two halfspaces, disagreement outside a band, and the disagreement coefficient. We use these results to significantly generalize prior results for margin-based active learning, disagreement-based active learning, and passive learning of intersections of halfspaces. Our analysis of geometric properties of
s
s
-concave distributions might be of independent interest to optimization more broadly."
neurips,https://proceedings.neurips.cc/paper/2017/file/3fab5890d8113d0b5a4178201dc842ad-Paper.pdf,Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee,"Alireza Aghasi, Afshin Abdi, Nam Nguyen, Justin Romberg","We introduce and analyze a new technique for model reduction for deep neural networks. While large networks are theoretically capable of learning arbitrarily complex models, overfitting and model redundancy negatively affects the prediction accuracy and model variance. Our Net-Trim algorithm prunes (sparsifies) a trained network layer-wise, removing connections at each layer by solving a convex optimization program. This program seeks a sparse set of weights at each layer that keeps the layer inputs and outputs consistent with the originally trained model. The algorithms and associated analysis are applicable to neural networks operating with the rectified linear unit (ReLU) as the nonlinear activation. We present both parallel and cascade versions of the algorithm. While the latter can achieve slightly simpler models with the same generalization performance, the former can be computed in a distributed manner. In both cases, Net-Trim significantly reduces the number of connections in the network, while also providing enough regularization to slightly reduce the generalization error. We also provide a mathematical analysis of the consistency between the initial network and the retrained model. To analyze the model sample complexity, we derive the general sufficient conditions for the recovery of a sparse transform matrix. For a single layer taking independent Gaussian random vectors of length
N
N
as inputs, we show that if the network response can be described using a maximum number of
s
s
non-zero weights per node, these weights can be learned from
O
(
s
log
N
)
O
samples."
neurips,https://proceedings.neurips.cc/paper/2017/file/3fb451ca2e89b3a13095b059d8705b15-Paper.pdf,"ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games","Yuandong Tian, Qucheng Gong, Wenling Shang, Yuxin Wu, C. Lawrence Zitnick",
neurips,https://proceedings.neurips.cc/paper/2017/file/3fc2c60b5782f641f76bcefc39fb2392-Paper.pdf,Task-based End-to-end Model Learning in Stochastic Optimization,"Priya Donti, Brandon Amos, J. Zico Kolter",
neurips,https://proceedings.neurips.cc/paper/2017/file/3fd60983292458bf7dee75f12d5e9e05-Paper.pdf,Fader Networks:Manipulating Images by Sliding Attributes,"Guillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine Bordes, Ludovic DENOYER, Marc'Aurelio Ranzato",
neurips,https://proceedings.neurips.cc/paper/2017/file/443dec3062d0286986e21dc0631734c9-Paper.pdf,VAE Learning via Stein Variational Gradient Descent,"Yuchen Pu, Zhe Gan, Ricardo Henao, Chunyuan Li, Shaobo Han, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2017/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf,Approximation and Convergence Properties of Generative Adversarial Learning,"Shuang Liu, Olivier Bousquet, Kamalika Chaudhuri",
neurips,https://proceedings.neurips.cc/paper/2017/file/44a2e0804995faf8d2e3b084a1e2db1d-Paper.pdf,VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning,"Akash Srivastava, Lazar Valkov, Chris Russell, Michael U. Gutmann, Charles Sutton",
neurips,https://proceedings.neurips.cc/paper/2017/file/44ac09ac6a149136a4102ee4b4103ae6-Paper.pdf,Local Aggregative Games,"Vikas Garg, Tommi Jaakkola",
neurips,https://proceedings.neurips.cc/paper/2017/file/4500e4037738e13c0c18db508e18d483-Paper.pdf,An Error Detection and Correction Framework for Connectomics,"Jonathan Zung, Ignacio Tartavull, Kisuk Lee, H. Sebastian Seung",
neurips,https://proceedings.neurips.cc/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf,Hindsight Experience Replay,"Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, Wojciech Zaremba",
neurips,https://proceedings.neurips.cc/paper/2017/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Paper.pdf,Fixed-Rank Approximation of a Positive-Semidefinite Matrix from Streaming Data,"Joel A. Tropp, Alp Yurtsever, Madeleine Udell, Volkan Cevher",
neurips,https://proceedings.neurips.cc/paper/2017/file/4588e674d3f0faf985047d4c3f13ed0d-Paper.pdf,The Numerics of GANs,"Lars Mescheder, Sebastian Nowozin, Andreas Geiger",
neurips,https://proceedings.neurips.cc/paper/2017/file/45fbc6d3e05ebd93369ce542e8f2322d-Paper.pdf,Cortical microcircuits as gated-recurrent neural networks,"Rui Costa, Ioannis Alexandros Assael, Brendan Shillingford, Nando de Freitas, TIm Vogels",
neurips,https://proceedings.neurips.cc/paper/2017/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf,Deep Lattice Networks and Partial Monotonic Functions,"Seungil You, David Ding, Kevin Canini, Jan Pfeifer, Maya Gupta",
neurips,https://proceedings.neurips.cc/paper/2017/file/4671aeaf49c792689533b00664a5c3ef-Paper.pdf,Zap Q-Learning,"Adithya M Devraj, Sean Meyn",
neurips,https://proceedings.neurips.cc/paper/2017/file/46922a0880a8f11f8f69cbb52b1396be-Paper.pdf,Contrastive Learning for Image Captioning,"Bo Dai, Dahua Lin",
neurips,https://proceedings.neurips.cc/paper/2017/file/46a558d97954d0692411c861cf78ef79-Paper.pdf,Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net,"Anirudh Goyal ALIAS PARTH GOYAL, Nan Rosemary Ke, Surya Ganguli, Yoshua Bengio",
neurips,https://proceedings.neurips.cc/paper/2017/file/473447ac58e1cd7e96172575f48dca3b-Paper.pdf,Linear Time Computation of Moments in Sum-Product Networks,"Han Zhao, Geoffrey J. Gordon",
neurips,https://proceedings.neurips.cc/paper/2017/file/489d0396e6826eb0c1e611d82ca8b215-Paper.pdf,SGD Learns the Conjugate Kernel Class of the Network,Amit Daniely,
neurips,https://proceedings.neurips.cc/paper/2017/file/48ab2f9b45957ab574cf005eb8a76760-Paper.pdf,Learning to Pivot with Adversarial Networks,"Gilles Louppe, Michael Kagan, Kyle Cranmer",
neurips,https://proceedings.neurips.cc/paper/2017/file/491442df5f88c6aa018e86dac21d3606-Paper.pdf,Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration,"Jason Altschuler, Jonathan Niles-Weed, Philippe Rigollet",
neurips,https://proceedings.neurips.cc/paper/2017/file/49182f81e6a13cf5eaa496d51fea6406-Paper.pdf,Universal Style Transfer via Feature Transforms,"Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang",
neurips,https://proceedings.neurips.cc/paper/2017/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf,Ensemble Sampling,"Xiuyuan Lu, Benjamin Van Roy",
neurips,https://proceedings.neurips.cc/paper/2017/file/49b8b4f95f02e055801da3b4f58e28b7-Paper.pdf,Practical Data-Dependent Metric Compression with Provable Guarantees,"Piotr Indyk, Ilya Razenshteyn, Tal Wagner",
neurips,https://proceedings.neurips.cc/paper/2017/file/4a2ddf148c5a9c42151a529e8cbdcc06-Paper.pdf,Partial Hard Thresholding: Towards A Principled Analysis of Support Recovery,"Jie Shen, Ping Li",
neurips,https://proceedings.neurips.cc/paper/2017/file/4a8423d5e91fda00bb7e46540e2b0cf1-Paper.pdf,Selective Classification for Deep Neural Networks,"Yonatan Geifman, Ran El-Yaniv",
neurips,https://proceedings.neurips.cc/paper/2017/file/4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf,Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space,"Liwei Wang, Alexander Schwing, Svetlana Lazebnik",
neurips,https://proceedings.neurips.cc/paper/2017/file/4b4edc2630fe75800ddc29a7b4070add-Paper.pdf,Deconvolutional Paragraph Representation Learning,"Yizhe Zhang, Dinghan Shen, Guoyin Wang, Zhe Gan, Ricardo Henao, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2017/file/4c56ff4ce4aaf9573aa5dff913df997a-Paper.pdf,Learning to See Physics via Visual De-animation,"Jiajun Wu, Erika Lu, Pushmeet Kohli, Bill Freeman, Josh Tenenbaum",
neurips,https://proceedings.neurips.cc/paper/2017/file/4cb811134b9d39fc3104bd06ce75abad-Paper.pdf,Adversarial Symmetric Variational Autoencoder,"Yuchen Pu, Weiyao Wang, Ricardo Henao, Liqun Chen, Zhe Gan, Chunyuan Li, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2017/file/4da04049a062f5adfe81b67dd755cecc-Paper.pdf,Model evidence from nonequilibrium simulations,Michael Habeck,
neurips,https://proceedings.neurips.cc/paper/2017/file/4db0f8b0fc895da263fd77fc8aecabe4-Paper.pdf,Estimating High-dimensional Non-Gaussian Multiple Index Models via Stein’s Lemma,"Zhuoran Yang, Krishnakumar Balasubramanian, Zhaoran Wang, Han Liu",
neurips,https://proceedings.neurips.cc/paper/2017/file/4e0928de075538c593fbdabb0c5ef2c3-Paper.pdf,Learning spatiotemporal piecewise-geodesic trajectories from longitudinal manifold-valued data,"Stéphanie ALLASSONNIERE, Juliette Chevallier, Stephane Oudard",
neurips,https://proceedings.neurips.cc/paper/2017/file/4e2a6330465c8ffcaa696a5a16639176-Paper.pdf,SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks,"Kyuhong Shim, Minjae Lee, Iksoo Choi, Yoonho Boo, Wonyong Sung",
neurips,https://proceedings.neurips.cc/paper/2017/file/4e732ced3463d06de0ca9a15b6153677-Paper.pdf,Concentration of Multilinear Functions of the Ising Model with Applications to Network Data,"Constantinos Daskalakis, Nishanth Dikkala, Gautam Kamath",
neurips,https://proceedings.neurips.cc/paper/2017/file/4e8412ad48562e3c9934f45c3e144d48-Paper.pdf,Rigorous Dynamics and Consistent Estimation in Arbitrarily Conditioned Linear Systems,"Alyson K. Fletcher, Mojtaba Sahraee-Ardakan, Sundeep Rangan, Philip Schniter",
neurips,https://proceedings.neurips.cc/paper/2017/file/4edaa105d5f53590338791951e38c3ad-Paper.pdf,OnACID: Online Analysis of Calcium Imaging Data in Real Time,"Andrea Giovannucci, Johannes Friedrich, Matt Kaufman, Anne Churchland, Dmitri Chklovskii, Liam Paninski, Eftychios A. Pnevmatikakis",
neurips,https://proceedings.neurips.cc/paper/2017/file/4fa177df22864518b2d7818d4db5db2d-Paper.pdf,Action Centered Contextual Bandits,"Kristjan Greenewald, Ambuj Tewari, Susan Murphy, Predag Klasnja",
neurips,https://proceedings.neurips.cc/paper/2017/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf,Cost efficient gradient boosting,"Sven Peter, Ferran Diego, Fred A. Hamprecht, Boaz Nadler",
neurips,https://proceedings.neurips.cc/paper/2017/file/50905d7b2216bfeccb5b41016357176b-Paper.pdf,Eigenvalue Decay Implies Polynomial-Time Learnability for Neural Networks,"Surbhi Goel, Adam Klivans",
neurips,https://proceedings.neurips.cc/paper/2017/file/50cf0763d8eb871776d4f28b39deb564-Paper.pdf,"On Separability of Loss Functions, and Revisiting Discriminative Vs Generative Models","Adarsh Prasad, Alexandru Niculescu-Mizil, Pradeep K. Ravikumar",
neurips,https://proceedings.neurips.cc/paper/2017/file/519c84155964659375821f7ca576f095-Paper.pdf,"ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events","Evan Racah, Christopher Beckham, Tegan Maharaj, Samira Ebrahimi Kahou, Mr. Prabhat, Chris Pal",
neurips,https://proceedings.neurips.cc/paper/2017/file/51e6d6e679953c6311757004d8cbbba9-Paper.pdf,A Meta-Learning Perspective on Cold-Start Recommendations for Items,"Manasi Vartak, Arvind Thiagarajan, Conrado Miranda, Jeshua Bratman, Hugo Larochelle",
neurips,https://proceedings.neurips.cc/paper/2017/file/51ef186e18dc00c2d31982567235c559-Paper.pdf,Learning Unknown Markov Decision Processes: A Thompson Sampling Approach,"Yi Ouyang, Mukul Gagrani, Ashutosh Nayyar, Rahul Jain","We consider the problem of learning an unknown Markov Decision Process (MDP) that is weakly communicating in the infinite horizon setting. We propose a Thompson Sampling-based reinforcement learning algorithm with dynamic episodes (TSDE). At the beginning of each episode, the algorithm generates a sample from the posterior distribution over the unknown model parameters. It then follows the optimal stationary policy for the sampled model for the rest of the episode. The duration of each episode is dynamically determined by two stopping criteria. The first stopping criterion controls the growth rate of episode length. The second stopping criterion happens when the number of visits to any state-action pair is doubled. We establish
~
O
(
H
S
√
A
T
)
O
bounds on expected regret under a Bayesian setting, where
S
S
and
A
A
are the sizes of the state and action spaces,
T
T
is time, and
H
H
is the bound of the span. This regret bound matches the best available bound for weakly communicating MDPs. Numerical results show it to perform better than existing algorithms for infinite horizon MDPs."
neurips,https://proceedings.neurips.cc/paper/2017/file/5227b6aaf294f5f027273aebf16015f2-Paper.pdf,Deep Hyperspherical Learning,"Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, Le Song",
neurips,https://proceedings.neurips.cc/paper/2017/file/52292e0c763fd027c6eba6b8f494d2eb-Paper.pdf,Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts,"Raymond Yeh, Jinjun Xiong, Wen-Mei Hwu, Minh Do, Alexander Schwing",
neurips,https://proceedings.neurips.cc/paper/2017/file/5352696a9ca3397beb79f116f3a33991-Paper.pdf,Off-policy evaluation for slate recommendation,"Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miro Dudik, John Langford, Damien Jose, Imed Zitouni",
neurips,https://proceedings.neurips.cc/paper/2017/file/54e36c5ff5f6a1802925ca009f3ebb68-Paper.pdf,Unbiased estimates for linear regression via volume sampling,"Michal Derezinski, Manfred K. K. Warmuth","Given a full rank matrix X with more columns than rows consider the task of estimating the pseudo inverse
X
+
X
based on the pseudo inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). The resulting estimator is unbiased and surprisingly the covariance of the estimator also has a closed form: It equals a specific factor times
X
+
X
+
⊤
X
. Pseudo inverse plays an important part in solving the linear least squares problem, where we try to predict a label for each column of
X
X
. We assume labels are expensive and we are only given the labels for the small subset of columns we sample from
X
X
. Using our methods we show that the weight vector of the solution for the sub problem is an unbiased estimator of the optimal solution for the whole problem based on all column labels. We believe that these new formulas establish a fundamental connection between linear least squares and volume sampling. We use our methods to obtain an algorithm for volume sampling that is faster than state-of-the-art and for obtaining bounds for the total loss of the estimated least-squares solution on all labeled columns."
neurips,https://proceedings.neurips.cc/paper/2017/file/556f391937dfd4398cbac35e050a2177-Paper.pdf,Revisiting Perceptron: Efficient and Label-Optimal Learning of Halfspaces,"Songbai Yan, Chicheng Zhang","It has been a long-standing problem to efficiently learn a halfspace using as few labels as possible in the presence of noise. In this work, we propose an efficient Perceptron-based algorithm for actively learning homogeneous halfspaces under the uniform distribution over the unit sphere. Under the bounded noise condition~\cite{MN06}, where each label is flipped with probability at most
η
<
1
2
η
, our algorithm achieves a near-optimal label complexity of
~
O
(
d
(
1
−
2
η
)
2
ln
1
ϵ
)
O
in time
~
O
(
d
2
ϵ
(
1
−
2
η
)
3
)
O
. Under the adversarial noise condition~\cite{ABL14, KLS09, KKMS08}, where at most a
~
Ω
(
ϵ
)
Ω
fraction of labels can be flipped, our algorithm achieves a near-optimal label complexity of
~
O
(
d
ln
1
ϵ
)
O
in time
~
O
(
d
2
ϵ
)
O
. Furthermore, we show that our active learning algorithm can be converted to an efficient passive learning algorithm that has near-optimal sample complexities with respect to
ϵ
ϵ
and
d
d
."
neurips,https://proceedings.neurips.cc/paper/2017/file/56584778d5a8ab88d6393cc4cd11e090-Paper.pdf,Renyi Differential Privacy Mechanisms for Posterior Sampling,"Joseph Geumlek, Shuang Song, Kamalika Chaudhuri",
neurips,https://proceedings.neurips.cc/paper/2017/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Paper.pdf,Variable Importance Using Decision Trees,"Jalil Kazemitabar, Arash Amini, Adam Bloniarz, Ameet S. Talwalkar",
neurips,https://proceedings.neurips.cc/paper/2017/file/57aeee35c98205091e18d1140e9f38cf-Paper.pdf,A simple model of recognition and recall memory,"Nisheeth Srivastava, Edward Vul",
neurips,https://proceedings.neurips.cc/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf,Implicit Regularization in Matrix Factorization,"Suriya Gunasekar, Blake E. Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, Nati Srebro","We study implicit regularization when optimizing an underdetermined quadratic objective over a matrix
X
X
with gradient descent on a factorization of X. We conjecture and provide empirical and theoretical evidence that with small enough step sizes and initialization close enough to the origin, gradient descent on a full dimensional factorization converges to the minimum nuclear norm solution."
neurips,https://proceedings.neurips.cc/paper/2017/file/58238e9ae2dd305d79c2ebc8c1883422-Paper.pdf,Continuous DR-submodular Maximization: Structure and Algorithms,"An Bian, Kfir Levy, Andreas Krause, Joachim M. Buhmann",
neurips,https://proceedings.neurips.cc/paper/2017/file/58d4d1e7b1e97b258c9ed0b37e02d087-Paper.pdf,"Decoupling ""when to update"" from ""how to update""","Eran Malach, Shai Shalev-Shwartz",
neurips,https://proceedings.neurips.cc/paper/2017/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Paper.pdf,Regret Analysis for Continuous Dueling Bandit,Wataru Kumagai,"The dueling bandit is a learning framework where the feedback information in the learning process is restricted to noisy comparison between a pair of actions. In this paper, we address a dueling bandit problem based on a cost function over a continuous space. We propose a stochastic mirror descent algorithm and show that the algorithm achieves an
O
(
√
T
log
T
)
O
-regret bound under strong convexity and smoothness assumptions for the cost function. Then, we clarify the equivalence between regret minimization in dueling bandit and convex optimization for the cost function. Moreover, considering a lower bound in convex optimization, it is turned out that our algorithm achieves the optimal convergence rate in convex optimization and the optimal regret in dueling bandit except for a logarithmic factor."
neurips,https://proceedings.neurips.cc/paper/2017/file/59b90e1005a220e2ebc542eb9d950b1e-Paper.pdf,One-Sided Unsupervised Domain Mapping,"Sagie Benaim, Lior Wolf","In unsupervised domain mapping, the learner is given two unmatched datasets
A
A
and
B
B
. The goal is to learn a mapping
G
A
B
G
that translates a sample in
A
A
to the analog sample in
B
B
. Recent approaches have shown that when learning simultaneously both
G
A
B
G
and the inverse mapping
G
B
A
G
, convincing mappings are obtained. In this work, we present a method of learning
G
A
B
G
without learning
G
B
A
G
. This is done by learning a mapping that maintains the distance between a pair of samples. Moreover, good mappings are obtained, even by maintaining the distance between different parts of the same sample before and after mapping. We present experimental results that the new method not only allows for one sided mapping learning, but also leads to preferable numerical results over the existing circularity-based constraint. Our entire code is made publicly available at~\url{https://github.com/sagiebenaim/DistanceGAN}."
neurips,https://proceedings.neurips.cc/paper/2017/file/59dfa2df42d9e3d41f5b02bfc32229dd-Paper.pdf,Poincaré Embeddings for Learning Hierarchical Representations,"Maximillian Nickel, Douwe Kiela",
neurips,https://proceedings.neurips.cc/paper/2017/file/5a142a55461d5fef016acfb927fee0bd-Paper.pdf,Variance-based Regularization with Convex Objectives,"Hongseok Namkoong, John C. Duchi",
neurips,https://proceedings.neurips.cc/paper/2017/file/5abdf8b8520b71f3a528c7547ee92428-Paper.pdf,"A Sharp Error Analysis for the Fused Lasso, with Application to Approximate Changepoint Screening","Kevin Lin, James L. Sharpnack, Alessandro Rinaldo, Ryan J. Tibshirani","In the 1-dimensional multiple changepoint detection problem, we derive a new fast error rate for the fused lasso estimator, under the assumption that the mean vector has a sparse number of changepoints. This rate is seen to be suboptimal (compared to the minimax rate) by only a factor of
log
log
n
log
. Our proof technique is centered around a novel construction that we call a lower interpolant. We extend our results to misspecified models and exponential family distributions. We also describe the implications of our error analysis for the approximate screening of changepoints."
neurips,https://proceedings.neurips.cc/paper/2017/file/5b970a1d9be0fd100063fd6cd688b73e-Paper.pdf,Cross-Spectral Factor Analysis,"Neil Gallagher, Kyle R. Ulrich, Austin Talbot, Kafui Dzirasa, Lawrence Carin, David E. Carlson",
neurips,https://proceedings.neurips.cc/paper/2017/file/5d44ee6f2c3f71b73125876103c8f6c4-Paper.pdf,Self-Normalizing Neural Networks,"Günter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter",
neurips,https://proceedings.neurips.cc/paper/2017/file/5d6646aad9bcc0be55b2c82f69750387-Paper.pdf,Fast amortized inference of neural activity from calcium imaging data with variational autoencoders,"Artur Speiser, Jinyao Yan, Evan W. Archer, Lars Buesing, Srinivas C. Turaga, Jakob H. Macke",
neurips,https://proceedings.neurips.cc/paper/2017/file/5dc126b503e374b0e08231344a7f493f-Paper.pdf,Asynchronous Parallel Coordinate Minimization for MAP Inference,"Ofer Meshi, Alexander Schwing",
neurips,https://proceedings.neurips.cc/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf,Inductive Representation Learning on Large Graphs,"Will Hamilton, Zhitao Ying, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2017/file/5eac43aceba42c8757b54003a58277b5-Paper.pdf,Data-Efficient Reinforcement Learning in Continuous State-Action Gaussian-POMDPs,"Rowan McAllister, Carl Edward Rasmussen",
neurips,https://proceedings.neurips.cc/paper/2017/file/5ef0b4eba35ab2d6180b0bca7e46b6f9-Paper.pdf,Coded Distributed Computing for Inverse Problems,"Yaoqing Yang, Pulkit Grover, Soummya Kar","Computationally intensive distributed and parallel computing is often bottlenecked by a small set of slow workers known as stragglers. In this paper, we utilize the emerging idea of
coded computation'' to design a novel error-correcting-code inspired technique for solving linear inverse problems under specific iterative methods in a parallelized implementation affected by stragglers. Example machine-learning applications include inverse problems such as personalized PageRank and sampling on graphs. We provably show that our coded-computation technique can reduce the mean-squared error under a computational deadline constraint. In fact, the ratio of mean-squared error of replication-based and coded techniques diverges to infinity as the deadline increases. Our experiments for personalized PageRank performed on real systems and real social networks show that this ratio can be as large as
10
4
10
. Further, unlike coded-computation techniques proposed thus far, our strategy combines outputs of all workers, including the stragglers, to produce more accurate estimates at the computational deadline. This also ensures that the accuracy degrades
gracefully'' in the event that the number of stragglers is large."
neurips,https://proceedings.neurips.cc/paper/2017/file/5ef698cd9fe650923ea331c15af3b160-Paper.pdf,"Dykstra's Algorithm, ADMM, and Coordinate Descent: Connections, Insights, and Extensions",Ryan J. Tibshirani,
neurips,https://proceedings.neurips.cc/paper/2017/file/5f14615696649541a025d3d0f8e0447f-Paper.pdf,Training recurrent networks to generate hypotheses about how the brain solves hard navigation problems,"Ingmar Kanitscheider, Ila Fiete",
neurips,https://proceedings.neurips.cc/paper/2017/file/6048ff4e8cb07aa60b6777b6f7384d52-Paper.pdf,SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted Cloud,"Zahra Ghodsi, Tianyu Gu, Siddharth Garg",
neurips,https://proceedings.neurips.cc/paper/2017/file/619205da514e83f869515c782a328d3c-Paper.pdf,Improved Graph Laplacian via Geometric Self-Consistency,"Dominique Joncas, Marina Meila, James McQueen",
neurips,https://proceedings.neurips.cc/paper/2017/file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf,Generalization Properties of Learning with Random Features,"Alessandro Rudi, Lorenzo Rosasco","We study the generalization properties of ridge regression with random features in the statistical learning framework. We show for the first time that
O
(
1
/
√
n
)
O
learning bounds can be achieved with only
O
(
√
n
log
n
)
O
random features rather than
O
(
n
)
O
as suggested by previous results. Further, we prove faster learning rates and show that they might require more random features, unless they are sampled according to a possibly problem dependent distribution. Our results shed light on the statistical computational trade-offs in large scale kernelized learning, showing the potential effectiveness of random features in reducing the computational complexity while keeping optimal generalization properties."
neurips,https://proceedings.neurips.cc/paper/2017/file/61b4a64be663682e8cb037d9719ad8cd-Paper.pdf,Predictive-State Decoders: Encoding the Future into Recurrent Networks,"Arun Venkatraman, Nicholas Rhinehart, Wen Sun, Lerrel Pinto, Martial Hebert, Byron Boots, Kris Kitani, J. Bagnell",
neurips,https://proceedings.neurips.cc/paper/2017/file/6211080fa89981f66b1a0c9d55c61d0f-Paper.pdf,Federated Multi-Task Learning,"Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, Ameet S. Talwalkar",
neurips,https://proceedings.neurips.cc/paper/2017/file/62889e73828c756c961c5a6d6c01a463-Paper.pdf,Learning Causal Structures Using Regression Invariance,"AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, Kun Zhang",
neurips,https://proceedings.neurips.cc/paper/2017/file/62dad6e273d32235ae02b7d321578ee8-Paper.pdf,Practical Hash Functions for Similarity Estimation and Dimensionality Reduction,"Søren Dahlgaard, Mathias Knudsen, Mikkel Thorup",
neurips,https://proceedings.neurips.cc/paper/2017/file/62f91ce9b820a491ee78c108636db089-Paper.pdf,Gaussian Quadrature for Kernel Features,"Tri Dao, Christopher M. De Sa, Christopher Ré","Kernel methods have recently attracted resurgent interest, showing performance competitive with deep neural networks in tasks such as speech recognition. The random Fourier features map is a technique commonly used to scale up kernel machines, but employing the randomized feature map means that
O
(
ϵ
−
2
)
O
samples are required to achieve an approximation error of at most
ϵ
ϵ
. We investigate some alternative schemes for constructing feature maps that are deterministic, rather than random, by approximating the kernel in the frequency domain using Gaussian quadrature. We show that deterministic feature maps can be constructed, for any
γ
>
0
γ
, to achieve error
ϵ
ϵ
with
O
(
e
e
γ
+
ϵ
−
1
/
γ
)
O
samples as
ϵ
ϵ
goes to 0. Our method works particularly well with sparse ANOVA kernels, which are inspired by the convolutional layer of CNNs. We validate our methods on datasets in different domains, such as MNIST and TIMIT, showing that deterministic features are faster to generate and achieve accuracy comparable to the state-of-the-art kernel methods based on random Fourier features."
neurips,https://proceedings.neurips.cc/paper/2017/file/632cee946db83e7a52ce5e8d6f0fed35-Paper.pdf,Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets,"Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gaurav Sukhatme, Joseph J. Lim",
neurips,https://proceedings.neurips.cc/paper/2017/file/63538fe6ef330c13a05a3ed7e599d5f7-Paper.pdf,Greedy Algorithms for Cone Constrained Optimization with Convergence Guarantees,"Francesco Locatello, Michael Tschannen, Gunnar Raetsch, Martin Jaggi",
neurips,https://proceedings.neurips.cc/paper/2017/file/635440afdfc39fe37995fed127d7df4f-Paper.pdf,On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel Methods and Neural Networks,"Arturs Backurs, Piotr Indyk, Ludwig Schmidt",
neurips,https://proceedings.neurips.cc/paper/2017/file/643de7cf7ba769c7466ccbc4adfd7fac-Paper.pdf,Acceleration and Averaging in Stochastic Descent Dynamics,"Walid Krichene, Peter L. Bartlett",
neurips,https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf,LightGBM: A Highly Efficient Gradient Boosting Decision Tree,"Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2017/file/6463c88460bd63bbe256e495c63aa40b-Paper.pdf,The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process,"Hongyuan Mei, Jason M. Eisner",
neurips,https://proceedings.neurips.cc/paper/2017/file/64a08e5f1e6c39faeb90108c430eb120-Paper.pdf,Bayesian Optimization with Gradients,"Jian Wu, Matthias Poloczek, Andrew G. Wilson, Peter Frazier",
neurips,https://proceedings.neurips.cc/paper/2017/file/654ad60ebd1ae29cedc37da04b6b0672-Paper.pdf,Visual Reference Resolution using Attention Memory for Visual Dialog,"Paul Hongsuck Seo, Andreas Lehrmann, Bohyung Han, Leonid Sigal",
neurips,https://proceedings.neurips.cc/paper/2017/file/663772ea088360f95bac3dc7ffb841be-Paper.pdf,Straggler Mitigation in Distributed Optimization Through Data Encoding,"Can Karakus, Yifan Sun, Suhas Diggavi, Wotao Yin",
neurips,https://proceedings.neurips.cc/paper/2017/file/6786f3c62fbf9021694f6e51cc07fe3c-Paper.pdf,Using Options and Covariance Testing for Long Horizon Off-Policy Policy Evaluation,"Zhaohan Guo, Philip S. Thomas, Emma Brunskill",
neurips,https://proceedings.neurips.cc/paper/2017/file/67c6a1e7ce56d3d6fa748ab6d9af3fd7-Paper.pdf,Attentional Pooling for Action Recognition,"Rohit Girdhar, Deva Ramanan",
neurips,https://proceedings.neurips.cc/paper/2017/file/67d16d00201083a2b118dd5128dd6f59-Paper.pdf,Testing and Learning on Distributions with Symmetric Noise Invariance,"Ho Chung Law, Christopher Yau, Dino Sejdinovic",
neurips,https://proceedings.neurips.cc/paper/2017/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf,Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results,"Antti Tarvainen, Harri Valpola",
neurips,https://proceedings.neurips.cc/paper/2017/file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,"Ryan Lowe, YI WU, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, Igor Mordatch",
neurips,https://proceedings.neurips.cc/paper/2017/file/69a5b5995110b36a9a347898d97a610e-Paper.pdf,Log-normality and Skewness of Estimated State/Action Values in Reinforcement Learning,"Liangpeng Zhang, Ke Tang, Xin Yao",
neurips,https://proceedings.neurips.cc/paper/2017/file/69d1fc78dbda242c43ad6590368912d4-Paper.pdf,Bayesian Compression for Deep Learning,"Christos Louizos, Karen Ullrich, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2017/file/69dafe8b58066478aea48f3d0f384820-Paper.pdf,Is Input Sparsity Time Possible for Kernel Low-Rank Approximation?,"Cameron Musco, David Woodruff","Low-rank approximation is a common tool used to accelerate kernel methods: the
n
×
n
n
kernel matrix
K
K
is approximated via a rank-
k
k
matrix
~
K
K
which can be stored in much less space and processed more quickly. In this work we study the limits of computationally efficient low-rank kernel approximation. We show that for a broad class of kernels, including the popular Gaussian and polynomial kernels, computing a relative error
k
k
-rank approximation to
K
K
is at least as difficult as multiplying the input data matrix
A
∈
R
n
×
d
A
by an arbitrary matrix
C
∈
R
d
×
k
C
. Barring a breakthrough in fast matrix multiplication, when
k
k
is not too large, this requires
Ω
(
n
n
z
(
A
)
k
)
Ω
time where
n
n
z
(
A
)
n
is the number of non-zeros in
A
A
. This lower bound matches, in many parameter regimes, recent work on subquadratic time algorithms for low-rank approximation of general kernels [MM16,MW17], demonstrating that these algorithms are unlikely to be significantly improved, in particular to
O
(
n
n
z
(
A
)
)
O
input sparsity runtimes. At the same time there is hope: we show for the first time that
O
(
n
n
z
(
A
)
)
O
time approximation is possible for general radial basis function kernels (e.g., the Gaussian kernel) for the closely related problem of low-rank approximation of the kernelized dataset."
neurips,https://proceedings.neurips.cc/paper/2017/file/6a2feef8ed6a9fe76d6b3f30f02150b4-Paper.pdf,Convergent Block Coordinate Descent for Training Tikhonov Regularized Deep Neural Networks,"Ziming Zhang, Matthew Brand",
neurips,https://proceedings.neurips.cc/paper/2017/file/6a508a60aa3bf9510ea6acb021c94b48-Paper.pdf,Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes,"Ahmed M. Alaa, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2017/file/6aca97005c68f1206823815f66102863-Paper.pdf,Learning Overcomplete HMMs,"Vatsal Sharan, Sham M. Kakade, Percy S. Liang, Gregory Valiant",
neurips,https://proceedings.neurips.cc/paper/2017/file/6ad4174eba19ecb5fed17411a34ff5e6-Paper.pdf,Convolutional Phase Retrieval,"Qing Qu, Yuqian Zhang, Yonina Eldar, John Wright","We study the convolutional phase retrieval problem, which asks us to recover an unknown signal
x
x
of length
n
n
from
m
m
measurements consisting of the magnitude of its cyclic convolution with a known kernel
a
a
of length
m
m
. This model is motivated by applications to channel estimation, optics, and underwater acoustic communication, where the signal of interest is acted on by a given channel/filter, and phase information is difficult or impossible to acquire. We show that when
a
a
is random and
m
≥
Ω
(
∥
C
x
∥
2
∥
x
∥
2
n
p
o
l
y
log
n
)
m
,
x
x
can be efficiently recovered up to a global phase using a combination of spectral initialization and generalized gradient descent. The main challenge is coping with dependencies in the measurement operator; we overcome this challenge by using ideas from decoupling theory, suprema of chaos processes and the restricted isometry property of random circulant matrices, and recent analysis for alternating minimizing methods."
neurips,https://proceedings.neurips.cc/paper/2017/file/6aed000af86a084f9cb0264161e29dd3-Paper.pdf,Stochastic and Adversarial Online Learning without Hyperparameters,"Ashok Cutkosky, Kwabena A. Boahen","Most online optimization algorithms focus on one of two things: performing well in adversarial settings by adapting to unknown data parameters (such as Lipschitz constants), typically achieving
O
(
√
T
)
O
regret, or performing well in stochastic settings where they can leverage some structure in the losses (such as strong convexity), typically achieving
O
(
log
(
T
)
)
O
regret. Algorithms that focus on the former problem hitherto achieved
O
(
√
T
)
O
in the stochastic setting rather than
O
(
log
(
T
)
)
O
. Here we introduce an online optimization algorithm that achieves
O
(
log
4
(
T
)
)
O
regret in a wide class of stochastic settings while gracefully degrading to the optimal
O
(
√
T
)
O
regret in adversarial settings (up to logarithmic factors). Our algorithm does not require any prior knowledge about the data or tuning of parameters to achieve superior performance."
neurips,https://proceedings.neurips.cc/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf,Masked Autoregressive Flow for Density Estimation,"George Papamakarios, Theo Pavlakou, Iain Murray",
neurips,https://proceedings.neurips.cc/paper/2017/file/6c340f25839e6acdc73414517203f5f0-Paper.pdf,QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding,"Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, Milan Vojnovic",
neurips,https://proceedings.neurips.cc/paper/2017/file/6c349155b122aa8ad5c877007e05f24f-Paper.pdf,Learning Hierarchical Information Flow with Recurrent Neural Modules,"Danijar Hafner, Alexander Irpan, James Davidson, Nicolas Heess",
neurips,https://proceedings.neurips.cc/paper/2017/file/6c3cf77d52820cd0fe646d38bc2145ca-Paper.pdf,Deanonymization in the Bitcoin P2P Network,"Giulia Fanti, Pramod Viswanath",
neurips,https://proceedings.neurips.cc/paper/2017/file/6c524f9d5d7027454a783c841250ba71-Paper.pdf,Learning with Average Top-k Loss,"Yanbo Fan, Siwei Lyu, Yiming Ying, Baogang Hu","In this work, we introduce the average top-
k
k
(\atk) loss as a new ensemble loss for supervised learning. The \atk loss provides a natural generalization of the two widely used ensemble losses, namely the average loss and the maximum loss. Furthermore, the \atk loss combines the advantages of them and can alleviate their corresponding drawbacks to better adapt to different data distributions. We show that the \atk loss affords an intuitive interpretation that reduces the penalty of continuous and convex individual losses on correctly classified data. The \atk loss can lead to convex optimization problems that can be solved effectively with conventional sub-gradient based method. We further study the Statistical Learning Theory of \matk by establishing its classification calibration and statistical consistency of \matk which provide useful insights on the practical choice of the parameter
k
k
. We demonstrate the applicability of \matk learning combined with different individual loss functions for binary and multi-class classification and regression using synthetic and real datasets."
neurips,https://proceedings.neurips.cc/paper/2017/file/6c9882bbac1c7093bd25041881277658-Paper.pdf,MaskRNN: Instance Level Video Object Segmentation,"Yuan-Ting Hu, Jia-Bin Huang, Alexander Schwing",
neurips,https://proceedings.neurips.cc/paper/2017/file/6d0f846348a856321729a2f36734d1a7-Paper.pdf,Max-Margin Invariant Features from Transformed Unlabelled Data,"Dipan Pal, Ashwin Kannan, Gautam Arakalgud, Marios Savvides",
neurips,https://proceedings.neurips.cc/paper/2017/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf,Sparse Approximate Conic Hulls,"Greg Van Buskirk, Benjamin Raichel, Nicholas Ruozzi",
neurips,https://proceedings.neurips.cc/paper/2017/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf,Label Distribution Learning Forests,"Wei Shen, KAI ZHAO, Yilu Guo, Alan L. Yuille",
neurips,https://proceedings.neurips.cc/paper/2017/file/6e5025ccc7d638ae4e724da8938450a6-Paper.pdf,Efficient Sublinear-Regret Algorithms for Online Sparse Linear Regression with Limited Observation,"Shinji Ito, Daisuke Hatano, Hanna Sumita, Akihiro Yabe, Takuro Fukunaga, Naonori Kakimura, Ken-Ichi Kawarabayashi","Online sparse linear regression is the task of applying linear regression analysis to examples arriving sequentially subject to a resource constraint that a limited number of features of examples can be observed. Despite its importance in many practical applications, it has been recently shown that there is no polynomial-time sublinear-regret algorithm unless NP
⊆
⊆
BPP, and only an exponential-time sublinear-regret algorithm has been found. In this paper, we introduce mild assumptions to solve the problem. Under these assumptions, we present polynomial-time sublinear-regret algorithms for the online sparse linear regression. In addition, thorough experiments with publicly available data demonstrate that our algorithms outperform other known algorithms."
neurips,https://proceedings.neurips.cc/paper/2017/file/6ef80bb237adf4b6f77d0700e1255907-Paper.pdf,Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds,"Yuanyuan Liu, Fanhua Shang, James Cheng, Hong Cheng, Licheng Jiao",
neurips,https://proceedings.neurips.cc/paper/2017/file/6f1d0705c91c2145201df18a1a0c7345-Paper.pdf,Hierarchical Implicit Models and Likelihood-Free Variational Inference,"Dustin Tran, Rajesh Ranganath, David Blei",
neurips,https://proceedings.neurips.cc/paper/2017/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf,Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding,"Mainak Jas, Tom Dupré la Tour, Umut Simsekli, Alexandre Gramfort","Neural time-series data contain a wide variety of prototypical signal waveforms (atoms) that are of significant importance in clinical and cognitive research. One of the goals for analyzing such data is hence to extract such `shift-invariant' atoms. Even though some success has been reported with existing algorithms, they are limited in applicability due to their heuristic nature. Moreover, they are often vulnerable to artifacts and impulsive noise, which are typically present in raw neural recordings. In this study, we address these issues and propose a novel probabilistic convolutional sparse coding (CSC) model for learning shift-invariant atoms from raw neural signals containing potentially severe artifacts. In the core of our model, which we call
α
α
CSC, lies a family of heavy-tailed distributions called
α
α
-stable distributions. We develop a novel, computationally efficient Monte Carlo expectation-maximization algorithm for inference. The maximization step boils down to a weighted CSC problem, for which we develop a computationally efficient optimization algorithm. Our results show that the proposed algorithm achieves state-of-the-art convergence speeds. Besides,
α
α
CSC is significantly more robust to artifacts when compared to three competing algorithms: it can extract spike bursts, oscillations, and even reveal more subtle phenomena such as cross-frequency coupling when applied to noisy neural time series."
neurips,https://proceedings.neurips.cc/paper/2017/file/6fab6e3aa34248ec1e34a4aeedecddc8-Paper.pdf,Modulating early visual processing by language,"Harm de Vries, Florian Strub, Jeremie Mary, Hugo Larochelle, Olivier Pietquin, Aaron C. Courville",
neurips,https://proceedings.neurips.cc/paper/2017/file/6fe131632103526e3a6e8114c78eb1e1-Paper.pdf,Discriminative State Space Models,"Vitaly Kuznetsov, Mehryar Mohri",
neurips,https://proceedings.neurips.cc/paper/2017/file/70222949cc0db89ab32c9969754d4758-Paper.pdf,Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols,"Serhii Havrylov, Ivan Titov",
neurips,https://proceedings.neurips.cc/paper/2017/file/70efdf2ec9b086079795c442636b55fb-Paper.pdf,"Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning","Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, David Barber",
neurips,https://proceedings.neurips.cc/paper/2017/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf,Online Influence Maximization under Independent Cascade Model with Semi-Bandit Feedback,"Zheng Wen, Branislav Kveton, Michal Valko, Sharan Vaswani",
neurips,https://proceedings.neurips.cc/paper/2017/file/71887f62f073a78511cbac56f8cab53f-Paper.pdf,Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization,"Ahmet Alacaoglu, Quoc Tran Dinh, Olivier Fercoq, Volkan Cevher",
neurips,https://proceedings.neurips.cc/paper/2017/file/71ad16ad2c4d81f348082ff6c4b20768-Paper.pdf,Linearly constrained Gaussian processes,"Carl Jidling, Niklas Wahlström, Adrian Wills, Thomas B. Schön",
neurips,https://proceedings.neurips.cc/paper/2017/file/72b386224056bf940cd5b01341f65e9d-Paper.pdf,Solid Harmonic Wavelet Scattering: Predicting Quantum Molecular Energy from Invariant Descriptors of 3D Electronic Densities,"Michael Eickenberg, Georgios Exarchakis, Matthew Hirn, Stephane Mallat",
neurips,https://proceedings.neurips.cc/paper/2017/file/7371364b3d72ac9a3ed8638e6f0be2c9-Paper.pdf,On Frank-Wolfe and Equilibrium Computation,"Jacob D. Abernethy, Jun-Kun Wang",
neurips,https://proceedings.neurips.cc/paper/2017/file/73e5080f0f3804cb9cf470a8ce895dac-Paper.pdf,Generalizing GANs: A Turing Perspective,"Roderich Gross, Yue Gu, Wei Li, Melvin Gauci",
neurips,https://proceedings.neurips.cc/paper/2017/file/73fed7fd472e502d8908794430511f4d-Paper.pdf,Predicting Scene Parsing and Motion Dynamics in the Future,"Xiaojie Jin, Huaxin Xiao, Xiaohui Shen, Jimei Yang, Zhe Lin, Yunpeng Chen, Zequn Jie, Jiashi Feng, Shuicheng Yan",
neurips,https://proceedings.neurips.cc/paper/2017/file/74071a673307ca7459bcf75fbd024e09-Paper.pdf,A Screening Rule for l1-Regularized Ising Model Estimation,"Zhaobin Kuang, Sinong Geng, David Page",
neurips,https://proceedings.neurips.cc/paper/2017/file/743394beff4b1282ba735e5e3723ed74-Paper.pdf,A Minimax Optimal Algorithm for Crowdsourcing,"Thomas Bonald, Richard Combes",
neurips,https://proceedings.neurips.cc/paper/2017/file/7486cef2522ee03547cfb970a404a874-Paper.pdf,Communication-Efficient Distributed Learning of Discrete Distributions,"Ilias Diakonikolas, Elena Grigorescu, Jerry Li, Abhiram Natarajan, Krzysztof Onak, Ludwig Schmidt",
neurips,https://proceedings.neurips.cc/paper/2017/file/748ba69d3e8d1af87f84fee909eef339-Paper.pdf,VAIN: Attentional Multi-agent Predictive Modeling,Yedid Hoshen,
neurips,https://proceedings.neurips.cc/paper/2017/file/752d25a1f8dbfb2d656bac3094bfb81c-Paper.pdf,Hierarchical Attentive Recurrent Tracking,"Adam Kosiorek, Alex Bewley, Ingmar Posner",
neurips,https://proceedings.neurips.cc/paper/2017/file/758a06618c69880a6cee5314ee42d52f-Paper.pdf,Sobolev Training for Neural Networks,"Wojciech M. Czarnecki, Simon Osindero, Max Jaderberg, Grzegorz Swirszcz, Razvan Pascanu",
neurips,https://proceedings.neurips.cc/paper/2017/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf,Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized Empirical Risk Minimization,"Tomoya Murata, Taiji Suzuki","We develop a new accelerated stochastic gradient method for efficiently solving the convex regularized empirical risk minimization problem in mini-batch settings. The use of mini-batches has become a golden standard in the machine learning community, because the mini-batch techniques stabilize the gradient estimate and can easily make good use of parallel computing. The core of our proposed method is the incorporation of our new
double acceleration'' technique and variance reduction technique. We theoretically analyze our proposed method and show that our method much improves the mini-batch efficiencies of previous accelerated stochastic methods, and essentially only needs size
√
n
n
mini-batches for achieving the optimal iteration complexities for both non-strongly and strongly convex objectives, where
n
n
is the training set size. Further, we show that even in non-mini-batch settings, our method achieves the best known convergence rate for non-strongly convex and strongly convex objectives."
neurips,https://proceedings.neurips.cc/paper/2017/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf,Learning with Feature Evolvable Streams,"Bo-Jian Hou, Lijun Zhang, Zhi-Hua Zhou",
neurips,https://proceedings.neurips.cc/paper/2017/file/766ebcd59621e305170616ba3d3dac32-Paper.pdf,Safe Model-based Reinforcement Learning with Stability Guarantees,"Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2017/file/769675d7c11f336ae6573e7e533570ec-Paper.pdf,"Time-dependent spatially varying graphical models, with application to brain fMRI data analysis","Kristjan Greenewald, Seyoung Park, Shuheng Zhou, Alexander Giessing",
neurips,https://proceedings.neurips.cc/paper/2017/file/7876acb66640bad41f1e1371ef30c180-Paper.pdf,Clone MCMC: Parallel High-Dimensional Gaussian Gibbs Sampling,"Andrei-Cristian Barbos, Francois Caron, Jean-François Giovannelli, Arnaud Doucet",
neurips,https://proceedings.neurips.cc/paper/2017/file/7884a9652e94555c70f96b6be63be216-Paper.pdf,Context Selection for Embedding Models,"Liping Liu, Francisco Ruiz, Susan Athey, David Blei",
neurips,https://proceedings.neurips.cc/paper/2017/file/788d986905533aba051261497ecffcbb-Paper.pdf,Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction,"Kristofer Bouchard, Alejandro Bujan, Fred Roosta, Shashanka Ubaru, Mr. Prabhat, Antoine Snijders, Jian-Hua Mao, Edward Chang, Michael W. Mahoney, Sharmodeep Bhattacharya","The increasing size and complexity of scientific data could dramatically enhance discovery and prediction for basic scientific applications, e.g., neuroscience, genetics, systems biology, etc. Realizing this potential, however, requires novel statistical analysis methods that are both interpretable and predictive. We introduce the Union of Intersections (UoI) method, a flexible, modular, and scalable framework for enhanced model selection and estimation. The method performs model selection and model estimation through intersection and union operations, respectively. We show that UoI can satisfy the bi-criteria of low-variance and nearly unbiased estimation of a small number of interpretable features, while maintaining high-quality prediction accuracy. We perform extensive numerical investigation to evaluate a UoI algorithm (
U
o
I
L
a
s
s
o
U
) on synthetic and real data. In doing so, we demonstrate the extraction of interpretable functional networks from human electrophysiology recordings as well as the accurate prediction of phenotypes from genotype-phenotype data with reduced features. We also show (with the
U
o
I
L
1
L
o
g
i
s
t
i
c
U
and
U
o
I
C
U
R
U
variants of the basic framework) improved prediction parsimony for classification and matrix factorization on several benchmark biomedical data sets. These results suggest that methods based on UoI framework could improve interpretation and prediction in data-driven discovery across scientific fields."
neurips,https://proceedings.neurips.cc/paper/2017/file/79514e888b8f2acacc68738d0cbb803e-Paper.pdf,Good Semi-supervised Learning That Requires a Bad GAN,"Zihang Dai, Zhilin Yang, Fan Yang, William W. Cohen, Russ R. Salakhutdinov",
neurips,https://proceedings.neurips.cc/paper/2017/file/7993e11204b215b27694b6f139e34ce8-Paper.pdf,Targeting EEG/LFP Synchrony with Neural Nets,"Yitong Li, michael Murias, samantha Major, geraldine Dawson, Kafui Dzirasa, Lawrence Carin, David E. Carlson",
neurips,https://proceedings.neurips.cc/paper/2017/file/7a006957be65e608e863301eb98e1808-Paper.pdf,Learning from uncertain curves: The 2-Wasserstein metric for Gaussian processes,"Anton Mallasto, Aasa Feragen",
neurips,https://proceedings.neurips.cc/paper/2017/file/7a6a74cbe87bc60030a4bd041dd47b78-Paper.pdf,Online Dynamic Programming,"Holakou Rahmanian, Manfred K. K. Warmuth",
neurips,https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf,Neural Discrete Representation Learning,"Aaron van den Oord, Oriol Vinyals, koray kavukcuoglu",
neurips,https://proceedings.neurips.cc/paper/2017/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf,Probabilistic Rule Realization and Selection,"Haizi Yu, Tianxi Li, Lav R. Varshney",
neurips,https://proceedings.neurips.cc/paper/2017/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf,A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning,"Marco Fraccaro, Simon Kamronn, Ulrich Paquet, Ole Winther",
neurips,https://proceedings.neurips.cc/paper/2017/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf,Stabilizing Training of Generative Adversarial Networks through Regularization,"Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, Thomas Hofmann",
neurips,https://proceedings.neurips.cc/paper/2017/file/7c82fab8c8f89124e2ce92984e04fb40-Paper.pdf,Training Deep Networks without Learning Rates Through Coin Betting,"Francesco Orabona, Tatiana Tommasi",
neurips,https://proceedings.neurips.cc/paper/2017/file/7cbbc409ec990f19c78c75bd1e06f215-Paper.pdf,Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis,"Jian Zhao, Lin Xiong, Panasonic Karlekar Jayashree, Jianshu Li, Fang Zhao, Zhecan Wang, Panasonic Sugiri Pranata, Panasonic Shengmei Shen, Shuicheng Yan, Jiashi Feng",
neurips,https://proceedings.neurips.cc/paper/2017/file/7cc234202e98d2722580858573fd0817-Paper.pdf,Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation,"Christian Borgs, Jennifer Chayes, Christina E. Lee, Devavrat Shah","The sparse matrix estimation problem consists of estimating the distribution of an
n
×
n
n
matrix
Y
Y
, from a sparsely observed single instance of this matrix where the entries of
Y
Y
are independent random variables. This captures a wide array of problems; special instances include matrix completion in the context of recommendation systems, graphon estimation, and community detection in (mixed membership) stochastic block models. Inspired by classical collaborative filtering for recommendation systems, we propose a novel iterative, collaborative filtering-style algorithm for matrix estimation in this generic setting. We show that the mean squared error (MSE) of our estimator converges to
0
0
at the rate of
O
(
d
2
(
p
n
)
−
2
/
5
)
O
as long as
ω
(
d
5
n
)
ω
random entries from a total of
n
2
n
entries of
Y
Y
are observed (uniformly sampled),
\E
[
Y
]
\E
has rank
d
d
, and the entries of
Y
Y
have bounded support. The maximum squared error across all entries converges to
0
0
with high probability as long as we observe a little more,
Ω
(
d
5
n
ln
5
(
n
)
)
Ω
entries. Our results are the best known sample complexity results in this generality."
neurips,https://proceedings.neurips.cc/paper/2017/file/7cce53cf90577442771720a370c3c723-Paper.pdf,Positive-Unlabeled Learning with Non-Negative Risk Estimator,"Ryuichi Kiryo, Gang Niu, Marthinus C. du Plessis, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2017/file/7e0a0209b929d097bd3e8ef30567a5c1-Paper.pdf,Gradient descent GAN optimization is locally stable,"Vaishnavh Nagarajan, J. Zico Kolter",
neurips,https://proceedings.neurips.cc/paper/2017/file/7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Paper.pdf,Faster and Non-ergodic O(1/K) Stochastic Alternating Direction Method of Multipliers,"Cong Fang, Feng Cheng, Zhouchen Lin",
neurips,https://proceedings.neurips.cc/paper/2017/file/7e7757b1e12abcb736ab9a754ffb617a-Paper.pdf,Group Sparse Additive Machine,"Hong Chen, Xiaoqian Wang, Cheng Deng, Heng Huang",
neurips,https://proceedings.neurips.cc/paper/2017/file/7e7e69ea3384874304911625ac34321c-Paper.pdf,PixelGAN Autoencoders,"Alireza Makhzani, Brendan J. Frey",
neurips,https://proceedings.neurips.cc/paper/2017/file/7edccc661418aeb5761dbcdc06ad490c-Paper.pdf,Excess Risk Bounds for the Bayes Risk using Variational Inference in Latent Gaussian Models,"Rishit Sheth, Roni Khardon",
neurips,https://proceedings.neurips.cc/paper/2017/file/7f018eb7b301a66658931cb8a93fd6e8-Paper.pdf,Online control of the false discovery rate with decaying memory,"Aaditya Ramdas, Fanny Yang, Martin J. Wainwright, Michael I. Jordan","In the online multiple testing problem, p-values corresponding to different null hypotheses are presented one by one, and the decision of whether to reject a hypothesis must be made immediately, after which the next p-value is presented. Alpha-investing algorithms to control the false discovery rate were first formulated by Foster and Stine and have since been generalized and applied to various settings, varying from quality-preserving databases for science to multiple A/B tests for internet commerce. This paper improves the class of generalized alpha-investing algorithms (GAI) in four ways : (a) we show how to uniformly improve the power of the entire class of GAI procedures under independence by awarding more alpha-wealth for each rejection, giving a near win-win resolution to a dilemma raised by Javanmard and Montanari, (b) we demonstrate how to incorporate prior weights to indicate domain knowledge of which hypotheses are likely to be null or non-null, (c) we allow for differing penalties for false discoveries to indicate that some hypotheses may be more meaningful/important than others, (d) we define a new quantity called the \emph{decaying memory false discovery rate, or
\memfdr
\memfdr
} that may be more meaningful for applications with an explicit time component, using a discount factor to incrementally forget past decisions and alleviate some potential problems that we describe and name
piggybacking'' and
alpha-death''. Our GAI++ algorithms incorporate all four generalizations (a, b, c, d) simulatenously, and reduce to more powerful variants of earlier algorithms when the weights and decay are all set to unity."
neurips,https://proceedings.neurips.cc/paper/2017/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf,Safe and Nested Subgame Solving for Imperfect-Information Games,"Noam Brown, Tuomas Sandholm",
neurips,https://proceedings.neurips.cc/paper/2017/file/7fea637fd6d02b8f0adf6f7dc36aed93-Paper.pdf,A PAC-Bayesian Analysis of Randomized Learning with Application to Stochastic Gradient Descent,Ben London,
neurips,https://proceedings.neurips.cc/paper/2017/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf,Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning,"El Mahdi El Mhamdi, Rachid Guerraoui, Hadrien Hendrikx, Alexandre Maurer",
neurips,https://proceedings.neurips.cc/paper/2017/file/819f46e52c25763a55cc642422644317-Paper.pdf,Toward Multimodal Image-to-Image Translation,"Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman",
neurips,https://proceedings.neurips.cc/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Paper.pdf,The Marginal Value of Adaptive Gradient Methods in Machine Learning,"Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, Benjamin Recht",
neurips,https://proceedings.neurips.cc/paper/2017/file/81c650caac28cdefce4de5ddc18befa0-Paper.pdf,Mean Field Residual Networks: On the Edge of Chaos,"Ge Yang, Samuel Schoenholz",
neurips,https://proceedings.neurips.cc/paper/2017/file/81ca0262c82e712e50c580c032d99b60-Paper.pdf,Non-convex Finite-Sum Optimization Via SCSG Methods,"Lihua Lei, Cheng Ju, Jianbo Chen, Michael I. Jordan","We develop a class of algorithms, as variants of the stochastically controlled stochastic gradient (SCSG) methods , for the smooth nonconvex finite-sum optimization problem. Only assuming the smoothness of each component, the complexity of SCSG to reach a stationary point with
E
∥
∇
f
(
x
)
∥
2
≤
ϵ
E
is
O
(
min
{
ϵ
−
5
/
3
,
ϵ
−
1
n
2
/
3
}
)
O
, which strictly outperforms the stochastic gradient descent. Moreover, SCSG is never worse than the state-of-the-art methods based on variance reduction and it significantly outperforms them when the target accuracy is low. A similar acceleration is also achieved when the functions satisfy the Polyak-Lojasiewicz condition. Empirical experiments demonstrate that SCSG outperforms stochastic gradient methods on training multi-layers neural networks in terms of both training and validation loss."
neurips,https://proceedings.neurips.cc/paper/2017/file/81e5f81db77c596492e6f1a5a792ed53-Paper.pdf,First-Order Adaptive Sample Size Methods to Reduce Complexity of Empirical Risk Minimization,"Aryan Mokhtari, Alejandro Ribeiro",
neurips,https://proceedings.neurips.cc/paper/2017/file/8208974663db80265e9bfe7b222dcb18-Paper.pdf,Doubly Stochastic Variational Inference for Deep Gaussian Processes,"Hugh Salimbeni, Marc Deisenroth",
neurips,https://proceedings.neurips.cc/paper/2017/file/82161242827b703e6acf9c726942a1e4-Paper.pdf,From Parity to Preference-based Notions of Fairness in Classification,"Muhammad Bilal Zafar, Isabel Valera, Manuel Rodriguez, Krishna Gummadi, Adrian Weller",
neurips,https://proceedings.neurips.cc/paper/2017/file/821fa74b50ba3f7cba1e6c53e8fa6845-Paper.pdf,Nonparametric Online Regression while Learning the Metric,"Ilja Kuzborskij, Nicolò Cesa-Bianchi","We study algorithms for online nonparametric regression that learn the directions along which the regression function is smoother. Our algorithm learns the Mahalanobis metric based on the gradient outer product matrix
G
G
of the regression function (automatically adapting to the effective rank of this matrix), while simultaneously bounding the regret ---on the same data sequence--- in terms of the spectrum of
G
G
. As a preliminary step in our analysis, we extend a nonparametric online learning algorithm by Hazan and Megiddo enabling it to compete against functions whose Lipschitzness is measured with respect to an arbitrary Mahalanobis metric."
neurips,https://proceedings.neurips.cc/paper/2017/file/82b8a3434904411a9fdc43ca87cee70c-Paper.pdf,Stochastic Optimization with Variance Reduction for Infinite Datasets with Finite Sum Structure,"Alberto Bietti, Julien Mairal",
neurips,https://proceedings.neurips.cc/paper/2017/file/831caa1b600f852b7844499430ecac17-Paper.pdf,Working hard to know your neighbor's margins: Local descriptor learning loss,"Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic, Jiri Matas",
neurips,https://proceedings.neurips.cc/paper/2017/file/838e8afb1ca34354ac209f53d90c3a43-Paper.pdf,Hiding Images in Plain Sight: Deep Steganography,Shumeet Baluja,
neurips,https://proceedings.neurips.cc/paper/2017/file/83f97f4825290be4cb794ec6a234595f-Paper.pdf,Lookahead Bayesian Optimization with Inequality Constraints,"Remi Lam, Karen Willcox",
neurips,https://proceedings.neurips.cc/paper/2017/file/8420d359404024567b5aefda1231af24-Paper.pdf,Online Learning with Transductive Regret,"Mehryar Mohri, Scott Yang",
neurips,https://proceedings.neurips.cc/paper/2017/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf,Pixels to Graphs by Associative Embedding,"Alejandro Newell, Jia Deng",
neurips,https://proceedings.neurips.cc/paper/2017/file/84b20b1f5a0d103f5710bb67a043cd78-Paper.pdf,Accelerated Stochastic Greedy Coordinate Descent by Soft Thresholding Projection onto Simplex,"Chaobing Song, Shaobo Cui, Yong Jiang, Shu-Tao Xia","In this paper we study the well-known greedy coordinate descent (GCD) algorithm to solve
ℓ
1
ℓ
-regularized problems and improve GCD by the two popular strategies: Nesterov's acceleration and stochastic optimization. Firstly, we propose a new rule for greedy selection based on an
ℓ
1
ℓ
-norm square approximation which is nontrivial to solve but convex; then an efficient algorithm called
SOft ThreshOlding PrOjection (SOTOPO)'' is proposed to exactly solve the
ℓ
1
ℓ
-regularized
ℓ
1
ℓ
-norm square approximation problem, which is induced by the new rule. Based on the new rule and the SOTOPO algorithm, the Nesterov's acceleration and stochastic optimization strategies are then successfully applied to the GCD algorithm. The resulted algorithm called accelerated stochastic greedy coordinate descent (ASGCD) has the optimal convergence rate
O
(
√
1
/
ϵ
)
O
; meanwhile, it reduces the iteration complexity of greedy selection up to a factor of sample size. Both theoretically and empirically, we show that ASGCD has better performance for high-dimensional and dense problems with sparse solution."
neurips,https://proceedings.neurips.cc/paper/2017/file/84c6494d30851c63a55cdb8cb047fadd-Paper.pdf,Reinforcement Learning under Model Mismatch,"Aurko Roy, Huan Xu, Sebastian Pokutta",
neurips,https://proceedings.neurips.cc/paper/2017/file/84ddfb34126fc3a48ee38d7044e87276-Paper.pdf,Concrete Dropout,"Yarin Gal, Jiri Hron, Alex Kendall",
neurips,https://proceedings.neurips.cc/paper/2017/file/850af92f8d9903e7a4e0559a98ecc857-Paper.pdf,Multiresolution Kernel Approximation for Gaussian Process Regression,"Yi Ding, Risi Kondor, Jonathan Eskreis-Winkler","Gaussian process regression generally does not scale to beyond a few thousands data points without applying some sort of kernel approximation method. Most approximations focus on the high eigenvalue part of the spectrum of the kernel matrix,
K
K
, which leads to bad performance when the length scale of the kernel is small. In this paper we introduce Multiresolution Kernel Approximation (MKA), the first true broad bandwidth kernel approximation algorithm. Important points about MKA are that it is memory efficient, and it is a direct method, which means that it also makes it easy to approximate
K
−
1
K
and
det
(
K
)
det
."
neurips,https://proceedings.neurips.cc/paper/2017/file/851300ee84c2b80ed40f51ed26d866fc-Paper.pdf,Near Minimax Optimal Players for the Finite-Time 3-Expert Prediction Problem,"Yasin Abbasi Yadkori, Peter L. Bartlett, Victor Gabillon","We study minimax strategies for the online prediction problem with expert advice. It has been conjectured that a simple adversary strategy, called COMB, is near optimal in this game for any number of experts. Our results and new insights make progress in this direction by showing that, up to a small additive term, COMB is minimax optimal in the finite-time three expert problem. In addition, we provide for this setting a new near minimax optimal COMB-based learner. Prior to this work, in this problem, learners obtaining the optimal multiplicative constant in their regret rate were known only when
K
=
2
K
or
K
→
∞
K
. We characterize, when
K
=
3
K
, the regret of the game scaling as
√
8
/
(
9
π
)
T
±
log
(
T
)
2
8
which gives for the first time the optimal constant in the leading (
√
T
T
) term of the regret."
neurips,https://proceedings.neurips.cc/paper/2017/file/8597a6cfa74defcbde3047c891d78f90-Paper.pdf,Learned D-AMP: Principled Neural Network based Compressive Image Recovery,"Chris Metzler, Ali Mousavi, Richard Baraniuk","Compressive image recovery is a challenging problem that requires fast and accurate algorithms. Recently, neural networks have been applied to this problem with promising results. By exploiting massively parallel GPU processing architectures and oodles of training data, they can run orders of magnitude faster than existing techniques. However, these methods are largely unprincipled black boxes that are difficult to train and often-times specific to a single measurement matrix. It was recently demonstrated that iterative sparse-signal-recovery algorithms can be
unrolled’' to form interpretable deep networks. Taking inspiration from this work, we develop a novel neural network architecture that mimics the behavior of the denoising-based approximate message passing (D-AMP) algorithm. We call this new network {\em Learned} D-AMP (LDAMP). The LDAMP network is easy to train, can be applied to a variety of different measurement matrices, and comes with a state-evolution heuristic that accurately predicts its performance. Most importantly, it outperforms the state-of-the-art BM3D-AMP and NLR-CS algorithms in terms of both accuracy and run time. At high resolutions, and when used with sensing matrices that have fast implementations, LDAMP runs over
50
×
50
faster than BM3D-AMP and hundreds of times faster than NLR-CS."
neurips,https://proceedings.neurips.cc/paper/2017/file/861dc9bd7f4e7dd3cccd534d0ae2a2e9-Paper.pdf,Deep Multi-task Gaussian Processes for Survival Analysis with Competing Risks,,
neurips,https://proceedings.neurips.cc/paper/2017/file/86a1793f65aeef4aeef4b479fc9b2bca-Paper.pdf,Unsupervised Transformation Learning via Convex Relaxations,"Tatsunori B. Hashimoto, Percy S. Liang, John C. Duchi",
neurips,https://proceedings.neurips.cc/paper/2017/file/86b122d4358357d834a87ce618a55de0-Paper.pdf,Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations,"Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, Luc V. Gool",
neurips,https://proceedings.neurips.cc/paper/2017/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf,Accuracy First: Selecting a Differential Privacy Level for Accuracy Constrained ERM,"Katrina Ligett, Seth Neel, Aaron Roth, Bo Waggoner, Steven Z. Wu",
neurips,https://proceedings.neurips.cc/paper/2017/file/86e78499eeb33fb9cac16b7555b50767-Paper.pdf,Triple Generative Adversarial Nets,"Chongxuan LI, Taufik Xu, Jun Zhu, Bo Zhang",
neurips,https://proceedings.neurips.cc/paper/2017/file/883e881bb4d22a7add958f2d6b052c9f-Paper.pdf,Deep Learning with Topological Signatures,"Christoph Hofer, Roland Kwitt, Marc Niethammer, Andreas Uhl",
neurips,https://proceedings.neurips.cc/paper/2017/file/884d79963bd8bc0ae9b13a1aa71add73-Paper.pdf,Revenue Optimization with Approximate Bid Predictions,"Andres Munoz, Sergei Vassilvitskii",
neurips,https://proceedings.neurips.cc/paper/2017/file/8929c70f8d710e412d38da624b21c3c8-Paper.pdf,Mapping distinct timescales of functional interactions among brain networks,"Mali Sundaresan, Arshed Nabeel, Devarajan Sridharan",
neurips,https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf,Improved Training of Wasserstein GANs,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron C. Courville",
neurips,https://proceedings.neurips.cc/paper/2017/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf,Adaptive stimulus selection for optimizing neural population responses,"Benjamin Cowley, Ryan Williamson, Katerina Clemens, Matthew Smith, Byron M. Yu",
neurips,https://proceedings.neurips.cc/paper/2017/file/89d4402dc03d3b7318bbac10203034ab-Paper.pdf,Matrix Norm Estimation from a Few Entries,"Ashish Khetan, Sewoong Oh","Singular values of a data in a matrix form provide insights on the structure of the data, the effective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative filtering and network analysis, we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering various spectral properties of the underlying matrix from a sampling of its entries. We propose a framework of first estimating the Schatten
k
k
-norms of a matrix for several values of
k
k
, and using these as surrogates for estimating spectral properties of interest, such as the spectrum itself or the rank. This paper focuses on the technical challenges in accurately estimating the Schatten norms from a sampling of a matrix. We introduce a novel unbiased estimator based on counting small structures in a graph and provide guarantees that match its empirical performances. Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we significantly improve upon a competing approach of using matrix completion methods."
neurips,https://proceedings.neurips.cc/paper/2017/file/89f0fd5c927d466d6ec9a21b9ac34ffa-Paper.pdf,On the Power of Truncated SVD for General High-rank Matrix Estimation Problems,"Simon S. Du, Yining Wang, Aarti Singh","We show that given an estimate
ˆ
\mat
A
\mat
that is close to a general high-rank positive semi-definite (PSD) matrix
\mat
A
\mat
in spectral norm (i.e.,
∥
ˆ
\mat
A
−
\mat
A
∥
2
≤
δ
‖
), the simple truncated Singular Value Decomposition of
ˆ
\mat
A
\mat
produces a multiplicative approximation of
\mat
A
\mat
in Frobenius norm. This observation leads to many interesting results on general high-rank matrix estimation problems: 1.High-rank matrix completion: we show that it is possible to recover a {general high-rank matrix}
\mat
A
\mat
up to
(
1
+
ε
)
(
relative error in Frobenius norm from partial observations, with sample complexity independent of the spectral gap of
\mat
A
\mat
. 2.High-rank matrix denoising: we design algorithms that recovers a matrix
\mat
A
\mat
with relative error in Frobenius norm from its noise-perturbed observations, without assuming
\mat
A
\mat
is exactly low-rank. 3.Low-dimensional estimation of high-dimensional covariance: given
N
N
i.i.d.~samples of dimension
n
n
from
N
n
(
\mat
0
,
\mat
A
)
N
, we show that it is possible to estimate the covariance matrix
\mat
A
\mat
with relative error in Frobenius norm with
N
≈
n
N
,improving over classical covariance estimation results which requires
N
≈
n
2
N
."
neurips,https://proceedings.neurips.cc/paper/2017/file/89fcd07f20b6785b92134bd6c1d0fa42-Paper.pdf,TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning,"Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li",
neurips,https://proceedings.neurips.cc/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter",
neurips,https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf,A Unified Approach to Interpreting Model Predictions,"Scott M. Lundberg, Su-In Lee",
neurips,https://proceedings.neurips.cc/paper/2017/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf,Nonbacktracking Bounds on the Influence in Independent Cascade Models,"Emmanuel Abbe, Sanjeev Kulkarni, Eun Jee Lee",
neurips,https://proceedings.neurips.cc/paper/2017/file/8b8388180314a337c9aa3c5aa8e2f37a-Paper.pdf,Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls,"Zeyuan Allen-Zhu, Elad Hazan, Wei Hu, Yuanzhi Li",
neurips,https://proceedings.neurips.cc/paper/2017/file/8bb88f80d334b1869781beb89f7b73be-Paper.pdf,Fully Decentralized Policies for Multi-Agent Systems: An Information Theoretic Approach,"Roel Dobbe, David Fridovich-Keil, Claire Tomlin",
neurips,https://proceedings.neurips.cc/paper/2017/file/8c249675aea6c3cbd91661bbae767ff1-Paper.pdf,Neural system identification for large populations separating “what” and “where”,"David Klindt, Alexander S. Ecker, Thomas Euler, Matthias Bethge",
neurips,https://proceedings.neurips.cc/paper/2017/file/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Paper.pdf,Learning Active Learning from Data,"Ksenia Konyushkova, Raphael Sznitman, Pascal Fua",
neurips,https://proceedings.neurips.cc/paper/2017/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Paper.pdf,Controllable Invariance through Adversarial Feature Learning,"Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, Graham Neubig",
neurips,https://proceedings.neurips.cc/paper/2017/file/8cbd005a556ccd4211ce43f309bc0eac-Paper.pdf,Visual Interaction Networks: Learning a Physics Simulator from Video,"Nicholas Watters, Daniel Zoran, Theophane Weber, Peter Battaglia, Razvan Pascanu, Andrea Tacchetti",
neurips,https://proceedings.neurips.cc/paper/2017/file/8ce6790cc6a94e65f17f908f462fae85-Paper.pdf,Repeated Inverse Reinforcement Learning,"Kareem Amin, Nan Jiang, Satinder Singh",
neurips,https://proceedings.neurips.cc/paper/2017/file/8d3bba7425e7c98c50f52ca1b52d3735-Paper.pdf,Inference in Graphical Models via Semidefinite Programming Hierarchies,"Murat A. Erdogdu, Yash Deshpande, Andrea Montanari","Maximum A posteriori Probability (MAP) inference in graphical models amounts to solving a graph-structured combinatorial optimization problem. Popular inference algorithms such as belief propagation (BP) and generalized belief propagation (GBP) are intimately related to linear programming (LP) relaxation within the Sherali-Adams hierarchy. Despite the popularity of these algorithms, it is well understood that the Sum-of-Squares (SOS) hierarchy based on semidefinite programming (SDP) can provide superior guarantees. Unfortunately, SOS relaxations for a graph with
n
n
vertices require solving an SDP with
n
Θ
(
d
)
n
variables where
d
d
is the degree in the hierarchy. In practice, for
d
≥
4
d
, this approach does not scale beyond a few tens of variables. In this paper, we propose binary SDP relaxations for MAP inference using the SOS hierarchy with two innovations focused on computational efficiency. Firstly, in analogy to BP and its variants, we only introduce decision variables corresponding to contiguous regions in the graphical model. Secondly, we solve the resulting SDP using a non-convex Burer-Monteiro style method, and develop a sequential rounding procedure. We demonstrate that the resulting algorithm can solve problems with tens of thousands of variables within minutes, and outperforms BP and GBP on practical problems such as image denoising and Ising spin glasses. Finally, for specific graph types, we establish a sufficient condition for the tightness of the proposed partial SOS relaxation."
neurips,https://proceedings.neurips.cc/paper/2017/file/8d420fa35754d1f1c19969c88780314d-Paper.pdf,Gauging Variational Inference,"Sung-Soo Ahn, Michael Chertkov, Jinwoo Shin",
neurips,https://proceedings.neurips.cc/paper/2017/file/8e68c3c7bf14ad0bcaba52babfa470bd-Paper.pdf,Teaching Machines to Describe Images with Natural Language Feedback,"huan ling, Sanja Fidler",
neurips,https://proceedings.neurips.cc/paper/2017/file/8edd72158ccd2a879f79cb2538568fdc-Paper.pdf,Associative Embedding: End-to-End Learning for Joint Detection and Grouping,"Alejandro Newell, Zhiao Huang, Jia Deng",
neurips,https://proceedings.neurips.cc/paper/2017/file/8fb5f8be2aa9d6c64a04e3ab9f63feee-Paper.pdf,"Information Theoretic Properties of Markov Random Fields, and their Algorithmic Applications","Linus Hamilton, Frederic Koehler, Ankur Moitra","Markov random fields are a popular model for high-dimensional probability distributions. Over the years, many mathematical, statistical and algorithmic problems on them have been studied. Until recently, the only known algorithms for provably learning them relied on exhaustive search, correlation decay or various incoherence assumptions. Bresler gave an algorithm for learning general Ising models on bounded degree graphs. His approach was based on a structural result about mutual information in Ising models. Here we take a more conceptual approach to proving lower bounds on the mutual information. Our proof generalizes well beyond Ising models, to arbitrary Markov random fields with higher order interactions. As an application, we obtain algorithms for learning Markov random fields on bounded degree graphs on
n
n
nodes with
r
r
-order interactions in
n
r
n
time and
log
n
log
sample complexity. Our algorithms also extend to various partial observation models."
neurips,https://proceedings.neurips.cc/paper/2017/file/8fecb20817b3847419bb3de39a609afe-Paper.pdf,Subset Selection and Summarization in Sequential Data,"Ehsan Elhamifar, M. Clara De Paolis Kaluza",
neurips,https://proceedings.neurips.cc/paper/2017/file/900c563bfd2c48c16701acca83ad858a-Paper.pdf,Z-Forcing: Training Stochastic Recurrent Networks,"Anirudh Goyal ALIAS PARTH GOYAL, Alessandro Sordoni, Marc-Alexandre Côté, Nan Rosemary Ke, Yoshua Bengio",
neurips,https://proceedings.neurips.cc/paper/2017/file/90599c8fdd2f6e7a03ad173e2f535751-Paper.pdf,Regret Minimization in MDPs with Options without Prior Knowledge,"Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, Emma Brunskill",
neurips,https://proceedings.neurips.cc/paper/2017/file/907edb0aa6986220dbffb79a788596ee-Paper.pdf,Learning Identifiable Gaussian Bayesian Networks in Polynomial Time and Sample Complexity,"Asish Ghoshal, Jean Honorio","Learning the directed acyclic graph (DAG) structure of a Bayesian network from observational data is a notoriously difficult problem for which many non-identifiability and hardness results are known. In this paper we propose a provably polynomial-time algorithm for learning sparse Gaussian Bayesian networks with equal noise variance --- a class of Bayesian networks for which the DAG structure can be uniquely identified from observational data --- under high-dimensional settings. We show that
O
(
k
4
log
p
)
O
number of samples suffices for our method to recover the true DAG structure with high probability, where
p
p
is the number of variables and
k
k
is the maximum Markov blanket size. We obtain our theoretical guarantees under a condition called \emph{restricted strong adjacency faithfulness} (RSAF), which is strictly weaker than strong faithfulness --- a condition that other methods based on conditional independence testing need for their success. The sample complexity of our method matches the information-theoretic limits in terms of the dependence on
p
p
. We validate our theoretical findings through synthetic experiments."
neurips,https://proceedings.neurips.cc/paper/2017/file/908c9a564a86426585b29f5335b619bc-Paper.pdf,Learning Neural Representations of Human Cognition across Many fMRI Studies,"Arthur Mensch, Julien Mairal, Danilo Bzdok, Bertrand Thirion, Gael Varoquaux",
neurips,https://proceedings.neurips.cc/paper/2017/file/9185f3ec501c674c7c788464a36e7fb3-Paper.pdf,Conic Scan-and-Cover algorithms for nonparametric topic modeling,"Mikhail Yurochkin, Aritra Guha, XuanLong Nguyen",
neurips,https://proceedings.neurips.cc/paper/2017/file/92a0e7a415d64ebafcb16a8ca817cde4-Paper.pdf,Online Learning for Multivariate Hawkes Processes,"Yingxiang Yang, Jalal Etesami, Niao He, Negar Kiyavash","We develop a nonparametric and online learning algorithm that estimates the triggering functions of a multivariate Hawkes process (MHP). The approach we take approximates the triggering function
f
i
,
j
(
t
)
f
by functions in a reproducing kernel Hilbert space (RKHS), and maximizes a time-discretized version of the log-likelihood, with Tikhonov regularization. Theoretically, our algorithm achieves an
\calO
(
log
T
)
\calO
regret bound. Numerical results show that our algorithm offers a competing performance to that of the nonparametric batch learning algorithm, with a run time comparable to the parametric online learning algorithm."
neurips,https://proceedings.neurips.cc/paper/2017/file/92af93f73faf3cefc129b6bc55a748a9-Paper.pdf,An Empirical Study on The Properties of Random Bases for Kernel Methods,"Maximilian Alber, Pieter-Jan Kindermans, Kristof Schütt, Klaus-Robert Müller, Fei Sha",
neurips,https://proceedings.neurips.cc/paper/2017/file/934815ad542a4a7c5e8a2dfa04fea9f5-Paper.pdf,"Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions","Aryeh Kontorovich, Sivan Sabato, Roi Weiss",
neurips,https://proceedings.neurips.cc/paper/2017/file/94b5bde6de888ddf9cde6748ad2523d1-Paper.pdf,Causal Effect Inference with Deep Latent-Variable Models,"Christos Louizos, Uri Shalit, Joris M. Mooij, David Sontag, Richard Zemel, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2017/file/95f8d9901ca8878e291552f001f67692-Paper.pdf,Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach,"Emmanouil Platanios, Hoifung Poon, Tom M. Mitchell, Eric J. Horvitz",
neurips,https://proceedings.neurips.cc/paper/2017/file/96671501524948bc3937b4b30d0e57b9-Paper.pdf,A Decomposition of Forecast Error in Prediction Markets,"Miro Dudik, Sebastien Lahaie, Ryan M. Rogers, Jennifer Wortman Vaughan",
neurips,https://proceedings.neurips.cc/paper/2017/file/97416ac0f58056947e2eb5d5d253d4f2-Paper.pdf,Ranking Data with Continuous Labels through Oriented Recursive Partitions,"Stéphan Clémençon, Mastane Achab",
neurips,https://proceedings.neurips.cc/paper/2017/file/976abf49974d4686f87192efa0513ae0-Paper.pdf,Scalable Log Determinants for Gaussian Process Kernel Learning,"Kun Dong, David Eriksson, Hannes Nickisch, David Bindel, Andrew G. Wilson",
neurips,https://proceedings.neurips.cc/paper/2017/file/978fce5bcc4eccc88ad48ce3914124a2-Paper.pdf,Fair Clustering Through Fairlets,"Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, Sergei Vassilvitskii",
neurips,https://proceedings.neurips.cc/paper/2017/file/979d472a84804b9f647bc185a877a8b5-Paper.pdf,A Linear-Time Kernel Goodness-of-Fit Test,"Wittawat Jitkrittum, Wenkai Xu, Zoltan Szabo, Kenji Fukumizu, Arthur Gretton",
neurips,https://proceedings.neurips.cc/paper/2017/file/97d98119037c5b8a9663cb21fb8ebf47-Paper.pdf,Rotting Bandits,"Nir Levine, Koby Crammer, Shie Mannor",
neurips,https://proceedings.neurips.cc/paper/2017/file/98b17f068d5d9b7668e19fb8ae470841-Paper.pdf,Scalable Planning with Tensorflow for Hybrid Nonlinear Domains,"Ga Wu, Buser Say, Scott Sanner",
neurips,https://proceedings.neurips.cc/paper/2017/file/98dce83da57b0395e163467c9dae521b-Paper.pdf,Probabilistic Models for Integration Error in the Assessment of Functional Cardiac Models,"Chris Oates, Steven Niederer, Angela Lee, François-Xavier Briol, Mark Girolami","This paper studies the numerical computation of integrals, representing estimates or predictions, over the output
f
(
x
)
f
of a computational model with respect to a distribution
p
(
d
x
)
p
over uncertain inputs
x
x
to the model. For the functional cardiac models that motivate this work, neither
f
f
nor
p
p
possess a closed-form expression and evaluation of either requires
≈
≈
100 CPU hours, precluding standard numerical integration methods. Our proposal is to treat integration as an estimation problem, with a joint model for both the a priori unknown function
f
f
and the a priori unknown distribution
p
p
. The result is a posterior distribution over the integral that explicitly accounts for dual sources of numerical approximation error due to a severely limited computational budget. This construction is applied to account, in a statistically principled manner, for the impact of numerical errors that (at present) are confounding factors in functional cardiac model assessment."
neurips,https://proceedings.neurips.cc/paper/2017/file/995665640dc319973d3173a74a03860c-Paper.pdf,Bandits Dueling on Partially Ordered Sets,"Julien Audiffren, Liva Ralaivola",
neurips,https://proceedings.neurips.cc/paper/2017/file/99adff456950dd9629a5260c4de21858-Paper.pdf,Decomposition-Invariant Conditional Gradient for General Polytopes with Line Search,"Mohammad Ali Bashiri, Xinhua Zhang",
neurips,https://proceedings.neurips.cc/paper/2017/file/99c5e07b4d5de9d18c350cdf64c5aa3d-Paper.pdf,Multiscale Semi-Markov Dynamics for Intracortical Brain-Computer Interfaces,"Daniel Milstein, Jason Pacheco, Leigh Hochberg, John D. Simeral, Beata Jarosiewicz, Erik Sudderth",
neurips,https://proceedings.neurips.cc/paper/2017/file/9a1756fd0c741126d7bbd4b692ccbd91-Paper.pdf,Fast Black-box Variational Inference through Stochastic Trust-Region Optimization,"Jeffrey Regier, Michael I. Jordan, Jon McAuliffe",
neurips,https://proceedings.neurips.cc/paper/2017/file/9a3d458322d70046f63dfd8b0153ece4-Paper.pdf,Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU with Generalized Hamming Network,Lixin Fan,
neurips,https://proceedings.neurips.cc/paper/2017/file/9a49a25d845a483fae4be7e341368e36-Paper.pdf,Optimized Pre-Processing for Discrimination Prevention,"Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, Kush R. Varshney",
neurips,https://proceedings.neurips.cc/paper/2017/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf,Scalable Demand-Aware Recommendation,"Jinfeng Yi, Cho-Jui Hsieh, Kush R. Varshney, Lijun Zhang, Yao Li",
neurips,https://proceedings.neurips.cc/paper/2017/file/9c838d2e45b2ad1094d42f4ef36764f6-Paper.pdf,Learning a Multi-View Stereo Machine,"Abhishek Kar, Christian Häne, Jitendra Malik",
neurips,https://proceedings.neurips.cc/paper/2017/file/9c8661befae6dbcd08304dbf4dcaf0db-Paper.pdf,On Blackbox Backpropagation and Jacobian Sensing,"Krzysztof M. Choromanski, Vikas Sindhwani",
neurips,https://proceedings.neurips.cc/paper/2017/file/9cb9ed4f35cf7c2f295cc2bc6f732a84-Paper.pdf,Learning Disentangled Representations with Semi-Supervised Deep Generative Models,"Siddharth N, Brooks Paige, Jan-Willem van de Meent, Alban Desmaison, Noah Goodman, Pushmeet Kohli, Frank Wood, Philip Torr",
neurips,https://proceedings.neurips.cc/paper/2017/file/9cf81d8026a9018052c429cc4e56739b-Paper.pdf,GP CaKe: Effective brain connectivity with causal kernels,"Luca Ambrogioni, Max Hinne, Marcel Van Gerven, Eric Maris",
neurips,https://proceedings.neurips.cc/paper/2017/file/9d7311ba459f9e45ed746755a32dcd11-Paper.pdf,Certified Defenses for Data Poisoning Attacks,"Jacob Steinhardt, Pang Wei W. Koh, Percy S. Liang",
neurips,https://proceedings.neurips.cc/paper/2017/file/9ddb9dd5d8aee9a76bf217a2a3c54833-Paper.pdf,Towards Generalization and Simplicity in Continuous Control,"Aravind Rajeswaran, Kendall Lowrey, Emanuel V. Todorov, Sham M. Kakade",
neurips,https://proceedings.neurips.cc/paper/2017/file/9e82757e9a1c12cb710ad680db11f6f1-Paper.pdf,Imagination-Augmented Agents for Deep Reinforcement Learning,"Sébastien Racanière, Theophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adrià Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, Demis Hassabis, David Silver, Daan Wierstra",
neurips,https://proceedings.neurips.cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf,Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles,"Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell",
neurips,https://proceedings.neurips.cc/paper/2017/file/9f44e956e3a2b7b5598c625fcc802c36-Paper.pdf,Adaptive Active Hypothesis Testing under Limited Information,"Fabio Cecchi, Nidhi Hegde",
neurips,https://proceedings.neurips.cc/paper/2017/file/9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf,Translation Synchronization via Truncated Least Squares,"Xiangru Huang, Zhenxiao Liang, Chandrajit Bajaj, Qixing Huang",
neurips,https://proceedings.neurips.cc/paper/2017/file/a00e5eb0973d24649a4a920fc53d9564-Paper.pdf,Limitations on Variance-Reduction and Acceleration Schemes for Finite Sums Optimization,Yossi Arjevani,"We study the conditions under which one is able to efficiently apply variance-reduction and acceleration schemes on finite sums problems. First, we show that perhaps surprisingly, the finite sum structure, by itself, is not sufficient for obtaining a complexity bound of
~
\cO
(
(
n
+
L
/
μ
)
ln
(
1
/
ϵ
)
)
\cO
for
L
L
-smooth and
μ
μ
-strongly convex finite sums - one must also know exactly which individual function is being referred to by the oracle at each iteration. Next, we show that for a broad class of first-order and coordinate-descent finite sums algorithms (including, e.g., SDCA, SVRG, SAG), it is not possible to get an `accelerated' complexity bound of
~
\cO
(
(
n
+
√
n
L
/
μ
)
ln
(
1
/
ϵ
)
)
\cO
, unless the strong convexity parameter is given explicitly. Lastly, we show that when this class of algorithms is used for minimizing
L
L
-smooth and non-strongly convex finite sums, the optimal complexity bound is
~
\cO
(
n
+
L
/
ϵ
)
\cO
, assuming that (on average) the same update rule is used for any iteration, and
~
\cO
(
n
+
√
n
L
/
ϵ
)
\cO
, otherwise."
neurips,https://proceedings.neurips.cc/paper/2017/file/a0160709701140704575d499c997b6ca-Paper.pdf,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks,"Urs Köster, Tristan Webb, Xin Wang, Marcel Nassar, Arjun K. Bansal, William Constable, Oguz Elibol, Scott Gray, Stewart Hall, Luke Hornof, Amir Khosrowshahi, Carey Kloss, Ruby J. Pai, Naveen Rao",
neurips,https://proceedings.neurips.cc/paper/2017/file/a03fa30821986dff10fc66647c84c9c3-Paper.pdf,Recursive Sampling for the Nystrom Method,"Cameron Musco, Christopher Musco",
neurips,https://proceedings.neurips.cc/paper/2017/file/a081cab429ff7a3b96e0a07319f1049e-Paper.pdf,Early stopping for kernel boosting algorithms: A general analysis with localized complexities,"Yuting Wei, Fanny Yang, Martin J. Wainwright","Early stopping of iterative algorithms is a widely-used form of regularization in statistical learning, commonly used in conjunction with boosting and related gradient-type algorithms. Although consistency results have been established in some settings, such estimators are less well-understood than their analogues based on penalized regularization. In this paper, for a relatively broad class of loss functions and boosting algorithms (including
L
2
L
-boost, LogitBoost and AdaBoost, among others), we connect the performance of a stopped iterate to the localized Rademacher/Gaussian complexity of the associated function class. This connection allows us to show that local fixed point analysis, now standard in the analysis of penalized estimators, can be used to derive optimal stopping rules. We derive such stopping rules in detail for various kernel classes, and illustrate the correspondence of our theory with practice for Sobolev kernel classes."
neurips,https://proceedings.neurips.cc/paper/2017/file/a1d7311f2a312426d710e1c617fcbc8c-Paper.pdf,Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning,"Shixiang (Shane) Gu, Timothy Lillicrap, Richard E. Turner, Zoubin Ghahramani, Bernhard Schölkopf, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2017/file/a2186aa7c086b46ad4e8bf81e2a3a19b-Paper.pdf,Parameter-Free Online Learning via Model Selection,"Dylan J. Foster, Satyen Kale, Mehryar Mohri, Karthik Sridharan","We introduce an efficient algorithmic framework for model selection in online learning, also known as parameter-free online learning. Departing from previous work, which has focused on highly structured function classes such as nested balls in Hilbert space, we propose a generic meta-algorithm framework that achieves online model selection oracle inequalities under minimal structural assumptions. We give the first computationally efficient parameter-free algorithms that work in arbitrary Banach spaces under mild smoothness assumptions; previous results applied only to Hilbert spaces. We further derive new oracle inequalities for matrix classes, non-nested convex sets, and
R
d
R
with generic regularizers. Finally, we generalize these results by providing oracle inequalities for arbitrary non-linear classes in the online supervised learning model. These results are all derived through a unified meta-algorithm scheme using a novel ""multi-scale"" algorithm for prediction with expert advice based on random playout, which may be of independent interest."
neurips,https://proceedings.neurips.cc/paper/2017/file/a34bacf839b923770b2c360eefa26748-Paper.pdf,Predicting User Activity Level In Point Processes With Mass Transport Equation,"Yichen Wang, Xiaojing Ye, Hongyuan Zha, Le Song",
neurips,https://proceedings.neurips.cc/paper/2017/file/a36e841c5230a79c2102036d2e259848-Paper.pdf,The Importance of Communities for Learning to Influence,"Eric Balkanski, Nicole Immorlica, Yaron Singer",
neurips,https://proceedings.neurips.cc/paper/2017/file/a4666cd9e1ab0e4abf05a0fb232f4ad3-Paper.pdf,Gradients of Generative Models for Improved Discriminative Analysis of Tandem Mass Spectra,"John T. Halloran, David M. Rocke",
neurips,https://proceedings.neurips.cc/paper/2017/file/a48564053b3c7b54800246348c7fa4a0-Paper.pdf,On the Optimization Landscape of Tensor Decompositions,"Rong Ge, Tengyu Ma","Non-convex optimization with local search heuristics has been widely used in machine learning, achieving many state-of-art results. It becomes increasingly important to understand why they can work for these NP-hard problems on typical data. The landscape of many objective functions in learning has been conjectured to have the geometric property that
all local optima are (approximately) global optima'', and thus they can be solved efficiently by local search algorithms. However, establishing such property can be very difficult. In this paper, we analyze the optimization landscape of the random over-complete tensor decomposition problem, which has many applications in unsupervised leaning, especially in learning latent variable models. In practice, it can be efficiently solved by gradient ascent on a non-convex objective. We show that for any small constant
ϵ
>
0
ϵ
, among the set of points with function values
(
1
+
ϵ
)
(
-factor larger than the expectation of the function, all the local maxima are approximate global maxima. Previously, the best-known result only characterizes the geometry in small neighborhoods around the true components. Our result implies that even with an initialization that is barely better than the random guess, the gradient ascent algorithm is guaranteed to solve this problem. Our main technique uses Kac-Rice formula and random matrix theory. To our best knowledge, this is the first time when Kac-Rice formula is successfully applied to counting the number of local minima of a highly-structured random polynomial with dependent coefficients."
neurips,https://proceedings.neurips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf,Counterfactual Fairness,"Matt J. Kusner, Joshua Loftus, Chris Russell, Ricardo Silva",
neurips,https://proceedings.neurips.cc/paper/2017/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf,Efficient Online Linear Optimization with Approximation Algorithms,Dan Garber,"We revisit the problem of Online Linear Optimization in case the set of feasible actions is accessible through an approximated linear optimization oracle with a factor
α
α
multiplicative approximation guarantee. This setting is in particular interesting since it captures natural online extensions of well-studied offline linear optimization problems which are NP-hard, yet admit efficient approximation algorithms. The goal here is to minimize the
α
α
-regret which is the natural extension of the standard regret in online learning to this setting. We present new algorithms with significantly improved oracle complexity for both the full information and bandit variants of the problem. Mainly, for both variants, we present
α
α
-regret bounds of
O
(
T
−
1
/
3
)
O
, were
T
T
is the number of prediction rounds, using only
O
(
log
(
T
)
)
O
calls to the approximation oracle per iteration, on average. These are the first results to obtain both average oracle complexity of
O
(
log
(
T
)
)
O
(or even poly-logarithmic in
T
T
) and
α
α
-regret bound
O
(
T
−
c
)
O
for a positive constant
c
c
, for both variants."
neurips,https://proceedings.neurips.cc/paper/2017/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf,Inhomogeneous Hypergraph Clustering with Applications,"Pan Li, Olgica Milenkovic",
neurips,https://proceedings.neurips.cc/paper/2017/file/a51fb975227d6640e4fe47854476d133-Paper.pdf,Runtime Neural Pruning,"Ji Lin, Yongming Rao, Jiwen Lu, Jie Zhou",
neurips,https://proceedings.neurips.cc/paper/2017/file/a5e0ff62be0b08456fc7f1e88812af3d-Paper.pdf,"Train longer, generalize better: closing the generalization gap in large batch training of neural networks","Elad Hoffer, Itay Hubara, Daniel Soudry",
neurips,https://proceedings.neurips.cc/paper/2017/file/a6d259bfbfa2062843ef543e21d7ec8e-Paper.pdf,Monte-Carlo Tree Search by Best Arm Identification,"Emilie Kaufmann, Wouter M. Koolen",
neurips,https://proceedings.neurips.cc/paper/2017/file/a6db4ed04f1621a119799fd3d7545d3d-Paper.pdf,Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model,"Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, Wang-chun WOO",
neurips,https://proceedings.neurips.cc/paper/2017/file/a6ea8471c120fe8cc35a2954c9b9c595-Paper.pdf,Scalable Model Selection for Belief Networks,"Zhao Song, Yusuke Muraoka, Ryohei Fujimaki, Lawrence Carin","We propose a scalable algorithm for model selection in sigmoid belief networks (SBNs), based on the factorized asymptotic Bayesian (FAB) framework. We derive the corresponding generalized factorized information criterion (gFIC) for the SBN, which is proven to be statistically consistent with the marginal log-likelihood. To capture the dependencies within hidden variables in SBNs, a recognition network is employed to model the variational distribution. The resulting algorithm, which we call FABIA, can simultaneously execute both model selection and inference by maximizing the lower bound of gFIC. On both synthetic and real data, our experiments suggest that FABIA, when compared to state-of-the-art algorithms for learning SBNs,
(
i
)
(
produces a more concise model, thus enabling faster testing;
(
i
i
)
(
improves predictive performance;
(
i
i
i
)
(
accelerates convergence; and
(
i
v
)
(
prevents overfitting."
neurips,https://proceedings.neurips.cc/paper/2017/file/a74c3bae3e13616104c1b25f9da1f11f-Paper.pdf,Collaborative Deep Learning in Fixed Topology Networks,"Zhanhong Jiang, Aditya Balu, Chinmay Hegde, Soumik Sarkar",
neurips,https://proceedings.neurips.cc/paper/2017/file/a78482ce76496fcf49085f2190e675b4-Paper.pdf,On the Complexity of Learning Neural Networks,"Le Song, Santosh Vempala, John Wilmes, Bo Xie",
neurips,https://proceedings.neurips.cc/paper/2017/file/a7a3d70c6d17a73140918996d03c014f-Paper.pdf,A Sample Complexity Measure with Applications to Learning Optimal Auctions,Vasilis Syrgkanis,"We introduce a new sample complexity measure, which we refer to as split-sample growth rate. For any hypothesis
H
H
and for any sample
S
S
of size
m
m
, the split-sample growth rate
^
τ
H
(
m
)
τ
counts how many different hypotheses can empirical risk minimization output on any sub-sample of
S
S
of size
m
/
2
m
. We show that the expected generalization error is upper bounded by
O
(
√
log
(
^
τ
H
(
2
m
)
)
m
)
O
. Our result is enabled by a strengthening of the Rademacher complexity analysis of the expected generalization error. We show that this sample complexity measure, greatly simplifies the analysis of the sample complexity of optimal auction design, for many auction classes studied in the literature. Their sample complexity can be derived solely by noticing that in these auction classes, ERM on any sample or sub-sample will pick parameters that are equal to one of the points in the sample."
neurips,https://proceedings.neurips.cc/paper/2017/file/a82d922b133be19c1171534e6594f754-Paper.pdf,On Optimal Generalizability in Parametric Learning,"Ahmad Beirami, Meisam Razaviyayn, Shahin Shahrampour, Vahid Tarokh",
neurips,https://proceedings.neurips.cc/paper/2017/file/a8345c3bb9e3896ea538ce77ffaf2c20-Paper.pdf,K-Medoids For K-Means Seeding,"James Newling, François Fleuret",
neurips,https://proceedings.neurips.cc/paper/2017/file/a869ccbcbd9568808b8497e28275c7c8-Paper.pdf,Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction,"Dan Xu, Wanli Ouyang, Xavier Alameda-Pineda, Elisa Ricci, Xiaogang Wang, Nicu Sebe",
neurips,https://proceedings.neurips.cc/paper/2017/file/a8abb4bb284b5b27aa7cb790dc20f80b-Paper.pdf,Geometric Descent Method for Convex Composite Minimization,"Shixiang Chen, Shiqian Ma, Wei Liu","In this paper, we extend the geometric descent method recently proposed by Bubeck, Lee and Singh to tackle nonsmooth and strongly convex composite problems. We prove that our proposed algorithm, dubbed geometric proximal gradient method (GeoPG), converges with a linear rate
(
1
−
1
/
√
κ
)
(
and thus achieves the optimal rate among first-order methods, where
κ
κ
is the condition number of the problem. Numerical results on linear regression and logistic regression with elastic net regularization show that GeoPG compares favorably with Nesterov's accelerated proximal gradient method, especially when the problem is ill-conditioned."
neurips,https://proceedings.neurips.cc/paper/2017/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf,Label Efficient Learning of Transferable Representations acrosss Domains and Tasks,"Zelun Luo, Yuliang Zou, Judy Hoffman, Li F. Fei-Fei",
neurips,https://proceedings.neurips.cc/paper/2017/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf,Improving Regret Bounds for Combinatorial Semi-Bandits with Probabilistically Triggered Arms and Its Applications,"Qinshi Wang, Wei Chen",
neurips,https://proceedings.neurips.cc/paper/2017/file/a8ecbabae151abacba7dbde04f761c37-Paper.pdf,Matching neural paths: transfer from recognition to correspondence search,"Nikolay Savinov, Lubor Ladicky, Marc Pollefeys",
neurips,https://proceedings.neurips.cc/paper/2017/file/a96b65a721e561e1e3de768ac819ffbb-Paper.pdf,Convergence Analysis of Two-layer Neural Networks with ReLU Activation,"Yuanzhi Li, Yang Yuan","In recent years, stochastic gradient descent (SGD) based techniques has become the standard tools for training neural networks. However, formal theoretical understanding of why SGD can train neural networks in practice is largely missing. In this paper, we make progress on understanding this mystery by providing a convergence analysis for SGD on a rich subset of two-layer feedforward networks with ReLU activations. This subset is characterized by a special structure called ""identity mapping"". We prove that, if input follows from Gaussian distribution, with standard
O
(
1
/
√
d
)
O
initialization of the weights, SGD converges to the global minimum in polynomial number of steps. Unlike normal vanilla networks, the ""identity mapping"" makes our network asymmetric and thus the global minimum is unique. To complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks. Our convergence theorem differs from traditional non-convex optimization techniques. We show that SGD converges to optimal in ""two phases"": In phase I, the gradient points to the wrong direction, however, a potential function
g
g
gradually decreases. Then in phase II, SGD enters a nice one point convex region and converges. We also show that the identity mapping is necessary for convergence, as it moves the initial point to a better place for optimization. Experiment verifies our claims."
neurips,https://proceedings.neurips.cc/paper/2017/file/a9813e9550fee3110373c21fa012eee7-Paper.pdf,Quantifying how much sensory information in a neural code is relevant for behavior,"Giuseppe Pica, Eugenio Piasini, Houman Safaai, Caroline Runyan, Christopher Harvey, Mathew Diamond, Christoph Kayser, Tommaso Fellin, Stefano Panzeri","Determining how much of the sensory information carried by a neural code contributes to behavioral performance is key to understand sensory function and neural information flow. However, there are as yet no analytical tools to compute this information that lies at the intersection between sensory coding and behavioral readout. Here we develop a novel measure, termed the information-theoretic intersection information
\III
(
S
;
R
;
C
)
\III
, that quantifies how much of the sensory information carried by a neural response
R
R
is used for behavior during perceptual discrimination tasks. Building on the Partial Information Decomposition framework, we define
\III
(
S
;
R
;
C
)
\III
as the part of the mutual information between the stimulus
S
S
and the response
R
R
that also informs the consequent behavioral choice
C
C
. We compute
\III
(
S
;
R
;
C
)
\III
in the analysis of two experimental cortical datasets, to show how this measure can be used to compare quantitatively the contributions of spike timing and spike rates to task performance, and to identify brain areas or neural populations that specifically transform sensory information into choice."
neurips,https://proceedings.neurips.cc/paper/2017/file/ab452534c5ce28c4fbb0e102d4a4fb2e-Paper.pdf,Self-supervised Learning of Motion Capture,"Hsiao-Yu Tung, Hsiao-Wei Tung, Ersin Yumer, Katerina Fragkiadaki",
neurips,https://proceedings.neurips.cc/paper/2017/file/ab541d874c7bc19ab77642849e02b89f-Paper.pdf,Toward Goal-Driven Neural Network Models for the Rodent Whisker-Trigeminal System,"Chengxu Zhuang, Jonas Kubilius, Mitra JZ Hartmann, Daniel L. Yamins",
neurips,https://proceedings.neurips.cc/paper/2017/file/ab7314887865c4265e896c6e209d1cd6-Paper.pdf,Clustering Billions of Reads for DNA Data Storage,"Cyrus Rashtchian, Konstantin Makarychev, Miklos Racz, Siena Ang, Djordje Jevdjic, Sergey Yekhanin, Luis Ceze, Karin Strauss",
neurips,https://proceedings.neurips.cc/paper/2017/file/acab0116c354964a558e65bdd07ff047-Paper.pdf,AIDE: An algorithm for measuring the accuracy of probabilistic inference algorithms,"Marco Cusumano-Towner, Vikash K. Mansinghka",
neurips,https://proceedings.neurips.cc/paper/2017/file/ad71c82b22f4f65b9398f76d8be4c615-Paper.pdf,Information-theoretic analysis of generalization capability of learning algorithms,"Aolin Xu, Maxim Raginsky",
neurips,https://proceedings.neurips.cc/paper/2017/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf,MarrNet: 3D Shape Reconstruction via 2.5D Sketches,"Jiajun Wu, Yifan Wang, Tianfan Xue, Xingyuan Sun, Bill Freeman, Josh Tenenbaum",
neurips,https://proceedings.neurips.cc/paper/2017/file/addfa9b7e234254d26e9c7f2af1005cb-Paper.pdf,Flexible statistical inference for mechanistic models of neural dynamics,"Jan-Matthis Lueckmann, Pedro J. Goncalves, Giacomo Bassetto, Kaan Öcal, Marcel Nonnenmacher, Jakob H. Macke",
neurips,https://proceedings.neurips.cc/paper/2017/file/ade55409d1224074754035a5a937d2e0-Paper.pdf,ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching,"Chunyuan Li, Hao Liu, Changyou Chen, Yuchen Pu, Liqun Chen, Ricardo Henao, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2017/file/ae5e3ce40e0404a45ecacaaf05e5f735-Paper.pdf,Speeding Up Latent Variable Gaussian Graphical Model Estimation via Nonconvex Optimization,"Pan Xu, Jian Ma, Quanquan Gu",
neurips,https://proceedings.neurips.cc/paper/2017/file/aebf7782a3d445f43cf30ee2c0d84dee-Paper.pdf,Sparse convolutional coding for neuronal assembly detection,"Sven Peter, Elke Kirschbaum, Martin Both, Lee Campbell, Brandon Harvey, Conor Heins, Daniel Durstewitz, Ferran Diego, Fred A. Hamprecht",
neurips,https://proceedings.neurips.cc/paper/2017/file/b0169350cd35566c47ba83c6ec1d6f82-Paper.pdf,Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons,"Nikhil Parthasarathy, Eleanor Batty, William Falcon, Thomas Rutten, Mohit Rajpal, E.J. Chichilnisky, Liam Paninski",
neurips,https://proceedings.neurips.cc/paper/2017/file/b030afbb3a8af8fb0759241c97466ee4-Paper.pdf,"Plan, Attend, Generate: Planning for Sequence-to-Sequence Models","Caglar Gulcehre, Francis Dutil, Adam Trischler, Yoshua Bengio",
neurips,https://proceedings.neurips.cc/paper/2017/file/b069b3415151fa7217e870017374de7c-Paper.pdf,Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems,"Yonatan Belinkov, James Glass",
neurips,https://proceedings.neurips.cc/paper/2017/file/b06f50d1f89bd8b2a0fb771c1a69c2b0-Paper.pdf,Multi-Task Learning for Contextual Bandits,"Aniket Anand Deshmukh, Urun Dogan, Clay Scott",
neurips,https://proceedings.neurips.cc/paper/2017/file/b166b57d195370cd41f80dd29ed523d9-Paper.pdf,Temporal Coherency based Criteria for Predicting Video Frames using Deep Multi-stage Generative Adversarial Networks,"Prateep Bhattacharjee, Sukhendu Das",
neurips,https://proceedings.neurips.cc/paper/2017/file/b19aa25ff58940d974234b48391b9549-Paper.pdf,Improving the Expected Improvement Algorithm,"Chao Qin, Diego Klabjan, Daniel Russo",
neurips,https://proceedings.neurips.cc/paper/2017/file/b1a59b315fc9a3002ce38bbe070ec3f5-Paper.pdf,Towards Accurate Binary Convolutional Neural Network,"Xiaofan Lin, Cong Zhao, Wei Pan",
neurips,https://proceedings.neurips.cc/paper/2017/file/b22b257ad0519d4500539da3c8bcf4dd-Paper.pdf,Spectrally-normalized margin bounds for neural networks,"Peter L. Bartlett, Dylan J. Foster, Matus J. Telgarsky",
neurips,https://proceedings.neurips.cc/paper/2017/file/b24d516bb65a5a58079f0f3526c87c57-Paper.pdf,Consistent Multitask Learning with Nonlinear Output Relations,"Carlo Ciliberto, Alessandro Rudi, Lorenzo Rosasco, Massimiliano Pontil",
neurips,https://proceedings.neurips.cc/paper/2017/file/b2531e7bb29bf22e1daae486fae3417a-Paper.pdf,Deep Recurrent Neural Network-Based Identification of Precursor microRNAs,"Seunghyun Park, Seonwoo Min, Hyun-Soo Choi, Sungroh Yoon",
neurips,https://proceedings.neurips.cc/paper/2017/file/b299ad862b6f12cb57679f0538eca514-Paper.pdf,Boltzmann Exploration Done Right,"Nicolò Cesa-Bianchi, Claudio Gentile, Gabor Lugosi, Gergely Neu","Boltzmann exploration is a classic strategy for sequential decision-making under uncertainty, and is one of the most standard tools in Reinforcement Learning (RL). Despite its widespread use, there is virtually no theoretical understanding about the limitations or the actual benefits of this exploration scheme. Does it drive exploration in a meaningful way? Is it prone to misidentifying the optimal actions or spending too much time exploring the suboptimal ones? What is the right tuning for the learning rate? In this paper, we address several of these questions for the classic setup of stochastic multi-armed bandits. One of our main results is showing that the Boltzmann exploration strategy with any monotone learning-rate sequence will induce suboptimal behavior. As a remedy, we offer a simple non-monotone schedule that guarantees near-optimal performance, albeit only when given prior access to key problem parameters that are typically not available in practical situations (like the time horizon
T
T
and the suboptimality gap
Δ
Δ
). More importantly, we propose a novel variant that uses different learning rates for different arms, and achieves a distribution-dependent regret bound of order
K
log
2
T
Δ
K
and a distribution-independent bound of order
√
K
T
log
K
K
without requiring such prior knowledge. To demonstrate the flexibility of our technique, we also propose a variant that guarantees the same performance bounds even if the rewards are heavy-tailed."
neurips,https://proceedings.neurips.cc/paper/2017/file/b2ab001909a8a6f04b51920306046ce5-Paper.pdf,End-to-end Differentiable Proving,"Tim Rocktäschel, Sebastian Riedel",
neurips,https://proceedings.neurips.cc/paper/2017/file/b2eeb7362ef83deff5c7813a67e14f0a-Paper.pdf,Matching on Balanced Nonlinear Representations for Treatment Effects Estimation,"Sheng Li, Yun Fu",
neurips,https://proceedings.neurips.cc/paper/2017/file/b3b43aeeacb258365cc69cdaf42a68af-Paper.pdf,Tomography of the London Underground: a Scalable Model for Origin-Destination Data,"Nicolò Colombo, Ricardo Silva, Soong Moon Kang",
neurips,https://proceedings.neurips.cc/paper/2017/file/b3b4d2dbedc99fe843fd3dedb02f086f-Paper.pdf,Gaussian process based nonlinear latent structure discovery in multivariate spike train data,"Anqi Wu, Nicholas A. Roy, Stephen Keeley, Jonathan W. Pillow",
neurips,https://proceedings.neurips.cc/paper/2017/file/b432f34c5a997c8e7c806a895ecc5e25-Paper.pdf,Multi-Objective Non-parametric Sequential Prediction,"Guy Uziel, Ran El-Yaniv",
neurips,https://proceedings.neurips.cc/paper/2017/file/b4d168b48157c623fbd095b4a565b5bb-Paper.pdf,Optimal Sample Complexity of M-wise Data for Top-K Ranking,"Minje Jang, Sunghyun Kim, Changho Suh, Sewoong Oh","We explore the top-K rank aggregation problem in which one aims to recover a consistent ordering that focuses on top-K ranked items based on partially revealed preference information. We examine an M-wise comparison model that builds on the Plackett-Luce (PL) model where for each sample, M items are ranked according to their perceived utilities modeled as noisy observations of their underlying true utilities. As our result, we characterize the minimax optimality on the sample size for top-K ranking. The optimal sample size turns out to be inversely proportional to M. We devise an algorithm that effectively converts M-wise samples into pairwise ones and employs a spectral method using the refined data. In demonstrating its optimality, we develop a novel technique for deriving tight
ℓ
∞
ℓ
estimation error bounds, which is key to accurately analyzing the performance of top-K ranking algorithms, but has been challenging. Recent work relied on an additional maximum-likelihood estimation (MLE) stage merged with a spectral method to attain good estimates in
ℓ
∞
ℓ
error to achieve the limit for the pairwise model. In contrast, although it is valid in slightly restricted regimes, our result demonstrates a spectral method alone to be sufficient for the general M-wise model. We run numerical experiments using synthetic data and confirm that the optimal sample size decreases at the rate of 1/M. Moreover, running our algorithm on real-world data, we find that its applicability extends to settings that may not fit the PL model."
neurips,https://proceedings.neurips.cc/paper/2017/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf,From which world is your graph,"Cheng Li, Felix MF Wong, Zhenming Liu, Varun Kanade",
neurips,https://proceedings.neurips.cc/paper/2017/file/b60c5ab647a27045b462934977ccad9a-Paper.pdf,An Empirical Bayes Approach to Optimizing Machine Learning Algorithms,James McInerney,
neurips,https://proceedings.neurips.cc/paper/2017/file/b6617980ce90f637e68c3ebe8b9be745-Paper.pdf,Multiscale Quantization for Fast Similarity Search,"Xiang Wu, Ruiqi Guo, Ananda Theertha Suresh, Sanjiv Kumar, Daniel N. Holtmann-Rice, David Simcha, Felix Yu",
neurips,https://proceedings.neurips.cc/paper/2017/file/b6e710870acb098e584277457ba89d68-Paper.pdf,Bregman Divergence for Stochastic Variance Reduction: Saddle-Point and Adversarial Prediction,"Zhan Shi, Xinhua Zhang, Yaoliang Yu",
neurips,https://proceedings.neurips.cc/paper/2017/file/b75bd27b5a48a1b48987a18d831f6336-Paper.pdf,Perturbative Black Box Variational Inference,"Robert Bamler, Cheng Zhang, Manfred Opper, Stephan Mandt","Black box variational inference (BBVI) with reparameterization gradients triggered the exploration of divergence measures other than the Kullback-Leibler (KL) divergence, such as alpha divergences. In this paper, we view BBVI with generalized divergences as a form of estimating the marginal likelihood via biased importance sampling. The choice of divergence determines a bias-variance trade-off between the tightness of a bound on the marginal likelihood (low bias) and the variance of its gradient estimators. Drawing on variational perturbation theory of statistical physics, we use these insights to construct a family of new variational bounds. Enumerated by an odd integer order
K
K
, this family captures the standard KL bound for
K
=
1
K
, and converges to the exact marginal likelihood as
K
→
∞
K
. Compared to alpha-divergences, our reparameterization gradients have a lower variance. We show in experiments on Gaussian Processes and Variational Autoencoders that the new bounds are more mass covering, and that the resulting posterior covariances are closer to the true posterior and lead to higher likelihoods on held-out data."
neurips,https://proceedings.neurips.cc/paper/2017/file/b7fede84c2be02ccb9c77107956560eb-Paper.pdf,Kernel Feature Selection via Conditional Covariance Minimization,"Jianbo Chen, Mitchell Stern, Martin J. Wainwright, Michael I. Jordan",
neurips,https://proceedings.neurips.cc/paper/2017/file/b87470782489389f344c4fa4ceb5260c-Paper.pdf,Active Learning from Peers,"Keerthiram Murugesan, Jaime Carbonell",
neurips,https://proceedings.neurips.cc/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf,On Fairness and Calibration,"Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, Kilian Q. Weinberger",
neurips,https://proceedings.neurips.cc/paper/2017/file/ba3866600c3540f67c1e9575e213be0a-Paper.pdf,One-Shot Imitation Learning,"Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, Wojciech Zaremba",
neurips,https://proceedings.neurips.cc/paper/2017/file/bbeb0c1b1fd44e392c7ce2fdbd137e87-Paper.pdf,Triangle Generative Adversarial Networks,"Zhe Gan, Liqun Chen, Weiyao Wang, Yuchen Pu, Yizhe Zhang, Hao Liu, Chunyuan Li, Lawrence Carin","A Triangle Generative Adversarial Network (
Δ
Δ
-GAN) is developed for semi-supervised cross-domain joint distribution matching, where the training data consists of samples from each domain, and supervision of domain correspondence is provided by only a few paired samples.
Δ
Δ
-GAN consists of four neural networks, two generators and two discriminators. The generators are designed to learn the two-way conditional distributions between the two domains, while the discriminators implicitly define a ternary discriminative function, which is trained to distinguish real data pairs and two kinds of fake data pairs. The generators and discriminators are trained together using adversarial learning. Under mild assumptions, in theory the joint distributions characterized by the two generators concentrate to the data distribution. In experiments, three different kinds of domain pairs are considered, image-label, image-image and image-attribute pairs. Experiments on semi-supervised image classification, image-to-image translation and attribute-based image generation demonstrate the superiority of the proposed approach."
neurips,https://proceedings.neurips.cc/paper/2017/file/bc4e356fee1972242c8f7eabf4dff517-Paper.pdf,Learning Populations of Parameters,"Kevin Tian, Weihao Kong, Gregory Valiant","Consider the following estimation problem: there are
n
n
entities, each with an unknown parameter
p
i
∈
[
0
,
1
]
p
, and we observe
n
n
independent random variables,
X
1
,
…
,
X
n
X
, with
X
i
∼
X
Binomial
(
t
,
p
i
)
(
. How accurately can one recover the
histogram'' (i.e. cumulative density function) of the
p
i
p
's? While the empirical estimates would recover the histogram to earth mover distance
Θ
(
1
√
t
)
Θ
(equivalently,
ℓ
1
ℓ
distance between the CDFs), we show that, provided
n
n
is sufficiently large, we can achieve error
O
(
1
t
)
O
which is information theoretically optimal. We also extend our results to the multi-dimensional parameter case, capturing settings where each member of the population has multiple associated parameters. Beyond the theoretical results, we demonstrate that the recovery algorithm performs well in practice on a variety of datasets, providing illuminating insights into several domains, including politics, sports analytics, and variation in the gender ratio of offspring."
neurips,https://proceedings.neurips.cc/paper/2017/file/bd0cc810b580b35884bd9df37c0e8b0f-Paper.pdf,Multi-Armed Bandits with Metric Movement Costs,"Tomer Koren, Roi Livni, Yishay Mansour","We consider the non-stochastic Multi-Armed Bandit problem in a setting where there is a fixed and known metric on the action space that determines a cost for switching between any pair of actions. The loss of the online learner has two components: the first is the usual loss of the selected actions, and the second is an additional loss due to switching between actions. Our main contribution gives a tight characterization of the expected minimax regret in this setting, in terms of a complexity measure
C
C
of the underlying metric which depends on its covering numbers. In finite metric spaces with
k
k
actions, we give an efficient algorithm that achieves regret of the form
˜
(
max
\set
C
1
/
3
T
2
/
3
,
√
k
T
)
(
, and show that this is the best possible. Our regret bound generalizes previous known regret bounds for some special cases: (i) the unit-switching cost regret
˜
Θ
(
max
\set
k
1
/
3
T
2
/
3
,
√
k
T
)
Θ
where
C
=
Θ
(
k
)
C
, and (ii) the interval metric with regret
˜
Θ
(
max
\set
T
2
/
3
,
√
k
T
)
Θ
where
C
=
Θ
(
1
)
C
. For infinite metrics spaces with Lipschitz loss functions, we derive a tight regret bound of
˜
Θ
(
T
d
+
1
d
+
2
)
Θ
where
d
≥
1
d
is the Minkowski dimension of the space, which is known to be tight even when there are no switching costs."
neurips,https://proceedings.neurips.cc/paper/2017/file/bd686fd640be98efaae0091fa301e613-Paper.pdf,Structured Embedding Models for Grouped Data,"Maja Rudolph, Francisco Ruiz, Susan Athey, David Blei",
neurips,https://proceedings.neurips.cc/paper/2017/file/bdc4626aa1d1df8e14d80d345b2a442d-Paper.pdf,Conservative Contextual Linear Bandits,"Abbas Kazerouni, Mohammad Ghavamzadeh, Yasin Abbasi Yadkori, Benjamin Van Roy",
neurips,https://proceedings.neurips.cc/paper/2017/file/bea5955b308361a1b07bc55042e25e54-Paper.pdf,Regularized Modal Regression with Applications in Cognitive Impairment Prediction,"Xiaoqian Wang, Hong Chen, Weidong Cai, Dinggang Shen, Heng Huang",
neurips,https://proceedings.neurips.cc/paper/2017/file/bf201d5407a6509fa536afc4b380577e-Paper.pdf,Adversarial Ranking for Language Generation,"Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, Ming-ting Sun",
neurips,https://proceedings.neurips.cc/paper/2017/file/bf424cb7b0dea050a42b9739eb261a3a-Paper.pdf,Diving into the shallows: a computational perspective on large-scale shallow learning,"SIYUAN MA, Mikhail Belkin",
neurips,https://proceedings.neurips.cc/paper/2017/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf,Integration Methods and Optimization Algorithms,"Damien Scieur, Vincent Roulet, Francis Bach, Alexandre d'Aspremont",
neurips,https://proceedings.neurips.cc/paper/2017/file/bf8229696f7a3bb4700cfddef19fa23f-Paper.pdf,The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings,"Krzysztof M. Choromanski, Mark Rowland, Adrian Weller",
neurips,https://proceedings.neurips.cc/paper/2017/file/c02f9de3c2f3040751818aacc7f60b74-Paper.pdf,A KL-LUCB algorithm for Large-Scale Crowdsourcing,"Ervin Tanczos, Robert Nowak, Bob Mankoff",
neurips,https://proceedings.neurips.cc/paper/2017/file/c0e90532fb42ac6de18e25e95db73047-Paper.pdf,Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes,"Jianshu Chen, Chong Wang, Lin Xiao, Ji He, Lihong Li, Li Deng",
neurips,https://proceedings.neurips.cc/paper/2017/file/c182f930a06317057d31c73bb2fedd4f-Paper.pdf,Streaming Weak Submodularity: Interpreting Neural Networks on the Fly,"Ethan Elenberg, Alexandros G. Dimakis, Moran Feldman, Amin Karbasi",
neurips,https://proceedings.neurips.cc/paper/2017/file/c1fea270c48e8079d8ddf7d06d26ab52-Paper.pdf,Decomposable Submodular Function Minimization: Discrete and Continuous,"Alina Ene, Huy Nguyen, László A. Végh",
neurips,https://proceedings.neurips.cc/paper/2017/file/c22abfa379f38b5b0411bc11fa9bf92f-Paper.pdf,Learning Affinity via Spatial Propagation Networks,"Sifei Liu, Shalini De Mello, Jinwei Gu, Guangyu Zhong, Ming-Hsuan Yang, Jan Kautz",
neurips,https://proceedings.neurips.cc/paper/2017/file/c24cd76e1ce41366a4bbe8a49b02a028-Paper.pdf,Gated Recurrent Convolution Neural Network for OCR,"Jianfeng Wang, Xiaolin Hu",
neurips,https://proceedings.neurips.cc/paper/2017/file/c2964caac096f26db222cb325aa267cb-Paper.pdf,Multi-view Matrix Factorization for Linear Dynamical System Estimation,"Mahdi Karami, Martha White, Dale Schuurmans, Csaba Szepesvari",
neurips,https://proceedings.neurips.cc/paper/2017/file/c2ba1bc54b239208cb37b901c0d3b363-Paper.pdf,Policy Gradient With Value Function Approximation For Collective Multiagent Planning,"Duc Thien Nguyen, Akshat Kumar, Hoong Chuin Lau",
neurips,https://proceedings.neurips.cc/paper/2017/file/c2f32522a84d5e6357e6abac087f1b0b-Paper.pdf,Stochastic Submodular Maximization: The Case of Coverage Functions,"Mohammad Karimi, Mario Lucic, Hamed Hassani, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2017/file/c30fb4dc55d801fc7473840b5b161dfa-Paper.pdf,Stochastic Approximation for Canonical Correlation Analysis,"Raman Arora, Teodor Vanislavov Marinov, Poorya Mianjy, Nati Srebro","We propose novel first-order stochastic approximation algorithms for canonical correlation analysis (CCA). Algorithms presented are instances of inexact matrix stochastic gradient (MSG) and inexact matrix exponentiated gradient (MEG), and achieve
ϵ
ϵ
-suboptimality in the population objective in
poly
(
1
ϵ
)
poly
iterations. We also consider practical variants of the proposed algorithms and compare them with other methods for CCA both theoretically and empirically."
neurips,https://proceedings.neurips.cc/paper/2017/file/c32d9bf27a3da7ec8163957080c8628e-Paper.pdf,Linear regression without correspondence,"Daniel J. Hsu, Kevin Shi, Xiaorui Sun",
neurips,https://proceedings.neurips.cc/paper/2017/file/c3535febaff29fcb7c0d20cbe94391c7-Paper.pdf,Structured Generative Adversarial Networks,"Zhijie Deng, Hao Zhang, Xiaodan Liang, Luona Yang, Shizhen Xu, Jun Zhu, Eric P. Xing",
neurips,https://proceedings.neurips.cc/paper/2017/file/c366c2c97d47b02b24c3ecade4c40a01-Paper.pdf,Dynamic-Depth Context Tree Weighting,"Joao V. Messias, Shimon Whiteson",
neurips,https://proceedings.neurips.cc/paper/2017/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf,"Fast, Sample-Efficient Algorithms for Structured Phase Retrieval","Gauri Jagatap, Chinmay Hegde",
neurips,https://proceedings.neurips.cc/paper/2017/file/c44e503833b64e9f27197a484f4257c0-Paper.pdf,Hierarchical Methods of Moments,"Matteo Ruffini, Guillaume Rabusseau, Borja Balle",
neurips,https://proceedings.neurips.cc/paper/2017/file/c4b31ce7d95c75ca70d50c19aef08bf1-Paper.pdf,A New Alternating Direction Method for Linear Programming,"Sinong Wang, Ness Shroff","It is well known that, for a linear program (LP) with constraint matrix
A
∈
R
m
×
n
A
, the Alternating Direction Method of Multiplier converges globally and linearly at a rate
O
(
(
∥
A
∥
2
F
+
m
n
)
log
(
1
/
ϵ
)
)
O
. However, such a rate is related to the problem dimension and the algorithm exhibits a slow and fluctuating
tail convergence'' in practice. In this paper, we propose a new variable splitting method of LP and prove that our method has a convergence rate of
O
(
∥
A
∥
2
log
(
1
/
ϵ
)
)
O
. The proof is based on simultaneously estimating the distance from a pair of primal dual iterates to the optimal primal and dual solution set by certain residuals. In practice, we result in a new first-order LP solver that can exploit both the sparsity and the specific structure of matrix
A
A
and a significant speedup for important problems such as basis pursuit, inverse covariance matrix estimation, L1 SVM and nonnegative matrix factorization problem compared with current fastest LP solvers."
neurips,https://proceedings.neurips.cc/paper/2017/file/c4de8ced6214345614d33fb0b16a8acd-Paper.pdf,Near Optimal Sketching of Low-Rank Tensor Regression,"Xingguo Li, Jarvis Haupt, David Woodruff","We study the least squares regression problem
min
Θ
∈
\RR
p
1
×
⋯
×
p
D
∥
\cA
(
Θ
)
−
b
∥
2
2
min
, where
Θ
Θ
is a low-rank tensor, defined as
Θ
=
∑
R
r
=
1
θ
(
r
)
1
∘
⋯
∘
θ
(
r
)
D
Θ
, for vectors
θ
(
r
)
d
∈
R
p
d
θ
for all
r
∈
[
R
]
r
and
d
∈
[
D
]
d
. %
R
R
is small compared with
p
1
,
…
,
p
D
p
, Here,
∘
∘
denotes the outer product of vectors, and
\cA
(
Θ
)
\cA
is a linear function on
Θ
Θ
. This problem is motivated by the fact that the number of parameters in
Θ
Θ
is only
R
⋅
∑
D
d
=
1
p
D
R
, which is significantly smaller than the
∏
D
d
=
1
p
d
∏
number of parameters in ordinary least squares regression. We consider the above CP decomposition model of tensors
Θ
Θ
, as well as the Tucker decomposition. For both models we show how to apply data dimensionality reduction techniques based on {\it sparse} random projections
Φ
∈
\RR
m
×
n
Φ
, with
m
≪
n
m
, to reduce the problem to a much smaller problem
min
Θ
∥
Φ
\cA
(
Θ
)
−
Φ
b
∥
2
2
min
, for which
∥
Φ
\cA
(
Θ
)
−
Φ
b
∥
2
2
=
(
1
±
ε
)
∥
\cA
(
Θ
)
−
b
∥
2
2
‖
holds simultaneously for all
Θ
Θ
. We obtain a significantly smaller dimension and sparsity in the randomized linear mapping
Φ
Φ
than is possible for ordinary least squares regression. Finally, we give a number of numerical simulations supporting our theory."
neurips,https://proceedings.neurips.cc/paper/2017/file/c54e7837e0cd0ced286cb5995327d1ab-Paper.pdf,Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models,Sergey Ioffe,
neurips,https://proceedings.neurips.cc/paper/2017/file/c57168a952f5d46724cf35dfc3d48a7f-Paper.pdf,Position-based Multiple-play Bandit Problem with Unknown Position Bias,"Junpei Komiyama, Junya Honda, Akiko Takeda",
neurips,https://proceedings.neurips.cc/paper/2017/file/c59b469d724f7919b7d35514184fdc0f-Paper.pdf,Deep Voice 2: Multi-Speaker Neural Text-to-Speech,"Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, Jonathan Raiman, Yanqi Zhou",
neurips,https://proceedings.neurips.cc/paper/2017/file/c5a4e7e6882845ea7bb4d9462868219b-Paper.pdf,Eigen-Distortions of Hierarchical Representations,"Alexander Berardino, Valero Laparra, Johannes Ballé, Eero Simoncelli",
neurips,https://proceedings.neurips.cc/paper/2017/file/c5dc3e08849bec07e33ca353de62ea04-Paper.pdf,Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon,"Xin Dong, Shangyu Chen, Sinno Pan",
neurips,https://proceedings.neurips.cc/paper/2017/file/c6036a69be21cb660499b75718a3ef24-Paper.pdf,Deliberation Networks: Sequence Generation Beyond One-Pass Decoding,"Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, Tie-Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2017/file/c61f571dbd2fb949d3fe5ae1608dd48b-Paper.pdf,Do Deep Neural Networks Suffer from Crowding?,"Anna Volokitin, Gemma Roig, Tomaso A. Poggio",
neurips,https://proceedings.neurips.cc/paper/2017/file/c65d7bd70fe3e5e3a2f3de681edc193d-Paper.pdf,Non-Stationary Spectral Kernels,"Sami Remes, Markus Heinonen, Samuel Kaski",
neurips,https://proceedings.neurips.cc/paper/2017/file/c7558e9d1f956b016d1fdba7ea132378-Paper.pdf,Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations,"Marcel Nonnenmacher, Srinivas C. Turaga, Jakob H. Macke",
neurips,https://proceedings.neurips.cc/paper/2017/file/c75b6f114c23a4d7ea11331e7c00e73c-Paper.pdf,Minimizing a Submodular Function from Samples,"Eric Balkanski, Yaron Singer",
neurips,https://proceedings.neurips.cc/paper/2017/file/c850371fda6892fbfd1c5a5b457e5777-Paper.pdf,A graph-theoretic approach to multitasking,"Noga Alon, Daniel Reichman, Igor Shinkar, Tal Wagner, Sebastian Musslick, Jonathan D. Cohen, Tom Griffiths, Biswadip dey, Kayhan Ozcimder","A key feature of neural network architectures is their ability to support the simultaneous interaction among large numbers of units in the learning and processing of representations. However, how the richness of such interactions trades off against the ability of a network to simultaneously carry out multiple independent processes -- a salient limitation in many domains of human cognition -- remains largely unexplored. In this paper we use a graph-theoretic analysis of network architecture to address this question, where tasks are represented as edges in a bipartite graph
G
=
(
A
∪
B
,
E
)
G
. We define a new measure of multitasking capacity of such networks, based on the assumptions that tasks that \emph{need} to be multitasked rely on independent resources, i.e., form a matching, and that tasks \emph{can} be performed without interference if they form an induced matching. Our main result is an inherent tradeoff between the multitasking capacity and the average degree of the network that holds \emph{regardless of the network architecture}. These results are also extended to networks of depth greater than
2
2
. On the positive side, we demonstrate that networks that are random-like (e.g., locally sparse) can have desirable multitasking properties. Our results shed light into the parallel-processing limitations of neural systems and provide insights that may be useful for the analysis and design of parallel architectures."
neurips,https://proceedings.neurips.cc/paper/2017/file/c86a7ee3d8ef0b551ed58e354a836f2b-Paper.pdf,Adversarial Surrogate Losses for Ordinal Regression,"Rizal Fathony, Mohammad Ali Bashiri, Brian Ziebart",
neurips,https://proceedings.neurips.cc/paper/2017/file/c8862fc1a32725712838863fb1a260b9-Paper.pdf,Self-Supervised Intrinsic Image Decomposition,"Michael Janner, Jiajun Wu, Tejas D. Kulkarni, Ilker Yildirim, Josh Tenenbaum",
neurips,https://proceedings.neurips.cc/paper/2017/file/c902b497eb972281fb5b4e206db38ee6-Paper.pdf,On-the-fly Operation Batching in Dynamic Computation Graphs,"Graham Neubig, Yoav Goldberg, Chris Dyer",
neurips,https://proceedings.neurips.cc/paper/2017/file/c930eecd01935feef55942cc445f708f-Paper.pdf,Fitting Low-Rank Tensors in Constant Time,"Kohei Hayashi, Yuichi Yoshida","In this paper, we develop an algorithm that approximates the residual error of Tucker decomposition, one of the most popular tensor decomposition methods, with a provable guarantee. Given an order-
K
K
tensor
X
∈
R
N
1
×
⋯
×
N
K
X
, our algorithm randomly samples a constant number
s
s
of indices for each mode and creates a
mini'' tensor
~
X
∈
R
s
×
⋯
×
s
X
, whose elements are given by the intersection of the sampled indices on
X
X
. Then, we show that the residual error of the Tucker decomposition of
~
X
X
is sufficiently close to that of
X
X
with high probability. This result implies that we can figure out how much we can fit a low-rank tensor to
X
X
\emph{in constant time}, regardless of the size of
X
X
. This is useful for guessing the favorable rank of Tucker decomposition. Finally, we demonstrate how the sampling method works quickly and accurately using multiple real datasets."
neurips,https://proceedings.neurips.cc/paper/2017/file/ca3ec598002d2e7662e2ef4bdd58278b-Paper.pdf,Random Projection Filter Bank for Time Series Data,"Amir-massoud Farahmand, Sepideh Pourazarm, Daniel Nikovski",
neurips,https://proceedings.neurips.cc/paper/2017/file/cb8acb1dc9821bf74e6ca9068032d623-Paper.pdf,Dynamic Revenue Sharing,"Santiago Balseiro, Max Lin, Vahab Mirrokni, Renato Leme, IIIS Song Zuo",
neurips,https://proceedings.neurips.cc/paper/2017/file/cb8da6767461f2812ae4290eac7cbc42-Paper.pdf,Prototypical Networks for Few-shot Learning,"Jake Snell, Kevin Swersky, Richard Zemel",
neurips,https://proceedings.neurips.cc/paper/2017/file/cbcb58ac2e496207586df2854b17995f-Paper.pdf,Unsupervised learning of object frames by dense equivariant image labelling,"James Thewlis, Hakan Bilen, Andrea Vedaldi",
neurips,https://proceedings.neurips.cc/paper/2017/file/ccbd8ca962b80445df1f7f38c57759f0-Paper.pdf,Unified representation of tractography and diffusion-weighted MRI data using sparse multidimensional arrays,"Cesar F. Caiafa, Olaf Sporns, Andrew Saykin, Franco Pestilli",
neurips,https://proceedings.neurips.cc/paper/2017/file/cd3afef9b8b89558cd56638c3631868a-Paper.pdf,Random Permutation Online Isotonic Regression,"Wojciech Kotlowski, Wouter M. Koolen, Alan Malek",
neurips,https://proceedings.neurips.cc/paper/2017/file/cdd96eedd7f695f4d61802f8105ba2b0-Paper.pdf,PRUNE: Preserving Proximity and Global Ranking for Network Embedding,"Yi-An Lai, Chin-Chi Hsu, Wen Hao Chen, Mi-Yen Yeh, Shou-De Lin",
neurips,https://proceedings.neurips.cc/paper/2017/file/ce5140df15d046a66883807d18d0264b-Paper.pdf,"Online to Offline Conversions, Universality and Adaptive Minibatch Sizes",Kfir Levy,
neurips,https://proceedings.neurips.cc/paper/2017/file/ced556cd9f9c0c8315cfbe0744a3baf0-Paper.pdf,Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network,"Wengong Jin, Connor Coley, Regina Barzilay, Tommi Jaakkola",
neurips,https://proceedings.neurips.cc/paper/2017/file/cedebb6e872f539bef8c3f919874e9d7-Paper.pdf,Inferring Generative Model Structure with Static Analysis,"Paroma Varma, Bryan D. He, Payal Bajaj, Nishith Khandwala, Imon Banerjee, Daniel Rubin, Christopher Ré",
neurips,https://proceedings.neurips.cc/paper/2017/file/cf2226ddd41b1a2d0ae51dab54d32c36-Paper.pdf,"Influence Maximization with
ε
ε
-Almost Submodular Threshold Functions","Qiang Li, Wei Chen, Institute of Computing Xiaoming Sun, Institute of Computing Jialin Zhang","Influence maximization is the problem of selecting
k
k
nodes in a social network to maximize their influence spread. The problem has been extensively studied but most works focus on the submodular influence diffusion models. In this paper, motivated by empirical evidences, we explore influence maximization in the non-submodular regime. In particular, we study the general threshold model in which a fraction of nodes have non-submodular threshold functions, but their threshold functions are closely upper- and lower-bounded by some submodular functions (we call them
ε
ε
-almost submodular). We first show a strong hardness result: there is no
1
/
n
γ
/
c
1
approximation for influence maximization (unless P = NP) for all networks with up to
n
γ
n
ε
ε
-almost submodular nodes, where
γ
γ
is in (0,1) and
c
c
is a parameter depending on
ε
ε
. This indicates that influence maximization is still hard to approximate even though threshold functions are close to submodular. We then provide
(
1
−
ε
)
ℓ
(
1
−
1
/
e
)
(
approximation algorithms when the number of
ε
ε
-almost submodular nodes is
ℓ
ℓ
. Finally, we conduct experiments on a number of real-world datasets, and the results demonstrate that our approximation algorithms outperform other baseline algorithms."
neurips,https://proceedings.neurips.cc/paper/2017/file/cfee398643cbc3dc5eefc89334cacdc1-Paper.pdf,Improved Dynamic Regret for Non-degenerate Functions,"Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, Zhi-Hua Zhou",
neurips,https://proceedings.neurips.cc/paper/2017/file/d0010a6f34908640a4a6da2389772a78-Paper.pdf,AdaGAN: Boosting Generative Models,"Ilya O. Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann SIMON-GABRIEL, Bernhard Schölkopf",
neurips,https://proceedings.neurips.cc/paper/2017/file/d10ec7c16cbe9de8fbb1c42787c3ec26-Paper.pdf,Large-Scale Quadratically Constrained Quadratic Program via Low-Discrepancy Sequences,"Kinjal Basu, Ankan Saha, Shaunak Chatterjee",
neurips,https://proceedings.neurips.cc/paper/2017/file/d1e946f4e67db4b362ad23818a6fb78a-Paper.pdf,Graph Matching via Multiplicative Update Algorithm,"Bo Jiang, Jin Tang, Chris Ding, Yihong Gong, Bin Luo",
neurips,https://proceedings.neurips.cc/paper/2017/file/d2cd33e9c0236a8c2d8bd3fa91ad3acf-Paper.pdf,Neural Expectation Maximization,"Klaus Greff, Sjoerd van Steenkiste, Jürgen Schmidhuber",
neurips,https://proceedings.neurips.cc/paper/2017/file/d2ddea18f00665ce8623e36bd4e3c7c5-Paper.pdf,"Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs","Saurabh Verma, Zhi-Li Zhang",
neurips,https://proceedings.neurips.cc/paper/2017/file/d38901788c533e8286cb6400b40b386d-Paper.pdf,Expectation Propagation with Stochastic Kinetic Model in Complex Interaction Systems,"Le Fang, Fan Yang, Wen Dong, Tong Guan, Chunming Qiao",
neurips,https://proceedings.neurips.cc/paper/2017/file/d3a7f48c12e697d50c8a7ae7684644ef-Paper.pdf,Welfare Guarantees from Data,"Darrell Hoy, Denis Nekipelov, Vasilis Syrgkanis",
neurips,https://proceedings.neurips.cc/paper/2017/file/d3d80b656929a5bc0fa34381bf42fbdd-Paper.pdf,Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference,"Abhishek Kumar, Prasanna Sattigeri, Tom Fletcher",
neurips,https://proceedings.neurips.cc/paper/2017/file/d494020ff8ec181ef98ed97ac3f25453-Paper.pdf,Houdini: Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples,"Moustapha M. Cisse, Yossi Adi, Natalia Neverova, Joseph Keshet",
neurips,https://proceedings.neurips.cc/paper/2017/file/d54ce9de9df77c579775a7b6b1a4bdc0-Paper.pdf,Clustering Stable Instances of Euclidean k-means.,"Aravindan Vijayaraghavan, Abhratanu Dutta, Alex Wang",
neurips,https://proceedings.neurips.cc/paper/2017/file/d594b1a945b5d645e59e21f88bd2d83b-Paper.pdf,Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin,"Ritambhara Singh, Jack Lanchantin, Arshdeep Sekhon, Yanjun Qi",
neurips,https://proceedings.neurips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf,Deep Reinforcement Learning from Human Preferences,"Paul F. Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei",
neurips,https://proceedings.neurips.cc/paper/2017/file/d7a84628c025d30f7b2c52c958767e76-Paper.pdf,Subset Selection under Noise,"Chao Qian, Jing-Cheng Shi, Yang Yu, Ke Tang, Zhi-Hua Zhou","The problem of selecting the best
k
k
-element subset from a universe is involved in many applications. While previous studies assumed a noise-free environment or a noisy monotone submodular objective function, this paper considers a more realistic and general situation where the evaluation of a subset is a noisy monotone function (not necessarily submodular), with both multiplicative and additive noises. To understand the impact of the noise, we firstly show the approximation ratio of the greedy algorithm and POSS, two powerful algorithms for noise-free subset selection, in the noisy environments. We then propose to incorporate a noise-aware strategy into POSS, resulting in the new PONSS algorithm. We prove that PONSS can achieve a better approximation ratio under some assumption such as i.i.d. noise distribution. The empirical results on influence maximization and sparse regression problems show the superior performance of PONSS."
neurips,https://proceedings.neurips.cc/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,"Charles Ruizhongtai Qi, Li Yi, Hao Su, Leonidas J. Guibas",
neurips,https://proceedings.neurips.cc/paper/2017/file/d8d31bd778da8bdd536187c36e48892b-Paper.pdf,"Approximation Bounds for Hierarchical Clustering: Average Linkage, Bisecting K-means, and Local Search","Benjamin Moseley, Joshua Wang",
neurips,https://proceedings.neurips.cc/paper/2017/file/d8e1344e27a5b08cdfd5d027d9b8d6de-Paper.pdf,Thinking Fast and Slow with Deep Learning and Tree Search,"Thomas Anthony, Zheng Tian, David Barber",
neurips,https://proceedings.neurips.cc/paper/2017/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Paper.pdf,Learning Combinatorial Optimization Algorithms over Graphs,"Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, Le Song",
neurips,https://proceedings.neurips.cc/paper/2017/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf,Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice,"Jeffrey Pennington, Samuel Schoenholz, Surya Ganguli",
neurips,https://proceedings.neurips.cc/paper/2017/file/d9ff90f4000eacd3a6c9cb27f78994cf-Paper.pdf,Adaptive Classification for Prediction Under a Budget,"Feng Nan, Venkatesh Saligrama",
neurips,https://proceedings.neurips.cc/paper/2017/file/da0d1111d2dc5d489242e60ebcbaf988-Paper.pdf,Online Convex Optimization with Stochastic Constraints,"Hao Yu, Michael Neely, Xiaohan Wei","This paper considers online convex optimization (OCO) with stochastic constraints, which generalizes Zinkevich's OCO over a known simple fixed set by introducing multiple stochastic functional constraints that are i.i.d. generated at each round and are disclosed to the decision maker only after the decision is made. This formulation arises naturally when decisions are restricted by stochastic environments or deterministic environments with noisy observations. It also includes many important problems as special case, such as OCO with long term constraints, stochastic constrained convex optimization, and deterministic constrained convex optimization. To solve this problem, this paper proposes a new algorithm that achieves
O
(
√
T
)
O
expected regret and constraint violations and
O
(
√
T
log
(
T
)
)
O
high probability regret and constraint violations. Experiments on a real-world data center scheduling problem further verify the performance of the new algorithm."
neurips,https://proceedings.neurips.cc/paper/2017/file/dab49080d80c724aad5ebf158d63df41-Paper.pdf,Structured Bayesian Pruning via Log-Normal Multiplicative Noise,"Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, Dmitry P. Vetrov",
neurips,https://proceedings.neurips.cc/paper/2017/file/db5cea26ca37aa09e5365f3e7f5dd9eb-Paper.pdf,Clustering with Noisy Queries,"Arya Mazumdar, Barna Saha","In this paper, we provide a rigorous theoretical study of clustering with noisy queries. Given a set of
n
n
elements, our goal is to recover the true clustering by asking minimum number of pairwise queries to an oracle. Oracle can answer queries of the form
do elements
u
u
and
v
v
belong to the same cluster?''-the queries can be asked interactively (adaptive queries), or non-adaptively up-front, but its answer can be erroneous with probability
p
p
. In this paper, we provide the first information theoretic lower bound on the number of queries for clustering with noisy oracle in both situations. We design novel algorithms that closely match this query complexity lower bound, even when the number of clusters is unknown. Moreover, we design computationally efficient algorithms both for the adaptive and non-adaptive settings. The problem captures/generalizes multiple application scenarios. It is directly motivated by the growing body of work that use crowdsourcing for {\em entity resolution}, a fundamental and challenging data mining task aimed to identify all records in a database referring to the same entity. Here crowd represents the noisy oracle, and the number of queries directly relates to the cost of crowdsourcing. Another application comes from the problem of sign edge prediction in social network, where social interactions can be both positive and negative, and one must identify the sign of all pair-wise interactions by querying a few pairs. Furthermore, clustering with noisy oracle is intimately connected to correlation clustering, leading to improvement therein. Finally, it introduces a new direction of study in the popular stochastic block model where one has an incomplete stochastic block model matrix to recover the clusters."
neurips,https://proceedings.neurips.cc/paper/2017/file/db85e2590b6109813dafa101ceb2faeb-Paper.pdf,Compression-aware Training of Deep Networks,"Jose M. Alvarez, Mathieu Salzmann",
neurips,https://proceedings.neurips.cc/paper/2017/file/db98dc0dbafde48e8f74c0de001d35e4-Paper.pdf,Maxing and Ranking with Few Assumptions,"Moein Falahatgar, Yi Hao, Alon Orlitsky, Venkatadheeraj Pichapati, Vaishakh Ravindrakumar","PAC maximum selection (maxing) and ranking of
n
n
elements via random pairwise comparisons have diverse applications and have been studied under many models and assumptions. With just one simple natural assumption: strong stochastic transitivity, we show that maxing can be performed with linearly many comparisons yet ranking requires quadratically many. With no assumptions at all, we show that for the Borda-score metric, maximum selection can be performed with linearly many comparisons and ranking can be performed with
O
(
n
log
n
)
O
comparisons."
neurips,https://proceedings.neurips.cc/paper/2017/file/dbb422937d7ff56e049d61da730b3e11-Paper.pdf,Subspace Clustering via Tangent Cones,"Amin Jalali, Rebecca Willett","Given samples lying on any of a number of subspaces, subspace clustering is the task of grouping the samples based on the their corresponding subspaces. Many subspace clustering methods operate by assigning a measure of affinity to each pair of points and feeding these affinities into a graph clustering algorithm. This paper proposes a new paradigm for subspace clustering that computes affinities based on the corresponding conic geometry. The proposed conic subspace clustering (CSC) approach considers the convex hull of a collection of normalized data points and the corresponding tangent cones. The union of subspaces underlying the data imposes a strong association between the tangent cone at a sample
x
x
and the original subspace containing
x
x
. In addition to describing this novel geometric perspective, this paper provides a practical algorithm for subspace clustering that leverages this perspective, where a tangent cone membership test is used to estimate the affinities. This algorithm is accompanied with deterministic and stochastic guarantees on the properties of the learned affinity matrix, on the true and false positive rates and spread, which directly translate into the overall clustering accuracy."
neurips,https://proceedings.neurips.cc/paper/2017/file/dbd22ba3bd0df8f385bdac3e9f8be207-Paper.pdf,DropoutNet: Addressing Cold Start in Recommender Systems,"Maksims Volkovs, Guangwei Yu, Tomi Poutanen",
neurips,https://proceedings.neurips.cc/paper/2017/file/dc6a6489640ca02b0d42dabeb8e46bb7-Paper.pdf,Unsupervised Image-to-Image Translation Networks,"Ming-Yu Liu, Thomas Breuel, Jan Kautz",
neurips,https://proceedings.neurips.cc/paper/2017/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf,SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability,"Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein",
neurips,https://proceedings.neurips.cc/paper/2017/file/dc960c46c38bd16e953d97cdeefdbc68-Paper.pdf,Fast Rates for Bandit Optimization with Upper-Confidence Frank-Wolfe,"Quentin Berthet, Vianney Perchet",
neurips,https://proceedings.neurips.cc/paper/2017/file/dcda54e29207294d8e7e1b537338b1c0-Paper.pdf,Identifying Outlier Arms in Multi-Armed Bandit,"Honglei Zhuang, Chi Wang, Yifan Wang",
neurips,https://proceedings.neurips.cc/paper/2017/file/dcf6070a4ab7f3afbfd2809173e0824b-Paper.pdf,Discovering Potential Correlations via Hypercontractivity,"Hyeji Kim, Weihao Gao, Sreeram Kannan, Sewoong Oh, Pramod Viswanath",
neurips,https://proceedings.neurips.cc/paper/2017/file/dd8eb9f23fbd362da0e3f4e70b878c16-Paper.pdf,A Dirichlet Mixture Model of Hawkes Processes for Event Sequence Clustering,"Hongteng Xu, Hongyuan Zha",
neurips,https://proceedings.neurips.cc/paper/2017/file/ddcbe25988981920c872c1787382f04d-Paper.pdf,Efficient Approximation Algorithms for Strings Kernel Based Sequence Classification,"Muhammad Farhan, Juvaria Tariq, Arif Zaman, Mudassir Shabbir, Imdad Ullah Khan",
neurips,https://proceedings.neurips.cc/paper/2017/file/dea9ddb25cbf2352cf4dec30222a02a5-Paper.pdf,Multi-output Polynomial Networks and Factorization Machines,"Mathieu Blondel, Vlad Niculae, Takuma Otsuka, Naonori Ueda",
neurips,https://proceedings.neurips.cc/paper/2017/file/deb54ffb41e085fd7f69a75b6359c989-Paper.pdf,Tractability in Structured Probability Spaces,"Arthur Choi, Yujia Shen, Adnan Darwiche",
neurips,https://proceedings.neurips.cc/paper/2017/file/df0aab058ce179e4f7ab135ed4e641a9-Paper.pdf,Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search,"Luigi Acerbi, Wei Ji Ma",
neurips,https://proceedings.neurips.cc/paper/2017/file/df1f1d20ee86704251795841e6a9405a-Paper.pdf,Multi-Information Source Optimization,"Matthias Poloczek, Jialei Wang, Peter Frazier",
neurips,https://proceedings.neurips.cc/paper/2017/file/dfce06801e1a85d6d06f1fdd4475dacd-Paper.pdf,Differentially private Bayesian learning on distributed data,"Mikko Heikkilä, Eemil Lagerspetz, Samuel Kaski, Kana Shimizu, Sasu Tarkoma, Antti Honkela",
neurips,https://proceedings.neurips.cc/paper/2017/file/dfd7468ac613286cdbb40872c8ef3b06-Paper.pdf,MMD GAN: Towards Deeper Understanding of Moment Matching Network,"Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, Barnabas Poczos","Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing {\it adversarial kernel learning} techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD-GAN. The new distance measure in MMD-GAN is a meaningful loss that enjoys the advantage of weak
∗
∗
topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR-10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works."
neurips,https://proceedings.neurips.cc/paper/2017/file/dfeb9598fbfb97cc6bbcc0aff2c785d6-Paper.pdf,Convergence of Gradient EM on Multi-component Mixture of Gaussians,"Bowei Yan, Mingzhang Yin, Purnamrita Sarkar",
neurips,https://proceedings.neurips.cc/paper/2017/file/e00406144c1e7e35240afed70f34166a-Paper.pdf,Bayesian Dyadic Trees and Histograms for Regression,"Stéphanie van der Pas, Veronika Ročková",
neurips,https://proceedings.neurips.cc/paper/2017/file/e0126439e08ddfbdf4faa952dc910590-Paper.pdf,Efficient and Flexible Inference for Stochastic Systems,"Stefan Bauer, Nico S. Gorbach, Djordje Miladinovic, Joachim M. Buhmann",
neurips,https://proceedings.neurips.cc/paper/2017/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf,Learning ReLUs via Gradient Descent,Mahdi Soltanolkotabi,"In this paper we study the problem of learning Rectified Linear Units (ReLUs) which are functions of the form
\vct
x
↦
max
(
0
,
⟨
\vct
w
,
\vct
x
⟩
)
\vct
with
\vct
w
∈
\R
d
\vct
denoting the weight vector. We study this problem in the high-dimensional regime where the number of observations are fewer than the dimension of the weight vector. We assume that the weight vector belongs to some closed set (convex or nonconvex) which captures known side-information about its structure. We focus on the realizable model where the inputs are chosen i.i.d.~from a Gaussian distribution and the labels are generated according to a planted weight vector. We show that projected gradient descent, when initialized at
\vct
0
\vct
, converges at a linear rate to the planted model with a number of samples that is optimal up to numerical constants. Our results on the dynamics of convergence of these very shallow neural nets may provide some insights towards understanding the dynamics of deeper architectures."
neurips,https://proceedings.neurips.cc/paper/2017/file/e0688d13958a19e087e123148555e4b4-Paper.pdf,Learning Graph Representations with Embedding Propagation,"Alberto Garcia Duran, Mathias Niepert",
neurips,https://proceedings.neurips.cc/paper/2017/file/e077e1a544eec4f0307cf5c3c721d944-Paper.pdf,Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation,"Matthias Hein, Maksym Andriushchenko",
neurips,https://proceedings.neurips.cc/paper/2017/file/e0a209539d1e74ab9fe46b9e01a19a97-Paper.pdf,Collapsed variational Bayes for Markov jump processes,"Boqian Zhang, Jiangwei Pan, Vinayak A. Rao",
neurips,https://proceedings.neurips.cc/paper/2017/file/e0ab531ec312161511493b002f9be2ee-Paper.pdf,Is the Bellman residual a bad proxy?,"Matthieu Geist, Bilal Piot, Olivier Pietquin","This paper aims at theoretically and empirically comparing two standard optimization criteria for Reinforcement Learning: i) maximization of the mean value and ii) minimization of the Bellman residual. For that purpose, we place ourselves in the framework of policy search algorithms, that are usually designed to maximize the mean value, and derive a method that minimizes the residual
∥
T
∗
v
π
−
v
π
∥
1
,
ν
‖
over policies. A theoretical analysis shows how good this proxy is to policy optimization, and notably that it is better than its value-based counterpart. We also propose experiments on randomly generated generic Markov decision processes, specifically designed for studying the influence of the involved concentrability coefficient. They show that the Bellman residual is generally a bad proxy to policy optimization and that directly maximizing the mean value is much better, despite the current lack of deep theoretical analysis. This might seem obvious, as directly addressing the problem of interest is usually better, but given the prevalence of (projected) Bellman residual minimization in value-based reinforcement learning, we believe that this question is worth to be considered."
neurips,https://proceedings.neurips.cc/paper/2017/file/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Paper.pdf,Efficient Use of Limited-Memory Accelerators for Linear Learning on Heterogeneous Systems,"Celestine Dünner, Thomas Parnell, Martin Jaggi",
neurips,https://proceedings.neurips.cc/paper/2017/file/e11943a6031a0e6114ae69c257617980-Paper.pdf,Noise-Tolerant Interactive Learning Using Pairwise Comparisons,"Yichong Xu, Hongyang Zhang, Kyle Miller, Aarti Singh, Artur Dubrawski",
neurips,https://proceedings.neurips.cc/paper/2017/file/e139c454239bfde741e893edb46a06cc-Paper.pdf,Near-Optimal Edge Evaluation in Explicit Generalized Binomial Graphs,"Sanjiban Choudhury, Shervin Javdani, Siddhartha Srinivasa, Sebastian Scherer",
neurips,https://proceedings.neurips.cc/paper/2017/file/e19347e1c3ca0c0b97de5fb3b690855a-Paper.pdf,Minimal Exploration in Structured Stochastic Bandits,"Richard Combes, Stefan Magureanu, Alexandre Proutiere",
neurips,https://proceedings.neurips.cc/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf,Learning Efficient Object Detection Models with Knowledge Distillation,"Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, Manmohan Chandraker",
neurips,https://proceedings.neurips.cc/paper/2017/file/e22312179bf43e61576081a2f250f845-Paper.pdf,Learning Chordal Markov Networks via Branch and Bound,"Kari Rantanen, Antti Hyttinen, Matti Järvisalo",
neurips,https://proceedings.neurips.cc/paper/2017/file/e3408432c1a48a52fb6c74d926b38886-Paper.pdf,Efficient Optimization for Linear Dynamical Systems with Applications to Clustering and Sparse Coding,"Wenbing Huang, Mehrtash Harandi, Tong Zhang, Lijie Fan, Fuchun Sun, Junzhou Huang",
neurips,https://proceedings.neurips.cc/paper/2017/file/e369853df766fa44e1ed0ff613f563bd-Paper.pdf,Deep Subspace Clustering Networks,"Pan Ji, Tong Zhang, Hongdong Li, Mathieu Salzmann, Ian Reid",
neurips,https://proceedings.neurips.cc/paper/2017/file/e449b9317dad920c0dd5ad0a2a2d5e49-Paper.pdf,Robust Estimation of Neural Signals in Calcium Imaging,"Hakan Inan, Murat A. Erdogdu, Mark Schnitzer",
neurips,https://proceedings.neurips.cc/paper/2017/file/e4a93f0332b2519177ed55741ea4e5e7-Paper.pdf,Fast-Slow Recurrent Neural Networks,"Asier Mujika, Florian Meier, Angelika Steger",
neurips,https://proceedings.neurips.cc/paper/2017/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf,PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs,"Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, Philip S. Yu",
neurips,https://proceedings.neurips.cc/paper/2017/file/e60e81c4cbe5171cd654662d9887aec2-Paper.pdf,Dual Discriminator Generative Adversarial Nets,"Tu Nguyen, Trung Le, Hung Vu, Dinh Phung",
neurips,https://proceedings.neurips.cc/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf,Beyond Parity: Fairness Objectives for Collaborative Filtering,"Sirui Yao, Bert Huang",
neurips,https://proceedings.neurips.cc/paper/2017/file/e655c7716a4b3ea67f48c6322fc42ed6-Paper.pdf,Multitask Spectral Learning of Weighted Automata,"Guillaume Rabusseau, Borja Balle, Joelle Pineau",
neurips,https://proceedings.neurips.cc/paper/2017/file/e6acf4b0f69f6f6e60e9a815938aa1ff-Paper.pdf,A simple neural network module for relational reasoning,"Adam Santoro, David Raposo, David G. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap",
neurips,https://proceedings.neurips.cc/paper/2017/file/e6af401c28c1790eaef7d55c92ab6ab6-Paper.pdf,Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks,Arash Vahdat,
neurips,https://proceedings.neurips.cc/paper/2017/file/e6ba70fc093b4ce912d769ede1ceeba8-Paper.pdf,Stochastic Mirror Descent in Variationally Coherent Optimization Problems,"Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, Peter W. Glynn",
neurips,https://proceedings.neurips.cc/paper/2017/file/e6c2dc3dee4a51dcec3a876aa2339a78-Paper.pdf,Polynomial Codes: an Optimal Design for High-Dimensional Coded Matrix Multiplication,"Qian Yu, Mohammad Maddah-Ali, Salman Avestimehr",
neurips,https://proceedings.neurips.cc/paper/2017/file/e6cbc650cd5798a05dfd0f51d14cde5c-Paper.pdf,From Bayesian Sparsity to Gated Recurrent Nets,"Hao He, Bo Xin, Satoshi Ikehata, David Wipf",
neurips,https://proceedings.neurips.cc/paper/2017/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf,Compatible Reward Inverse Reinforcement Learning,"Alberto Maria Metelli, Matteo Pirotta, Marcello Restelli",
neurips,https://proceedings.neurips.cc/paper/2017/file/e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf,Consistent Robust Regression,"Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, Purushottam Kar",
neurips,https://proceedings.neurips.cc/paper/2017/file/e71e5cd119bbc5797164fb0cd7fd94a4-Paper.pdf,Scalable Variational Inference for Dynamical Systems,"Nico S. Gorbach, Stefan Bauer, Joachim M. Buhmann",
neurips,https://proceedings.neurips.cc/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf,Learning multiple visual domains with residual adapters,"Sylvestre-Alvise Rebuffi, Hakan Bilen, Andrea Vedaldi",
neurips,https://proceedings.neurips.cc/paper/2017/file/e7e23670481ac78b3c4122a99ba60573-Paper.pdf,Incorporating Side Information by Adaptive Convolution,"Di Kang, Debarun Dhar, Antoni Chan",
neurips,https://proceedings.neurips.cc/paper/2017/file/e8bf0f27d70d480d3ab793bb7619aaa5-Paper.pdf,Hierarchical Clustering Beyond the Worst-Case,"Vincent Cohen-Addad, Varun Kanade, Frederik Mallmann-Trenn",
neurips,https://proceedings.neurips.cc/paper/2017/file/e91068fff3d7fa1594dfdf3b4308433a-Paper.pdf,"Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference","Geoffrey Roeder, Yuhuai Wu, David K. Duvenaud",
neurips,https://proceedings.neurips.cc/paper/2017/file/e93028bdc1aacdfb3687181f2031765d-Paper.pdf,"Multiplicative Weights Update with Constant Step-Size in Congestion Games: Convergence, Limit Cycles and Chaos","Gerasimos Palaiopanos, Ioannis Panageas, Georgios Piliouras","The Multiplicative Weights Update (MWU) method is a ubiquitous meta-algorithm that works as follows: A distribution is maintained on a certain set, and at each step the probability assigned to action
γ
γ
is multiplied by
(
1
−
ϵ
C
(
γ
)
)
>
0
(
where
C
(
γ
)
C
is the
cost"" of action
γ
γ
and then rescaled to ensure that the new values form a distribution. We analyze MWU in congestion games where agents use \textit{arbitrary admissible constants} as learning rates
ϵ
ϵ
and prove convergence to \textit{exact Nash equilibria}. Interestingly, this convergence result does not carry over to the nearly homologous MWU variant where at each step the probability assigned to action
γ
γ
is multiplied by
(
1
−
ϵ
)
C
(
γ
)
(
even for the simplest case of two-agent, two-strategy load balancing games, where such dynamics can provably lead to limit cycles or even chaotic behavior."
neurips,https://proceedings.neurips.cc/paper/2017/file/e9412ee564384b987d086df32d4ce6b7-Paper.pdf,QMDP-Net: Deep Learning for Planning under Partial Observability,"Peter Karkus, David Hsu, Wee Sun Lee",
neurips,https://proceedings.neurips.cc/paper/2017/file/e94f63f579e05cb49c05c2d050ead9c0-Paper.pdf,Deep Supervised Discrete Hashing,"Qi Li, Zhenan Sun, Ran He, Tieniu Tan",
neurips,https://proceedings.neurips.cc/paper/2017/file/e94fe9ac8dc10dd8b9a239e6abee2848-Paper.pdf,"Approximation Algorithms for
ℓ
0
ℓ
-Low Rank Approximation","Karl Bringmann, Pavel Kolev, David Woodruff","We study the
ℓ
0
ℓ
-Low Rank Approximation Problem, where the goal is, given an
m
×
n
m
matrix
A
A
, to output a rank-
k
k
matrix
A
′
A
for which
∥
A
′
−
A
∥
0
‖
is minimized. Here, for a matrix
B
B
,
∥
B
∥
0
‖
denotes the number of its non-zero entries. This NP-hard variant of low rank approximation is natural for problems with no underlying metric, and its goal is to minimize the number of disagreeing data positions. We provide approximation algorithms which significantly improve the running time and approximation factor of previous work. For
k
>
1
k
, we show how to find, in poly
(
m
n
)
(
time for every
k
k
, a rank
O
(
k
log
(
n
/
k
)
)
O
matrix
A
′
A
for which
∥
A
′
−
A
∥
0
≤
O
(
k
2
log
(
n
/
k
)
)
\OPT
‖
. To the best of our knowledge, this is the first algorithm with provable guarantees for the
ℓ
0
ℓ
-Low Rank Approximation Problem for
k
>
1
k
, even for bicriteria algorithms. For the well-studied case when
k
=
1
k
, we give a
(
2
+
ϵ
)
(
-approximation in {\it sublinear time}, which is impossible for other variants of low rank approximation such as for the Frobenius norm. We strengthen this for the well-studied case of binary matrices to obtain a
(
1
+
O
(
ψ
)
)
(
-approximation in sublinear time, where
ψ
=
\OPT
/
\nnz
A
ψ
. For small
ψ
ψ
, our approximation factor is
1
+
o
(
1
)
1
."
neurips,https://proceedings.neurips.cc/paper/2017/file/e97ee2054defb209c35fe4dc94599061-Paper.pdf,ADMM without a Fixed Penalty Parameter: Faster Convergence with New Adaptive Penalization,"Yi Xu, Mingrui Liu, Qihang Lin, Tianbao Yang","Alternating direction method of multipliers (ADMM) has received tremendous interest for solving numerous problems in machine learning, statistics and signal processing. However, it is known that the performance of ADMM and many of its variants is very sensitive to the penalty parameter of a quadratic penalty applied to the equality constraints. Although several approaches have been proposed for dynamically changing this parameter during the course of optimization, they do not yield theoretical improvement in the convergence rate and are not directly applicable to stochastic ADMM. In this paper, we develop a new ADMM and its linearized variant with a new adaptive scheme to update the penalty parameter. Our methods can be applied under both deterministic and stochastic optimization settings for structured non-smooth objective function. The novelty of the proposed scheme lies at that it is adaptive to a local sharpness property of the objective function, which marks the key difference from previous adaptive scheme that adjusts the penalty parameter per-iteration based on certain conditions on iterates. On theoretical side, given the local sharpness characterized by an exponent
θ
∈
(
0
,
1
]
θ
, we show that the proposed ADMM enjoys an improved iteration complexity of
˜
O
(
1
/
ϵ
1
−
θ
)
O
\footnote{
˜
O
(
)
O
suppresses a logarithmic factor.} in the deterministic setting and an iteration complexity of
˜
O
(
1
/
ϵ
2
(
1
−
θ
)
)
O
in the stochastic setting without smoothness and strong convexity assumptions. The complexity in either setting improves that of the standard ADMM which only uses a fixed penalty parameter. On the practical side, we demonstrate that the proposed algorithms converge comparably to, if not much faster than, ADMM with a fine-tuned fixed penalty parameter."
neurips,https://proceedings.neurips.cc/paper/2017/file/e9fb2eda3d9c55a0d89c98d6c54b5b3e-Paper.pdf,A Learning Error Analysis for Structured Prediction with Approximate Inference,"Yuanbin Wu, Man Lan, Shiliang Sun, Qi Zhang, Xuanjing Huang",
neurips,https://proceedings.neurips.cc/paper/2017/file/ea159dc9788ffac311592613b7f71fbb-Paper.pdf,Simple strategies for recovering inner products from coarsely quantized random projections,"Ping Li, Martin Slawski",
neurips,https://proceedings.neurips.cc/paper/2017/file/ea204361fe7f024b130143eb3e189a18-Paper.pdf,Trimmed Density Ratio Estimation,"Song Liu, Akiko Takeda, Taiji Suzuki, Kenji Fukumizu",
neurips,https://proceedings.neurips.cc/paper/2017/file/ea6b2efbdd4255a9f1b3bbc6399b58f4-Paper.pdf,Adaptive Batch Size for Safe Policy Gradients,"Matteo Papini, Matteo Pirotta, Marcello Restelli",
neurips,https://proceedings.neurips.cc/paper/2017/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf,Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting,"Rebecca Morrison, Ricardo Baptista, Youssef Marzouk",
neurips,https://proceedings.neurips.cc/paper/2017/file/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Paper.pdf,"REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models","George Tucker, Andriy Mnih, Chris J. Maddison, John Lawson, Jascha Sohl-Dickstein",
neurips,https://proceedings.neurips.cc/paper/2017/file/eddb904a6db773755d2857aacadb1cb0-Paper.pdf,Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues,"Noga Alon, Moshe Babaioff, Yannai A. Gonczarowski, Yishay Mansour, Shay Moran, Amir Yehudayoff",
neurips,https://proceedings.neurips.cc/paper/2017/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf,Tensor Biclustering,"Soheil Feizi, Hamid Javadi, David Tse",
neurips,https://proceedings.neurips.cc/paper/2017/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf,On the Model Shrinkage Effect of Gamma Process Edge Partition Models,"Iku Ohama, Issei Sato, Takuya Kida, Hiroki Arimura","The edge partition model (EPM) is a fundamental Bayesian nonparametric model for extracting an overlapping structure from binary matrix. The EPM adopts a gamma process (
Γ
Γ
P) prior to automatically shrink the number of active atoms. However, we empirically found that the model shrinkage of the EPM does not typically work appropriately and leads to an overfitted solution. An analysis of the expectation of the EPM's intensity function suggested that the gamma priors for the EPM hyperparameters disturb the model shrinkage effect of the internal
Γ
Γ
P. In order to ensure that the model shrinkage effect of the EPM works in an appropriate manner, we proposed two novel generative constructions of the EPM: CEPM incorporating constrained gamma priors, and DEPM incorporating Dirichlet priors instead of the gamma priors. Furthermore, all DEPM's model parameters including the infinite atoms of the
Γ
Γ
P prior could be marginalized out, and thus it was possible to derive a truly infinite DEPM (IDEPM) that can be efficiently inferred using a collapsed Gibbs sampler. We experimentally confirmed that the model shrinkage of the proposed models works well and that the IDEPM indicated state-of-the-art performance in generalization ability, link prediction accuracy, mixing efficiency, and convergence speed."
neurips,https://proceedings.neurips.cc/paper/2017/file/ef72d53990bc4805684c9b61fa64a102-Paper.pdf,Estimating Mutual Information for Discrete-Continuous Mixtures,"Weihao Gao, Sreeram Kannan, Sewoong Oh, Pramod Viswanath",
neurips,https://proceedings.neurips.cc/paper/2017/file/efdf562ce2fb0ad460fd8e9d33e57f57-Paper.pdf,Reconstructing perceived faces from brain activations with deep adversarial neural decoding,"Yağmur Güçlütürk, Umut Güçlü, Katja Seeliger, Sander Bosch, Rob van Lier, Marcel A. J. van Gerven",
neurips,https://proceedings.neurips.cc/paper/2017/file/f016e59c7ad8b1d72903bb1aa5720d53-Paper.pdf,An inner-loop free solution to inverse problems using deep neural networks,"Kai Fan, Qi Wei, Lawrence Carin, Katherine A. Heller",
neurips,https://proceedings.neurips.cc/paper/2017/file/f0204e1d3ee3e4b05de4e2ddbd39e076-Paper.pdf,A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control,"Fanny Yang, Aaditya Ramdas, Kevin G. Jamieson, Martin J. Wainwright",
neurips,https://proceedings.neurips.cc/paper/2017/file/f0935e4cd5920aa6c7c996a5ee53a70f-Paper.pdf,Interactive Submodular Bandit,"Lin Chen, Andreas Krause, Amin Karbasi","In many machine learning applications, submodular functions have been used as a model for evaluating the utility or payoff of a set such as news items to recommend, sensors to deploy in a terrain, nodes to influence in a social network, to name a few. At the heart of all these applications is the assumption that the underlying utility/payoff function is known a priori, hence maximizing it is in principle possible. In real life situations, however, the utility function is not fully known in advance and can only be estimated via interactions. For instance, whether a user likes a movie or not can be reliably evaluated only after it was shown to her. Or, the range of influence of a user in a social network can be estimated only after she is selected to advertise the product. We model such problems as an interactive submodular bandit optimization, where in each round we receive a context (e.g., previously selected movies) and have to choose an action (e.g., propose a new movie). We then receive a noisy feedback about the utility of the action (e.g., ratings) which we model as a submodular function over the context-action space. We develop SM-UCB that efficiently trades off exploration (collecting more data) and exploration (proposing a good action given gathered data) and achieves a
O
(
√
T
)
O
regret bound after
T
T
rounds of interaction. Given a bounded-RKHS norm kernel over the context-action-payoff space that governs the smoothness of the utility function, SM-UCB keeps an upper-confidence bound on the payoff function that allows it to asymptotically achieve no-regret. Finally, we evaluate our results on four concrete applications, including movie recommendation (on the MovieLense data set), news recommendation (on Yahoo! Webscope dataset), interactive influence maximization (on a subset of the Facebook network), and personalized data summarization (on Reuters Corpus). In all these applications, we observe that SM-UCB consistently outperforms the prior art."
neurips,https://proceedings.neurips.cc/paper/2017/file/f0f6ba4b5e0000340312d33c212c3ae8-Paper.pdf,Hash Embeddings for Efficient Word Representations,"Dan Tito Svenstrup, Jonas Hansen, Ole Winther","We present hash embeddings, an efficient method for representing words in a continuous vector form. A hash embedding may be seen as an interpolation between a standard word embedding and a word embedding created using a random hash function (the hashing trick). In hash embeddings each token is represented by
k
k
d
d
-dimensional embeddings vectors and one
k
k
dimensional weight vector. The final
d
d
dimensional representation of the token is the product of the two. Rather than fitting the embedding vectors for each token these are selected by the hashing trick from a shared pool of
B
B
embedding vectors. Our experiments show that hash embeddings can easily deal with huge vocabularies consisting of millions tokens. When using a hash embedding there is no need to create a dictionary before training nor to perform any kind of vocabulary pruning after training. We show that models trained using hash embeddings exhibit at least the same level of performance as models trained using regular embeddings across a wide range of tasks. Furthermore, the number of parameters needed by such an embedding is only a fraction of what is required by a regular embedding. Since standard embeddings and embeddings constructed using the hashing trick are actually just special cases of a hash embedding, hash embeddings can be considered an extension and improvement over the existing regular embedding types."
neurips,https://proceedings.neurips.cc/paper/2017/file/f12ee9734e1edf70ed02d9829018b3d9-Paper.pdf,Learning Low-Dimensional Metrics,"Blake Mason, Lalit Jain, Robert Nowak",
neurips,https://proceedings.neurips.cc/paper/2017/file/f1981e4bd8a0d6d8462016d2fc6276b3-Paper.pdf,Unsupervised Sequence Classification using Sequential Output Statistics,"Yu Liu, Jianshu Chen, Li Deng",
neurips,https://proceedings.neurips.cc/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf,Deep Sets,"Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R. Salakhutdinov, Alexander J. Smola",
neurips,https://proceedings.neurips.cc/paper/2017/file/f231f2107df69eab0a3862d50018a9b2-Paper.pdf,Optimal Shrinkage of Singular Values Under Random Data Contamination,"Danny Barash, Matan Gavish",
neurips,https://proceedings.neurips.cc/paper/2017/file/f24ad6f72d6cc4cb51464f2b29ab69d3-Paper.pdf,Learning Mixture of Gaussians with Streaming Data,"Aditi Raghunathan, Prateek Jain, Ravishankar Krishnawamy","In this paper, we study the problem of learning a mixture of Gaussians with streaming data: given a stream of
N
N
points in
d
d
dimensions generated by an unknown mixture of
k
k
spherical Gaussians, the goal is to estimate the model parameters using a single pass over the data stream. We analyze a streaming version of the popular Lloyd's heuristic and show that the algorithm estimates all the unknown centers of the component Gaussians accurately if they are sufficiently separated. Assuming each pair of centers are
C
σ
C
distant with
C
=
Ω
(
(
k
log
k
)
1
/
4
σ
)
C
and where
σ
2
σ
is the maximum variance of any Gaussian component, we show that asymptotically the algorithm estimates the centers optimally (up to certain constants); our center separation requirement matches the best known result for spherical Gaussians \citep{vempalawang}. For finite samples, we show that a bias term based on the initial estimate decreases at
O
(
1
/
p
o
l
y
(
N
)
)
O
rate while variance decreases at nearly optimal rate of
σ
2
d
/
N
σ
. Our analysis requires seeding the algorithm with a good initial estimate of the true cluster centers for which we provide an online PCA based clustering algorithm. Indeed, the asymptotic per-step time complexity of our algorithm is the optimal
d
⋅
k
d
while space complexity of our algorithm is
O
(
d
k
log
k
)
O
. In addition to the bias and variance terms which tend to
0
0
, the hard-thresholding based updates of streaming Lloyd's algorithm is agnostic to the data distribution and hence incurs an \emph{approximation error} that cannot be avoided. However, by using a streaming version of the classical \emph{(soft-thresholding-based)} EM method that exploits the Gaussian distribution explicitly, we show that for a mixture of two Gaussians the true means can be estimated consistently, with estimation error decreasing at nearly optimal rate, and tending to
0
0
for
N
→
∞
N
."
neurips,https://proceedings.neurips.cc/paper/2017/file/f26dab9bf6a137c3b6782e562794c2f2-Paper.pdf,Learning to Compose Domain-Specific Transformations for Data Augmentation,"Alexander J. Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, Christopher Ré",
neurips,https://proceedings.neurips.cc/paper/2017/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf,Preventing Gradient Explosions in Gated Recurrent Units,"Sekitoshi Kanai, Yasuhiro Fujiwara, Sotetsu Iwamura",
neurips,https://proceedings.neurips.cc/paper/2017/file/f31b20466ae89669f9741e047487eb37-Paper.pdf,Streaming Sparse Gaussian Process Approximations,"Thang D. Bui, Cuong Nguyen, Richard E. Turner",
neurips,https://proceedings.neurips.cc/paper/2017/file/f337d999d9ad116a7b4f3d409fcc6480-Paper.pdf,Differentially Private Empirical Risk Minimization Revisited: Faster and More General,"Di Wang, Minwei Ye, Jinhui Xu","In this paper we study differentially private Empirical Risk Minimization(ERM) in different settings. For smooth (strongly) convex loss function with or without (non)-smooth regularization, we give algorithms which achieve either optimal or near optimal utility bound with less gradient complexity compared with previous work. For ERM with smooth convex loss function in high-dimension(
p
≫
n
p
) setting, we give an algorithm which achieves the upper bound with less gradient complexity than previous ones. At last, we generalize the expected excess empirical risk from convex to Polyak-Lojasiewicz condition and give a tighter upper bound of the utility comparing with the result in \cite{DBLP:journals/corr/ZhangZMW17}."
neurips,https://proceedings.neurips.cc/paper/2017/file/f44ee263952e65b3610b8ba51229d1f9-Paper.pdf,Unbounded cache model for online language modeling with open vocabulary,"Edouard Grave, Moustapha M. Cisse, Armand Joulin",
neurips,https://proceedings.neurips.cc/paper/2017/file/f4552671f8909587cf485ea990207f3b-Paper.pdf,Shape and Material from Sound,"Zhoutong Zhang, Qiujia Li, Zhengjia Huang, Jiajun Wu, Josh Tenenbaum, Bill Freeman",
neurips,https://proceedings.neurips.cc/paper/2017/file/f457c545a9ded88f18ecee47145a72c0-Paper.pdf,On the Consistency of Quick Shift,Heinrich Jiang,
neurips,https://proceedings.neurips.cc/paper/2017/file/f45a1078feb35de77d26b3f7a52ef502-Paper.pdf,Wasserstein Learning of Deep Generative Point Process Models,"Shuai Xiao, Mehrdad Farajtabar, Xiaojing Ye, Junchi Yan, Le Song, Hongyuan Zha",
neurips,https://proceedings.neurips.cc/paper/2017/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf,Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent,"Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, Julien Stainer","We study the resilience to Byzantine failures of distributed implementations of Stochastic Gradient Descent (SGD). So far, distributed machine learning frameworks have largely ignored the possibility of failures, especially arbitrary (i.e., Byzantine) ones. Causes of failures include software bugs, network asynchrony, biases in local datasets, as well as attackers trying to compromise the entire system. Assuming a set of
n
n
workers, up to
f
f
being Byzantine, we ask how resilient can SGD be, without limiting the dimension, nor the size of the parameter space. We first show that no gradient aggregation rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the aggregation rule capturing the basic requirements to guarantee convergence despite
f
f
Byzantine workers. We propose \emph{Krum}, an aggregation rule that satisfies our resilience property, which we argue is the first provably Byzantine-resilient algorithm for distributed SGD. We also report on experimental evaluations of Krum."
neurips,https://proceedings.neurips.cc/paper/2017/file/f507783927f2ec2737ba40afbd17efb5-Paper.pdf,Protein Interface Prediction using Graph Convolutional Networks,"Alex Fout, Jonathon Byrd, Basir Shariat, Asa Ben-Hur",
neurips,https://proceedings.neurips.cc/paper/2017/file/f55cadb97eaff2ba1980e001b0bd9842-Paper.pdf,Convergence rates of a partition based Bayesian multivariate density estimation method,"Linxi Liu, Dangna Li, Wing Hung Wong",
neurips,https://proceedings.neurips.cc/paper/2017/file/f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf,Avoiding Discrimination through Causal Reasoning,"Niki Kilbertus, Mateo Rojas Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, Bernhard Schölkopf",
neurips,https://proceedings.neurips.cc/paper/2017/file/f60bb6bb4c96d4df93c51bd69dcc15a0-Paper.pdf,Alternating Estimation for Structured High-Dimensional Multi-Response Models,"Sheng Chen, Arindam Banerjee",
neurips,https://proceedings.neurips.cc/paper/2017/file/f61d6947467ccd3aa5af24db320235dd-Paper.pdf,Multimodal Learning and Reasoning for Visual Question Answering,"Ilija Ilievski, Jiashi Feng",
neurips,https://proceedings.neurips.cc/paper/2017/file/f69e505b08403ad2298b9f262659929a-Paper.pdf,Generative Local Metric Learning for Kernel Regression,"Yung-Kyun Noh, Masashi Sugiyama, Kee-Eung Kim, Frank Park, Daniel D. Lee",
neurips,https://proceedings.neurips.cc/paper/2017/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf,Overcoming Catastrophic Forgetting by Incremental Moment Matching,"Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, Byoung-Tak Zhang",
neurips,https://proceedings.neurips.cc/paper/2017/file/f75526659f31040afeb61cb7133e4e6d-Paper.pdf,Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent,"Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, Ji Liu",
neurips,https://proceedings.neurips.cc/paper/2017/file/f79921bbae40a577928b76d2fc3edc2a-Paper.pdf,Gradient Descent Can Take Exponential Time to Escape Saddle Points,"Simon S. Du, Chi Jin, Jason D. Lee, Michael I. Jordan, Aarti Singh, Barnabas Poczos",
neurips,https://proceedings.neurips.cc/paper/2017/file/f7e0b956540676a129760a3eae309294-Paper.pdf,Dual Path Networks,"Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, Jiashi Feng",
neurips,https://proceedings.neurips.cc/paper/2017/file/f80bf05527157a8c2a7bb63b22f49aaa-Paper.pdf,Model-based Bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit,"Laurence Aitchison, Lloyd Russell, Adam M. Packer, Jinyao Yan, Philippe Castonguay, Michael Hausser, Srinivas C. Turaga",
neurips,https://proceedings.neurips.cc/paper/2017/file/f80ff32e08a25270b5f252ce39522f72-Paper.pdf,Universal consistency and minimax rates for online Mondrian Forests,"Jaouad Mourtada, Stéphane Gaïffas, Erwan Scornet","We establish the consistency of an algorithm of Mondrian Forests~\cite{lakshminarayanan2014mondrianforests,lakshminarayanan2016mondrianuncertainty}, a randomized classification algorithm that can be implemented online. First, we amend the original Mondrian Forest algorithm proposed in~\cite{lakshminarayanan2014mondrianforests}, that considers a \emph{fixed} lifetime parameter. Indeed, the fact that this parameter is fixed actually hinders statistical consistency of the original procedure. Our modified Mondrian Forest algorithm grows trees with increasing lifetime parameters
λ
n
λ
, and uses an alternative updating rule, allowing to work also in an online fashion. Second, we provide a theoretical analysis establishing simple conditions for consistency. Our theoretical analysis also exhibits a surprising fact: our algorithm achieves the minimax rate (optimal rate) for the estimation of a Lipschitz regression function, which is a strong extension of previous results~\cite{arlot2014purf_bias} to an \emph{arbitrary dimension}."
neurips,https://proceedings.neurips.cc/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf,Gradient Episodic Memory for Continual Learning,"David Lopez-Paz, Marc'Aurelio Ranzato",
neurips,https://proceedings.neurips.cc/paper/2017/file/f8da71e562ff44a2bc7edf3578c593da-Paper.pdf,Variational Inference for Gaussian Process Models with Linear Complexity,"Ching-An Cheng, Byron Boots",
neurips,https://proceedings.neurips.cc/paper/2017/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf,The Reversible Residual Network: Backpropagation Without Storing Activations,"Aidan N. Gomez, Mengye Ren, Raquel Urtasun, Roger B. Grosse",
neurips,https://proceedings.neurips.cc/paper/2017/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf,Language Modeling with Recurrent Highway Hypernetworks,Joseph Suarez,
neurips,https://proceedings.neurips.cc/paper/2017/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf,Parametric Simplex Method for Sparse Learning,"Haotian Pang, Han Liu, Robert J. Vanderbei, Tuo Zhao",
neurips,https://proceedings.neurips.cc/paper/2017/file/fa84632d742f2729dc32ce8cb5d49733-Paper.pdf,Filtering Variational Objectives,"Chris J. Maddison, John Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, Yee Teh",
neurips,https://proceedings.neurips.cc/paper/2017/file/faafda66202d234463057972460c04f5-Paper.pdf,Cold-Start Reinforcement Learning with Softmax Policy Gradient,"Nan Ding, Radu Soricut",
neurips,https://proceedings.neurips.cc/paper/2017/file/facf9f743b083008a894eee7baa16469-Paper.pdf,Bridging the Gap Between Value and Policy Based Reinforcement Learning,"Ofir Nachum, Mohammad Norouzi, Kelvin Xu, Dale Schuurmans",
neurips,https://proceedings.neurips.cc/paper/2017/file/fb2606a5068901da92473666256e6e5b-Paper.pdf,Asynchronous Coordinate Descent under More Realistic Assumptions,"Tao Sun, Robert Hannah, Wotao Yin",
neurips,https://proceedings.neurips.cc/paper/2017/file/fb3f76858cb38e5b7fd113e0bc1c0721-Paper.pdf,"EEG-GRAPH: A Factor-Graph-Based Model for Capturing Spatial, Temporal, and Observational Relationships in Electroencephalograms","Yogatheesan Varatharajah, Min Jin Chong, Krishnakant Saboo, Brent Berry, Benjamin Brinkmann, Gregory Worrell, Ravishankar Iyer",
neurips,https://proceedings.neurips.cc/paper/2017/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf,Natural Value Approximators: Learning when to Trust Past Estimates,"Zhongwen Xu, Joseph Modayil, Hado P. van Hasselt, Andre Barreto, David Silver, Tom Schaul",
neurips,https://proceedings.neurips.cc/paper/2017/file/fb89fd138b104dcf8e2077ad2a23954d-Paper.pdf,Active Exploration for Learning Symbolic Representations,"Garrett Andersen, George Konidaris",
neurips,https://proceedings.neurips.cc/paper/2017/file/fc79250f8c5b804390e8da280b4cf06e-Paper.pdf,Balancing information exposure in social networks,"Kiran Garimella, Aristides Gionis, Nikos Parotsidis, Nikolaj Tatti",
neurips,https://proceedings.neurips.cc/paper/2017/file/fca0789e7891cbc0583298a238316122-Paper.pdf,Nonlinear Acceleration of Stochastic Algorithms,"Damien Scieur, Francis Bach, Alexandre d'Aspremont",
neurips,https://proceedings.neurips.cc/paper/2017/file/fcdf25d6e191893e705819b177cddea0-Paper.pdf,Multi-way Interacting Regression via Factorization Machines,"Mikhail Yurochkin, XuanLong Nguyen, nikolaos Vasiloglou",
neurips,https://proceedings.neurips.cc/paper/2017/file/fd69dbe29f156a7ef876a40a94f65599-Paper.pdf,The Expxorcist: Nonparametric Graphical Models Via Conditional Exponential Densities,"Arun Suggala, Mladen Kolar, Pradeep K. Ravikumar",
neurips,https://proceedings.neurips.cc/paper/2017/file/fe2d010308a6b3799a3d9c728ee74244-Paper.pdf,Generating steganographic images via adversarial training,"Jamie Hayes, George Danezis",
neurips,https://proceedings.neurips.cc/paper/2017/file/fec8d47d412bcbeece3d9128ae855a7a-Paper.pdf,NeuralFDR: Learning Discovery Thresholds from Hypothesis Features,"Fei Xia, Martin J. Zhang, James Y. Zou, David Tse",
neurips,https://proceedings.neurips.cc/paper/2017/file/fed33392d3a48aa149a87a38b875ba4a-Paper.pdf,A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis,Tor Lattimore,
neurips,https://proceedings.neurips.cc/paper/2017/file/ffbd6cbb019a1413183c8d08f2929307-Paper.pdf,Value Prediction Network,"Junhyuk Oh, Satinder Singh, Honglak Lee",
neurips,https://proceedings.neurips.cc/paper/2017/file/ffeabd223de0d4eacb9a3e6e53e5448d-Paper.pdf,Detrended Partial Cross Correlation for Brain Connectivity Analysis,"Jaime Ide, Fábio Cappabianco, Fabio Faria, Chiang-shan R. Li",
