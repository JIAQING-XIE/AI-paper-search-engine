conference,url,title,authors,abstract
neurips,https://proceedings.neurips.cc/paper/2010/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf,Agnostic Active Learning Without Constraints,"Alina Beygelzimer, Daniel J. Hsu, John Langford, Tong Zhang",
neurips,https://proceedings.neurips.cc/paper/2010/file/00e26af6ac3b1c1c49d7c3d79c60d000-Paper.pdf,A Dirty Model for Multi-task Learning,"Ali Jalali, Sujay Sanghavi, Chao Ruan, Pradeep Ravikumar","We consider the multiple linear regression problem, in a setting where some of the set of relevant features could be shared across the tasks. A lot of recent research has studied the use of
ℓ
1
/
ℓ
q
ℓ
norm block-regularizations with
q
>
1
q
for such (possibly) block-structured problems, establishing strong guarantees on recovery even under high-dimensional scaling where the number of features scale with the number of observations. However, these papers also caution that the performance of such block-regularized methods are very dependent on the {\em extent} to which the features are shared across tasks. Indeed they show~\citep{NWJoint} that if the extent of overlap is less than a threshold, or even if parameter {\em values} in the shared features are highly uneven, then block
ℓ
1
/
ℓ
q
ℓ
regularization could actually perform {\em worse} than simple separate elementwise
ℓ
1
ℓ
regularization. We are far away from a realistic multi-task setting: not only do the set of relevant features have to be exactly the same across tasks, but their values have to as well. Here, we ask the question: can we leverage support and parameter overlap when it exists, but not pay a penalty when it does not? Indeed, this falls under a more general question of whether we can model such \emph{dirty data} which may not fall into a single neat structural bracket (all block-sparse, or all low-rank and so on). Here, we take a first step, focusing on developing a dirty model for the multiple regression problem. Our method uses a very simple idea: we decompose the parameters into two components and {\em regularize these differently.} We show both theoretically and empirically, our method strictly and noticeably outperforms both
ℓ
1
ℓ
and
ℓ
1
/
ℓ
q
ℓ
methods, over the entire range of possible overlaps. We also provide theoretical guarantees that the method performs well under high-dimensional scaling."
neurips,https://proceedings.neurips.cc/paper/2010/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf,Generative Local Metric Learning for Nearest Neighbor Classification,"Yung-kyun Noh, Byoung-tak Zhang, Daniel Lee",
neurips,https://proceedings.neurips.cc/paper/2010/file/01882513d5fa7c329e940dda99b12147-Paper.pdf,Relaxed Clipping: A Global Training Method for Robust Regression and Classification,"Min Yang, Linli Xu, Martha White, Dale Schuurmans, Yao-liang Yu",
neurips,https://proceedings.neurips.cc/paper/2010/file/0188e8b8b014829e2fa0f430f0a95961-Paper.pdf,Linear readout from a neural population with partial correlation data,"Adrien Wohrer, Ranulfo Romo, Christian K. Machens",
neurips,https://proceedings.neurips.cc/paper/2010/file/019d385eb67632a7e958e23f24bd07d7-Paper.pdf,On Herding and the Perceptron Cycling Theorem,"Andrew Gelfand, Yutian Chen, Laurens Maaten, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2010/file/01f78be6f7cad02658508fe4616098a9-Paper.pdf,Tiled convolutional neural networks,"Jiquan Ngiam, Zhenghao Chen, Daniel Chia, Pang Koh, Quoc Le, Andrew Ng",
neurips,https://proceedings.neurips.cc/paper/2010/file/03c6b06952c750899bb03d998e631860-Paper.pdf,Decomposing Isotonic Regression for Efficiently Solving Large Problems,"Ronny Luss, Saharon Rosset, Moni Shahar",
neurips,https://proceedings.neurips.cc/paper/2010/file/05049e90fa4f5039a8cadc6acbb4b2cc-Paper.pdf,Learning Kernels with Radiuses of Minimum Enclosing Balls,"Kun Gai, Guangyun Chen, Chang-shui Zhang",
neurips,https://proceedings.neurips.cc/paper/2010/file/06138bc5af6023646ede0e1f7c1eac75-Paper.pdf,Label Embedding Trees for Large Multi-Class Tasks,"Samy Bengio, Jason Weston, David Grangier",
neurips,https://proceedings.neurips.cc/paper/2010/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf,Deep Coding Network,"Yuanqing Lin, Tong Zhang, Shenghuo Zhu, Kai Yu",
neurips,https://proceedings.neurips.cc/paper/2010/file/06409663226af2f3114485aa4e0a23b4-Paper.pdf,Transduction with Matrix Completion: Three Birds with One Stone,"Andrew Goldberg, Ben Recht, Junming Xu, Robert Nowak, Jerry Zhu",
neurips,https://proceedings.neurips.cc/paper/2010/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf,Extended Bayesian Information Criteria for Gaussian Graphical Models,"Rina Foygel, Mathias Drton",
neurips,https://proceedings.neurips.cc/paper/2010/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf,Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces,"Abhinav Gupta, Martial Hebert, Takeo Kanade, David Blei",
neurips,https://proceedings.neurips.cc/paper/2010/file/07871915a8107172b3b5dc15a6574ad3-Paper.pdf,A Computational Decision Theory for Interactive Assistants,"Alan Fern, Prasad Tadepalli",
neurips,https://proceedings.neurips.cc/paper/2010/file/087408522c31eeb1f982bc0eaf81d35f-Paper.pdf,Large Margin Multi-Task Metric Learning,"Shibin Parameswaran, Kilian Q. Weinberger",
neurips,https://proceedings.neurips.cc/paper/2010/file/08b255a5d42b89b0585260b6f2360bdd-Paper.pdf,Evaluating neuronal codes for inference using Fisher information,"Haefner Ralf, Matthias Bethge",
neurips,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf,Guaranteed Rank Minimization via Singular Value Projection,"Prateek Jain, Raghu Meka, Inderjit Dhillon",
neurips,https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf,Double Q-learning,Hado Hasselt,
neurips,https://proceedings.neurips.cc/paper/2010/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf,Generalized roof duality and bisubmodular functions,Vladimir Kolmogorov,"Consider a convex relaxation
^
f
f
of a pseudo-boolean function
f
f
. We say that the relaxation is {\em totally half-integral} if
^
f
(
\bx
)
f
is a polyhedral function with half-integral extreme points
\bx
\bx
, and this property is preserved after adding an arbitrary combination of constraints of the form
x
i
=
x
j
x
,
x
i
=
1
−
x
j
x
, and
x
i
=
γ
x
where
γ
∈
{
0
,
1
,
1
2
}
γ
is a constant. A well-known example is the {\em roof duality} relaxation for quadratic pseudo-boolean functions
f
f
. We argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions. Our contributions are as follows. First, we provide a complete characterization of totally half-integral relaxations
^
f
f
by establishing a one-to-one correspondence with {\em bisubmodular functions}. Second, we give a new characterization of bisubmodular functions. Finally, we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality."
neurips,https://proceedings.neurips.cc/paper/2010/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf,"Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization","Feiping Nie, Heng Huang, Xiao Cai, Chris Ding",
neurips,https://proceedings.neurips.cc/paper/2010/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Paper.pdf,Repeated Games against Budgeted Adversaries,"Jacob D. Abernethy, Manfred K. K. Warmuth",
neurips,https://proceedings.neurips.cc/paper/2010/file/0aa1883c6411f7873cb83dacb17b0afc-Paper.pdf,Switching state space model for simultaneously estimating state transitions and nonstationary firing rates,"Ken Takiyama, Masato Okada",
neurips,https://proceedings.neurips.cc/paper/2010/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf,Why are some word orders more common than others? A uniform information density account,"Luke Maurits, Dan Navarro, Amy Perfors",
neurips,https://proceedings.neurips.cc/paper/2010/file/0d0871f0806eae32d30983b62252da50-Paper.pdf,Getting lost in space: Large sample analysis of the resistance distance,"Ulrike Luxburg, Agnes Radl, Matthias Hein",
neurips,https://proceedings.neurips.cc/paper/2010/file/0d0fd7c6e093f7b804fa0150b875b868-Paper.pdf,Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers,"Manas Pathak, Shantanu Rane, Bhiksha Raj",
neurips,https://proceedings.neurips.cc/paper/2010/file/0e9fa1f3e9e66792401a6972d477dcc3-Paper.pdf,Convex Multiple-Instance Learning by Estimating Likelihood Ratio,"Fuxin Li, Cristian Sminchisescu",
neurips,https://proceedings.neurips.cc/paper/2010/file/0f2c9a93eea6f38fabb3acb1c31488c6-Paper.pdf,Short-term memory in neuronal networks through dynamical compressed sensing,"Surya Ganguli, Haim Sompolinsky",
neurips,https://proceedings.neurips.cc/paper/2010/file/0f9cafd014db7a619ddb4276af0d692c-Paper.pdf,The Multidimensional Wisdom of Crowds,"Peter Welinder, Steve Branson, Pietro Perona, Serge Belongie",
neurips,https://proceedings.neurips.cc/paper/2010/file/0ff39bbbf981ac0151d340c9aa40e63e-Paper.pdf,Boosting Classifier Cascades,"Nuno Vasconcelos, Mohammad Saberian",
neurips,https://proceedings.neurips.cc/paper/2010/file/115f89503138416a242f40fb7d7f338e-Paper.pdf,Implicitly Constrained Gaussian Process Regression for Monocular Non-Rigid Pose Estimation,"Mathieu Salzmann, Raquel Urtasun",
neurips,https://proceedings.neurips.cc/paper/2010/file/11b921ef080f7736089c757404650e40-Paper.pdf,Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks,"Kentaro Katahira, Kazuo Okanoya, Masato Okada",
neurips,https://proceedings.neurips.cc/paper/2010/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf,Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains,"Martha White, Adam White",
neurips,https://proceedings.neurips.cc/paper/2010/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf,Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification,"Li-jia Li, Hao Su, Li Fei-fei, Eric Xing",
neurips,https://proceedings.neurips.cc/paper/2010/file/15d4e891d784977cacbfcbb00c48f133-Paper.pdf,Using body-anchored priors for identifying actions in single images,"Leonid Karlinsky, Michael Dinerstein, Shimon Ullman",
neurips,https://proceedings.neurips.cc/paper/2010/file/168908dd3227b8358eababa07fcaf091-Paper.pdf,Reward Design via Online Gradient Ascent,"Jonathan Sorg, Richard L. Lewis, Satinder Singh",
neurips,https://proceedings.neurips.cc/paper/2010/file/182be0c5cdcd5072bb1864cdee4d3d6e-Paper.pdf,Universal Consistency of Multi-Class Support Vector Classification,Tobias Glasmachers,
neurips,https://proceedings.neurips.cc/paper/2010/file/18997733ec258a9fcaf239cc55d53363-Paper.pdf,Supervised Clustering,"Pranjal Awasthi, Reza Zadeh",
neurips,https://proceedings.neurips.cc/paper/2010/file/19b650660b253761af189682e03501dd-Paper.pdf,Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics,"Kanaka Rajan, L Abbott, Haim Sompolinsky",
neurips,https://proceedings.neurips.cc/paper/2010/file/19f3cd308f1455b3fa09a282e0d496f4-Paper.pdf,Distributionally Robust Markov Decision Processes,"Huan Xu, Shie Mannor",
neurips,https://proceedings.neurips.cc/paper/2010/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Paper.pdf,Empirical Risk Minimization with Approximations of Probabilistic Grammars,"Noah A. Smith, Shay Cohen",
neurips,https://proceedings.neurips.cc/paper/2010/file/1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf,MAP Estimation for Graphical Models by Likelihood Maximization,"Akshat Kumar, Shlomo Zilberstein","Computing a {\em maximum a posteriori} (MAP) assignment in graphical models is a crucial inference problem for many practical applications. Several provably convergent approaches have been successfully developed using linear programming (LP) relaxation of the MAP problem. We present an alternative approach, which transforms the MAP problem into that of inference in a finite mixture of simple Bayes nets. We then derive the Expectation Maximization (EM) algorithm for this mixture that also monotonically increases a lower bound on the MAP assignment until convergence. The update equations for the EM algorithm are remarkably simple, both conceptually and computationally, and can be implemented using a graph-based message passing paradigm similar to max-product computation. We experiment on the real-world protein design dataset and show that EM's convergence rate is significantly higher than the previous LP relaxation based approach MPLP. EM achieves a solution quality within
95
95
\% of optimal for most instances and is often an order-of-magnitude faster than MPLP."
neurips,https://proceedings.neurips.cc/paper/2010/file/1cc3633c579a90cfdd895e64021e2163-Paper.pdf,Identifying graph-structured activation patterns in networks,"James Sharpnack, Aarti Singh",
neurips,https://proceedings.neurips.cc/paper/2010/file/1d72310edc006dadf2190caad5802983-Paper.pdf,Size Matters: Metric Visual Search Constraints from Monocular Metadata,"Mario Fritz, Kate Saenko, Trevor Darrell",
neurips,https://proceedings.neurips.cc/paper/2010/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf,Near-Optimal Bayesian Active Learning with Noisy Observations,"Daniel Golovin, Andreas Krause, Debajyoti Ray",
neurips,https://proceedings.neurips.cc/paper/2010/file/1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf,Probabilistic Belief Revision with Structural Constraints,"Peter Jones, Venkatesh Saligrama, Sanjoy Mitter",
neurips,https://proceedings.neurips.cc/paper/2010/file/1f50893f80d6830d62765ffad7721742-Paper.pdf,Structured Determinantal Point Processes,"Alex Kulesza, Ben Taskar",
neurips,https://proceedings.neurips.cc/paper/2010/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf,b-Bit Minwise Hashing for Estimating Three-Way Similarities,"Ping Li, Arnd Konig, Wenhao Gui","Computing two-way and multi-way set similarities is a fundamental problem. This study focuses on estimating 3-way resemblance (Jaccard similarity) using b-bit minwise hashing. While traditional minwise hashing methods store each hashed value using 64 bits, b-bit minwise hashing only stores the lowest b bits (where b>= 2 for 3-way). The extension to 3-way similarity from the prior work on 2-way similarity is technically non-trivial. We develop the precise estimator which is accurate and very complicated; and we recommend a much simplified estimator suitable for sparse data. Our analysis shows that
b
b
-bit minwise hashing can normally achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy of the 3-way resemblance."
neurips,https://proceedings.neurips.cc/paper/2010/file/20f07591c6fcb220ffe637cda29bb3f6-Paper.pdf,"Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike",Stella Yu,
neurips,https://proceedings.neurips.cc/paper/2010/file/242c100dc94f871b6d7215b868a875f8-Paper.pdf,Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting,"Chris Barber, Joseph Bockhorst, Paul Roebber",
neurips,https://proceedings.neurips.cc/paper/2010/file/24896ee4c6526356cc127852413ea3b4-Paper.pdf,Link Discovery using Graph Feature Tracking,"Emile Richard, Nicolas Baskiotis, Theodoros Evgeniou, Nicolas Vayatis",
neurips,https://proceedings.neurips.cc/paper/2010/file/25b2822c2f5a3230abfadd476e8b04c9-Paper.pdf,A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model,"Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz",
neurips,https://proceedings.neurips.cc/paper/2010/file/2723d092b63885e0d7c260cc007e8b9d-Paper.pdf,Sparse Inverse Covariance Selection via Alternating Linearization Methods,"Katya Scheinberg, Shiqian Ma, Donald Goldfarb","Gaussian graphical models are of great interest in statistical learning. Because the conditional independencies between different nodes correspond to zero entries in the inverse covariance matrix of the Gaussian distribution, one can learn the structure of the graph by estimating a sparse inverse covariance matrix from sample data, by solving a convex maximum likelihood problem with an
ℓ
1
ℓ
-regularization term. In this paper, we propose a first-order method based on an alternating linearization technique that exploits the problem's special structure; in particular, the subproblems solved in each iteration have closed-form solutions. Moreover, our algorithm obtains an
ϵ
ϵ
-optimal solution in
O
(
1
/
ϵ
)
O
iterations. Numerical experiments on both synthetic and real data from gene association networks show that a practical version of this algorithm outperforms other competitive algorithms."
neurips,https://proceedings.neurips.cc/paper/2010/file/27ed0fb950b856b06e1273989422e7d3-Paper.pdf,Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories,"George Konidaris, Scott Kuindersma, Roderic Grupen, Andrew Barto",
neurips,https://proceedings.neurips.cc/paper/2010/file/286674e3082feb7e5afb92777e48821f-Paper.pdf,Trading off Mistakes and Don't-Know Predictions,"Amin Sayedi, Morteza Zadimoghaddam, Avrim Blum",
neurips,https://proceedings.neurips.cc/paper/2010/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf,Evaluation of Rarity of Fingerprints in Forensics,"Chang Su, Sargur Srihari",
neurips,https://proceedings.neurips.cc/paper/2010/file/289dff07669d7a23de0ef88d2f7129e7-Paper.pdf,(RF)^2 -- Random Forest Random Field,"Nadia Payet, Sinisa Todorovic",
neurips,https://proceedings.neurips.cc/paper/2010/file/298923c8190045e91288b430794814c4-Paper.pdf,Online Learning in The Manifold of Low-Rank Matrices,"Uri Shalit, Daphna Weinshall, Gal Chechik",
neurips,https://proceedings.neurips.cc/paper/2010/file/2a084e55c87b1ebcdaad1f62fdbbac8e-Paper.pdf,Variational bounds for mixed-data factor analysis,"Mohammad Emtiyaz E. Khan, Guillaume Bouchard, Kevin P. Murphy, Benjamin M. Marlin",
neurips,https://proceedings.neurips.cc/paper/2010/file/2a79ea27c279e471f4d180b08d62b00a-Paper.pdf,Copula Bayesian Networks,Gal Elidan,
neurips,https://proceedings.neurips.cc/paper/2010/file/2ac2406e835bd49c70469acae337d292-Paper.pdf,Evidence-Specific Structures for Rich Tractable CRFs,"Anton Chechetka, Carlos Guestrin",
neurips,https://proceedings.neurips.cc/paper/2010/file/2b44928ae11fb9384c4cf38708677c48-Paper.pdf,Beyond Actions: Discriminative Models for Contextual Group Activities,"Tian Lan, Yang Wang, Weilong Yang, Greg Mori",
neurips,https://proceedings.neurips.cc/paper/2010/file/2b8a61594b1f4c4db0902a8a395ced93-Paper.pdf,Decoding Ipsilateral Finger Movements from ECoG Signals in Humans,"Yuzong Liu, Mohit Sharma, Charles Gaona, Jonathan Breshears, Jarod Roland, Zachary Freudenburg, Eric Leuthardt, Kilian Q. Weinberger",
neurips,https://proceedings.neurips.cc/paper/2010/file/2bcab9d935d219641434683dd9d18a03-Paper.pdf,New Adaptive Algorithms for Online Classification,"Francesco Orabona, Koby Crammer",
neurips,https://proceedings.neurips.cc/paper/2010/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf,Phoneme Recognition with Large Hierarchical Reservoirs,"Fabian Triefenbach, Azarakhsh Jalalvand, Benjamin Schrauwen, Jean-pierre Martens",
neurips,https://proceedings.neurips.cc/paper/2010/file/2cbca44843a864533ec05b321ae1f9d1-Paper.pdf,Learning Multiple Tasks using Manifold Regularization,"Arvind Agarwal, Samuel Gerber, Hal Daume",
neurips,https://proceedings.neurips.cc/paper/2010/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Paper.pdf,Group Sparse Coding with a Laplacian Scale Mixture Prior,"Pierre Garrigues, Bruno Olshausen",
neurips,https://proceedings.neurips.cc/paper/2010/file/2dace78f80bc92e6d7493423d729448e-Paper.pdf,Fractionally Predictive Spiking Neurons,"Jaldert Rombouts, Sander Bohte",
neurips,https://proceedings.neurips.cc/paper/2010/file/301ad0e3bd5cb1627a2044908a42fdc2-Paper.pdf,Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models,"Han Liu, Kathryn Roeder, Larry Wasserman","A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include
K
K
-fold cross-validation (
K
K
-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size asymptotically increases with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including
K
K
-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all competing procedures."
neurips,https://proceedings.neurips.cc/paper/2010/file/32b30a250abd6331e03a2a1f16466346-Paper.pdf,More data means less inference: A pseudo-max approach to structured learning,"David Sontag, Ofer Meshi, Amir Globerson, Tommi Jaakkola",
neurips,https://proceedings.neurips.cc/paper/2010/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf,Lifted Inference Seen from the Other Side : The Tractable Features,"Abhay Jha, Vibhav Gogate, Alexandra Meliou, Dan Suciu",
neurips,https://proceedings.neurips.cc/paper/2010/file/3473decccb0509fb264818a7512a8b9b-Paper.pdf,Predictive State Temporal Difference Learning,"Byron Boots, Geoffrey J. Gordon",
neurips,https://proceedings.neurips.cc/paper/2010/file/35051070e572e47d2c26c241ab88307f-Paper.pdf,Identifying Dendritic Processing,"Aurel A. Lazar, Yevgeniy Slutskiy",
neurips,https://proceedings.neurips.cc/paper/2010/file/35cf8659cfcb13224cbd47863a34fc58-Paper.pdf,On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient,"Tang Jie, Pieter Abbeel","Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (a) using the past experience to estimate {\em only} the gradient of the expected return
U
(
θ
)
U
at the current policy parameterization
θ
θ
, rather than to obtain a more complete estimate of
U
(
θ
)
U
, and (b) using past experience under the current policy {\em only} rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines---a new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds."
neurips,https://proceedings.neurips.cc/paper/2010/file/35f4a8d465e6e1edc05f3d8ab658c551-Paper.pdf,Functional Geometry Alignment and Localization of Brain Areas,"Georg Langs, Yanmei Tie, Laura Rigolo, Alexandra Golby, Polina Golland",
neurips,https://proceedings.neurips.cc/paper/2010/file/3621f1454cacf995530ea53652ddf8fb-Paper.pdf,Multi-View Active Learning in the Non-Realizable Case,"Wei Wang, Zhi-Hua Zhou","The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multi-view setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multi-view active learning can be
˜
O
(
log
1
ϵ
)
O
, contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is
˜
O
(
1
ϵ
)
O
, where the order of
1
/
ϵ
1
is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of
1
/
ϵ
1
is related to the parameter in Tsybakov noise."
neurips,https://proceedings.neurips.cc/paper/2010/file/37a749d808e46495a8da1e5352d03cae-Paper.pdf,Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures,"Kamiya Motwani, Nagesh Adluru, Chris Hinrichs, Andrew Alexander, Vikas Singh",
neurips,https://proceedings.neurips.cc/paper/2010/file/39059724f73a9969845dfe4146c5660e-Paper.pdf,Over-complete representations on recurrent neural networks can support persistent percepts,"Shaul Druckmann, Dmitri Chklovskii",
neurips,https://proceedings.neurips.cc/paper/2010/file/390e982518a50e280d8e2b535462ec1f-Paper.pdf,Non-Stochastic Bandit Slate Problems,"Satyen Kale, Lev Reyzin, Robert E. Schapire",
neurips,https://proceedings.neurips.cc/paper/2010/file/39461a19e9eddfb385ea76b26521ea48-Paper.pdf,Large Margin Learning of Upstream Scene Understanding Models,"Jun Zhu, Li-jia Li, Li Fei-fei, Eric Xing",
neurips,https://proceedings.neurips.cc/paper/2010/file/3a029f04d76d32e79367c4b3255dda4d-Paper.pdf,Switched Latent Force Models for Movement Segmentation,"Mauricio Alvarez, Jan Peters, Neil Lawrence, Bernhard Schölkopf",
neurips,https://proceedings.neurips.cc/paper/2010/file/3cf166c6b73f030b4f67eeaeba301103-Paper.pdf,Adaptive Multi-Task Lasso: with Application to eQTL Detection,"Seunghak Lee, Jun Zhu, Eric Xing",
neurips,https://proceedings.neurips.cc/paper/2010/file/3dc4876f3f08201c7c76cb71fa1da439-Paper.pdf,Categories and Functional Units: An Infinite Hierarchical Model for Brain Activations,"Danial Lashkari, Ramesh Sridharan, Polina Golland",
neurips,https://proceedings.neurips.cc/paper/2010/file/3def184ad8f4755ff269862ea77393dd-Paper.pdf,Random Projection Trees Revisited,"Aman Dhesi, Purushottam Kar",
neurips,https://proceedings.neurips.cc/paper/2010/file/41ae36ecb9b3eee609d05b90c14222fb-Paper.pdf,Joint Analysis of Time-Evolving Binary Matrices and Associated Documents,"Eric Wang, Dehong Liu, Jorge Silva, Lawrence Carin, David Dunson",
neurips,https://proceedings.neurips.cc/paper/2010/file/42998cf32d552343bc8e460416382dca-Paper.pdf,Discriminative Clustering by Regularized Information Maximization,"Andreas Krause, Pietro Perona, Ryan Gomes",
neurips,https://proceedings.neurips.cc/paper/2010/file/42a0e188f5033bc65bf8d78622277c4e-Paper.pdf,Learning to localise sounds with spiking neural networks,"Dan Goodman, Romain Brette",
neurips,https://proceedings.neurips.cc/paper/2010/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf,Dynamic Infinite Relational Model for Time-varying Relational Data Analysis,"Katsuhiko Ishiguro, Tomoharu Iwata, Naonori Ueda, Joshua Tenenbaum",
neurips,https://proceedings.neurips.cc/paper/2010/file/43baa6762fa81bb43b39c62553b2970d-Paper.pdf,Exact learning curves for Gaussian process regression on large random graphs,"Matthew Urry, Peter Sollich",
neurips,https://proceedings.neurips.cc/paper/2010/file/4476b929e30dd0c4e8bdbcc82c6ba23a-Paper.pdf,Sparse Instrumental Variables (SPIV) for Genome-Wide Studies,"Paul Mckeigue, Jon Krohn, Amos J. Storkey, Felix Agakov","This paper describes a probabilistic framework for studying associations between multiple genotypes, biomarkers, and phenotypic traits in the presence of noise and unobserved confounders for large genetic studies. The framework builds on sparse linear methods developed for regression and modified here for inferring causal structures of richer networks with latent variables. The method is motivated by the use of genotypes as
instruments'' to infer causal associations between phenotypic biomarkers and outcomes, without making the common restrictive assumptions of instrumental variable methods. The method may be used for an effective screening of potentially interesting genotype phenotype and biomarker-phenotype associations in genome-wide studies, which may have important implications for validating biomarkers as possible proxy endpoints for early stage clinical trials. Where the biomarkers are gene transcripts, the method can be used for fine mapping of quantitative trait loci (QTLs) detected in genetic linkage studies. The method is applied for examining effects of gene transcript levels in the liver on plasma HDL cholesterol levels for a sample of sequenced mice from a heterogeneous stock, with
∼
10
5
∼
genetic instruments and
∼
47
×
10
3
∼
gene transcripts."
neurips,https://proceedings.neurips.cc/paper/2010/file/44c4c17332cace2124a1a836d9fc4b6f-Paper.pdf,Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks,"Atsushi Miyamae, Yuichi Nagata, Isao Ono, Shigenobu Kobayashi",
neurips,https://proceedings.neurips.cc/paper/2010/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Paper.pdf,Kernel Descriptors for Visual Recognition,"Liefeng Bo, Xiaofeng Ren, Dieter Fox",
neurips,https://proceedings.neurips.cc/paper/2010/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf,Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning,"Matthias Broecheler, Lise Getoor",
neurips,https://proceedings.neurips.cc/paper/2010/file/46922a0880a8f11f8f69cbb52b1396be-Paper.pdf,Gaussian Process Preference Elicitation,"Shengbo Guo, Scott Sanner, Edwin V. Bonilla",
neurips,https://proceedings.neurips.cc/paper/2010/file/46fc943ecd56441056a560ba37d0b9e8-Paper.pdf,A Theory of Multiclass Boosting,"Indraneel Mukherjee, Robert E. Schapire",
neurips,https://proceedings.neurips.cc/paper/2010/file/470e7a4f017a5476afb7eeb3f8b96f9b-Paper.pdf,Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning,"Prateek Jain, Sudheendra Vijayanarasimhan, Kristen Grauman",
neurips,https://proceedings.neurips.cc/paper/2010/file/47a658229eb2368a99f1d032c8848542-Paper.pdf,Bootstrapping Apprenticeship Learning,"Abdeslam Boularias, Brahim Chaib-draa",
neurips,https://proceedings.neurips.cc/paper/2010/file/4a213d37242bdcad8e7300e202e7caa4-Paper.pdf,Co-regularization Based Semi-supervised Domain Adaptation,"Abhishek Kumar, Avishek Saha, Hal Daume",
neurips,https://proceedings.neurips.cc/paper/2010/file/4b0a59ddf11c58e7446c9df0da541a84-Paper.pdf,Structured sparsity-inducing norms through submodular functions,Francis Bach,
neurips,https://proceedings.neurips.cc/paper/2010/file/4c27cea8526af8cfee3be5e183ac9605-Paper.pdf,Optimal Web-Scale Tiering as a Flow Problem,"Gilbert Leung, Novi Quadrianto, Kostas Tsioutsiouliklis, Alex Smola",
neurips,https://proceedings.neurips.cc/paper/2010/file/4d5b995358e7798bc7e9d9db83c612a5-Paper.pdf,Slice sampling covariance hyperparameters of latent Gaussian models,"Iain Murray, Ryan P. Adams",
neurips,https://proceedings.neurips.cc/paper/2010/file/4da04049a062f5adfe81b67dd755cecc-Paper.pdf,Efficient Optimization for Discriminative Latent Class Models,"Armand Joulin, Jean Ponce, Francis Bach",
neurips,https://proceedings.neurips.cc/paper/2010/file/4e0cb6fb5fb446d1c92ede2ed8780188-Paper.pdf,Universal Kernels on Non-Standard Input Spaces,"Andreas Christmann, Ingo Steinwart","During the last years support vector machines (SVMs) have been successfully applied even in situations where the input space
X
X
is not necessarily a subset of
R
d
R
. Examples include SVMs using probability measures to analyse e.g. histograms or coloured images, SVMs for text classification and web mining, and SVMs for applications from computational biology using, e.g., kernels for trees and graphs. Moreover, SVMs are known to be consistent to the Bayes risk, if either the input space is a complete separable metric space and the reproducing kernel Hilbert space (RKHS)
H
⊂
L
p
(
P
X
)
H
is dense, or if the SVM is based on a universal kernel
k
k
. So far, however, there are no RKHSs of practical interest known that satisfy these assumptions on
\cH
\cH
or
k
k
if
X
⊄
R
d
X
. We close this gap by providing a general technique based on Taylor-type kernels to explicitly construct universal kernels on compact metric spaces which are not subset of
R
d
R
. We apply this technique for the following special cases: universal kernels on the set of probability measures, universal kernels based on Fourier transforms, and universal kernels for signal processing."
neurips,https://proceedings.neurips.cc/paper/2010/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Paper.pdf,Worst-Case Linear Discriminant Analysis,"Yu Zhang, Dit-Yan Yeung",
neurips,https://proceedings.neurips.cc/paper/2010/file/51d92be1c60d1db1d2e5e7a07da55b26-Paper.pdf,Learning Multiple Tasks with a Sparse Matrix-Normal Penalty,"Yi Zhang, Jeff Schneider",
neurips,https://proceedings.neurips.cc/paper/2010/file/537d9b6c927223c796cac288cced29df-Paper.pdf,Network Flow Algorithms for Structured Sparsity,"Julien Mairal, Rodolphe Jenatton, Francis Bach, Guillaume R. Obozinski","We consider a class of learning problems that involve a structured sparsity-inducing norm defined as the sum of
ℓ
∞
ℓ
-norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a specific hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network flow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost flow problem. We propose an efficient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of groups and variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems."
neurips,https://proceedings.neurips.cc/paper/2010/file/5487315b1286f907165907aa8fc96619-Paper.pdf,Active Learning by Querying Informative and Representative Examples,"Sheng-jun Huang, Rong Jin, Zhi-Hua Zhou",
neurips,https://proceedings.neurips.cc/paper/2010/file/550a141f12de6341fba65b0ad0433500-Paper.pdf,Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets,"Paolo Viappiani, Craig Boutilier",
neurips,https://proceedings.neurips.cc/paper/2010/file/556f391937dfd4398cbac35e050a2177-Paper.pdf,Semi-Supervised Learning with Adversarially Missing Label Information,"Umar Syed, Ben Taskar",
neurips,https://proceedings.neurips.cc/paper/2010/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Paper.pdf,Gated Softmax Classification,"Roland Memisevic, Christopher Zach, Marc Pollefeys, Geoffrey E. Hinton",
neurips,https://proceedings.neurips.cc/paper/2010/file/577ef1154f3240ad5b9b413aa7346a1e-Paper.pdf,Estimation of Rényi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs,"Dávid Pál, Barnabás Póczos, Csaba Szepesvári","We present simple and computationally efficient nonparametric estimators of R\'enyi entropy and mutual information based on an i.i.d. sample drawn from an unknown, absolutely continuous distribution over
\R
d
\R
. The estimators are calculated as the sum of
p
p
-th powers of the Euclidean lengths of the edges of the `generalized nearest-neighbor' graph of the sample and the empirical copula of the sample respectively. For the first time, we prove the almost sure consistency of these estimators and upper bounds on their rates of convergence, the latter of which under the assumption that the density underlying the sample is Lipschitz continuous. Experiments demonstrate their usefulness in independent subspace analysis."
neurips,https://proceedings.neurips.cc/paper/2010/file/5807a685d1a9ab3b599035bc566ce2b9-Paper.pdf,Causal discovery in multiple models from different experiments,"Tom Claassen, Tom Heskes",
neurips,https://proceedings.neurips.cc/paper/2010/file/59c33016884a62116be975a9bb8257e3-Paper.pdf,Learning Bounds for Importance Weighting,"Corinna Cortes, Yishay Mansour, Mehryar Mohri",
neurips,https://proceedings.neurips.cc/paper/2010/file/5c572eca050594c7bc3c36e7e8ab9550-Paper.pdf,A Reduction from Apprenticeship Learning to Classification,"Umar Syed, Robert E. Schapire","We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classification algorithm to learn to imitate the expert's behavior. Although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis. We prove that, if the learned classifier has error rate
\eps
\eps
, the difference between the value of the apprentice's policy and the expert's policy is
O
(
√
\eps
)
O
. Further, we prove that this difference is only
O
(
\eps
)
O
when the expert's policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difficult to obtain."
neurips,https://proceedings.neurips.cc/paper/2010/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf,Extensions of Generalized Binary Search to Group Identification and Exponential Costs,"Gowtham Bellala, Suresh Bhavnani, Clayton Scott",
neurips,https://proceedings.neurips.cc/paper/2010/file/5f0f5e5f33945135b874349cfbed4fb9-Paper.pdf,Feature Set Embedding for Incomplete Data,"David Grangier, Iain Melvin",
neurips,https://proceedings.neurips.cc/paper/2010/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf,Improving Human Judgments by Decontaminating Sequential Dependencies,"Michael C. Mozer, Harold Pashler, Matthew Wilder, Robert V. Lindsey, Matt Jones, Michael N. Jones",
neurips,https://proceedings.neurips.cc/paper/2010/file/632cee946db83e7a52ce5e8d6f0fed35-Paper.pdf,Scrambled Objects for Least-Squares Regression,"Odalric Maillard, Rémi Munos",
neurips,https://proceedings.neurips.cc/paper/2010/file/6395ebd0f4b478145ecfbaf939454fa4-Paper.pdf,Subgraph Detection Using Eigenvector L1 Norms,"Benjamin Miller, Nadya Bliss, Patrick Wolfe",
neurips,https://proceedings.neurips.cc/paper/2010/file/65cc2c8205a05d7379fa3a6386f710e1-Paper.pdf,Error Propagation for Approximate Policy and Value Iteration,"Amir-massoud Farahmand, Csaba Szepesvári, Rémi Munos",
neurips,https://proceedings.neurips.cc/paper/2010/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf,PAC-Bayesian Model Selection for Reinforcement Learning,"M. Fard, Joelle Pineau",
neurips,https://proceedings.neurips.cc/paper/2010/file/677e09724f0e2df9b6c000b75b5da10d-Paper.pdf,Learning to combine foveal glimpses with a third-order Boltzmann machine,"Hugo Larochelle, Geoffrey E. Hinton",
neurips,https://proceedings.neurips.cc/paper/2010/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf,Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm,"Nathan Srebro, Russ R. Salakhutdinov",
neurips,https://proceedings.neurips.cc/paper/2010/file/67f7fb873eaf29526a11a9b7ac33bfac-Paper.pdf,Worst-case bounds on the quality of max-product fixed-points,"Meritxell Vinyals, Jes\'us Cerquides, Alessandro Farinelli, Juan Rodríguez-aguilar",
neurips,https://proceedings.neurips.cc/paper/2010/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf,A POMDP Extension with Belief-dependent Rewards,"Mauricio Araya, Olivier Buffet, Vincent Thomas, Françcois Charpillet",
neurips,https://proceedings.neurips.cc/paper/2010/file/68a83eeb494a308fe5295da69428a507-Paper.pdf,Infinite Relational Modeling of Functional Connectivity in Resting State fMRI,"Morten Mørup, Kristoffer Madsen, Anne-marie Dogonowski, Hartwig Siebner, Lars K. Hansen",
neurips,https://proceedings.neurips.cc/paper/2010/file/68b1fbe7f16e4ae3024973f12f3cb313-Paper.pdf,An Alternative to Low-level-Sychrony-Based Methods for Speech Detection,"Javier Movellan, Paul Ruvolo",
neurips,https://proceedings.neurips.cc/paper/2010/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf,A rational decision making framework for inhibitory control,"Pradeep Shenoy, Angela J. Yu, Rajesh PN Rao",
neurips,https://proceedings.neurips.cc/paper/2010/file/69421f032498c97020180038fddb8e24-Paper.pdf,Policy gradients in linearly-solvable MDPs,Emanuel Todorov,
neurips,https://proceedings.neurips.cc/paper/2010/file/6c524f9d5d7027454a783c841250ba71-Paper.pdf,Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework,"Hongbo Zhou, Qiang Cheng",
neurips,https://proceedings.neurips.cc/paper/2010/file/6c8dba7d0df1c4a79dd07646be9a26c8-Paper.pdf,Learning concept graphs from text with stick-breaking priors,"America Chambers, Padhraic Smyth, Mark Steyvers",
neurips,https://proceedings.neurips.cc/paper/2010/file/6cdd60ea0045eb7a6ec44c54d29ed402-Paper.pdf,Fast Large-scale Mixture Modeling with Component-specific Data Partitions,"Bo Thiesson, Chong Wang",
neurips,https://proceedings.neurips.cc/paper/2010/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf,Improvements to the Sequence Memoizer,"Jan Gasthaus, Yee Teh",
neurips,https://proceedings.neurips.cc/paper/2010/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf,Simultaneous Object Detection and Ranking with Weak Supervision,"Matthew Blaschko, Andrea Vedaldi, Andrew Zisserman",
neurips,https://proceedings.neurips.cc/paper/2010/file/6e7d2da6d3953058db75714ac400b584-Paper.pdf,Generating more realistic images using gated MRF's,"Marc'aurelio Ranzato, Volodymyr Mnih, Geoffrey E. Hinton",
neurips,https://proceedings.neurips.cc/paper/2010/file/6ea2ef7311b482724a9b7b0bc0dd85c6-Paper.pdf,Efficient Minimization of Decomposable Submodular Functions,"Peter Stobbe, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2010/file/6ecbdd6ec859d284dc13885a37ce8d81-Paper.pdf,Implicit Differentiation by Perturbation,Justin Domke,
neurips,https://proceedings.neurips.cc/paper/2010/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf,The Maximal Causes of Natural Scenes are Edge Filters,"Jose Puertas, Joerg Bornschein, Jörg Lücke",
neurips,https://proceedings.neurips.cc/paper/2010/file/6f3e29a35278d71c7f65495871231324-Paper.pdf,Regularized estimation of image statistics by Score Matching,"Durk P. Kingma, Yann Cun",
neurips,https://proceedings.neurips.cc/paper/2010/file/6f3ef77ac0e3619e98159e9b6febf557-Paper.pdf,Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch,"Zeeshan Syed, John Guttag",
neurips,https://proceedings.neurips.cc/paper/2010/file/704afe073992cbe4813cae2f7715336f-Paper.pdf,Movement extraction by detecting dynamics switches and repetitions,"Silvia Chiappa, Jan Peters",
neurips,https://proceedings.neurips.cc/paper/2010/file/705f2172834666788607efbfca35afb3-Paper.pdf,Exact inference and learning for cumulative distribution functions on loopy graphs,"Nebojsa Jojic, Chris Meek, Jim Huang",
neurips,https://proceedings.neurips.cc/paper/2010/file/7143d7fbadfa4693b9eec507d9d37443-Paper.pdf,Spectral Regularization for Support Estimation,"Ernesto Vito, Lorenzo Rosasco, Alessandro Toigo",
neurips,https://proceedings.neurips.cc/paper/2010/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf,Online Learning for Latent Dirichlet Allocation,"Matthew Hoffman, Francis Bach, David Blei",
neurips,https://proceedings.neurips.cc/paper/2010/file/73278a4a86960eeb576a8fd4c9ec6997-Paper.pdf,"Random Projections for
k
k
-means Clustering","Christos Boutsidis, Anastasios Zouzias, Petros Drineas","This paper discusses the topic of dimensionality reduction for
k
k
-means clustering. We prove that any set of
n
n
points in
d
d
dimensions (rows in a matrix
A
∈
\RR
n
×
d
A
) can be projected into
t
=
Ω
(
k
/
\eps
2
)
t
dimensions, for any
\eps
∈
(
0
,
1
/
3
)
\eps
, in
O
(
n
d
⌈
\eps
−
2
k
/
log
(
d
)
⌉
)
O
time, such that with constant probability the optimal
k
k
-partition of the point set is preserved within a factor of
2
+
\eps
2
. The projection is done by post-multiplying
A
A
with a
d
×
t
d
random matrix
R
R
having entries
+
1
/
√
t
+
or
−
1
/
√
t
−
with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results."
neurips,https://proceedings.neurips.cc/paper/2010/file/766d856ef1a6b02f93d894415e6bfa0e-Paper.pdf,Inference and communication in the game of Password,"Yang Xu, Charles Kemp",
neurips,https://proceedings.neurips.cc/paper/2010/file/76cf99d3614e23eabab16fb27e944bf9-Paper.pdf,"Smoothness, Low Noise and Fast Rates","Nathan Srebro, Karthik Sridharan, Ambuj Tewari",
neurips,https://proceedings.neurips.cc/paper/2010/file/7810ccd41bf26faaa2c4e1f20db70a71-Paper.pdf,Energy Disaggregation via Discriminative Sparse Coding,"J. Kolter, Siddharth Batra, Andrew Ng",
neurips,https://proceedings.neurips.cc/paper/2010/file/7940ab47468396569a906f75ff3f20ef-Paper.pdf,Random Conic Pursuit for Semidefinite Programming,"Ariel Kleiner, Ali Rahimi, Michael Jordan",
neurips,https://proceedings.neurips.cc/paper/2010/file/7a53928fa4dd31e82c6ef826f341daec-Paper.pdf,Deterministic Single-Pass Algorithm for LDA,"Issei Sato, Kenichi Kurihara, Hiroshi Nakagawa",
neurips,https://proceedings.neurips.cc/paper/2010/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf,A Bayesian Framework for Figure-Ground Interpretation,"Vicky Froyen, Jacob Feldman, Manish Singh",
neurips,https://proceedings.neurips.cc/paper/2010/file/7bb060764a818184ebb1cc0d43d382aa-Paper.pdf,Online Markov Decision Processes under Bandit Feedback,"Gergely Neu, Andras Antos, András György, Csaba Szepesvári",
neurips,https://proceedings.neurips.cc/paper/2010/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf,A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration,"Yi-da Wu, Shi-jie Lin, Hsin Chen",
neurips,https://proceedings.neurips.cc/paper/2010/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf,"SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system","Sylvain Chevallier, Hél\`ene Paugam-moisy, Michele Sebag",
neurips,https://proceedings.neurips.cc/paper/2010/file/7cbbc409ec990f19c78c75bd1e06f215-Paper.pdf,Permutation Complexity Bound on Out-Sample Error,Malik Magdon-Ismail,
neurips,https://proceedings.neurips.cc/paper/2010/file/7cce53cf90577442771720a370c3c723-Paper.pdf,Fast global convergence rates of gradient methods for high-dimensional statistical recovery,"Alekh Agarwal, Sahand Negahban, Martin J. Wainwright","Many statistical
M
M
-estimators are based on convex optimization problems formed by the weighted sum of a loss function with a norm-based regularizer. We analyze the convergence rates of first-order gradient methods for solving such problems within a high-dimensional framework that allows the data dimension
d
d
to grow with (and possibly exceed) the sample size
n
n
. This high-dimensional structure precludes the usual global assumptions---namely, strong convexity and smoothness conditions---that underlie classical optimization analysis. We define appropriately restricted versions of these conditions, and show that they are satisfied with high probability for various statistical models. Under these conditions, our theory guarantees that Nesterov's first-order method~\cite{Nesterov07} has a globally geometric rate of convergence up to the statistical precision of the model, meaning the typical Euclidean distance between the true unknown parameter
θ
∗
θ
and the optimal solution
ˆ
θ
θ
. This globally linear rate is substantially faster than previous analyses of global convergence for specific methods that yielded only sublinear rates. Our analysis applies to a wide range of
M
M
-estimators and statistical models, including sparse linear regression using Lasso (
ℓ
1
ℓ
-regularized regression), group Lasso, block sparsity, and low-rank matrix recovery using nuclear norm regularization. Overall, this result reveals an interesting connection between statistical precision and computational efficiency in high-dimensional estimation."
neurips,https://proceedings.neurips.cc/paper/2010/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Paper.pdf,Attractor Dynamics with Synaptic Depression,"K. Wong, He Wang, Si Wu, Chi Fung",
neurips,https://proceedings.neurips.cc/paper/2010/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Paper.pdf,Layer-wise analysis of deep networks with Gaussian kernels,"Grégoire Montavon, Klaus-Robert Müller, Mikio Braun",
neurips,https://proceedings.neurips.cc/paper/2010/file/7f5d04d189dfb634e6a85bb9d9adf21e-Paper.pdf,Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake,"Stefan Harmeling, Hirsch Michael, Bernhard Schölkopf",
neurips,https://proceedings.neurips.cc/paper/2010/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf,Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models,"Anand Singh, Renaud Jolivet, Pierre Magistretti, Bruno Weber",
neurips,https://proceedings.neurips.cc/paper/2010/file/818f4654ed39a1c147d1e51a00ffb4cb-Paper.pdf,Global Analytic Solution for Variational Bayesian Matrix Factorization,"Shinichi Nakajima, Masashi Sugiyama, Ryota Tomioka",
neurips,https://proceedings.neurips.cc/paper/2010/file/819f46e52c25763a55cc642422644317-Paper.pdf,Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices,"Yi Sun, Jürgen Schmidhuber, Faustino Gomez",
neurips,https://proceedings.neurips.cc/paper/2010/file/81dc9bdb52d04dc20036dbd8313ed055-Paper.pdf,Linear Complementarity for Regularized Policy Evaluation and Improvement,"Jeffrey Johns, Christopher Painter-wakefield, Ronald Parr",
neurips,https://proceedings.neurips.cc/paper/2010/file/81e74d678581a3bb7a720b019f4f1a93-Paper.pdf,Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks,"Dirk Husmeier, Frank Dondelinger, Sophie Lebre",
neurips,https://proceedings.neurips.cc/paper/2010/file/821fa74b50ba3f7cba1e6c53e8fa6845-Paper.pdf,Graph-Valued Regression,"Han Liu, Xi Chen, Larry Wasserman, John Lafferty","Undirected graphical models encode in a graph
G
G
the dependency structure of a random vector
Y
Y
. In many applications, it is of interest to model
Y
Y
given another random vector
X
X
as input. We refer to the problem of estimating the graph
G
(
x
)
G
of
Y
Y
conditioned on
X
=
x
X
as
graph-valued regression''. In this paper, we propose a semiparametric method for estimating
G
(
x
)
G
that builds a tree on the
X
X
space just as in CART (classification and regression trees), but at each leaf of the tree estimates a graph. We call the method
Graph-optimized CART'', or Go-CART. We study the theoretical properties of Go-CART using dyadic partitioning trees, establishing oracle inequalities on risk minimization and tree partition consistency. We also demonstrate the application of Go-CART to a meteorological dataset, showing how graph-valued regression can provide a useful tool for analyzing complex data."
neurips,https://proceedings.neurips.cc/paper/2010/file/839ab46820b524afda05122893c2fe8e-Paper.pdf,Probabilistic Multi-Task Feature Selection,"Yu Zhang, Dit-Yan Yeung, Qian Xu","Recently, some variants of the
l
1
l
norm, particularly matrix norms such as the
l
1
,
2
l
and
l
1
,
∞
l
norms, have been widely used in multi-task learning, compressed sensing and other related areas to enforce sparsity via joint regularization. In this paper, we unify the
l
1
,
2
l
and
l
1
,
∞
l
norms by considering a family of
l
1
,
q
l
norms for
1
<
q
≤
∞
1
and study the problem of determining the most appropriate sparsity enforcing norm to use in the context of multi-task feature selection. Using the generalized normal distribution, we provide a probabilistic interpretation of the general multi-task feature selection problem using the
l
1
,
q
l
norm. Based on this probabilistic interpretation, we develop a probabilistic model using the noninformative Jeffreys prior. We also extend the model to learn and exploit more general types of pairwise relationships between tasks. For both versions of the model, we devise expectation-maximization~(EM) algorithms to learn all model parameters, including
q
q
, automatically. Experiments have been conducted on two cancer classification applications using microarray gene expression data."
neurips,https://proceedings.neurips.cc/paper/2010/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf,Penalized Principal Component Regression on Graphs for Analysis of Subnetworks,"Ali Shojaie, George Michailidis",
neurips,https://proceedings.neurips.cc/paper/2010/file/847cc55b7032108eee6dd897f3bca8a5-Paper.pdf,Bayesian Action-Graph Games,"Albert Jiang, Kevin Leyton-brown",
neurips,https://proceedings.neurips.cc/paper/2010/file/884d247c6f65a96a7da4d1105d584ddd-Paper.pdf,A Family of Penalty Functions for Structured Sparsity,"Jean Morales, Charles Micchelli, Massimiliano Pontil","We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern. We present a family of convex penalty functions, which encode this prior knowledge by means of a set of constraints on the absolute values of the regression coefficients. This family subsumes the
ℓ
1
ℓ
norm and is flexible enough to include different models of sparsity patterns, which are of practical and theoretical importance. We establish some important properties of these functions and discuss some examples where they can be computed explicitly. Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions. Numerical simulations highlight the benefit of structured sparsity and the advantage offered by our approach over the Lasso and other related methods."
neurips,https://proceedings.neurips.cc/paper/2010/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf,Spike timing-dependent plasticity as dynamic filter,"Joscha Schmiedt, Christian Albers, Klaus Pawelzik",
neurips,https://proceedings.neurips.cc/paper/2010/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Paper.pdf,The Neural Costs of Optimal Control,"Samuel Gershman, Robert Wilson",
neurips,https://proceedings.neurips.cc/paper/2010/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf,Sample Complexity of Testing the Manifold Hypothesis,"Hariharan Narayanan, Sanjoy Mitter","The hypothesis that high dimensional data tends to lie in the vicinity of a low dimensional manifold is the basis of a collection of methodologies termed Manifold Learning. In this paper, we study statistical aspects of the question of fitting a manifold with a nearly optimal least squared error. Given upper bounds on the dimension, volume, and curvature, we show that Empirical Risk Minimization can produce a nearly optimal manifold using a number of random samples that is {\it independent} of the ambient dimension of the space in which data lie. We obtain an upper bound on the required number of samples that depends polynomially on the curvature, exponentially on the intrinsic dimension, and linearly on the intrinsic volume. For constant error, we prove a matching minimax lower bound on the sample complexity that shows that this dependence on intrinsic dimension, volume and curvature is unavoidable. Whether the known lower bound of
O
(
k
\eps
2
+
log
1
\de
\eps
2
)
O
for the sample complexity of Empirical Risk minimization on
k
−
k
means applied to data in a unit ball of arbitrary dimension is tight, has been an open question since 1997 \cite{bart2}. Here
\eps
\eps
is the desired bound on the error and
\de
\de
is a bound on the probability of failure. We improve the best currently known upper bound \cite{pontil} of
O
(
k
2
\eps
2
+
log
1
\de
\eps
2
)
O
to
O
(
k
\eps
2
(
min
(
k
,
log
4
k
\eps
\eps
2
)
)
+
log
1
\de
\eps
2
)
O
. Based on these results, we devise a simple algorithm for
k
−
k
means and another that uses a family of convex programs to fit a piecewise linear curve of a specified length to high dimensional data, where the sample complexity is independent of the ambient dimension."
neurips,https://proceedings.neurips.cc/paper/2010/file/8c235f89a8143a28a1d6067e959dd858-Paper.pdf,A biologically plausible network for the computation of orientation dominance,"Kritika Muralidharan, Nuno Vasconcelos","The determination of dominant orientation at a given image location is formulated as a decision-theoretic question. This leads to a novel measure for the dominance of a given orientation
θ
θ
, which is similar to that used by SIFT. It is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of V1. The measure can thus be seen as a biologically plausible version of SIFT, and is denoted as bioSIFT. The network units are shown to exhibit trademark properties of V1 neurons, such as cross-orientation suppression, sparseness and independence. The connection between SIFT and biological vision provides a justification for the success of SIFT-like features and reinforces the importance of contrast normalization in computer vision. We illustrate this by replacing the Gabor units of an HMAX network with the new bioSIFT units. This is shown to lead to significant gains for classification tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems."
neurips,https://proceedings.neurips.cc/paper/2010/file/8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf,A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups,"Sofia Mosci, Silvia Villa, Alessandro Verri, Lorenzo Rosasco",
neurips,https://proceedings.neurips.cc/paper/2010/file/8d317bdcf4aafcfc22149d77babee96d-Paper.pdf,Heavy-Tailed Process Priors for Selective Shrinkage,"Fabian L. Wauthier, Michael Jordan",
neurips,https://proceedings.neurips.cc/paper/2010/file/8dd48d6a2e2cad213179a3992c0be53c-Paper.pdf,A New Probabilistic Model for Rank Aggregation,"Tao Qin, Xiubo Geng, Tie-yan Liu",
neurips,https://proceedings.neurips.cc/paper/2010/file/8df707a948fac1b4a0f97aa554886ec8-Paper.pdf,Learning Networks of Stochastic Differential Equations,"José Pereira, Morteza Ibrahimi, Andrea Montanari",
neurips,https://proceedings.neurips.cc/paper/2010/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf,Predictive Subspace Learning for Multi-view Data: a Large Margin Approach,"Ning Chen, Jun Zhu, Eric Xing",
neurips,https://proceedings.neurips.cc/paper/2010/file/8edd72158ccd2a879f79cb2538568fdc-Paper.pdf,A Bayesian Approach to Concept Drift,"Stephen Bach, Mark Maloof",
neurips,https://proceedings.neurips.cc/paper/2010/file/8f121ce07d74717e0b1f21d122e04521-Paper.pdf,Multivariate Dyadic Regression Trees for Sparse Learning Problems,"Han Liu, Xi Chen","We propose a new nonparametric learning method based on multivariate dyadic regression trees (MDRTs). Unlike traditional dyadic decision trees (DDTs) or classification and regression trees (CARTs), MDRTs are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty. Theoretically, we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rates of convergence (in a minimax sense) for the class of
(
α
,
C
)
(
-smooth functions. Empirically, MDRTs can simultaneously conduct function estimation and variable selection in high dimensions. To make MDRTs applicable for large-scale learning problems, we propose a greedy heuristics. The superior performance of MDRTs are demonstrated on both synthetic and real datasets."
neurips,https://proceedings.neurips.cc/paper/2010/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf,A novel family of non-parametric cumulative based divergences for point processes,"Sohan Seth, Park Il, Austin Brockmeier, Mulugeta Semework, John Choi, Joseph Francis, Jose Principe",
neurips,https://proceedings.neurips.cc/paper/2010/file/93fb9d4b16aa750c7475b6d601c35c2c-Paper.pdf,Sidestepping Intractable Inference with Structured Ensemble Cascades,"David Weiss, Benjamin Sapp, Ben Taskar",
neurips,https://proceedings.neurips.cc/paper/2010/file/94f6d7e04a4d452035300f18b984988c-Paper.pdf,Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings,"Ziv Bar-joseph, Hai-son Le",
neurips,https://proceedings.neurips.cc/paper/2010/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf,t-logistic regression,"Nan Ding, S.v.n. Vishwanathan",
neurips,https://proceedings.neurips.cc/paper/2010/file/9778d5d219c5080b9a6a17bef029331c-Paper.pdf,Occlusion Detection and Motion Estimation with Convex Optimization,"Alper Ayvaci, Michalis Raptis, Stefano Soatto",
neurips,https://proceedings.neurips.cc/paper/2010/file/98dce83da57b0395e163467c9dae521b-Paper.pdf,Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach,"Alessandro Bergamo, Lorenzo Torresani",
neurips,https://proceedings.neurips.cc/paper/2010/file/995665640dc319973d3173a74a03860c-Paper.pdf,Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression,"Ling Huang, Jinzhu Jia, Bin Yu, Byung-gon Chun, Petros Maniatis, Mayur Naik",
neurips,https://proceedings.neurips.cc/paper/2010/file/996009f2374006606f4c0b0fda878af1-Paper.pdf,"Humans Learn Using Manifolds, Reluctantly","Tim Rogers, Chuck Kalish, Joseph Harrison, Jerry Zhu, Bryan Gibson",
neurips,https://proceedings.neurips.cc/paper/2010/file/9adeb82fffb5444e81fa0ce8ad8afe7a-Paper.pdf,Sphere Embedding: An Application to Part-of-Speech Induction,"Yariv Maron, Michael Lamar, Elie Bienenstock",
neurips,https://proceedings.neurips.cc/paper/2010/file/9b04d152845ec0a378394003c96da594-Paper.pdf,Tight Sample Complexity of Large-Margin Learning,"Sivan Sabato, Nathan Srebro, Naftali Tishby",
neurips,https://proceedings.neurips.cc/paper/2010/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf,Minimum Average Cost Clustering,"Kiyohito Nagano, Yoshinobu Kawahara, Satoru Iwata",
neurips,https://proceedings.neurips.cc/paper/2010/file/9cfdf10e8fc047a44b08ed031e1f0ed1-Paper.pdf,Online Classification with Specificity Constraints,"Andrey Bernstein, Shie Mannor, Nahum Shimkin",
neurips,https://proceedings.neurips.cc/paper/2010/file/9f60ab2b55468f104055b16df8f69e81-Paper.pdf,Construction of Dependent Dirichlet Processes based on Poisson Processes,"Dahua Lin, Eric Grimson, John Fisher",
neurips,https://proceedings.neurips.cc/paper/2010/file/9fe8593a8a330607d76796b35c64c600-Paper.pdf,Practical Large-Scale Optimization for Max-norm Regularization,"Jason D. Lee, Ben Recht, Nathan Srebro, Joel Tropp, Russ R. Salakhutdinov",
neurips,https://proceedings.neurips.cc/paper/2010/file/a0160709701140704575d499c997b6ca-Paper.pdf,Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication,"Guy Isely, Christopher Hillar, Fritz Sommer",
neurips,https://proceedings.neurips.cc/paper/2010/file/a01610228fe998f515a72dd730294d87-Paper.pdf,Learning Convolutional Feature Hierarchies for Visual Recognition,"Koray Kavukcuoglu, Pierre Sermanet, Y-lan Boureau, Karol Gregor, Michael Mathieu, Yann Cun",
neurips,https://proceedings.neurips.cc/paper/2010/file/a01a0380ca3c61428c26a231f0e49a09-Paper.pdf,Multiple Kernel Learning and the SMO Algorithm,"Zhaonan Sun, Nawanol Ampornpunt, Manik Varma, S.v.n. Vishwanathan","Our objective is to train
p
p
-norm Multiple Kernel Learning (MKL) and, more generally, linear MKL regularised by the Bregman divergence, using the Sequential Minimal Optimization (SMO) algorithm. The SMO algorithm is simple, easy to implement and adapt, and efficiently scales to large problems. As a result, it has gained widespread acceptance and SVMs are routinely trained using SMO in diverse real world applications. Training using SMO has been a long standing goal in MKL for the very same reasons. Unfortunately, the standard MKL dual is not differentiable, and therefore can not be optimised using SMO style co-ordinate ascent. In this paper, we demonstrate that linear MKL regularised with the
p
p
-norm squared, or with certain Bregman divergences, can indeed be trained using SMO. The resulting algorithm retains both simplicity and efficiency and is significantly faster than the state-of-the-art specialised
p
p
-norm MKL solvers. We show that we can train on a hundred thousand kernels in approximately seven minutes and on fifty thousand points in less than half an hour on a single core."
neurips,https://proceedings.neurips.cc/paper/2010/file/a0a080f42e6f13b3a2df133f073095dd-Paper.pdf,Segmentation as Maximum-Weight Independent Set,"William Brendel, Sinisa Todorovic",
neurips,https://proceedings.neurips.cc/paper/2010/file/a1d33d0dfec820b41b54430b50e96b5c-Paper.pdf,Static Analysis of Binary Executables Using Structural SVMs,Nikos Karampatziakis,
neurips,https://proceedings.neurips.cc/paper/2010/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf,Factorized Latent Spaces with Structured Sparsity,"Yangqing Jia, Mathieu Salzmann, Trevor Darrell",
neurips,https://proceedings.neurips.cc/paper/2010/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf,A unified model of short-range and long-range motion perception,"Shuang Wu, Xuming He, Hongjing Lu, Alan L. Yuille",
neurips,https://proceedings.neurips.cc/paper/2010/file/a51fb975227d6640e4fe47854476d133-Paper.pdf,Structural epitome: a way to summarize one’s visual experience,"Nebojsa Jojic, Alessandro Perina, Vittorio Murino",
neurips,https://proceedings.neurips.cc/paper/2010/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf,Tree-Structured Stick Breaking for Hierarchical Data,"Zoubin Ghahramani, Michael Jordan, Ryan P. Adams",
neurips,https://proceedings.neurips.cc/paper/2010/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf,LSTD with Random Projections,"Mohammad Ghavamzadeh, Alessandro Lazaric, Odalric Maillard, Rémi Munos",
neurips,https://proceedings.neurips.cc/paper/2010/file/a8f15eda80c50adb0e71943adc8015cf-Paper.pdf,Feature Construction for Inverse Reinforcement Learning,"Sergey Levine, Zoran Popovic, Vladlen Koltun",
neurips,https://proceedings.neurips.cc/paper/2010/file/a9b7ba70783b617e9998dc4dd82eb3c5-Paper.pdf,An analysis on negative curvature induced by singularity in multi-layer neural-network learning,"Eiji Mizutani, Stuart Dreyfus",
neurips,https://proceedings.neurips.cc/paper/2010/file/aa169b49b583a2b5af89203c2b78c67c-Paper.pdf,Joint Cascade Optimization Using A Product Of Boosted Classifiers,"Leonidas Lefakis, Francois Fleuret",
neurips,https://proceedings.neurips.cc/paper/2010/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf,Parallelized Stochastic Gradient Descent,"Martin Zinkevich, Markus Weimer, Lihong Li, Alex Smola",
neurips,https://proceedings.neurips.cc/paper/2010/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf,Shadow Dirichlet for Restricted Probability Modeling,"Bela Frigyik, Maya Gupta, Yihua Chen",
neurips,https://proceedings.neurips.cc/paper/2010/file/adc8ca1b15e20915c3ea6008fc2f52ed-Paper.pdf,MAP estimation in Binary MRFs via Bipartite Multi-cuts,"Sashank J. Reddi, Sunita Sarawagi, Sundar Vishwanathan",
neurips,https://proceedings.neurips.cc/paper/2010/file/addfa9b7e234254d26e9c7f2af1005cb-Paper.pdf,Approximate Inference by Compilation to Arithmetic Circuits,"Daniel Lowd, Pedro Domingos",
neurips,https://proceedings.neurips.cc/paper/2010/file/aeb3135b436aa55373822c010763dd54-Paper.pdf,Random Walk Approach to Regret Minimization,"Hariharan Narayanan, Alexander Rakhlin",
neurips,https://proceedings.neurips.cc/paper/2010/file/afda332245e2af431fb7b672a68b659d-Paper.pdf,Unsupervised Kernel Dimension Reduction,"Meihong Wang, Fei Sha, Michael Jordan",
neurips,https://proceedings.neurips.cc/paper/2010/file/aff1621254f7c1be92f64550478c56e6-Paper.pdf,Multitask Learning without Label Correspondences,"Novi Quadrianto, James Petterson, Tibério Caetano, Alex Smola, S.v.n. Vishwanathan",
neurips,https://proceedings.neurips.cc/paper/2010/file/b20bb95ab626d93fd976af958fbc61ba-Paper.pdf,CUR from a Sparse Optimization Viewpoint,"Jacob Bien, Ya Xu, Michael W. Mahoney",
neurips,https://proceedings.neurips.cc/paper/2010/file/b2eb7349035754953b57a32e2841bda5-Paper.pdf,Robust Clustering as Ensembles of Affinity Relations,"Hairong Liu, Longin Latecki, Shuicheng Yan",
neurips,https://proceedings.neurips.cc/paper/2010/file/b2f627fff19fda463cb386442eac2b3d-Paper.pdf,Optimal learning rates for Kernel Conjugate Gradient regression,"Gilles Blanchard, Nicole Krämer",
neurips,https://proceedings.neurips.cc/paper/2010/file/b534ba68236ba543ae44b22bd110a1d6-Paper.pdf,Rates of convergence for the cluster tree,"Kamalika Chaudhuri, Sanjoy Dasgupta",
neurips,https://proceedings.neurips.cc/paper/2010/file/b56a18e0eacdf51aa2a5306b0f533204-Paper.pdf,Throttling Poisson Processes,"Uwe Dick, Peter Haider, Thomas Vanck, Michael Brückner, Tobias Scheffer",
neurips,https://proceedings.neurips.cc/paper/2010/file/b5dc4e5d9b495d0196f61d45b26ef33e-Paper.pdf,An Approximate Inference Approach to Temporal Optimization in Optimal Control,"Konrad Rawlik, Marc Toussaint, Sethu Vijayakumar",
neurips,https://proceedings.neurips.cc/paper/2010/file/b73ce398c39f506af761d2277d853a92-Paper.pdf,Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine,"George Dahl, Marc'aurelio Ranzato, Abdel-rahman Mohamed, Geoffrey E. Hinton",
neurips,https://proceedings.neurips.cc/paper/2010/file/bbf94b34eb32268ada57a3be5062fe7d-Paper.pdf,Sparse Coding for Learning Interpretable Spatio-Temporal Primitives,"Taehwan Kim, Gregory Shakhnarovich, Raquel Urtasun",
neurips,https://proceedings.neurips.cc/paper/2010/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf,Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference,"Vikas Sindhwani, Aurelie C. Lozano",
neurips,https://proceedings.neurips.cc/paper/2010/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf,Reverse Multi-Label Learning,"James Petterson, Tibério Caetano",
neurips,https://proceedings.neurips.cc/paper/2010/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf,Efficient Relational Learning with Hidden Variable Detection,"Ni Lao, Jun Zhu, Liu Liu, Yandong Liu, William W. Cohen",
neurips,https://proceedings.neurips.cc/paper/2010/file/c0f168ce8900fa56e57789e2a2f2c9d0-Paper.pdf,Learning from Logged Implicit Exploration Data,"Alex Strehl, John Langford, Lihong Li, Sham M. Kakade",
neurips,https://proceedings.neurips.cc/paper/2010/file/c2626d850c80ea07e7511bbae4c76f4b-Paper.pdf,Parametric Bandits: The Generalized Linear Case,"Sarah Filippi, Olivier Cappe, Aurélien Garivier, Csaba Szepesvári",
neurips,https://proceedings.neurips.cc/paper/2010/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf,Basis Construction from Power Series Expansions of Value Functions,"Sridhar Mahadevan, Bo Liu",
neurips,https://proceedings.neurips.cc/paper/2010/file/c410003ef13d451727aeff9082c29a5c-Paper.pdf,A Novel Kernel for Learning a Neuron Model from Spike Train Data,"Nicholas Fisher, Arunava Banerjee",
neurips,https://proceedings.neurips.cc/paper/2010/file/c44e503833b64e9f27197a484f4257c0-Paper.pdf,Nonparametric Bayesian Policy Priors for Reinforcement Learning,"Finale Doshi-velez, David Wingate, Nicholas Roy, Joshua Tenenbaum",
neurips,https://proceedings.neurips.cc/paper/2010/file/c73dfe6c630edb4c1692db67c510f65c-Paper.pdf,On the Theory of Learnining with Privileged Information,"Dmitry Pechyony, Vladimir Vapnik",
neurips,https://proceedings.neurips.cc/paper/2010/file/c7e1249ffc03eb9ded908c236bd1996d-Paper.pdf,Accounting for network effects in neuronal responses using L1 regularized point process models,"Ryan Kelly, Matthew Smith, Robert Kass, Tai Lee",
neurips,https://proceedings.neurips.cc/paper/2010/file/c850371fda6892fbfd1c5a5b457e5777-Paper.pdf,Probabilistic latent variable models for distinguishing between cause and effect,"Oliver Stegle, Dominik Janzing, Kun Zhang, Joris M. Mooij, Bernhard Schölkopf",
neurips,https://proceedings.neurips.cc/paper/2010/file/c8ffe9a587b126f152ed3d89a146b445-Paper.pdf,Two-Layer Generalization Analysis for Ranking Using Rademacher Average,"Wei Chen, Tie-yan Liu, Zhi-ming Ma",
neurips,https://proceedings.neurips.cc/paper/2010/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf,Learning from Candidate Labeling Sets,"Jie Luo, Francesco Orabona",
neurips,https://proceedings.neurips.cc/paper/2010/file/ca8155f4d27f205953f9d3d7974bdd70-Paper.pdf,Direct Loss Minimization for Structured Prediction,"Tamir Hazan, Joseph Keshet, David McAllester",
neurips,https://proceedings.neurips.cc/paper/2010/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf,Variational Inference over Combinatorial Spaces,"Alexandre Bouchard-côté, Michael Jordan",
neurips,https://proceedings.neurips.cc/paper/2010/file/cc1aa436277138f61cda703991069eaf-Paper.pdf,Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development,"Diane Hu, Laurens Maaten, Youngmin Cho, Sorin Lerner, Lawrence Saul",
neurips,https://proceedings.neurips.cc/paper/2010/file/ccb0989662211f61edae2e26d58ea92f-Paper.pdf,Avoiding False Positive in Multi-Instance Learning,"Yanjun Han, Qing Tao, Jue Wang",
neurips,https://proceedings.neurips.cc/paper/2010/file/ccb1d45fb76f7c5a0bf619f979c6cf36-Paper.pdf,Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles,"Kaiming Li, Lei Guo, Carlos Faraco, Dajiang Zhu, Fan Deng, Tuo Zhang, Xi Jiang, Degang Zhang, Hanbo Chen, Xintao Hu, Steve Miller, Tianming Liu",
neurips,https://proceedings.neurips.cc/paper/2010/file/cd758e8f59dfdf06a852adad277986ca-Paper.pdf,On the Convexity of Latent Social Network Inference,"Seth Myers, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2010/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf,Global seismic monitoring as probabilistic inference,"Nimar Arora, Stuart J. Russell, Paul Kidwell, Erik Sudderth",
neurips,https://proceedings.neurips.cc/paper/2010/file/d09bf41544a3365a46c9077ebb5e35c3-Paper.pdf,Gaussian sampling by local perturbations,"George Papandreou, Alan L. Yuille",
neurips,https://proceedings.neurips.cc/paper/2010/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf,Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model,"Peggy Series, David Reichert, Amos J. Storkey",
neurips,https://proceedings.neurips.cc/paper/2010/file/d490d7b4576290fa60eb31b5fc917ad1-Paper.pdf,Moreau-Yosida Regularization for Grouped Tree Structure Learning,"Jun Liu, Jieping Ye",
neurips,https://proceedings.neurips.cc/paper/2010/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf,Pose-Sensitive Embedding by Nonlinear NCA Regression,"Graham W. Taylor, Rob Fergus, George Williams, Ian Spiro, Christoph Bregler",
neurips,https://proceedings.neurips.cc/paper/2010/file/d64a340bcb633f536d56e51874281454-Paper.pdf,Synergies in learning words and their referents,"Mark Johnson, Katherine Demuth, Bevan Jones, Michael Black",
neurips,https://proceedings.neurips.cc/paper/2010/file/d6723e7cd6735df68d1ce4c704c29a04-Paper.pdf,Approximate inference in continuous time Gaussian-Jump processes,"Manfred Opper, Andreas Ruttor, Guido Sanguinetti",
neurips,https://proceedings.neurips.cc/paper/2010/file/d6ef5f7fa914c19931a55bb262ec879c-Paper.pdf,Empirical Bernstein Inequalities for U-Statistics,"Thomas Peel, Sandrine Anthoine, Liva Ralaivola",
neurips,https://proceedings.neurips.cc/paper/2010/file/d7a728a67d909e714c0774e22cb806f2-Paper.pdf,Active Estimation of F-Measures,"Christoph Sawade, Niels Landwehr, Tobias Scheffer",
neurips,https://proceedings.neurips.cc/paper/2010/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf,Inductive Regularized Learning of Kernel Functions,"Prateek Jain, Brian Kulis, Inderjit Dhillon","In this paper we consider the fundamental problem of semi-supervised kernel function learning. We propose a general regularized framework for learning a kernel matrix, and then demonstrate an equivalence between our proposed kernel matrix learning framework and a general linear transformation learning problem. Our result shows that the learned kernel matrices parameterize a linear transformation kernel function and can be applied inductively to new data points. Furthermore, our result gives a constructive method for kernelizing most existing Mahalanobis metric learning formulations. To make our results practical for large-scale data, we modify our framework to limit the number of parameters in the optimization process. We also consider the problem of kernelized inductive dimensionality reduction in the semi-supervised setting. We introduce a novel method for this problem by considering a special case of our general kernel learning framework where we select the trace norm function as the regularizer. We empirically demonstrate that our framework learns useful kernel functions, improving the
k
k
-NN classification accuracy significantly in a variety of domains. Furthermore, our kernelized dimensionality reduction technique significantly reduces the dimensionality of the feature space while achieving competitive classification accuracies."
neurips,https://proceedings.neurips.cc/paper/2010/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf,Active Learning Applied to Patient-Adaptive Heartbeat Classification,"Jenna Wiens, John Guttag",
neurips,https://proceedings.neurips.cc/paper/2010/file/da4fb5c6e93e74d3df8527599fa62642-Paper.pdf,Large-Scale Matrix Factorization with Missing Data under Additional Constraints,"Kaushik Mitra, Sameer Sheorey, Rama Chellappa","Matrix factorization in the presence of missing data is at the core of many computer vision problems such as structure from motion (SfM), non-rigid SfM and photometric stereo. We formulate the problem of matrix factorization with missing data as a low-rank semidefinite program (LRSDP) with the advantage that:
1
)
1
an efficient quasi-Newton implementation of the LRSDP enables us to solve large-scale factorization problems, and
2
)
2
additional constraints such as ortho-normality, required in orthographic SfM, can be directly incorporated in the new formulation. Our empirical evaluations suggest that, under the conditions of matrix completion theory, the proposed algorithm finds the optimal solution, and also requires fewer observations compared to the current state-of-the-art algorithms. We further demonstrate the effectiveness of the proposed algorithm in solving the affine SfM problem, non-rigid SfM and photometric stereo problems."
neurips,https://proceedings.neurips.cc/paper/2010/file/dabd8d2ce74e782c65a973ef76fd540b-Paper.pdf,Probabilistic Deterministic Infinite Automata,"David Pfau, Nicholas Bartlett, Frank Wood",
neurips,https://proceedings.neurips.cc/paper/2010/file/db576a7d2453575f29eab4bac787b919-Paper.pdf,Brain covariance selection: better individual functional connectivity models using population prior,"Gael Varoquaux, Alexandre Gramfort, Jean-baptiste Poline, Bertrand Thirion",
neurips,https://proceedings.neurips.cc/paper/2010/file/db85e2590b6109813dafa101ceb2faeb-Paper.pdf,Word Features for Latent Dirichlet Allocation,"James Petterson, Wray Buntine, Shravan Narayanamurthy, Tibério Caetano, Alex Smola",
neurips,https://proceedings.neurips.cc/paper/2010/file/dc912a253d1e9ba40e2c597ed2376640-Paper.pdf,A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction,"Tamir Hazan, Raquel Urtasun",
neurips,https://proceedings.neurips.cc/paper/2010/file/df7f28ac89ca37bf1abd2f6c184fe1cf-Paper.pdf,Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions,"Achintya Kundu, Vikram Tankasali, Chiranjib Bhattacharyya, Aharon Ben-tal",
neurips,https://proceedings.neurips.cc/paper/2010/file/e00406144c1e7e35240afed70f34166a-Paper.pdf,"Online Learning: Random Averages, Combinatorial Parameters, and Learnability","Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari",
neurips,https://proceedings.neurips.cc/paper/2010/file/e0c641195b27425bb056ac56f8953d24-Paper.pdf,Variable margin losses for classifier design,"Hamed Masnadi-shirazi, Nuno Vasconcelos",
neurips,https://proceedings.neurips.cc/paper/2010/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf,Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable,"Lauren Hannah, Warren Powell, David Blei",
neurips,https://proceedings.neurips.cc/paper/2010/file/e2230b853516e7b05d79744fbd4c9c13-Paper.pdf,Mixture of time-warped trajectory models for movement decoding,"Elaine Corbett, Eric Perreault, Konrad Koerding",
neurips,https://proceedings.neurips.cc/paper/2010/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Paper.pdf,A Discriminative Latent Model of Image Region and Object Tag Correspondence,"Yang Wang, Greg Mori",
neurips,https://proceedings.neurips.cc/paper/2010/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf,Lower Bounds on Rate of Convergence of Cutting Plane Methods,"Xinhua Zhang, Ankan Saha, S.v.n. Vishwanathan","In a recent paper Joachims (2006) presented SVM-Perf, a cutting plane method (CPM) for training linear Support Vector Machines (SVMs) which converges to an
ϵ
ϵ
accurate solution in
O
(
1
/
ϵ
2
)
O
iterations. By tightening the analysis, Teo et al. (2010) showed that
O
(
1
/
ϵ
)
O
iterations suffice. Given the impressive convergence speed of CPM on a number of practical problems, it was conjectured that these rates could be further improved. In this paper we disprove this conjecture. We present counter examples which are not only applicable for training linear SVMs with hinge loss, but also hold for support vector methods which optimize a \emph{multivariate} performance score. However, surprisingly, these problems are not inherently hard. By exploiting the structure of the objective function we can devise an algorithm that converges in
O
(
1
/
√
ϵ
)
O
iterations."
neurips,https://proceedings.neurips.cc/paper/2010/file/e57c6b956a6521b28495f2886ca0977a-Paper.pdf,Self-Paced Learning for Latent Variable Models,"M. Kumar, Benjamin Packer, Daphne Koller",
neurips,https://proceedings.neurips.cc/paper/2010/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Paper.pdf,Learning Efficient Markov Networks,"Vibhav Gogate, William Webb, Pedro Domingos",
neurips,https://proceedings.neurips.cc/paper/2010/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf,Multi-Stage Dantzig Selector,"Ji Liu, Peter Wonka, Jieping Ye","We consider the following sparse signal recovery (or feature selection) problem: given a design matrix
X
∈
R
n
×
m
X
(
m
≫
n
)
(
and a noisy observation vector
y
∈
R
n
y
satisfying
y
=
X
β
∗
+
ϵ
y
where
ϵ
ϵ
is the noise vector following a Gaussian distribution
N
(
0
,
σ
2
I
)
N
, how to recover the signal (or parameter vector)
β
∗
β
when the signal is sparse? The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively refines the target signal
β
∗
β
. We show that if
X
X
obeys a certain condition, then with a large probability the difference between the solution
^
β
β
estimated by the proposed method and the true solution
β
∗
β
measured in terms of the
l
p
l
norm (
p
≥
1
p
) is bounded as
∥
^
β
−
β
∗
∥
p
≤
(
C
(
s
−
N
)
1
/
p
√
log
m
+
Δ
)
σ
,
‖β^−β∗‖p≤(C(s−N)1/plog⁡m+Δ)σ,
C
C
is a constant,
s
s
is the number of nonzero entries in
β
∗
β
,
Δ
Δ
is independent of
m
m
and is much smaller than the first term, and
N
N
is the number of entries of
β
∗
β
larger than a certain value in the order of
O
(
σ
√
log
m
)
O
. The proposed method improves the estimation bound of the standard Dantzig selector approximately from
C
s
1
/
p
√
log
m
σ
C
to
C
(
s
−
N
)
1
/
p
√
log
m
σ
C
where the value
N
N
depends on the number of large entries in
β
∗
β
. When
N
=
s
N
, the proposed algorithm achieves the oracle solution with a high probability. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector."
neurips,https://proceedings.neurips.cc/paper/2010/file/e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf,Batch Bayesian Optimization via Simulation Matching,"Javad Azimi, Alan Fern, Xiaoli Fern",
neurips,https://proceedings.neurips.cc/paper/2010/file/e995f98d56967d946471af29d7bf99f1-Paper.pdf,Inference with Multivariate Heavy-Tails in Linear Models,"Danny Bickson, Carlos Guestrin",
neurips,https://proceedings.neurips.cc/paper/2010/file/eb160de1de89d9058fcb0b968dbbbd68-Paper.pdf,Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models,"Congcong Li, Adarsh Kowdle, Ashutosh Saxena, Tsuhan Chen",
neurips,https://proceedings.neurips.cc/paper/2010/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Paper.pdf,Learning the context of a category,Dan Navarro,
neurips,https://proceedings.neurips.cc/paper/2010/file/eddea82ad2755b24c4e168c5fc2ebd40-Paper.pdf,Learning via Gaussian Herding,"Koby Crammer, Daniel Lee",
neurips,https://proceedings.neurips.cc/paper/2010/file/edfbe1afcf9246bb0d40eb4d8027d90f-Paper.pdf,Monte-Carlo Planning in Large POMDPs,"David Silver, Joel Veness",
neurips,https://proceedings.neurips.cc/paper/2010/file/eecca5b6365d9607ee5a9d336962c534-Paper.pdf,Spatial and anatomical regularization of SVM for brain image analysis,"Remi Cuingnet, Marie Chupin, Habib Benali, Olivier Colliot",
neurips,https://proceedings.neurips.cc/paper/2010/file/f033ab37c30201f73f142449d037028d-Paper.pdf,Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform,Siwei Lyu,
neurips,https://proceedings.neurips.cc/paper/2010/file/f2201f5191c4e92cc5af043eebfd0946-Paper.pdf,"Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models","Felipe Gerhard, Wulfram Gerstner",
neurips,https://proceedings.neurips.cc/paper/2010/file/f29c21d4897f78948b91f03172341b7b-Paper.pdf,Active Instance Sampling via Matrix Partition,Yuhong Guo,
neurips,https://proceedings.neurips.cc/paper/2010/file/f4552671f8909587cf485ea990207f3b-Paper.pdf,Functional form of motion priors in human motion perception,"Hongjing Lu, Tungyou Lin, Alan Lee, Luminita Vese, Alan L. Yuille",
neurips,https://proceedings.neurips.cc/paper/2010/file/f47330643ae134ca204bf6b2481fec47-Paper.pdf,The LASSO risk: asymptotic results and real world examples,"Mohsen Bayati, José Pereira, Andrea Montanari",
neurips,https://proceedings.neurips.cc/paper/2010/file/f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf,"Layered image motion with explicit occlusions, temporal consistency, and depth ordering","Deqing Sun, Erik Sudderth, Michael Black",
neurips,https://proceedings.neurips.cc/paper/2010/file/f93882cbd8fc7fb794c1011d63be6fb6-Paper.pdf,Towards Property-Based Classification of Clustering Paradigms,"Margareta Ackerman, Shai Ben-David, David Loker",
neurips,https://proceedings.neurips.cc/paper/2010/file/f9a40a4780f5e1306c46f1c8daecee3b-Paper.pdf,Implicit encoding of prior probabilities in optimal neural populations,"Deep Ganguli, Eero Simoncelli",
neurips,https://proceedings.neurips.cc/paper/2010/file/faa9afea49ef2ff029a833cccc778fd0-Paper.pdf,Distributed Dual Averaging In Networks,"Alekh Agarwal, Martin J. Wainwright, John C. Duchi",
neurips,https://proceedings.neurips.cc/paper/2010/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf,Probabilistic Inference and Differential Privacy,"Oliver Williams, Frank Mcsherry",
neurips,https://proceedings.neurips.cc/paper/2010/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf,Copula Processes,"Andrew G. Wilson, Zoubin Ghahramani",
neurips,https://proceedings.neurips.cc/paper/2010/file/fccb60fb512d13df5083790d64c4d5dd-Paper.pdf,Learning invariant features using the Transformed Indian Buffet Process,"Joseph Austerweil, Thomas Griffiths",
neurips,https://proceedings.neurips.cc/paper/2010/file/fd06b8ea02fe5b1c2496fe1700e9d16c-Paper.pdf,An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA,"Matthias Hein, Thomas Bühler",
neurips,https://proceedings.neurips.cc/paper/2010/file/fe709c654eac84d5239d1a12a4f71877-Paper.pdf,Fast detection of multiple change-points shared by many signals using group LARS,"Jean-philippe Vert, Kevin Bleakley",
neurips,https://proceedings.neurips.cc/paper/2010/file/fe73f687e5bc5280214e0486b273a5f9-Paper.pdf,Learning To Count Objects in Images,"Victor Lempitsky, Andrew Zisserman",
neurips,https://proceedings.neurips.cc/paper/2010/file/fe8c15fed5f808006ce95eddb7366e35-Paper.pdf,Robust PCA via Outlier Pursuit,"Huan Xu, Constantine Caramanis, Sujay Sanghavi",
neurips,https://proceedings.neurips.cc/paper/2010/file/feab05aa91085b7a8012516bc3533958-Paper.pdf,Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition,"Serhat Bucak, Rong Jin, Anil Jain","Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity of kernel learning in computer vision problems. In this work, we develop an efficient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under consideration share the same combination of kernel functions, and the objective is to find the optimal kernel combination that benefits all the classes. Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge frequently encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis shows that the complexity of our algorithm is
O
(
m
1
/
3
√
l
n
m
)
O
, where
m
m
is the number of classes. Empirical studies with object recognition show that while achieving similar classification accuracy, the proposed method is significantly more efficient than the state-of-the-art algorithms for ML-MKL."
neurips,https://proceedings.neurips.cc/paper/2010/file/ffeabd223de0d4eacb9a3e6e53e5448d-Paper.pdf,Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors,"Alessandro Chiuso, Gianluigi Pillonetto",
