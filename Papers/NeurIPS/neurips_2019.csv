conference,url,title,authors,abstract
neurips,https://proceedings.neurips.cc/paper/2019/file/00989c20ff1386dc386d8124ebcba1a5-Paper.pdf,Compositional Plan Vectors,"Coline Devin, Daniel Geng, Pieter Abbeel, Trevor Darrell, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2019/file/00ac8ed3b4327bdd4ebbebcb2ba10a00-Paper.pdf,Learning to Propagate for Graph Meta-Learning,"LU LIU, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/00e26af6ac3b1c1c49d7c3d79c60d000-Paper.pdf,XNAS: Neural Architecture Search with Expert Advice,"Niv Nayman, Asaf Noy, Tal Ridnik, Itamar Friedman, Rong Jin, Lihi Zelnik",
neurips,https://proceedings.neurips.cc/paper/2019/file/0118a063b4aae95277f0bc1752c75abf-Paper.pdf,Multi-resolution Multi-task Gaussian Processes,"Oliver Hamelijnck, Theodoros Damoulas, Kangrui Wang, Mark Girolami",
neurips,https://proceedings.neurips.cc/paper/2019/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf,Deep Equilibrium Models,"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",
neurips,https://proceedings.neurips.cc/paper/2019/file/01894d6f048493d2cacde3c579c315a3-Paper.pdf,Cross Attention Network for Few-shot Classification,"Ruibing Hou, Hong Chang, Bingpeng MA, Shiguang Shan, Xilin Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/018b59ce1fd616d874afad0f44ba338d-Paper.pdf,Order Optimal One-Shot Distributed Learning,"Arsalan Sharifnassab, Saber Salehkaleybar, S. Jamaloddin Golestani","We consider distributed statistical optimization in one-shot setting, where there are
m
m
machines each observing
n
n
i.i.d samples. Based on its observed samples, each machine then sends an
O
(
log
(
m
n
)
)
O
-length message to a server, at which a parameter minimizing an expected loss is to be estimated. We propose an algorithm called Multi-Resolution Estimator (MRE) whose expected error is no larger than
~
O
(
m
−
1
/
max
(
d
,
2
)
n
−
1
/
2
)
O
, where
d
d
is the dimension of the parameter space. This error bound meets existing lower bounds up to poly-logarithmic factors, and is thereby order optimal. The expected error of MRE, unlike existing algorithms, tends to zero as the number of machines (
m
m
) goes to infinity, even when the number of samples per machine (
n
n
) remains upper bounded by a constant. This property of the MRE algorithm makes it applicable in new machine learning paradigms where
m
m
is much larger than
n
n
."
neurips,https://proceedings.neurips.cc/paper/2019/file/01ce84968c6969bdd5d51c5eeaa3946a-Paper.pdf,Exact Gaussian Processes on a Million Data Points,"Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kilian Q. Weinberger, Andrew Gordon Wilson","Gaussian processes (GPs) are flexible non-parametric models, with a capacity that grows with the available data. However, computational constraints with standard inference procedures have limited exact GPs to problems with fewer than about ten thousand training points, necessitating approximations for larger datasets. In this paper, we develop a scalable approach for exact GPs that leverages multi-GPU parallelization and methods like linear conjugate gradients, accessing the kernel matrix only through matrix multiplication. By partitioning and distributing kernel matrix multiplies, we demonstrate that an exact GP can be trained on over a million points, a task previously thought to be impossible with current computing hardware. Moreover, our approach is generally applicable, without constraints to grid data or specific kernel classes. Enabled by this scalability, we perform the first-ever comparison of exact GPs against scalable GP approximations on datasets with
10
4
−
10
6
10
data points, showing dramatic performance improvements."
neurips,https://proceedings.neurips.cc/paper/2019/file/01d8bae291b1e4724443375634ccfa0e-Paper.pdf,Asymmetric Valleys: Beyond Sharp and Flat Local Minima,"Haowei He, Gao Huang, Yang Yuan",
neurips,https://proceedings.neurips.cc/paper/2019/file/021e1ea77bd91aaa0fc4d01a943a654e-Paper.pdf,Calculating Optimistic Likelihoods Using (Geodesically) Convex Optimization,"Viet Anh Nguyen, Soroosh Shafieezadeh Abadeh, Man-Chung Yue, Daniel Kuhn, Wolfram Wiesemann",
neurips,https://proceedings.neurips.cc/paper/2019/file/0224cd598e48c5041c7947fd5cb20d53-Paper.pdf,"Think out of the ""Box"": Generically-Constrained Asynchronous Composite Optimization and Hedging","Pooria Joulani, András György, Csaba Szepesvari",
neurips,https://proceedings.neurips.cc/paper/2019/file/0234c510bc6d908b28c70ff313743079-Paper.pdf,Improved Precision and Recall Metric for Assessing Generative Models,"Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, Timo Aila",
neurips,https://proceedings.neurips.cc/paper/2019/file/024d2d699e6c1a82c9ba986386f4d824-Paper.pdf,A Direct tilde{O}(1/epsilon) Iteration Parallel Algorithm for Optimal Transport,"Arun Jambulapati, Aaron Sidford, Kevin Tian","Optimal transportation, or computing the Wasserstein or
earth mover's'' distance between two
n
n
-dimensional distributions, is a fundamental primitive which arises in many learning and statistical settings. We give an algorithm which solves the problem to additive
ϵ
ϵ
accuracy with
~
O
(
1
/
ϵ
)
O
parallel depth and
~
O
(
n
2
/
ϵ
)
O
work. \cite{BlanchetJKS18, Quanrud19} obtained this runtime through reductions to positive linear programming and matrix scaling. However, these reduction-based algorithms use subroutines which may be impractical due to requiring solvers for second-order iterations (matrix scaling) or non-parallelizability (positive LP). Our methods match the previous-best work bounds by \cite{BlanchetJKS18, Quanrud19} while either improving parallelization or removing the need for linear system solves, and improve upon the previous best first-order methods running in time
~
O
(
min
(
n
2
/
ϵ
2
,
n
2.5
/
ϵ
)
)
O
\cite{DvurechenskyGK18, LinHJ19}. We obtain our results by a primal-dual extragradient method, motivated by recent theoretical improvements to maximum flow \cite{Sherman17}."
neurips,https://proceedings.neurips.cc/paper/2019/file/0266e33d3f546cb5436a10798e657d97-Paper.pdf,Zero-Shot Semantic Segmentation,"Maxime Bucher, Tuan-Hung VU, Matthieu Cord, Patrick Pérez",
neurips,https://proceedings.neurips.cc/paper/2019/file/02a32ad2669e6fe298e607fe7cc0e1a0-Paper.pdf,Hyperspherical Prototype Networks,"Pascal Mettes, Elise van der Pol, Cees Snoek",
neurips,https://proceedings.neurips.cc/paper/2019/file/02bf86214e264535e3412283e817deaa-Paper.pdf,Lower Bounds on Adversarial Robustness from Optimal Transport,"Arjun Nitin Bhagoji, Daniel Cullina, Prateek Mittal","While progress has been made in understanding the robustness of machine learning classifiers to test-time adversaries (evasion attacks), fundamental questions remain unresolved. In this paper, we use optimal transport to characterize the maximum achievable accuracy in an adversarial classification scenario. In this setting, an adversary receives a random labeled example from one of two classes, perturbs the example subject to a neighborhood constraint, and presents the modified example to the classifier. We define an appropriate cost function such that the minimum transportation cost between the distributions of the two classes determines the \emph{minimum
0
−
1
0
loss for any classifier}. When the classifier comes from a restricted hypothesis class, the optimal transportation cost provides a lower bound. We apply our framework to the case of Gaussian data with norm-bounded adversaries and explicitly show matching bounds for the classification and transport problems and the optimality of linear classifiers. We also characterize the sample complexity of learning in this setting, deriving and extending previously known results as a special case. Finally, we use our framework to study the gap between the optimal classification performance possible and that currently achieved by state-of-the-art robustly trained neural networks for datasets of interest, namely, MNIST, Fashion MNIST and CIFAR-10."
neurips,https://proceedings.neurips.cc/paper/2019/file/02e656adee09f8394b402d9958389b7d-Paper.pdf,A Nonconvex Approach for Exact and Efficient Multichannel Sparse Blind Deconvolution,"Qing Qu, Xiao Li, Zhihui Zhu","We study the multi-channel sparse blind deconvolution (MCS-BD) problem, whose task is to simultaneously recover a kernel
a
a
and multiple sparse inputs
{
x
i
}
p
i
=
1
{
from their circulant convolution
y
i
=
\mb
a
⊛
\mb
x
i
y
(
i
=
1
,
⋯
,
p
i
). We formulate the task as a nonconvex optimization problem over the sphere. Under mild statistical assumptions of the data, we prove that the vanilla Riemannian gradient descent (RGD) method, with random initializations, provably recovers both the kernel
a
a
and the signals
{
x
i
}
p
i
=
1
{
up to a signed shift ambiguity. In comparison with state-of-the-art results, our work shows significant improvements in terms of sample complexity and computational efficiency. Our theoretical results are corroborated by numerical experiments, which demonstrate superior performance of the proposed approach over the previous methods on both synthetic and real datasets."
neurips,https://proceedings.neurips.cc/paper/2019/file/02ed812220b0705fabb868ddbf17ea20-Paper.pdf,Generalization of Reinforcement Learners with Working and Episodic Memory,"Meire Fortunato, Melissa Tan, Ryan Faulkner, Steven Hansen, Adrià Puigdomènech Badia, Gavin Buttimore, Charles Deck, Joel Z. Leibo, Charles Blundell",
neurips,https://proceedings.neurips.cc/paper/2019/file/02f063c236c7eef66324b432b748d15d-Paper.pdf,DTWNet: a Dynamic Time Warping Network,"Xingyu Cai, Tingyang Xu, Jinfeng Yi, Junzhou Huang, Sanguthevar Rajasekaran",
neurips,https://proceedings.neurips.cc/paper/2019/file/030e65da2b1c944090548d36b244b28d-Paper.pdf,Learning Mean-Field Games,"Xin Guo, Anran Hu, Renyuan Xu, Junzi Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/0336dcbab05b9d5ad24f4333c7658a0e-Paper.pdf,Learning Erdos-Renyi Random Graphs via Edge Detecting Queries,"Zihan Li, Matthias Fresacher, Jonathan Scarlett","In this paper, we consider the problem of learning an unknown graph via queries on groups of nodes, with the result indicating whether or not at least one edge is present among those nodes. While learning arbitrary graphs with
n
n
nodes and
k
k
edges is known to be hard in the sense of requiring
Ω
(
min
{
k
2
log
n
,
n
2
}
)
Ω
tests (even when a small probability of error is allowed), we show that learning an Erd\H{o}s-R\'enyi random graph with an average of
\kbar
\kbar
edges is much easier; namely, one can attain asymptotically vanishing error probability with only
O
(
\kbar
log
n
)
O
tests. We establish such bounds for a variety of algorithms inspired by the group testing problem, with explicit constant factors indicating a near-optimal number of tests, and in some cases asymptotic optimality including constant factors. In addition, we present an alternative design that permits a near-optimal sublinear decoding time of
O
(
\kbar
log
2
\kbar
+
\kbar
log
n
)
O
."
neurips,https://proceedings.neurips.cc/paper/2019/file/03573b32b2746e6e8ca98b9123f2249b-Paper.pdf,Cormorant: Covariant Molecular Neural Networks,"Brandon Anderson, Truong Son Hy, Risi Kondor",
neurips,https://proceedings.neurips.cc/paper/2019/file/03793ef7d06ffd63d34ade9d091f1ced-Paper.pdf,Flattening a Hierarchical Clustering through Active Learning,"Fabio Vitale, Anand Rajagopalan, Claudio Gentile",
neurips,https://proceedings.neurips.cc/paper/2019/file/0394ea68951e3299bcdfa75a097d7c11-Paper.pdf,Random Projections and Sampling Algorithms for Clustering of High-Dimensional Polygonal Curves,"Stefan Meintrup, Alexander Munteanu, Dennis Rohde","We study the
k
k
-median clustering problem for high-dimensional polygonal curves with finite but unbounded number of vertices. We tackle the computational issue that arises from the high number of dimensions by defining a Johnson-Lindenstrauss projection for polygonal curves. We analyze the resulting error in terms of the Fr\'echet distance, which is a tractable and natural dissimilarity measure for curves. Our clustering algorithms achieve sublinear dependency on the number of input curves via subsampling. Also, we show that the Fr\'echet distance can not be approximated within any factor of less than
√
2
2
by probabilistically reducing the dependency on the number of vertices of the curves. As a consequence we provide a fast, CUDA-parallelized version of the Alt and Godau algorithm for computing the Fr\'echet distance and use it to evaluate our results empirically."
neurips,https://proceedings.neurips.cc/paper/2019/file/03b264c595403666634ac75d828439bc-Paper.pdf,Explicit Explore-Exploit Algorithms in Continuous State Spaces,Mikael Henaff,
neurips,https://proceedings.neurips.cc/paper/2019/file/04115ec378e476c56d19d827bcf8db56-Paper.pdf,How degenerate is the parametrization of neural networks with the ReLU activation function?,"Dennis Maximilian Elbrächter, Julius Berner, Philipp Grohs",
neurips,https://proceedings.neurips.cc/paper/2019/file/0415740eaa4d9decbc8da001d3fd805f-Paper.pdf,Hyperbolic Graph Convolutional Neural Networks,"Ines Chami, Zhitao Ying, Christopher Ré, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2019/file/043ab21fc5a1607b381ac3896176dac6-Paper.pdf,Spherical Text Embedding,"Yu Meng, Jiaxin Huang, Guangyuan Wang, Chao Zhang, Honglei Zhuang, Lance Kaplan, Jiawei Han",
neurips,https://proceedings.neurips.cc/paper/2019/file/043c2ec6c6390dd0ac5519190a57c88c-Paper.pdf,Random Tessellation Forests,"Shufei Ge, Shijia Wang, Yee Whye Teh, Liangliang Wang, Lloyd Elliott",
neurips,https://proceedings.neurips.cc/paper/2019/file/044a23cadb567653eb51d4eb40acaa88-Paper.pdf,SpArSe: Sparse Architecture Search for CNNs on Resource-Constrained Microcontrollers,"Igor Fedorov, Ryan P. Adams, Matthew Mattina, Paul Whatmough",
neurips,https://proceedings.neurips.cc/paper/2019/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf,Capacity Bounded Differential Privacy,"Kamalika Chaudhuri, Jacob Imola, Ashwin Machanavajjhala",
neurips,https://proceedings.neurips.cc/paper/2019/file/05ae14d7ae387b93370d142d82220f1b-Paper.pdf,Information-Theoretic Generalization Bounds for SGLD via Data-Dependent Estimates,"Jeffrey Negrea, Mahdi Haghifam, Gintare Karolina Dziugaite, Ashish Khisti, Daniel M. Roy",
neurips,https://proceedings.neurips.cc/paper/2019/file/05d0abb9a864ae4981e933685b8b915c-Paper.pdf,Efficient Algorithms for Smooth Minimax Optimization,"Kiran K. Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh","This paper studies first order methods for solving smooth minimax optimization problems
min
x
max
y
g
(
x
,
y
)
min
where
g
(
⋅
,
⋅
)
g
is smooth and
g
(
x
,
⋅
)
g
is concave for each
x
x
. In terms of
g
(
⋅
,
y
)
g
, we consider two settings -- strongly convex and nonconvex -- and improve upon the best known rates in both. For strongly-convex
g
(
⋅
,
y
)
,
 ∀
y
g
, we propose a new direct optimal algorithm combining Mirror-Prox and Nesterov's AGD, and show that it can find global optimum in
˜
O
(
1
/
k
2
)
O
iterations, improving over current state-of-the-art rate of
O
(
1
/
k
)
O
. We use this result along with an inexact proximal point method to provide
˜
O
(
1
/
k
1
/
3
)
O
rate for finding stationary points in the nonconvex setting where
g
(
⋅
,
y
)
g
can be nonconvex. This improves over current best-known rate of
O
(
1
/
k
1
/
5
)
O
. Finally, we instantiate our result for finite nonconvex minimax problems, i.e.,
min
x
max
1
≤
i
≤
m
f
i
(
x
)
min
, with nonconvex
f
i
(
⋅
)
f
, to obtain convergence rate of
O
(
m
1
/
3
√
log
m
/
k
1
/
3
)
O
."
neurips,https://proceedings.neurips.cc/paper/2019/file/05e97c207235d63ceb1db43c60db7bbb-Paper.pdf,Uniform convergence may be unable to explain generalization in deep learning,"Vaishnavh Nagarajan, J. Zico Kolter","Aimed at explaining the surprisingly good generalization behavior of overparameterized deep networks, recent works have developed a variety of generalization bounds for deep learning, all based on the fundamental learning-theoretic technique of uniform convergence. While it is well-known that many of these existing bounds are numerically large, through numerous experiments, we bring to light a more concerning aspect of these bounds: in practice, these bounds can {\em increase} with the training dataset size. Guided by our observations, we then present examples of overparameterized linear classifiers and neural networks trained by gradient descent (GD) where uniform convergence provably cannot
explain generalization'' -- even if we take into account the implicit bias of GD {\em to the fullest extent possible}. More precisely, even if we consider only the set of classifiers output by GD, which have test errors less than some small
ϵ
ϵ
in our settings, we show that applying (two-sided) uniform convergence on this set of classifiers will yield only a vacuous generalization guarantee larger than
1
−
ϵ
1
. Through these findings, we cast doubt on the power of uniform convergence-based generalization bounds to provide a complete picture of why overparameterized deep networks generalize well."
neurips,https://proceedings.neurips.cc/paper/2019/file/0609154fa35b3194026346c9cac2a248-Paper.pdf,First order expansion of convex regularized estimators,"Pierre Bellec, Arun Kuchibhotla","We consider first order expansions of convex penalized estimators in high-dimensional regression problems with random designs. Our setting includes linear regression and logistic regression as special cases. For a given penalty function
h
h
and the corresponding penalized estimator
\hbeta
\hbeta
, we construct a quantity
η
η
, the first order expansion of
\hbeta
\hbeta
, such that the distance between
\hbeta
\hbeta
and
η
η
is an order of magnitude smaller than the estimation error
∥
^
β
−
β
∗
∥
‖
. In this sense, the first order expansion
η
η
can be thought of as a generalization of influence functions from the mathematical statistics literature to regularized estimators in high-dimensions. Such first order expansion implies that the risk of
^
β
β
is asymptotically the same as the risk of
η
η
which leads to a precise characterization of the MSE of
\hbeta
\hbeta
; this characterization takes a particularly simple form for isotropic design. Such first order expansion also leads to inference results based on
^
β
β
. We provide sufficient conditions for the existence of such first order expansion for three regularizers: the Lasso in its constrained form, the lasso in its penalized form, and the Group-Lasso. The results apply to general loss functions under some conditions and those conditions are satisfied for the squared loss in linear regression and for the logistic loss in the logistic model."
neurips,https://proceedings.neurips.cc/paper/2019/file/060fd70a06ead2e1079d27612b84aff4-Paper.pdf,Robust exploration in linear quadratic reinforcement learning,"Jack Umenberger, Mina Ferizbegovic, Thomas B. Schön, Håkan Hjalmarsson",
neurips,https://proceedings.neurips.cc/paper/2019/file/063e26c670d07bb7c4d30e6fc69fe056-Paper.pdf,Modeling Uncertainty by Learning a Hierarchy of Deep Neural Connections,"Raanan Yehezkel Rohekar, Yaniv Gurwicz, Shami Nisimov, Gal Novik",
neurips,https://proceedings.neurips.cc/paper/2019/file/0668e20b3c9e9185b04b3d2a9dc8fa2d-Paper.pdf,Meta-Surrogate Benchmarking for Hyperparameter Optimization,"Aaron Klein, Zhenwen Dai, Frank Hutter, Neil Lawrence, Javier Gonzalez",
neurips,https://proceedings.neurips.cc/paper/2019/file/067a26d87265ea39030f5bd82408ce7c-Paper.pdf,Time/Accuracy Tradeoffs for Learning a ReLU with respect to Gaussian Marginals,"Surbhi Goel, Sushrut Karmalkar, Adam Klivans","We consider the problem of computing the best-fitting ReLU with respect to square-loss on a training set when the examples have been drawn according to a spherical Gaussian distribution (the labels can be arbitrary). Let
\opt
<
1
\opt
be the population loss of the best-fitting ReLU. We prove:
\begin{itemize}
\item Finding a ReLU with square-loss $\opt + \epsilon$ is as
  hard as the problem of learning sparse parities with noise, widely thought
  to be computationally intractable.  This is the first hardness
  result for learning a ReLU with respect to Gaussian marginals, and
  our results imply --{\em unconditionally}-- that gradient descent cannot
  converge to the global minimum in polynomial time.
\item There exists an efficient approximation algorithm for finding the
  best-fitting ReLU that achieves error $O(\opt^{2/3})$.  The
  algorithm uses a novel reduction to noisy halfspace learning with
  respect to $0/1$ loss. 
\end{itemize}
\begin{itemize}\item Finding a ReLU with square-loss $\opt + \epsilon$ is as  hard as the problem of learning sparse parities with noise, widely thought  to be computationally intractable.  This is the first hardness  result for learning a ReLU with respect to Gaussian marginals, and  our results imply --{\em unconditionally}-- that gradient descent cannot  converge to the global minimum in polynomial time.\item There exists an efficient approximation algorithm for finding the  best-fitting ReLU that achieves error $O(\opt^{2/3})$.  The  algorithm uses a novel reduction to noisy halfspace learning with  respect to $0/1$ loss. \end{itemize}
Prior work due to Soltanolkotabi \cite{soltanolkotabi2017learning} showed that gradient descent {\em can} find the best-fitting ReLU with respect to Gaussian marginals, if the training set is {\em exactly} labeled by a ReLU."
neurips,https://proceedings.neurips.cc/paper/2019/file/06a50e3f66db4a334202d3adfd31c589-Paper.pdf,Bayesian Optimization under Heavy-tailed Payoffs,"Sayak Ray Chowdhury, Aditya Gopalan","We consider black box optimization of an unknown function in the nonparametric Gaussian process setting when the noise in the observed function values can be heavy tailed. This is in contrast to existing literature that typically assumes sub-Gaussian noise distributions for queries. Under the assumption that the unknown function belongs to the Reproducing Kernel Hilbert Space (RKHS) induced by a kernel, we first show that an adaptation of the well-known GP-UCB algorithm with reward truncation enjoys sublinear
~
O
(
T
2
+
α
2
(
1
+
α
)
)
O
regret even with only the
(
1
+
α
)
(
-th moments,
α
∈
(
0
,
1
]
α
, of the reward distribution being bounded (
~
O
O
hides logarithmic factors). However, for the common squared exponential (SE) and Mat\'{e}rn kernels, this is seen to be significantly larger than a fundamental
Ω
(
T
1
1
+
α
)
Ω
lower bound on regret. We resolve this gap by developing novel Bayesian optimization algorithms, based on kernel approximation techniques, with regret bounds matching the lower bound in order for the SE kernel. We numerically benchmark the algorithms on environments based on both synthetic models and real-world data sets."
neurips,https://proceedings.neurips.cc/paper/2019/file/06bf16f1f0372a63d520eac6cf7c5af7-Paper.pdf,Distribution Learning of a Random Spatial Field with a Location-Unaware Mobile Sensor,"Meera Pai, Animesh Kumar",
neurips,https://proceedings.neurips.cc/paper/2019/file/070dbb6024b5ef93784428afc71f2146-Paper.pdf,State Aggregation Learning from Markov Transition Data,"Yaqi Duan, Tracy Ke, Mengdi Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/07211688a0869d995947a8fb11b215d6-Paper.pdf,Reliable training and estimation of variance networks,"Nicki Skafte, Martin Jørgensen, Søren Hauberg",
neurips,https://proceedings.neurips.cc/paper/2019/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf,Meta-Learning with Implicit Gradients,"Aravind Rajeswaran, Chelsea Finn, Sham M. Kakade, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2019/file/074177d3eb6371e32c16c55a3b8f706b-Paper.pdf,Differentially Private Markov Chain Monte Carlo,"Mikko Heikkilä, Joonas Jälkö, Onur Dikmen, Antti Honkela",
neurips,https://proceedings.neurips.cc/paper/2019/file/07a4e20a7bbeeb7a736682b26b16ebe8-Paper.pdf,Universal Boosting Variational Inference,"Trevor Campbell, Xinglong Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/07a9d3fed4c5ea6b17e80258dee231fa-Paper.pdf,LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning,"Yali Du, Lei Han, Meng Fang, Ji Liu, Tianhong Dai, Dacheng Tao",
neurips,https://proceedings.neurips.cc/paper/2019/file/07cb5f86508f146774a2fac4373a8e50-Paper.pdf,A Normative Theory for Causal Inference and Bayes Factor Computation in Neural Circuits,"Wenhao Zhang, Si Wu, Brent Doiron, Tai Sing Lee",
neurips,https://proceedings.neurips.cc/paper/2019/file/0801b20e08c3242125d512808cd74302-Paper.pdf,The Geometry of Deep Networks: Power Diagram Subdivision,"Randall Balestriero, Romain Cosentino, Behnaam Aazhang, Richard Baraniuk",
neurips,https://proceedings.neurips.cc/paper/2019/file/08040837089cdf46631a10aca5258e16-Paper.pdf,Visual Sequence Learning in Hierarchical Prediction Networks and Primate Visual Cortex,,
neurips,https://proceedings.neurips.cc/paper/2019/file/084afd913ab1e6ea58b8ca73f6cb41a6-Paper.pdf,Equal Opportunity in Online Classification with Partial Feedback,"Yahav Bechavod, Katrina Ligett, Aaron Roth, Bo Waggoner, Steven Z. Wu",
neurips,https://proceedings.neurips.cc/paper/2019/file/08b7dc6e8b36bcaac15847827b7951a9-Paper.pdf,Semi-Parametric Efficient Policy Learning with Continuous Actions,"Victor Chernozhukov, Mert Demirer, Greg Lewis, Vasilis Syrgkanis",
neurips,https://proceedings.neurips.cc/paper/2019/file/091bc5440296cc0e41dd60ce22fbaf88-Paper.pdf,Concentration of risk measures: A Wasserstein distance approach,"Sanjay P. Bhat, Prashanth L.A.",
neurips,https://proceedings.neurips.cc/paper/2019/file/0937fb5864ed06ffb59ae5f9b5ed67a9-Paper.pdf,Interior-Point Methods Strike Back: Solving the Wasserstein Barycenter Problem,"DongDong Ge, Haoyue Wang, Zikai Xiong, Yinyu Ye",
neurips,https://proceedings.neurips.cc/paper/2019/file/093b60fd0557804c8ba0cbf1453da22f-Paper.pdf,Coda: An End-to-End Neural Program Decompiler,"Cheng Fu, Huili Chen, Haolan Liu, Xinyun Chen, Yuandong Tian, Farinaz Koushanfar, Jishen Zhao",
neurips,https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,"Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, zhifeng Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf,DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node,"Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vardhan Simhadri, Ravishankar Krishnawamy, Rohan Kadekodi",
neurips,https://proceedings.neurips.cc/paper/2019/file/09a8a8976abcdfdee15128b4cc02f33a-Paper.pdf,Linear Stochastic Bandits Under Safety Constraints,"Sanae Amani, Mahnoosh Alizadeh, Christos Thrampoulidis",
neurips,https://proceedings.neurips.cc/paper/2019/file/09ab23b6b607496f095feed7aaa1259b-Paper.pdf,Power analysis of knockoff filters for correlated designs,"Jingbo Liu, Philippe Rigollet","The knockoff filter introduced by Barber and Cand\`es 2016 is an elegant framework for controlling the false discovery rate in variable selection. While empirical results indicate that this methodology is not too conservative, there is no conclusive theoretical result on its power. When the predictors are i.i.d.\ Gaussian, it is known that as the signal to noise ratio tend to infinity, the knockoff filter is consistent in the sense that one can make FDR go to 0 and power go to 1 simultaneously. In this work we study the case where the predictors have a general covariance matrix
\bsigma
\bsigma
. We introduce a simple functional called \emph{effective signal deficiency (ESD)} of the covariance matrix of the predictors that predicts consistency of various variable selection methods. In particular, ESD reveals that the structure of the precision matrix plays a central role in consistency and therefore, so does the conditional independence structure of the predictors. To leverage this connection, we introduce \emph{Conditional Independence knockoff}, a simple procedure that is able to compete with the more sophisticated knockoff filters and that is defined when the predictors obey a Gaussian tree graphical models (or when the graph is sufficiently sparse). Our theoretical results are supported by numerical evidence on synthetic data."
neurips,https://proceedings.neurips.cc/paper/2019/file/09fb05dd477d4ae6479985ca56c5a12d-Paper.pdf,Implicitly learning to reason in first-order logic,"Vaishak Belle, Brendan Juba",
neurips,https://proceedings.neurips.cc/paper/2019/file/0a3df70393993583a13c0dd6686f3f32-Paper.pdf,Low-Rank Bandit Methods for High-Dimensional Dynamic Pricing,"Jonas W. Mueller, Vasilis Syrgkanis, Matt Taddy",
neurips,https://proceedings.neurips.cc/paper/2019/file/0a4bbceda17a6253386bc9eb45240e25-Paper.pdf,Learning Stable Deep Dynamics Models,"J. Zico Kolter, Gaurav Manek",
neurips,https://proceedings.neurips.cc/paper/2019/file/0a9fdbb17feb6ccb7ec405cfb85222c4-Paper.pdf,Beyond the Single Neuron Convex Barrier for Neural Network Certification,"Gagandeep Singh, Rupanshu Ganvir, Markus Püschel, Martin Vechev",
neurips,https://proceedings.neurips.cc/paper/2019/file/0ae775a8cb3b499ad1fca944e6f5c836-Paper.pdf,Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models,"Yuge Shi, Siddharth N, Brooks Paige, Philip Torr",
neurips,https://proceedings.neurips.cc/paper/2019/file/0af787945872196b42c9f73ead2565c8-Paper.pdf,Language as an Abstraction for Hierarchical Deep Reinforcement Learning,"YiDing Jiang, Shixiang (Shane) Gu, Kevin P. Murphy, Chelsea Finn",
neurips,https://proceedings.neurips.cc/paper/2019/file/0b105cf1504c4e241fcc6d519ea962fb-Paper.pdf,High-dimensional multivariate forecasting with low-rank Gaussian Copula Processes,"David Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico, Jan Gasthaus",
neurips,https://proceedings.neurips.cc/paper/2019/file/0bfce127947574733b19da0f30739fcd-Paper.pdf,Learning Macroscopic Brain Connectomes via Group-Sparse Factorization,"Farzane Aminmansour, Andrew Patterson, Lei Le, Yisu Peng, Daniel Mitchell, Franco Pestilli, Cesar F. Caiafa, Russell Greiner, Martha White",
neurips,https://proceedings.neurips.cc/paper/2019/file/0c215f194276000be6a6df6528067151-Paper.pdf,Optimal Sketching for Kronecker Product Regression and Low Rank Approximation,"Huaian Diao, Rajesh Jayaram, Zhao Song, Wen Sun, David Woodruff","We study the Kronecker product regression problem, in which the design matrix is a Kronecker product of two or more matrices. Formally, given
A
i
∈
\R
n
i
×
d
i
A
for
i
=
1
,
2
,
…
,
q
i
where
n
i
≫
d
i
n
for each
i
i
, and
b
∈
\R
n
1
n
2
⋯
n
q
b
, let
A
=
A
i
⊗
A
2
⊗
⋯
⊗
A
q
A
. Then for
p
∈
[
1
,
2
]
p
, the goal is to find
x
∈
\R
d
1
⋯
d
q
x
that approximately minimizes
∥
A
x
−
b
∥
p
‖
. Recently, Diao, Song, Sun, and Woodruff (AISTATS, 2018) gave an algorithm which is faster than forming the Kronecker product
A
∈
\R
n
1
⋯
n
q
×
d
1
⋯
d
q
A
. Specifically, for
p
=
2
p
they achieve a running time of
O
(
∑
q
i
=
1
nnz
(
A
i
)
+
nnz
(
b
)
)
O
, where
nnz
(
A
i
)
nnz
is the number of non-zero entries in
A
i
A
. Note that
nnz
(
b
)
nnz
can be as large as
Θ
(
n
1
⋯
n
q
)
Θ
. For
p
=
1
,
p
q
=
2
q
and
n
1
=
n
2
n
, they achieve a worse bound of
O
(
n
3
/
2
1
poly
(
d
1
d
2
)
+
nnz
(
b
)
)
O
. In this work, we provide significantly faster algorithms. For
p
=
2
p
, our running time is
O
(
∑
q
i
=
1
nnz
(
A
i
)
)
O
, which has no dependence on
nnz
(
b
)
nnz
. For
p
<
2
p
, our running time is
O
(
∑
q
i
=
1
nnz
(
A
i
)
+
nnz
(
b
)
)
O
, which matches the prior best running time for
p
=
2
p
. We also consider the related all-pairs regression problem, where given
A
∈
\R
n
×
d
,
b
∈
\R
n
A
, we want to solve
min
x
∈
\R
d
∥
¯
A
x
−
¯
b
∥
p
min
, where
¯
A
∈
\R
n
2
×
d
,
¯
b
∈
\R
n
2
A
consist of all pairwise differences of the rows of
A
,
b
A
. We give an
O
(
nnz
(
A
)
)
O
time algorithm for
p
∈
[
1
,
2
]
p
, improving the
Ω
(
n
2
)
Ω
time required to form
¯
A
A
. Finally, we initiate the study of Kronecker product low rank and and low-trank approximation. For input
A
A
as above, we give
O
(
∑
q
i
=
1
nnz
(
A
i
)
)
O
time algorithms, which is much faster than computing
A
A
."
neurips,https://proceedings.neurips.cc/paper/2019/file/0c4b1eeb45c90b52bfb9d07943d855ab-Paper.pdf,Deep Gamblers: Learning to Abstain with Portfolio Theory,"Ziyin Liu, Zhikang Wang, Paul Pu Liang, Russ R. Salakhutdinov, Louis-Philippe Morency, Masahito Ueda","We deal with the selective classification problem (supervised-learning problem with a rejection option), where we want to achieve the best performance at a certain level of coverage of the data. We transform the original
m
m
-class classification problem to (m+1)-class where the (m+1)-th class represents the model abstaining from making a prediction due to disconfidence. Inspired by portfolio theory, we propose a loss function for the selective classification problem based on the doubling rate of gambling. Minimizing this loss function corresponds naturally to maximizing the return of a horse race, where a player aims to balance between betting on an outcome (making a prediction) when confident and reserving one's winnings (abstaining) when not confident. This loss function allows us to train neural networks and characterize the disconfidence of prediction in an end-to-end fashion. In comparison with previous methods, our method requires almost no modification to the model inference algorithm or model architecture. Experiments show that our method can identify uncertainty in data points, and achieves strong results on SVHN and CIFAR10 at various coverages of the data."
neurips,https://proceedings.neurips.cc/paper/2019/file/0c72cb7ee1512f800abe27823a792d03-Paper.pdf,DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs,"Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, Daisy Zhe Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/0cb929eae7a499e50248a3a78f7acfc7-Paper.pdf,Combinatorial Inference against Label Noise,"Paul Hongsuck Seo, Geeho Kim, Bohyung Han","Label noise is one of the critical sources that degrade generalization performance of deep neural networks significantly. To handle the label noise issue in a principled way, we propose a unique classification framework of constructing multiple models in heterogeneous coarse-grained meta-class spaces and making joint inference of the trained models for the final predictions in the original (base) class space. Our approach reduces noise level by simply constructing meta-classes and improves accuracy via combinatorial inferences over multiple constituent classifiers. Since the proposed framework has distinct and complementary properties for the given problem, we can even incorporate additional off-the-shelf learning algorithms to improve accuracy further. We also introduce techniques to organize multiple heterogeneous meta-class sets using
k
k
-means clustering and identify a desirable subset leading to learn compact models. Our extensive experiments demonstrate outstanding performance in terms of accuracy and efficiency compared to the state-of-the-art methods under various synthetic noise configurations and in a real-world noisy dataset."
neurips,https://proceedings.neurips.cc/paper/2019/file/0cbed40c0d920b94126eaf5e707be1f5-Paper.pdf,Localized Structured Prediction,"Carlo Ciliberto, Francis Bach, Alessandro Rudi",
neurips,https://proceedings.neurips.cc/paper/2019/file/0d0fd7c6e093f7b804fa0150b875b868-Paper.pdf,Fast Low-rank Metric Learning for Large-scale and High-dimensional Data,"Han Liu, Zhizhong Han, Yu-Shen Liu, Ming Gu",
neurips,https://proceedings.neurips.cc/paper/2019/file/0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf,Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent,"Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, Jeffrey Pennington",
neurips,https://proceedings.neurips.cc/paper/2019/file/0d2b2061826a5df3221116a5085a6052-Paper.pdf,Retrosynthesis Prediction with Conditional Graph Logic Network,"Hanjun Dai, Chengtao Li, Connor Coley, Bo Dai, Le Song",
neurips,https://proceedings.neurips.cc/paper/2019/file/0d441de75945e5acbc865406fc9a2559-Paper.pdf,Efficient Pure Exploration in Adaptive Round model,"Tianyuan Jin, Jieming SHI, Xiaokui Xiao, Enhong Chen","In the adaptive setting, many multi-armed bandit applications allow the learner to adaptively draw samples and adjust sampling strategy in rounds. In many real applications, not only the query complexity but also the round complexity need to be optimized. In this paper, we study both PAC and exact top-
k
k
arm identification problems and design efficient algorithms considering both round complexity and query complexity. For PAC problem, we achieve optimal query complexity and use only
O
(
log
∗
k
δ
(
n
)
)
O
rounds, which matches the lower bound of round complexity, while most of existing works need
Θ
(
log
n
k
)
Θ
rounds. For exact top-
k
k
arm identification, we improve the round complexity factor from
log
n
log
to
log
∗
1
δ
(
n
)
log
, and achieve near optimal query complexity. In experiments, our algorithms conduct far fewer rounds, and outperform state of the art by orders of magnitude with respect to query cost."
neurips,https://proceedings.neurips.cc/paper/2019/file/0dd1bc593a91620daecf7723d2235624-Paper.pdf,Unsupervised Emergence of Egocentric Spatial Structure from Sensorimotor Prediction,"Alban Laflaquière, Michael Garcia Ortiz",
neurips,https://proceedings.neurips.cc/paper/2019/file/0defd533d51ed0a10c5c9dbf93ee78a5-Paper.pdf,Adversarial Robustness through Local Linearization,"Chongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Krishnamurthy Dvijotham, Alhussein Fawzi, Soham De, Robert Stanforth, Pushmeet Kohli",
neurips,https://proceedings.neurips.cc/paper/2019/file/0e095e054ee94774d6a496099eb1cf6a-Paper.pdf,Generalized Off-Policy Actor-Critic,"Shangtong Zhang, Wendelin Boehmer, Shimon Whiteson",
neurips,https://proceedings.neurips.cc/paper/2019/file/0e1feae55e360ff05fef58199b3fa521-Paper.pdf,"Average Individual Fairness: Algorithms, Generalization and Experiments","Saeed Sharifi-Malvajerdi, Michael Kearns, Aaron Roth",
neurips,https://proceedings.neurips.cc/paper/2019/file/0e2db0cb2c4645904a054261104b7a14-Paper.pdf,"Comparing distributions:
ℓ
1
ℓ
geometry improves kernel two-sample testing","meyer scetbon, Gael Varoquaux","Are two sets of observations drawn from the same distribution? This problem is a two-sample test. Kernel methods lead to many appealing properties. Indeed state-of-the-art approaches use the
L
2
L
distance between kernel-based distribution representatives to derive their test statistics. Here, we show that
L
p
L
distances (with
p
≥
1
p
) between these distribution representatives give metrics on the space of distributions that are well-behaved to detect differences between distributions as they metrize the weak convergence. Moreover, for analytic kernels, we show that the
L
1
L
geometry gives improved testing power for scalable computational procedures. Specifically, we derive a finite dimensional approximation of the metric given as the
ℓ
1
ℓ
norm of a vector which captures differences of expectations of analytic functions evaluated at spatial locations or frequencies (i.e, features). The features can be chosen to maximize the differences of the distributions and give interpretable indications of how they differs. Using an
ℓ
1
ℓ
norm gives better detection because differences between representatives are dense as we use analytic kernels (non-zero almost everywhere). The tests are consistent, while much faster than state-of-the-art quadratic-time kernel-based tests. Experiments on artificial and real-world problems demonstrate improved power/time tradeoff than the state of the art, based on
ℓ
2
ℓ
norms, and in some cases, better outright power than even the most expensive quadratic-time tests. This performance gain is retained even in high dimensions."
neurips,https://proceedings.neurips.cc/paper/2019/file/0e4f5cc9f4f3f7f1651a6b9f9214e5b1-Paper.pdf,Nonstochastic Multiarmed Bandits with Unrestricted Delays,"Tobias Sommer Thune, Nicolò Cesa-Bianchi, Yevgeny Seldin","We investigate multiarmed bandits with delayed feedback, where the delays need neither be identical nor bounded. We first prove that ""delayed"" Exp3 achieves the
O
(
√
(
K
T
+
D
)
ln
K
)
O
regret bound conjectured by Cesa-Bianchi et al. [2016] in the case of variable, but bounded delays. Here,
K
K
is the number of actions and
D
D
is the total delay over
T
T
rounds. We then introduce a new algorithm that lifts the requirement of bounded delays by using a wrapper that skips rounds with excessively large delays. The new algorithm maintains the same regret bound, but similar to its predecessor requires prior knowledge of
D
D
and
T
T
. For this algorithm we then construct a novel doubling scheme that forgoes the prior knowledge requirement under the assumption that the delays are available at action time (rather than at loss observation time). This assumption is satisfied in a broad range of applications, including interaction with servers and service providers. The resulting oracle regret bound is of order
min
β
(
|
S
β
|
+
β
ln
K
+
(
K
T
+
D
β
)
/
β
)
min
, where
|
S
β
|
|
is the number of observations with delay exceeding
β
β
, and
D
β
D
is the total delay of observations with delay below
β
β
. The bound relaxes to
O
(
√
(
K
T
+
D
)
ln
K
)
O
, but we also provide examples where
D
β
≪
D
D
and the oracle bound has a polynomially better dependence on the problem parameters."
neurips,https://proceedings.neurips.cc/paper/2019/file/0e57098d0318a954d1443e2974a38fac-Paper.pdf,Approximate Bayesian Inference for a Mechanistic Model of Vesicle Release at a Ribbon Synapse,"Cornelius Schröder, Ben James, Leon Lagnado, Philipp Berens",
neurips,https://proceedings.neurips.cc/paper/2019/file/0e79548081b4bd0df3c77c5ba2c23289-Paper.pdf,Data-dependent Sample Complexity of Deep Neural Networks via Lipschitz Augmentation,"Colin Wei, Tengyu Ma",
neurips,https://proceedings.neurips.cc/paper/2019/file/0e7c7d6c41c76b9ee6445ae01cc0181d-Paper.pdf,Semi-supervisedly Co-embedding Attributed Networks,"Zaiqiao Meng, Shangsong Liang, Jinyuan Fang, Teng Xiao",
neurips,https://proceedings.neurips.cc/paper/2019/file/0e900ad84f63618452210ab8baae0218-Paper.pdf,Adaptive Auxiliary Task Weighting for Reinforcement Learning,"Xingyu Lin, Harjatin Baweja, George Kantor, David Held",
neurips,https://proceedings.neurips.cc/paper/2019/file/0ec04cb3912c4f08874dd03716f80df1-Paper.pdf,Continuous Hierarchical Representations with Poincaré Variational Auto-Encoders,"Emile Mathieu, Charline Le Lan, Chris J. Maddison, Ryota Tomioka, Yee Whye Teh",
neurips,https://proceedings.neurips.cc/paper/2019/file/0ed9422357395a0d4879191c66f4faa2-Paper.pdf,Training Image Estimators without Image Ground Truth,"Zhihao Xia, Ayan Chakrabarti",
neurips,https://proceedings.neurips.cc/paper/2019/file/0ee8b85a85a49346fdff9665312a5cc4-Paper.pdf,On the Convergence Rate of Training Recurrent Neural Networks,"Zeyuan Allen-Zhu, Yuanzhi Li, Zhao Song","How can local-search methods such as stochastic gradient descent (SGD) avoid bad local minima in training multi-layer neural networks? Why can they fit random labels even given non-convex and non-smooth architectures? Most existing theory only covers networks with one hidden layer, so can we go deeper? In this paper, we focus on recurrent neural networks (RNNs) which are multi-layer networks widely used in natural language processing. They are harder to analyze than feedforward neural networks, because the \emph{same} recurrent unit is repeatedly applied across the entire time horizon of length
L
L
, which is analogous to feedforward networks of depth
L
L
. We show when the number of neurons is sufficiently large, meaning polynomial in the training data size and in
L
L
, then SGD is capable of minimizing the regression loss in the linear convergence rate. This gives theoretical evidence of how RNNs can memorize data. More importantly, in this paper we build general toolkits to analyze multi-layer networks with ReLU activations. For instance, we prove why ReLU activations can prevent exponential gradient explosion or vanishing, and build a perturbation theory to analyze first-order approximation of multi-layer networks."
neurips,https://proceedings.neurips.cc/paper/2019/file/0f9cafd014db7a619ddb4276af0d692c-Paper.pdf,Minimizers of the Empirical Risk and Risk Monotonicity,"Marco Loog, Tom Viering, Alexander Mey",
neurips,https://proceedings.neurips.cc/paper/2019/file/0fc170ecbb8ff1afb2c6de48ea5343e7-Paper.pdf,Factor Group-Sparse Regularization for Efficient Low-Rank Matrix Recovery,"Jicong Fan, Lijun Ding, Yudong Chen, Madeleine Udell","This paper develops a new class of nonconvex regularizers for low-rank matrix recovery. Many regularizers are motivated as convex relaxations of the \emph{matrix rank} function. Our new factor group-sparse regularizers are motivated as a relaxation of the \emph{number of nonzero columns} in a factorization of the matrix. These nonconvex regularizers are sharper than the nuclear norm; indeed, we show they are related to Schatten-
p
p
norms with arbitrarily small
0
<
p
≤
1
0
. Moreover, these factor group-sparse regularizers can be written in a factored form that enables efficient and effective nonconvex optimization; notably, the method does not use singular value decomposition. We provide generalization error bounds for low-rank matrix completion which show improved upper bounds for Schatten-
p
p
norm reglarization as
p
p
decreases. Compared to the max norm and the factored formulation of the nuclear norm, factor group-sparse regularizers are more efficient, accurate, and robust to the initial guess of rank. Experiments show promising performance of factor group-sparse regularization for low-rank matrix completion and robust principal component analysis."
neurips,https://proceedings.neurips.cc/paper/2019/file/0fd7e4f42a8b4b4ef33394d35212b13e-Paper.pdf,Möbius Transformation for Fast Inner Product Search on Graph,"Zhixin Zhou, Shulong Tan, Zhaozhuo Xu, Ping Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/1019c8091693ef5c5f55970346633f92-Paper.pdf,The Label Complexity of Active Learning from Observational Data,"Songbai Yan, Kamalika Chaudhuri, Tara Javidi","Prior work on this problem uses disagreement-based active learning, along with an importance weighted loss estimator to account for counterfactuals, which leads to a high label complexity. We show how to instead incorporate a more efficient counterfactual risk minimizer into the active learning algorithm. This requires us to modify both the counterfactual risk to make it amenable to active learning, as well as the active learning process to make it amenable to the risk. We provably demonstrate that the result of this is an algorithm which is statistically consistent as well as more label-efficient than prior work."
neurips,https://proceedings.neurips.cc/paper/2019/file/103303dd56a731e377d01f6a37badae3-Paper.pdf,Hyperbolic Graph Neural Networks,"Qi Liu, Maximilian Nickel, Douwe Kiela",
neurips,https://proceedings.neurips.cc/paper/2019/file/10493aa88605cad5ab4752b04a63d172-Paper.pdf,Learning Fairness in Multi-Agent Systems,"Jiechuan Jiang, Zongqing Lu",
neurips,https://proceedings.neurips.cc/paper/2019/file/107878346e1d8f8fe6af7a7a588aa807-Paper.pdf,On Robustness to Adversarial Examples and Polynomial Optimization,"Pranjal Awasthi, Abhratanu Dutta, Aravindan Vijayaraghavan","The main contribution of this work is to exhibit a strong connection between achieving robustness to adversarial examples, and a rich class of polynomial optimization problems, thereby making progress on the above questions. In particular, we leverage this connection to (a) design computationally efficient robust algorithms with provable guarantees for a large class of hypothesis, namely linear classifiers and degree-2 polynomial threshold functions~(PTFs), (b) give a precise characterization of the price of achieving robustness in a computationally efficient manner for these classes, (c) design efficient algorithms to certify robustness and generate adversarial attacks in a principled manner for 2-layer neural networks. We empirically demonstrate the effectiveness of these attacks on real data."
neurips,https://proceedings.neurips.cc/paper/2019/file/1091660f3dff84fd648efe31391c5524-Paper.pdf,In-Place Zero-Space Memory Protection for CNN,"Hui Guan, Lin Ning, Zhen Lin, Xipeng Shen, Huiyang Zhou, Seung-Hwan Lim",
neurips,https://proceedings.neurips.cc/paper/2019/file/10a5ab2db37feedfdeaab192ead4ac0e-Paper.pdf,Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs,"Max Simchowitz, Kevin G. Jamieson","This paper establishes that optimistic algorithms attain gap-dependent and non-asymptotic logarithmic regret for episodic MDPs. In contrast to prior work, our bounds do not suffer a dependence on diameter-like quantities or ergodicity, and smoothly interpolate between the gap dependent logarithmic-regret, and the
˜
O
(
√
H
S
A
T
)
O
-minimax rate. The key technique in our analysis is a novel
clipped'' regret decomposition which applies to a broad family of recent optimistic algorithms for episodic MDPs."
neurips,https://proceedings.neurips.cc/paper/2019/file/10ff0b5e85e5b85cc3095d431d8c08b4-Paper.pdf,Discovery of Useful Questions as Auxiliary Tasks,"Vivek Veeriah, Matteo Hessel, Zhongwen Xu, Janarthanan Rajendran, Richard L. Lewis, Junhyuk Oh, Hado P. van Hasselt, David Silver, Satinder Singh",
neurips,https://proceedings.neurips.cc/paper/2019/file/110209d8fae7417509ba71ad97c17639-Paper.pdf,Sequential Neural Processes,"Gautam Singh, Jaesik Yoon, Youngsung Son, Sungjin Ahn",
neurips,https://proceedings.neurips.cc/paper/2019/file/1113d7a76ffceca1bb350bfe145467c6-Paper.pdf,"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask","Hattie Zhou, Janice Lan, Rosanne Liu, Jason Yosinski",
neurips,https://proceedings.neurips.cc/paper/2019/file/1138d90ef0a0848a542e57d1595f58ea-Paper.pdf,Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes,"James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, Richard E. Turner",
neurips,https://proceedings.neurips.cc/paper/2019/file/118921efba23fc329e6560b27861f0c2-Paper.pdf,A Simple Baseline for Bayesian Uncertainty in Deep Learning,"Wesley J. Maddox, Pavel Izmailov, Timur Garipov, Dmitry P. Vetrov, Andrew Gordon Wilson",
neurips,https://proceedings.neurips.cc/paper/2019/file/11b9842e0a271ff252c1903e7132cd68-Paper.pdf,CPM-Nets: Cross Partial Multi-View Networks,"Changqing Zhang, Zongbo Han, yajie cui, Huazhu Fu, Joey Tianyi Zhou, Qinghua Hu",
neurips,https://proceedings.neurips.cc/paper/2019/file/122e27d57ae8ecb37f3f1da67abb33cb-Paper.pdf,Low-Complexity Nonparametric Bayesian Online Prediction with Universal Guarantees,"Alix LHERITIER, Frederic Cazals",
neurips,https://proceedings.neurips.cc/paper/2019/file/124c3e4ada4a529aa0fedece80bb42ab-Paper.pdf,Finding the Needle in the Haystack with Convolutions: on the benefits of architectural bias,"Stéphane d'Ascoli, Levent Sagun, Giulio Biroli, Joan Bruna","We introduce a method that maps a CNN to its equivalent FCN (denoted as eFCN). Such an embedding enables the comparison of CNN and FCN training dynamics directly in the FCN space. We use this method to test a new training protocol, which consists in training a CNN, embedding it to FCN space at a certain
relax time'', then resuming the training in FCN space. We observe that for all relax times, the deviation from the CNN subspace is small, and the final performance reached by the eFCN is higher than that reachable by a standard FCN of same architecture. More surprisingly, for some intermediate relax times, the eFCN outperforms the CNN it stemmed, by combining the prior information of the CNN and the expressivity of the FCN in a complementary way. The practical interest of our protocol is limited by the very large size of the highly sparse eFCN. However, it offers interesting insights into the persistence of architectural bias under stochastic gradient dynamics. It shows the existence of some rare basins in the FCN loss landscape associated with very good generalization. These can only be accessed thanks to the CNN prior, which helps navigate the landscape during the early stages of optimization."
neurips,https://proceedings.neurips.cc/paper/2019/file/125b93c9b50703fe9dac43ec231f5f83-Paper.pdf,Efficiently avoiding saddle points with zero order methods: No gradients required,"Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, Georgios Piliouras","We consider the case of derivative-free algorithms for non-convex optimization, also known as zero order algorithms, that use only function evaluations rather than gradients. For a wide variety of gradient approximators based on finite differences, we establish asymptotic convergence to second order stationary points using a carefully tailored application of the Stable Manifold Theorem. Regarding efficiency, we introduce a noisy zero-order method that converges to second order stationary points, i.e avoids saddle points. Our algorithm uses only
~
O
(
1
/
ϵ
2
)
O
approximate gradient calculations and, thus, it matches the converge rate guarantees of their exact gradient counterparts up to constants. In contrast to previous work, our convergence rate analysis avoids imposing additional dimension dependent slowdowns in the number of iterations required for non-convex zero order optimization."
neurips,https://proceedings.neurips.cc/paper/2019/file/12780ea688a71dabc284b064add459a4-Paper.pdf,Learning metrics for persistence-based summaries and applications for graph classification,"Qi Zhao, Yusu Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/12b1e42dc0746f22cf361267de07073f-Paper.pdf,PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph,"Yikang LI, Tao Ma, Yeqi Bai, Nan Duan, Sining Wei, Xiaogang Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/12e59a33dea1bf0630f46edfe13d6ea2-Paper.pdf,Learning Local Search Heuristics for Boolean Satisfiability,"Emre Yolcu, Barnabas Poczos",
neurips,https://proceedings.neurips.cc/paper/2019/file/131f383b434fdf48079bff1e44e2d9a5-Paper.pdf,Learning to Perform Local Rewriting for Combinatorial Optimization,"Xinyun Chen, Yuandong Tian",
neurips,https://proceedings.neurips.cc/paper/2019/file/13384ffc9d8bdb21c53c6f72d46f7866-Paper.pdf,A Unified Bellman Optimality Principle Combining Reward Maximization and Empowerment,"Felix Leibfried, Sergio Pascual-Díaz, Jordi Grau-Moya",
neurips,https://proceedings.neurips.cc/paper/2019/file/1359aa933b48b754a2f54adb688bfa77-Paper.pdf,Learning Representations for Time Series Clustering,"Qianli Ma, Jiawei Zheng, Sen Li, Gary W. Cottrell",
neurips,https://proceedings.neurips.cc/paper/2019/file/13d4635deccc230c944e4ff6e03404b5-Paper.pdf,Statistical-Computational Tradeoff in Single Index Models,"Lingxiao Wang, Zhuoran Yang, Zhaoran Wang","We study the statistical-computational tradeoffs in a high dimensional single index model
Y
=
f
(
X
⊤
β
∗
)
+
ϵ
Y
, where
f
f
is unknown,
X
X
is a Gaussian vector and
β
∗
β
is
s
s
-sparse with unit norm. When
\cov
(
Y
,
X
⊤
β
∗
)
≠
0
\cov
, \cite{plan2016generalized} shows that the direction and support of
β
∗
β
can be recovered using a generalized version of Lasso. In this paper, we investigate the case when this critical assumption fails to hold, where the problem becomes considerably harder. Using the statistical query model to characterize the computational cost of an algorithm, we show that when
\cov
(
Y
,
X
⊤
β
∗
)
=
0
\cov
and
\cov
(
Y
,
(
X
⊤
β
∗
)
2
)
>
0
\cov
, no computationally tractable algorithms can achieve the information-theoretic limit of the minimax risk. This implies that one must pay an extra computational cost for the nonlinearity involved in the model."
neurips,https://proceedings.neurips.cc/paper/2019/file/13e5ebb0fa112fe1b31a1067962d74a7-Paper.pdf,Probabilistic Logic Neural Networks for Reasoning,"Meng Qu, Jian Tang",
neurips,https://proceedings.neurips.cc/paper/2019/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf,Joint-task Self-supervised Learning for Temporal Correspondence,"Xueting Li, Sifei Liu, Shalini De Mello, Xiaolong Wang, Jan Kautz, Ming-Hsuan Yang",
neurips,https://proceedings.neurips.cc/paper/2019/file/14678db82874f1456031fcc05a3afaf6-Paper.pdf,Learning Sparse Distributions using Iterative Hard Thresholding,"Jacky Y. Zhang, Rajiv Khanna, Anastasios Kyrillidis, Oluwasanmi O. Koyejo",
neurips,https://proceedings.neurips.cc/paper/2019/file/148148d62be67e0916a833931bd32b26-Paper.pdf,On Distributed Averaging for Stochastic k-PCA,"Aditya Bhaskara, Pruthuvi Maheshakya Wijewardena",
neurips,https://proceedings.neurips.cc/paper/2019/file/149815eb972b3c370dee3b89d645ae14-Paper.pdf,Learning dynamic polynomial proofs,"Alhussein Fawzi, Mateusz Malinowski, Hamza Fawzi, Omar Fawzi",
neurips,https://proceedings.neurips.cc/paper/2019/file/14cfdb59b5bda1fc245aadae15b1984a-Paper.pdf,Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control,"Sai Qian Zhang, Qi Zhang, Jieyu Lin","Multi-agent reinforcement learning (MARL) has recently received considerable attention due to its applicability to a wide range of real-world applications. However, achieving efficient communication among agents has always been an overarching problem in MARL. In this work, we propose Variance Based Control (VBC), a simple yet efficient technique to improve communication efficiency in MARL. By limiting the variance of the exchanged messages between agents during the training phase, the noisy component in the messages can be eliminated effectively, while the useful part can be preserved and utilized by the agents for better performance. Our evaluation using multiple MARL benchmarks indicates that our method achieves
2
−
10
×
2
lower in communication overhead than state-of-the-art MARL algorithms, while allowing agents to achieve better overall performance."
neurips,https://proceedings.neurips.cc/paper/2019/file/14da15db887a4b50efe5c1bc66537089-Paper.pdf,Global Convergence of Gradient Descent for Deep Linear Residual Networks,"Lei Wu, Qingcan Wang, Chao Ma","We analyze the global convergence of gradient descent for deep linear residual networks by proposing a new initialization: zero-asymmetric (ZAS) initialization. It is motivated by avoiding stable manifolds of saddle points. We prove that under the ZAS initialization, for an arbitrary target matrix, gradient descent converges to an
ε
ε
-optimal point in
O
(
L
3
log
(
1
/
ε
)
)
O
iterations, which scales polynomially with the network depth
L
L
. Our result and the
exp
(
Ω
(
L
)
)
exp
convergence time for the standard initialization (Xavier or near-identity) \cite{shamir2018exponential} together demonstrate the importance of the residual structure and the initialization in the optimization for deep linear neural networks, especially when
L
L
is large."
neurips,https://proceedings.neurips.cc/paper/2019/file/151de84cca69258b17375e2f44239191-Paper.pdf,Dying Experts: Efficient Algorithms with Optimal Regret Bounds,"Hamid Shayestehmanesh, Sajjad Azami, Nishant A. Mehta",
neurips,https://proceedings.neurips.cc/paper/2019/file/15212f24321aa2c3dc8e9acf820f3c15-Paper.pdf,A Bayesian Theory of Conformity in Collective Decision Making,"Koosha Khalvati, Saghar Mirbagheri, Seongmin A. Park, Jean-Claude Dreher, Rajesh PN Rao",
neurips,https://proceedings.neurips.cc/paper/2019/file/1543843a4723ed2ab08e18053ae6dc5b-Paper.pdf,Poisson-Randomized Gamma Dynamical Systems,"Aaron Schein, Scott Linderman, Mingyuan Zhou, David Blei, Hanna Wallach",
neurips,https://proceedings.neurips.cc/paper/2019/file/154ff8944e6eac05d0675c95b5b8889d-Paper.pdf,Bayesian Layers: A Module for Neural Network Uncertainty,"Dustin Tran, Mike Dusenberry, Mark van der Wilk, Danijar Hafner",
neurips,https://proceedings.neurips.cc/paper/2019/file/1558417b096b5d8e7cbe0183ea9cbf26-Paper.pdf,Sequence Modeling with Unconstrained Generation Order,"Dmitrii Emelianenko, Elena Voita, Pavel Serdyukov",
neurips,https://proceedings.neurips.cc/paper/2019/file/15825aee15eb335cc13f9b559f166ee8-Paper.pdf,Online Continual Learning with Maximal Interfered Retrieval,"Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, Lucas Page-Caccia",
neurips,https://proceedings.neurips.cc/paper/2019/file/159c1ffe5b61b41b3c4d8f4c2150f6c4-Paper.pdf,Visualizing and Measuring the Geometry of BERT,"Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B. Viegas, Andy Coenen, Adam Pearce, Been Kim",
neurips,https://proceedings.neurips.cc/paper/2019/file/15cf76466b97264765356fcc56d801d1-Paper.pdf,Learning to Predict Without Looking Ahead: World Models Without Forward Prediction,"Daniel Freeman, David Ha, Luke Metz",
neurips,https://proceedings.neurips.cc/paper/2019/file/15d185eaa7c954e77f5343d941e25fbd-Paper.pdf,Deep Generalized Method of Moments for Instrumental Variable Analysis,"Andrew Bennett, Nathan Kallus, Tobias Schnabel",
neurips,https://proceedings.neurips.cc/paper/2019/file/15e122e839dfdaa7ce969536f94aecf6-Paper.pdf,Copulas as High-Dimensional Generative Models: Vine Copula Autoencoders,"Natasa Tagasovska, Damien Ackerer, Thibault Vatter",
neurips,https://proceedings.neurips.cc/paper/2019/file/15f99f2165aa8c86c9dface16fefd281-Paper.pdf,Implicit Semantic Data Augmentation for Deep Networks,"Yulin Wang, Xuran Pan, Shiji Song, Hong Zhang, Gao Huang, Cheng Wu",
neurips,https://proceedings.neurips.cc/paper/2019/file/16026d60ff9b54410b3435b403afd226-Paper.pdf,q-means: A quantum algorithm for unsupervised machine learning,"Iordanis Kerenidis, Jonas Landman, Alessandro Luongo, Anupam Prakash",
neurips,https://proceedings.neurips.cc/paper/2019/file/16105fb9cc614fc29e1bda00dab60d41-Paper.pdf,RUDDER: Return Decomposition for Delayed Rewards,"Jose A. Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes Brandstetter, Sepp Hochreiter",
neurips,https://proceedings.neurips.cc/paper/2019/file/1625abb8e458a79765c62009235e9d5b-Paper.pdf,Learning-Based Low-Rank Approximations,"Piotr Indyk, Ali Vakilian, Yang Yuan","We introduce a “learning-based” algorithm for the low-rank decomposition problem: given an
n
×
d
n
matrix
A
A
, and a parameter
k
k
, compute a rank-
k
k
matrix
A
′
A
that minimizes the approximation loss
∥
A
−
A
′
∥
F
‖
. The algorithm uses a training set of input matrices in order to optimize its performance. Specifically, some of the most efficient approximate algorithms for computing low-rank approximations proceed by computing a projection
S
A
S
, where
S
S
is a sparse random
m
×
n
m
“sketching matrix”, and then performing the singular value decomposition of
S
A
S
. We show how to replace the random matrix
S
S
with a “learned” matrix of the same sparsity to reduce the error. Our experiments show that, for multiple types of data sets, a learned sketch matrix can substantially reduce the approximation loss compared to a random matrix
S
S
, sometimes up to one order of magnitude. We also study mixed matrices where only some of the rows are trained and the remaining ones are random, and show that matrices still offer improved performance while retaining worst-case guarantees. Finally, to understand the theoretical aspects of our approach, we study the special case of
m
=
1
m
. In particular, we give an approximation algorithm for minimizing the empirical loss, with approximation factor depending on the stable rank of matrices in the training set. We also show generalization bounds for the sketch matrix learning problem."
neurips,https://proceedings.neurips.cc/paper/2019/file/165a59f7cf3b5c4396ba65953d679f17-Paper.pdf,Convergence Guarantees for Adaptive Bayesian Quadrature Methods,"Motonobu Kanagawa, Philipp Hennig",
neurips,https://proceedings.neurips.cc/paper/2019/file/169779d3852b32ce8b1a1724dbf5217d-Paper.pdf,A First-Order Algorithmic Framework for Distributionally Robust Logistic Regression,"JIAJIN LI, SEN HUANG, Anthony Man-Cho So",
neurips,https://proceedings.neurips.cc/paper/2019/file/16bda725ae44af3bb9316f416bd13b1b-Paper.pdf,Theoretical Analysis of Adversarial Learning: A Minimax Approach,"Zhuozhuo Tu, Jingwei Zhang, Dacheng Tao",
neurips,https://proceedings.neurips.cc/paper/2019/file/16fc18d787294ad5171100e33d05d4e2-Paper.pdf,Compositional De-Attention Networks,"Yi Tay, Anh Tuan Luu, Aston Zhang, Shuohang Wang, Siu Cheung Hui",
neurips,https://proceedings.neurips.cc/paper/2019/file/172ef5a94b4dd0aa120c6878fc29f70c-Paper.pdf,Robust Attribution Regularization,"Jiefeng Chen, Xi Wu, Vaibhav Rastogi, Yingyu Liang, Somesh Jha",
neurips,https://proceedings.neurips.cc/paper/2019/file/172fd0d638b3282151bd8f3d652cb640-Paper.pdf,Semantic-Guided Multi-Attention Localization for Zero-Shot Learning,"Yizhe Zhu, Jianwen Xie, Zhiqiang Tang, Xi Peng, Ahmed Elgammal",
neurips,https://proceedings.neurips.cc/paper/2019/file/1770ae9e1b6bc9f5fd2841f141557ffb-Paper.pdf,Distributionally Robust Optimization and Generalization in Kernel Methods,"Matthew Staib, Stefanie Jegelka",
neurips,https://proceedings.neurips.cc/paper/2019/file/17b3c7061788dbe82de5abe9f6fe22b3-Paper.pdf,Kernel Instrumental Variable Regression,"Rahul Singh, Maneesh Sahani, Arthur Gretton",
neurips,https://proceedings.neurips.cc/paper/2019/file/182bd81ea25270b7d1c2fe8353d17fe6-Paper.pdf,Metalearned Neural Memory,"Tsendsuren Munkhdalai, Alessandro Sordoni, TONG WANG, Adam Trischler",
neurips,https://proceedings.neurips.cc/paper/2019/file/187acf7982f3c169b3075132380986e4-Paper.pdf,Learning Bayesian Networks with Low Rank Conditional Probability Tables,"Adarsh Barik, Jean Honorio",
neurips,https://proceedings.neurips.cc/paper/2019/file/18cdf49ea54eec029238fcc95f76ce41-Paper.pdf,Large Scale Adversarial Representation Learning,"Jeff Donahue, Karen Simonyan",
neurips,https://proceedings.neurips.cc/paper/2019/file/195f15384c2a79cedf293e4a847ce85c-Paper.pdf,Hindsight Credit Assignment,"Anna Harutyunyan, Will Dabney, Thomas Mesnard, Mohammad Gheshlaghi Azar, Bilal Piot, Nicolas Heess, Hado P. van Hasselt, Gregory Wayne, Satinder Singh, Doina Precup, Remi Munos",
neurips,https://proceedings.neurips.cc/paper/2019/file/19ca14e7ea6328a42e0eb13d585e4c22-Paper.pdf,Zero-shot Learning via Simultaneous Generating and Learning,"Hyeonwoo Yu, Beomhee Lee",
neurips,https://proceedings.neurips.cc/paper/2019/file/1a04f965818a8533f5613003c7db243d-Paper.pdf,"Direct Optimization through
arg
max
arg
for Discrete Variational Auto-Encoder","Guy Lorberbom, Andreea Gane, Tommi Jaakkola, Tamir Hazan","Reparameterization of variational auto-encoders with continuous random variables is an effective method for reducing the variance of their gradient estimates. In the discrete case, one can perform reparametrization using the Gumbel-Max trick, but the resulting objective relies on an
arg
max
arg
operation and is non-differentiable. In contrast to previous works which resort to \emph{softmax}-based relaxations, we propose to optimize it directly by applying the \emph{direct loss minimization} approach. Our proposal extends naturally to structured discrete latent variable models when evaluating the
arg
max
arg
operation is tractable. We demonstrate empirically the effectiveness of the direct loss minimization technique in variational autoencoders with both unstructured and structured discrete latent variables."
neurips,https://proceedings.neurips.cc/paper/2019/file/1a638db8311430c6c018bf21e1a0b7fb-Paper.pdf,Generalization Error Analysis of Quantized Compressive Learning,"Xiaoyun Li, Ping Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/1b113258af3968aaf3969ca67e744ff8-Paper.pdf,Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning,"David Janz, Jiri Hron, Przemysław Mazur, Katja Hofmann, José Miguel Hernández-Lobato, Sebastian Tschiatschek",
neurips,https://proceedings.neurips.cc/paper/2019/file/1b33d16fc562464579b7199ca3114982-Paper.pdf,Trivializations for Gradient-Based Optimization on Manifolds,Mario Lezcano Casado,
neurips,https://proceedings.neurips.cc/paper/2019/file/1b486d7a5189ebe8d8c46afc64b0d1b4-Paper.pdf,On the Fairness of Disentangled Representations,"Francesco Locatello, Gabriele Abbati, Thomas Rainforth, Stefan Bauer, Bernhard Schölkopf, Olivier Bachem",
neurips,https://proceedings.neurips.cc/paper/2019/file/1b742ae215adf18b75449c6e272fd92d-Paper.pdf,When to use parametric models in reinforcement learning?,"Hado P. van Hasselt, Matteo Hessel, John Aslanides",
neurips,https://proceedings.neurips.cc/paper/2019/file/1b79b52d1bf6f71b2b1eb7ca08ed0776-Paper.pdf,Ouroboros: On Accelerating Training of Transformer-Based Language Models,"Qian Yang, Zhouyuan Huo, Wenlin Wang, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2019/file/1b9a80606d74d3da6db2f1274557e644-Paper.pdf,MonoForest framework for tree ensemble analysis,"Igor Kuralenok, Vasilii Ershov, Igor Labutin",
neurips,https://proceedings.neurips.cc/paper/2019/file/1bd2caf96a17d892c2c7e9959549cfc7-Paper.pdf,Correlation Priors for Reinforcement Learning,"Bastian Alt, Adrian Šošić, Heinz Koeppl",
neurips,https://proceedings.neurips.cc/paper/2019/file/1bd4b29a8e0afccd9923fe29cecb4b29-Paper.pdf,Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently,"Xiao Liu, Xiaolong Zou, Zilong Ji, Gengshuo Tian, Yuanyuan Mi, Tiejun Huang, K. Y. Michael Wong, Si Wu",
neurips,https://proceedings.neurips.cc/paper/2019/file/1c336b8080f82bcc2cd2499b4c57261d-Paper.pdf,Calibration tests in multi-class classification: A unifying framework,"David Widmann, Fredrik Lindsten, Dave Zachariah",
neurips,https://proceedings.neurips.cc/paper/2019/file/1c6a0198177bfcc9bd93f6aab94aad3c-Paper.pdf,Joint Optimization of Tree-based Index and Deep Model for Recommender Systems,"Han Zhu, Daqing Chang, Ziru Xu, Pengye Zhang, Xiang Li, Jie He, Han Li, Jian Xu, Kun Gai",
neurips,https://proceedings.neurips.cc/paper/2019/file/1cc8a8ea51cd0adddf5dab504a285915-Paper.pdf,Accurate Uncertainty Estimation and Decomposition in Ensemble Learning,"Jeremiah Liu, John Paisley, Marianthi-Anna Kioumourtzoglou, Brent Coull",
neurips,https://proceedings.neurips.cc/paper/2019/file/1cd035a313edec52ac8f69c27aba683f-Paper.pdf,Globally Optimal Learning for Structured Elliptical Losses,"Yoav Wald, Nofar Noy, Gal Elidan, Ami Wiesel","In structured problems, however, where there are multiple labels and structural constraints on the labels are imposed (or learned), robust optimization is challenging, and more often than not the loss used is simply the negative log-likelihood of a Gaussian Markov random field. Heavy tailed and contaminated data are common in various applications of machine learning. A standard technique to handle regression tasks that involve such data, is to use robust losses, e.g., the popular Huber’s loss. In structured problems, however, where there are multiple labels and structural constraints on the labels are imposed (or learned), robust optimization is challenging, and more often than not the loss used is simply the negative log-likelihood of a Gaussian Markov random field. In this work, we analyze robust alternatives. Theoretical understanding of such problems is quite limited, with guarantees on optimization given only for special cases and non-structured settings. The core of the difficulty is the non-convexity of the objective function, implying that standard optimization algorithms may converge to sub-optimal critical points. Our analysis focuses on loss functions that arise from elliptical distributions, which appealingly include most loss functions proposed in the literature as special cases. We show that, even though these problems are non-convex, they can be optimized efficiently. Concretely, we prove that at the limit of infinite training data, due to algebraic properties of the problem, all stationary points are globally optimal. Finally, we demonstrate the empirical appeal of using these losses for regression on synthetic and real-life data."
neurips,https://proceedings.neurips.cc/paper/2019/file/1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf,MixMatch: A Holistic Approach to Semi-Supervised Learning,"David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin A. Raffel",
neurips,https://proceedings.neurips.cc/paper/2019/file/1ce3e6e3f452828e23a0c94572bef9d9-Paper.pdf,Preventing Gradient Attenuation in Lipschitz Constrained Convolutional Networks,"Qiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger B. Grosse, Joern-Henrik Jacobsen",
neurips,https://proceedings.neurips.cc/paper/2019/file/1ce83e5d4135b07c0b82afffbe2b3436-Paper.pdf,Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder,"Ji Feng, Qi-Zhi Cai, Zhi-Hua Zhou",
neurips,https://proceedings.neurips.cc/paper/2019/file/1d01bd2e16f57892f0954902899f0692-Paper.pdf,Invariance-inducing regularization using worst-case transformations suffices to boost accuracy and spatial robustness,"Fanny Yang, Zuowen Wang, Christina Heinze-Deml",
neurips,https://proceedings.neurips.cc/paper/2019/file/1d0932d7f57ce74d9d9931a2c6db8a06-Paper.pdf,Attentive State-Space Modeling of Disease Progression,"Ahmed M. Alaa, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2019/file/1d54c76f48f146c3b2d66daf9d7f845e-Paper.pdf,On two ways to use determinantal point processes for Monte Carlo integration,"Guillaume Gautier, Rémi Bardenet, Michal Valko",
neurips,https://proceedings.neurips.cc/paper/2019/file/1d6408264d31d453d556c60fe7d0459e-Paper.pdf,ADDIS: an adaptive discarding algorithm for online FDR control with conservative nulls,"Jinjin Tian, Aaditya Ramdas",
neurips,https://proceedings.neurips.cc/paper/2019/file/1d72310edc006dadf2190caad5802983-Paper.pdf,Controllable Text-to-Image Generation,"Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, Philip Torr",
neurips,https://proceedings.neurips.cc/paper/2019/file/1d7c2aae840867027b7edd17b6aaa0e9-Paper.pdf,Exploring Algorithmic Fairness in Robust Graph Covering Problems,"Aida Rahmattalabi, Phebe Vayanos, Anthony Fulginiti, Eric Rice, Bryan Wilder, Amulya Yadav, Milind Tambe",
neurips,https://proceedings.neurips.cc/paper/2019/file/1da546f25222c1ee710cf7e2f7a3ff0c-Paper.pdf,Fast Convergence of Natural Gradient Descent for Over-Parameterized Neural Networks,"Guodong Zhang, James Martens, Roger B. Grosse",
neurips,https://proceedings.neurips.cc/paper/2019/file/1dba5eed8838571e1c80af145184e515-Paper.pdf,Reducing the variance in online optimization by transporting past gradients,"Sébastien Arnold, Pierre-Antoine Manzagol, Reza Babanezhad Harikandeh, Ioannis Mitliagkas, Nicolas Le Roux",
neurips,https://proceedings.neurips.cc/paper/2019/file/1e0feeaff84a19bf3936e693311fa66d-Paper.pdf,Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces,"Benyamin Allahgholizadeh Haghi, Spencer Kellis, Sahil Shah, Maitreyi Ashok, Luke Bashford, Daniel Kramer, Brian Lee, Charles Liu, Richard Andersen, Azita Emami",
neurips,https://proceedings.neurips.cc/paper/2019/file/1e44fdf9c44d7328fecc02d677ed704d-Paper.pdf,Graph Normalizing Flows,"Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, Kevin Swersky",
neurips,https://proceedings.neurips.cc/paper/2019/file/1e48c4420b7073bc11916c6c1de226bb-Paper.pdf,Cascaded Dilated Dense Network with Two-step Data Consistency for MRI Reconstruction,"Hao Zheng, Faming Fang, Guixu Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf,Neural networks grown and self-organized by noise,"Guruprasad Raghavan, Matt Thomson",
neurips,https://proceedings.neurips.cc/paper/2019/file/1e79596878b2320cac26dd792a6c51c9-Paper.pdf,Likelihood Ratios for Out-of-Distribution Detection,"Jie Ren, Peter J. Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon, Balaji Lakshminarayanan",
neurips,https://proceedings.neurips.cc/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf,Root Mean Square Layer Normalization,"Biao Zhang, Rico Sennrich",
neurips,https://proceedings.neurips.cc/paper/2019/file/1efa39bcaec6f3900149160693694536-Paper.pdf,HyperGCN: A New Method For Training Graph Convolutional Networks on Hypergraphs,"Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, Partha Talukdar",
neurips,https://proceedings.neurips.cc/paper/2019/file/1f36c15d6a3d18d52e8d493bc8187cb9-Paper.pdf,Asymptotics for Sketching in Least Squares Regression,"Edgar Dobriban, Sifan Liu",
neurips,https://proceedings.neurips.cc/paper/2019/file/1f6419b1cbe79c71410cb320fc094775-Paper.pdf,Gradient Dynamics of Shallow Univariate ReLU Networks,"Francis Williams, Matthew Trager, Daniele Panozzo, Claudio Silva, Denis Zorin, Joan Bruna",
neurips,https://proceedings.neurips.cc/paper/2019/file/1f88c7c5d7d94ae08bd752aa3d82108b-Paper.pdf,Chirality Nets for Human Pose Regression,"Raymond Yeh, Yuan-Ting Hu, Alexander Schwing",
neurips,https://proceedings.neurips.cc/paper/2019/file/1fa6269f58898f0e809575c9a48747ef-Paper.pdf,TAB-VCR: Tags and Attributes based VCR Baselines,"Jingxiang Lin, Unnat Jain, Alexander Schwing",
neurips,https://proceedings.neurips.cc/paper/2019/file/1fd09c5f59a8ff35d499c0ee25a1d47e-Paper.pdf,Multiclass Performance Metric Elicitation,"Gaurush Hiranandani, Shant Boodaghians, Ruta Mehta, Oluwasanmi O. Koyejo",
neurips,https://proceedings.neurips.cc/paper/2019/file/201d546992726352471cfea6b0df0a48-Paper.pdf,Assessing Social and Intersectional Biases in Contextualized Word Representations,"Yi Chern Tan, L. Elisa Celis",
neurips,https://proceedings.neurips.cc/paper/2019/file/20885c72ca35d75619d6a378edea9f76-Paper.pdf,Likelihood-Free Overcomplete ICA and Applications In Causal Discovery,"Chenwei DING, Mingming Gong, Kun Zhang, Dacheng Tao",
neurips,https://proceedings.neurips.cc/paper/2019/file/20c86a628232a67e7bd46f76fba7ce12-Paper.pdf,MaCow: Masked Convolutional Generative Flow,"Xuezhe Ma, Xiang Kong, Shanghang Zhang, Eduard Hovy",
neurips,https://proceedings.neurips.cc/paper/2019/file/20f07591c6fcb220ffe637cda29bb3f6-Paper.pdf,Batched Multi-armed Bandits Problem,"Zijun Gao, Yanjun Han, Zhimei Ren, Zhengqing Zhou",
neurips,https://proceedings.neurips.cc/paper/2019/file/2119b8d43eafcf353e07d7cb5554170b-Paper.pdf,High-Quality Self-Supervised Deep Image Denoising,"Samuli Laine, Tero Karras, Jaakko Lehtinen, Timo Aila",
neurips,https://proceedings.neurips.cc/paper/2019/file/216f44e2d28d4e175a194492bde9148f-Paper.pdf,Generalization in multitask deep neural classifiers: a statistical physics approach,"Anthony Ndirango, Tyler Lee",
neurips,https://proceedings.neurips.cc/paper/2019/file/2172fde49301047270b2897085e4319d-Paper.pdf,Causal Regularization,Dominik Janzing,
neurips,https://proceedings.neurips.cc/paper/2019/file/21b29648a47a45ad16bb0da0c004dfba-Paper.pdf,Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond,"Lin Chen, Hossein Esfandiari, Gang Fu, Vahab Mirrokni",
neurips,https://proceedings.neurips.cc/paper/2019/file/21be9a4bd4f81549a9d1d241981cec3c-Paper.pdf,Augmented Neural ODEs,"Emilien Dupont, Arnaud Doucet, Yee Whye Teh",
neurips,https://proceedings.neurips.cc/paper/2019/file/21ce689121e39821d07d04faab328370-Paper.pdf,Efficient Smooth Non-Convex Stochastic Compositional Optimization via Stochastic Recursive Gradient Descent,"Wenqing Hu, Chris Junchi Li, Xiangru Lian, Ji Liu, Huizhuo Yuan","Stochastic compositional optimization arises in many important machine learning tasks such as reinforcement learning and portfolio management. The objective function is the composition of two expectations of stochastic functions, and is more challenging to optimize than vanilla stochastic optimization problems. In this paper, we investigate the stochastic compositional optimization in the general smooth non-convex setting. We employ a recently developed idea of \textit{Stochastic Recursive Gradient Descent} to design a novel algorithm named SARAH-Compositional, and prove a sharp Incremental First-order Oracle (IFO) complexity upper bound for stochastic compositional optimization:
O
(
(
n
+
m
)
1
/
2
ε
−
2
)
O
in the finite-sum case and
O
(
ε
−
3
)
O
in the online case. Such a complexity is known to be the best one among IFO complexity results for non-convex stochastic compositional optimization. Numerical experiments validate the superior performance of our algorithm and theory."
neurips,https://proceedings.neurips.cc/paper/2019/file/21fe5b8ba755eeaece7a450849876228-Paper.pdf,Regularizing Trajectory Optimization with Denoising Autoencoders,"Rinu Boney, Norman Di Palo, Mathias Berglund, Alexander Ilin, Juho Kannala, Antti Rasmus, Harri Valpola",
neurips,https://proceedings.neurips.cc/paper/2019/file/2201611d7a08ffda97e3e8c6b667a1bc-Paper.pdf,Multi-Criteria Dimensionality Reduction with Applications to Fairness,"Uthaipon Tantipongpipat, Samira Samadi, Mohit Singh, Jamie H. Morgenstern, Santosh Vempala","Our main result is an exact polynomial-time algorithm for the two-criteria dimensionality reduction problem when the two criteria are increasing concave functions. As an application of this result, we obtain a polynomial time algorithm for Fair-PCA for k=2 groups, resolving an open problem of Samadi et al.[NeurIPS18], and a polynomial time algorithm for NSW objective for k=2 groups. We also give approximation algorithms for k>2. Our technical contribution in the above results is to prove new low-rank properties of extreme point solutions to semi-definite programs. We conclude with the results of several experiments indicating improved performance and generalized application of our algorithm on real-world datasets."
neurips,https://proceedings.neurips.cc/paper/2019/file/222afbe0d68c61de60374b96f1d86715-Paper.pdf,Structured and Deep Similarity Matching via Structured and Deep Hebbian Networks,"Dina Obeid, Hugo Ramambason, Cengiz Pehlevan",
neurips,https://proceedings.neurips.cc/paper/2019/file/227e072d131ba77451d8f27ab9afdfb7-Paper.pdf,Neural Trust Region/Proximal Policy Optimization Attains Globally Optimal Policy,"Boyi Liu, Qi Cai, Zhuoran Yang, Zhaoran Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/227f6afd3b7f89b96c4bb91f95d50f6d-Paper.pdf,ANODEV2: A Coupled Neural ODE Framework,"Tianjun Zhang, Zhewei Yao, Amir Gholami, Joseph E. Gonzalez, Kurt Keutzer, Michael W. Mahoney, George Biros",
neurips,https://proceedings.neurips.cc/paper/2019/file/2281f5c898351dbc6dace2ba201e7948-Paper.pdf,Learning Neural Networks with Adaptive Regularization,"Han Zhao, Yao-Hung Hubert Tsai, Russ R. Salakhutdinov, Geoffrey J. Gordon",
neurips,https://proceedings.neurips.cc/paper/2019/file/228499b55310264a8ea0e27b6e7c6ab6-Paper.pdf,Turbo Autoencoder: Deep learning based channel codes for point-to-point communication channels,"Yihan Jiang, Hyeji Kim, Himanshu Asnani, Sreeram Kannan, Sewoong Oh, Pramod Viswanath",
neurips,https://proceedings.neurips.cc/paper/2019/file/228b25587479f2fc7570428e8bcbabdc-Paper.pdf,DetNAS: Backbone Search for Object Detection,"Yukang Chen, Tong Yang, Xiangyu Zhang, GAOFENG MENG, Xinyu Xiao, Jian Sun",
neurips,https://proceedings.neurips.cc/paper/2019/file/2297607a5db8576d5ad6bbd83696ff60-Paper.pdf,Nonlinear scaling of resource allocation in sensory bottlenecks,"Laura Rose Edmondson, Alejandro Jimenez Rodriguez, Hannes P. Saal",
neurips,https://proceedings.neurips.cc/paper/2019/file/23755432da68528f115c9633c0d7834f-Paper.pdf,What the Vec? Towards Probabilistically Grounded Embeddings,"Carl Allen, Ivana Balazevic, Timothy Hospedales",
neurips,https://proceedings.neurips.cc/paper/2019/file/23c894276a2c5a16470e6a31f4618d73-Paper.pdf,Diffusion Improves Graph Learning,"Johannes Klicpera, Stefan Weißenberger, Stephan Günnemann",
neurips,https://proceedings.neurips.cc/paper/2019/file/24389bfe4fe2eba8bf9aa9203a44cdad-Paper.pdf,"Inverting Deep Generative models, One layer at a time","Qi Lei, Ajil Jalal, Inderjit S. Dhillon, Alexandros G. Dimakis","We show that for the realizable case, single layer inversion can be performed exactly in polynomial time, by solving a linear program. Further, we show that for two layers, inversion is NP-hard to recover binary latent code (even for the realizable case) and the pre-image set can be non-convex."
neurips,https://proceedings.neurips.cc/paper/2019/file/24646475ed957884ca39b0c1d9cc06b2-Paper.pdf,Sample Complexity of Learning Mixture of Sparse Linear Regressions,"Akshay Krishnamurthy, Arya Mazumdar, Andrew McGregor, Soumyabrata Pal",
neurips,https://proceedings.neurips.cc/paper/2019/file/246a3c5544feb054f3ea718f61adfa16-Paper.pdf,A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks,"Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, Pengchuan Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/248024541dbda1d3fd75fe49d1a4df4d-Paper.pdf,A Latent Variational Framework for Stochastic Optimization,Philippe Casgrain,
neurips,https://proceedings.neurips.cc/paper/2019/file/24e01830d213d75deb99c22b9cd91ddd-Paper.pdf,Escaping from saddle points on Riemannian manifolds,"Yue Sun, Nicolas Flammarion, Maryam Fazel","We consider minimizing a nonconvex, smooth function
f
f
on a Riemannian manifold
M
M
. We show that a perturbed version of the gradient descent algorithm converges to a second-order stationary point for this problem (and hence is able to escape saddle points on the manifold). While the unconstrained problem is well-studied, our result is the first to prove such a rate for nonconvex, manifold-constrained problems. The rate of convergence depends as
1
/
ϵ
2
1
on the accuracy
ϵ
ϵ
, which matches a rate known only for unconstrained smooth minimization. The convergence rate also has a polynomial dependence on the parameters denoting the curvature of the manifold and the smoothness of the function."
neurips,https://proceedings.neurips.cc/paper/2019/file/25048eb6a33209cb5a815bff0cf6887c-Paper.pdf,Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods,"Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D. Lee, Meisam Razaviyayn","Recent applications that arise in machine learning have surged significant interest in solving min-max saddle point games. This problem has been extensively studied in the convex-concave regime for which a global equilibrium solution can be computed efficiently. In this paper, we study the problem in the non-convex regime and show that an
ε
ε
--first order stationary point of the game can be computed when one of the player’s objective can be optimized to global optimality efficiently. In particular, we first consider the case where the objective of one of the players satisfies the Polyak-{\L}ojasiewicz (PL) condition. For such a game, we show that a simple multi-step gradient descent-ascent algorithm finds an
ε
ε
--first order stationary point of the problem in
˜
O
(
ε
−
2
)
O
iterations. Then we show that our framework can also be applied to the case where the objective of the
max-player"" is concave. In this case, we propose a multi-step gradient descent-ascent algorithm that finds an
ε
ε
--first order stationary point of the game in
˜
O
(
ε
−
3.5
)
O
iterations, which is the best known rate in the literature. We applied our algorithm to a fair classification problem of Fashion-MNIST dataset and observed that the proposed algorithm results in smoother training and better generalization."
neurips,https://proceedings.neurips.cc/paper/2019/file/251c5ffd6b62cc21c446c963c76cf214-Paper.pdf,The Option Keyboard: Combining Skills in Reinforcement Learning,"Andre Barreto, Diana Borsa, Shaobo Hou, Gheorghe Comanici, Eser Aygün, Philippe Hamel, Daniel Toyama, Jonathan hunt, Shibl Mourad, David Silver, Doina Precup",
neurips,https://proceedings.neurips.cc/paper/2019/file/253f7b5d921338af34da817c00f42753-Paper.pdf,On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective,"Lili Su, Pengkun Yang","We consider training over-parameterized two-layer neural networks with Rectified Linear Unit (ReLU) using gradient descent (GD) method. Inspired by a recent line of work, we study the evolutions of network prediction errors across GD iterations, which can be neatly described in a matrix form. When the network is sufficiently over-parameterized, these matrices individually approximate {\em an} integral operator which is determined by the feature vector distribution
ρ
ρ
only. Consequently, GD method can be viewed as {\em approximately} applying the powers of this integral operator on the underlying/target function
f
∗
f
that generates the responses/labels. We show that if
f
∗
f
admits a low-rank approximation with respect to the eigenspaces of this integral operator, then the empirical risk decreases to this low rank approximation error at a linear rate which is determined by
f
∗
f
and
ρ
ρ
only, i.e., the rate is independent of the sample size
n
n
. Furthermore, if
f
∗
f
has zero low-rank approximation error, then, as long as the width of the neural network is
Ω
(
n
log
n
)
Ω
, the empirical risk decreases to
Θ
(
1
/
√
n
)
Θ
. To the best of our knowledge, this is the first result showing the sufficiency of nearly-linear network over-parameterization. We provide an application of our general results to the setting where
ρ
ρ
is the uniform distribution on the spheres and
f
∗
f
is a polynomial. Throughout this paper, we consider the scenario where the input dimension
d
d
is fixed."
neurips,https://proceedings.neurips.cc/paper/2019/file/254ed7d2de3b23ab10936522dd547b78-Paper.pdf,Modeling Tabular data using Conditional GAN,"Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, Kalyan Veeramachaneni",
neurips,https://proceedings.neurips.cc/paper/2019/file/2557911c1bf75c2b643afb4ecbfc8ec2-Paper.pdf,"Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates","Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, Simon Lacoste-Julien",
neurips,https://proceedings.neurips.cc/paper/2019/file/25caef3a545a1fff2ff4055484f0e758-Paper.pdf,Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies,"Yonathan Efroni, Nadav Merlis, Mohammad Ghavamzadeh, Shie Mannor",
neurips,https://proceedings.neurips.cc/paper/2019/file/263fc48aae39f219b4c71d9d4bb4aed2-Paper.pdf,Weighted Linear Bandits for Non-Stationary Environments,"Yoan Russac, Claire Vernade, Olivier Cappé",
neurips,https://proceedings.neurips.cc/paper/2019/file/2647c1dba23bc0e0f9cdf75339e120d2-Paper.pdf,Neural Lyapunov Control,"Ya-Chien Chang, Nima Roohi, Sicun Gao",
neurips,https://proceedings.neurips.cc/paper/2019/file/26b58a41da329e0cbde0cbf956640a58-Paper.pdf,Stochastic Variance Reduced Primal Dual Algorithms for Empirical Composition Optimization,"Adithya M Devraj, Jianshu Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/26cd8ecadce0d4efd6cc8a8725cbd1f8-Paper.pdf,Hamiltonian Neural Networks,"Samuel Greydanus, Misko Dzamba, Jason Yosinski",
neurips,https://proceedings.neurips.cc/paper/2019/file/274a10ffa06e434f2a94df765cac6bf4-Paper.pdf,Better Transfer Learning with Inferred Successor Maps,"Tamas Madarasz, Tim Behrens",
neurips,https://proceedings.neurips.cc/paper/2019/file/2751fae77b24c37382cf6464173d145e-Paper.pdf,Random Quadratic Forms with Dependence: Applications to Restricted Isometry and Beyond,"Arindam Banerjee, Qilong Gu, Vidyashankar Sivakumar, Steven Z. Wu",
neurips,https://proceedings.neurips.cc/paper/2019/file/28659414dab9eca0219dd592b8136434-Paper.pdf,Energy-Inspired Models: Learning with Sampler-Induced Distributions,"John Lawson, George Tucker, Bo Dai, Rajesh Ranganath",
neurips,https://proceedings.neurips.cc/paper/2019/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf,Data-Dependence of Plateau Phenomenon in Learning with Neural Network --- Statistical Mechanical Analysis,"Yuki Yoshida, Masato Okada",
neurips,https://proceedings.neurips.cc/paper/2019/file/28f0b864598a1291557bed248a998d4e-Paper.pdf,Differentiable Cloth Simulation for Inverse Problems,"Junbang Liang, Ming Lin, Vladlen Koltun",
neurips,https://proceedings.neurips.cc/paper/2019/file/28f7241796510e838db4a1384ae1279d-Paper.pdf,Detecting Overfitting via Adversarial Examples,"Roman Werpachowski, András György, Csaba Szepesvari",
neurips,https://proceedings.neurips.cc/paper/2019/file/291597a100aadd814d197af4f4bab3a7-Paper.pdf,Region-specific Diffeomorphic Metric Mapping,"Zhengyang Shen, Francois-Xavier Vialard, Marc Niethammer",
neurips,https://proceedings.neurips.cc/paper/2019/file/2952351097998ac1240cb2ab7333a3d2-Paper.pdf,Teaching Multiple Concepts to a Forgetful Learner,"Anette Hunziker, Yuxin Chen, Oisin Mac Aodha, Manuel Gomez Rodriguez, Andreas Krause, Pietro Perona, Yisong Yue, Adish Singla",
neurips,https://proceedings.neurips.cc/paper/2019/file/2974788b53f73e7950e8aa49f3a306db-Paper.pdf,Domain Generalization via Model-Agnostic Learning of Semantic Features,"Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, Ben Glocker",
neurips,https://proceedings.neurips.cc/paper/2019/file/2a084e55c87b1ebcdaad1f62fdbbac8e-Paper.pdf,Unconstrained Monotonic Neural Networks,"Antoine Wehenkel, Gilles Louppe",
neurips,https://proceedings.neurips.cc/paper/2019/file/2a845d4d23b883acb632fefd814e175f-Paper.pdf,Efficient Identification in Linear Structural Causal Models with Instrumental Cutsets,"Daniel Kumor, Bryant Chen, Elias Bareinboim",
neurips,https://proceedings.neurips.cc/paper/2019/file/2afc4dfb14e55c6face649a1d0c1025b-Paper.pdf,Temporal FiLM: Capturing Long-Range Sequence Dependencies with Feature-Wise Modulations.,"Sawyer Birnbaum, Volodymyr Kuleshov, Zayd Enam, Pang Wei W. Koh, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2019/file/2afe4567e1bf64d32a5527244d104cea-Paper.pdf,Convolution with even-sized kernels and symmetric padding,"Shuang Wu, Guanrui Wang, Pei Tang, Feng Chen, Luping Shi",
neurips,https://proceedings.neurips.cc/paper/2019/file/2b8501af7b64d1aaae7dd832805f0709-Paper.pdf,Inducing brain-relevant bias in natural language processing models,"Dan Schwartz, Mariya Toneva, Leila Wehbe",
neurips,https://proceedings.neurips.cc/paper/2019/file/2b8f621e9244cea5007bac8f5d50e476-Paper.pdf,SMILe: Scalable Meta Inverse Reinforcement Learning through Context-Conditional Policies,"Seyed Kamyar Seyed Ghasemipour, Shixiang (Shane) Gu, Richard Zemel",
neurips,https://proceedings.neurips.cc/paper/2019/file/2bc8ae25856bc2a6a1333d1331a3b7a6-Paper.pdf,Learning Non-Convergent Non-Persistent Short-Run MCMC Toward Energy-Based Model,"Erik Nijkamp, Mitch Hill, Song-Chun Zhu, Ying Nian Wu",
neurips,https://proceedings.neurips.cc/paper/2019/file/2bd2e3373dce441c6c3bfadd1daa953e-Paper.pdf,Exploring Unexplored Tensor Network Decompositions for Convolutional Neural Networks,"Kohei Hayashi, Taiki Yamaguchi, Yohei Sugawara, Shin-ichi Maeda",
neurips,https://proceedings.neurips.cc/paper/2019/file/2bf283c05b601f21364d052ca0ec798d-Paper.pdf,Interval timing in deep reinforcement learning agents,"Ben Deverett, Ryan Faulkner, Meire Fortunato, Gregory Wayne, Joel Z. Leibo",
neurips,https://proceedings.neurips.cc/paper/2019/file/2c048d74b3410237704eb7f93a10c9d7-Paper.pdf,Shaping Belief States with Generative Environment Models for RL,"Karol Gregor, Danilo Jimenez Rezende, Frederic Besse, Yan Wu, Hamza Merzic, Aaron van den Oord",
neurips,https://proceedings.neurips.cc/paper/2019/file/2c3ddf4bf13852db711dd1901fb517fa-Paper.pdf,Uncertainty-based Continual Learning with Adaptive Regularization,"Hongjoon Ahn, Sungmin Cha, Donggyu Lee, Taesup Moon",
neurips,https://proceedings.neurips.cc/paper/2019/file/2c463dfdde588f3bfc60d53118c10d6b-Paper.pdf,Implicit Posterior Variational Inference for Deep Gaussian Processes,"Haibin YU, Yizhou Chen, Bryan Kian Hsiang Low, Patrick Jaillet, Zhongxiang Dai",
neurips,https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf,Are Sixteen Heads Really Better than One?,"Paul Michel, Omer Levy, Graham Neubig",
neurips,https://proceedings.neurips.cc/paper/2019/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf,Model Compression with Adversarial Robustness: A Unified Optimization Framework,"Shupeng Gui, Haotao Wang, Haichuan Yang, Chen Yu, Zhangyang Wang, Ji Liu",
neurips,https://proceedings.neurips.cc/paper/2019/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf,Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks,"Yiwen Guo, Ziang Yan, Changshui Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/2cb6b10338a7fc4117a80da24b582060-Paper.pdf,Combinatorial Bayesian Optimization using the Graph Cartesian Product,"Changyong Oh, Jakub Tomczak, Efstratios Gavves, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2019/file/2cfa8f9e50e0f510ede9d12338a5f564-Paper.pdf,Sample Adaptive MCMC,Michael Zhu,
neurips,https://proceedings.neurips.cc/paper/2019/file/2d36b5821f8affc6868b59dfc9af6c9f-Paper.pdf,Tree-Sliced Variants of Wasserstein Distances,"Tam Le, Makoto Yamada, Kenji Fukumizu, Marco Cuturi",
neurips,https://proceedings.neurips.cc/paper/2019/file/2d44e06a7038f2dd98f0f54c4be35e22-Paper.pdf,Integrating Markov processes with structural causal modeling enables counterfactual inference in complex systems,"Robert Ness, Kaushal Paneri, Olga Vitek",
neurips,https://proceedings.neurips.cc/paper/2019/file/2d71b2ae158c7c5912cc0bbde2bb9d95-Paper.pdf,An Adaptive Empirical Bayesian Method for Sparse Deep Learning,"Wei Deng, Xiao Zhang, Faming Liang, Guang Lin",
neurips,https://proceedings.neurips.cc/paper/2019/file/2d95666e2649fcfc6e3af75e09f5adb9-Paper.pdf,Topology-Preserving Deep Image Segmentation,"Xiaoling Hu, Fuxin Li, Dimitris Samaras, Chao Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/2e0d41e02c5be4668ec1b0730b3346a8-Paper.pdf,Stacked Capsule Autoencoders,"Adam Kosiorek, Sara Sabour, Yee Whye Teh, Geoffrey E. Hinton",
neurips,https://proceedings.neurips.cc/paper/2019/file/2e2c080d5490760af59d0baf5acbb84e-Paper.pdf,Progressive Augmentation of GANs,"Dan Zhang, Anna Khoreva",
neurips,https://proceedings.neurips.cc/paper/2019/file/2e65f2f2fdaf6c699b223c61b1b5ab89-Paper.pdf,Online sampling from log-concave distributions,"Holden Lee, Oren Mangoubi, Nisheeth Vishnoi","Given a sequence of convex functions
f
0
,
f
1
,
…
,
f
T
f
, we study the problem of sampling from the Gibbs distribution
π
t
∝
e
−
∑
t
k
=
0
f
k
π
for each epoch
t
t
in an {\em online} manner. Interest in this problem derives from applications in machine learning, Bayesian statistics, and optimization where, rather than obtaining all the observations at once, one constantly acquires new data, and must continuously update the distribution. Our main result is an algorithm that generates roughly independent samples from
π
t
π
for every epoch
t
t
and, under mild assumptions, makes
p
o
l
y
l
o
g
(
T
)
p
gradient evaluations per epoch. All previous results imply a bound on the number of gradient or function evaluations which is at least linear in
T
T
. Motivated by real-world applications, we assume that functions are smooth, their associated distributions have a bounded second moment, and their minimizer drifts in a bounded manner, but do not assume they are strongly convex. In particular, our assumptions hold for online Bayesian logistic regression, when the data satisfy natural regularity properties, giving a sampling algorithm with updates that are poly-logarithmic in
T
T
. In simulations, our algorithm achieves accuracy comparable to an algorithm specialized to logistic regression. Key to our algorithm is a novel stochastic gradient Langevin dynamics Markov chain with a carefully designed variance reduction step and constant batch size. Technically, lack of strong convexity is a significant barrier to analysis and, here, our main contribution is a martingale exit time argument that shows our Markov chain remains in a ball of radius roughly poly-logarithmic in
T
T
for enough time to reach within
ϵ
ϵ
of
π
t
π
."
neurips,https://proceedings.neurips.cc/paper/2019/file/2e6d9c6052e99fcdfa61d9b9da273ca2-Paper.pdf,Practical Two-Step Lookahead Bayesian Optimization,"Jian Wu, Peter Frazier",
neurips,https://proceedings.neurips.cc/paper/2019/file/2ecd2bd94734e5dd392d8678bc64cdab-Paper.pdf,Generalized Block-Diagonal Structure Pursuit: Learning Soft Latent Task Assignment against Negative Transfer,"Zhiyong Yang, Qianqian Xu, Yangbangyan Jiang, Xiaochun Cao, Qingming Huang","In multi-task learning, a major challenge springs from a notorious issue known as negative transfer, which refers to the phenomenon that sharing the knowledge with dissimilar and hard tasks often results in a worsened performance. To circumvent this issue, we propose a novel multi-task learning method, which simultaneously learns latent task representations and a block-diagonal Latent Task Assignment Matrix (LTAM). Different from most of the previous work, pursuing the Block-Diagonal structure of LTAM (assigning latent tasks to output tasks) alleviates negative transfer via collaboratively grouping latent tasks and output tasks such that inter-group knowledge transfer and sharing is suppressed. This goal is challenging, since 1) our notion of Block-Diagonal Property extends the traditional notion for square matrices where the
i
i
-th column and the
i
i
-th column represents the same concept; 2) marginal constraints on rows and columns are also required for avoiding isolated latent/output tasks. Facing such challenges, we propose a novel regularizer by means of an equivalent spectral condition realizing this generalized block-diagonal property. Practically, we provide a relaxation scheme which improves the flexibility of the model. With the objective function given, we then propose an alternating optimization method, which not only tells how negative transfer is alleviated in our method but also reveals an interesting connection between our method and the optimal transport problem. Finally, the method is demonstrated on a simulation dataset, three real-world benchmark datasets and further applied to personalized attribute predictions."
neurips,https://proceedings.neurips.cc/paper/2019/file/2edfeadfe636973b42d7b6ac315b896c-Paper.pdf,Regret Bounds for Thompson Sampling in Episodic Restless Bandit Problems,"Young Hun Jung, Ambuj Tewari","Restless bandit problems are instances of non-stationary multi-armed bandits. These problems have been studied well from the optimization perspective, where the goal is to efficiently find a near-optimal policy when system parameters are known. However, very few papers adopt a learning perspective, where the parameters are unknown. In this paper, we analyze the performance of Thompson sampling in episodic restless bandits with unknown parameters. We consider a general policy map to define our competitor and prove an
~
\bigO
(
√
T
)
\bigO
Bayesian regret bound. Our competitor is flexible enough to represent various benchmarks including the best fixed action policy, the optimal policy, the Whittle index policy, or the myopic policy. We also present empirical results that support our theoretical findings."
neurips,https://proceedings.neurips.cc/paper/2019/file/2ef35a8b78b572a47f56846acbeef5d3-Paper.pdf,Adaptive Sequence Submodularity,"Marko Mitrovic, Ehsan Kazemi, Moran Feldman, Andreas Krause, Amin Karbasi",
neurips,https://proceedings.neurips.cc/paper/2019/file/2f3926f0a9613f3c3cc21d52a3cdb4d9-Paper.pdf,"N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules","Shengchao Liu, Mehmet F. Demirel, Yingyu Liang",
neurips,https://proceedings.neurips.cc/paper/2019/file/2f3c6a4cd8af177f6456e7e51a916ff3-Paper.pdf,The spiked matrix model with generative priors,"Benjamin Aubin, Bruno Loureiro, Antoine Maillard, Florent Krzakala, Lenka Zdeborová",
neurips,https://proceedings.neurips.cc/paper/2019/file/2f4059ce1227f021edc5d9c6f0f17dc1-Paper.pdf,"The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure For Least Squares","Rong Ge, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli","Minimax optimal convergence rates for numerous classes of stochastic convex optimization problems are well characterized, where the majority of results utilize iterate averaged stochastic gradient descent (SGD) with polynomially decaying step sizes. In contrast, the behavior of SGD’s final iterate has received much less attention despite the widespread use in practice. Motivated by this observation, this work provides a detailed study of the following question: what rate is achievable using the final iterate of SGD for the streaming least squares regression problem with and without strong convexity? First, this work shows that even if the time horizon T (i.e. the number of iterations that SGD is run for) is known in advance, the behavior of SGD’s final iterate with any polynomially decaying learning rate scheme is highly sub-optimal compared to the statistical minimax rate (by a condition number factor in the strongly convex case and a factor of
√
T
T
in the non-strongly convex case). In contrast, this paper shows that Step Decay schedules, which cut the learning rate by a constant factor every constant number of epochs (i.e., the learning rate decays geometrically) offer significant improvements over any polynomially decaying step size schedule. In particular, the behavior of the final iterate with step decay schedules is off from the statistical minimax rate by only log factors (in the condition number for the strongly convex case, and in T in the non-strongly convex case). Finally, in stark contrast to the known horizon case, this paper shows that the anytime (i.e. the limiting) behavior of SGD’s final iterate is poor (in that it queries iterates with highly sub-optimal function value infinitely often, i.e. in a limsup sense) irrespective of the step size scheme employed. These results demonstrate the subtlety in establishing optimal learning rate schedules (for the final iterate) for stochastic gradient procedures in fixed time horizon settings."
neurips,https://proceedings.neurips.cc/paper/2019/file/2f4fe03d77724a7217006e5d16728874-Paper.pdf,Understanding and Improving Layer Normalization,"Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, Junyang Lin",
neurips,https://proceedings.neurips.cc/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf,Generative Modeling by Estimating Gradients of the Data Distribution,"Yang Song, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2019/file/300d1539c3b6aa1793b5678b857732cf-Paper.pdf,Hypothesis Set Stability and Generalization,"Dylan J. Foster, Spencer Greenberg, Satyen Kale, Haipeng Luo, Mehryar Mohri, Karthik Sridharan",
neurips,https://proceedings.neurips.cc/paper/2019/file/3070e6addcd702cb58de5d7897bfdae1-Paper.pdf,Balancing Efficiency and Fairness in On-Demand Ridesourcing,"Nixie S. Lesmana, Xuan Zhang, Xiaohui Bei","In this paper, we focus on both the system efficiency and the fairness among drivers and quantitatively analyze the trade-offs between these two objectives. In particular, we give an explicit answer to the question of whether there always exists an assignment that achieves any target efficiency and fairness. We also propose a simple reassignment algorithm that can achieve any selected trade-off. Finally, we demonstrate the effectiveness of the algorithms through extensive experiments on real-world datasets."
neurips,https://proceedings.neurips.cc/paper/2019/file/30c8e1ca872524fbf7ea5c519ca397ee-Paper.pdf,Backprop with Approximate Activations for Memory-efficient Network Training,"Ayan Chakrabarti, Benjamin Moseley",
neurips,https://proceedings.neurips.cc/paper/2019/file/30d411fdc0e6daf092a74354094359bb-Paper.pdf,Learning to Screen,"Alon Cohen, Avinatan Hassidim, Haim Kaplan, Yishay Mansour, Shay Moran","Imagine a large firm with multiple departments that plans a large recruitment. Candidates arrive one-by-one, and for each candidate the firm decides, based on her data (CV, skills, experience, etc), whether to summon her for an interview. The firm wants to recruit the best candidates while minimizing the number of interviews. We model such scenarios as an assignment problem between items (candidates) and categories (departments): the items arrive one-by-one in an online manner, and upon processing each item the algorithm decides, based on its value and the categories it can be matched with, whether to retain or discard it (this decision is irrevocable). The goal is to retain as few items as possible while guaranteeing that the set of retained items contains an optimal matching. We consider two variants of this problem: (i) in the first variant it is assumed that the
n
n
items are drawn independently from an unknown distribution
D
D
. (ii) In the second variant it is assumed that before the process starts, the algorithm has an access to a training set of
n
n
items drawn independently from the same unknown distribution (e.g.\ data of candidates from previous recruitment seasons). We give tight bounds on the minimum possible number of retained items in each of these variants. These results demonstrate that one can retain exponentially less items in the second variant (with the training set). Our algorithms and analysis utilize ideas and techniques from statistical learning theory and from discrete algorithms."
neurips,https://proceedings.neurips.cc/paper/2019/file/30d4e6422cd65c7913bc9ce62e078b79-Paper.pdf,A coupled autoencoder approach for multi-modal analysis of cell types,"Rohan Gala, Nathan Gouwens, Zizhen Yao, Agata Budzillo, Osnat Penn, Bosiljka Tasic, Gabe Murphy, Hongkui Zeng, Uygar Sümbül",
neurips,https://proceedings.neurips.cc/paper/2019/file/30de24287a6d8f07b37c716ad51623a7-Paper.pdf,Meta-Inverse Reinforcement Learning with Probabilistic Context Variables,"Lantao Yu, Tianhe Yu, Chelsea Finn, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2019/file/310cc7ca5a76a446f85c1a0d641ba96d-Paper.pdf,Precision-Recall Balanced Topic Modelling,"Seppo Virtanen, Mark Girolami",
neurips,https://proceedings.neurips.cc/paper/2019/file/312351bff07989769097660a56395065-Paper.pdf,Exact inference in structured prediction,"Kevin Bello, Jean Honorio",
neurips,https://proceedings.neurips.cc/paper/2019/file/3147da8ab4a0437c15ef51a5cc7f2dc4-Paper.pdf,Practical and Consistent Estimation of f-Divergences,"Paul Rubenstein, Olivier Bousquet, Josip Djolonga, Carlos Riquelme, Ilya O. Tolstikhin",
neurips,https://proceedings.neurips.cc/paper/2019/file/315f006f691ef2e689125614ea22cc61-Paper.pdf,Policy Poisoning in Batch Reinforcement Learning and Control,"Yuzhe Ma, Xuezhou Zhang, Wen Sun, Jerry Zhu",
neurips,https://proceedings.neurips.cc/paper/2019/file/3198dfd0aef271d22f7bcddd6f12f5cb-Paper.pdf,R2D2: Reliable and Repeatable Detector and Descriptor,"Jerome Revaud, Cesar De Souza, Martin Humenberger, Philippe Weinzaepfel",
neurips,https://proceedings.neurips.cc/paper/2019/file/31c0b36aef265d9221af80872ceb62f9-Paper.pdf,First Order Motion Model for Image Animation,"Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, Nicu Sebe",
neurips,https://proceedings.neurips.cc/paper/2019/file/31c0c178a9fc26ffecffd8670e6d746d-Paper.pdf,Scalable inference of topic evolution via models for latent geometric structures,"Mikhail Yurochkin, Zhiwei Fan, Aritra Guha, Paraschos Koutris, XuanLong Nguyen",
neurips,https://proceedings.neurips.cc/paper/2019/file/31ca0ca71184bbdb3de7b20a51e88e90-Paper.pdf,Anti-efficient encoding in emergent communication,"Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, Marco Baroni",
neurips,https://proceedings.neurips.cc/paper/2019/file/32508f53f24c46f685870a075eaaa29c-Paper.pdf,Improving Black-box Adversarial Attacks with a Transfer-based Prior,"Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu",
neurips,https://proceedings.neurips.cc/paper/2019/file/328347805873e9a9c700591812fb0ec2-Paper.pdf,REM: From Structural Entropy to Community Structure Deception,"Yiwei Liu, Jiamou Liu, Zijian Zhang, Liehuang Zhu, Angsheng Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/32bbf7b2bc4ed14eb1e9c2580056a989-Paper.pdf,Unsupervised Object Segmentation by Redrawing,"Mickaël Chen, Thierry Artières, Ludovic Denoyer",
neurips,https://proceedings.neurips.cc/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf,Unlabeled Data Improves Adversarial Robustness,"Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C. Duchi, Percy S. Liang","We demonstrate, theoretically and empirically, that adversarial robustness can significantly benefit from semisupervised learning. Theoretically, we revisit the simple Gaussian model of Schmidt et al. that shows a sample complexity gap between standard and robust classification. We prove that unlabeled data bridges this gap: a simple semisupervised learning procedure (self-training) achieves high robust accuracy using the same number of labels required for achieving high standard accuracy. Empirically, we augment CIFAR-10 with 500K unlabeled images sourced from 80 Million Tiny Images and use robust self-training to outperform state-of-the-art robust accuracies by over 5 points in (i)
ℓ
∞
ℓ
robustness against several strong attacks via adversarial training and (ii) certified
ℓ
2
ℓ
and
ℓ
∞
ℓ
robustness via randomized smoothing. On SVHN, adding the dataset's own extra training set with the labels removed provides gains of 4 to 10 points, within 1 point of the gain from using the extra labels."
neurips,https://proceedings.neurips.cc/paper/2019/file/332647f433a1c10fa2e2ae04abfdf83e-Paper.pdf,Optimal Stochastic and Online Learning with Individual Iterates,"Yunwen Lei, Peng Yang, Ke Tang, Ding-Xuan Zhou",
neurips,https://proceedings.neurips.cc/paper/2019/file/3335881e06d4d23091389226225e17c7-Paper.pdf,The Implicit Bias of AdaGrad on Separable Data,"Qian Qian, Xiaoyuan Qian",
neurips,https://proceedings.neurips.cc/paper/2019/file/333ac5d90817d69113471fbb6e531bee-Paper.pdf,iSplit LBI: Individualized Partial Ranking with Ties via Split LBI,"Qianqian Xu, Xinwei Sun, Zhiyong Yang, Xiaochun Cao, Qingming Huang, Yuan Yao",
neurips,https://proceedings.neurips.cc/paper/2019/file/3341f6f048384ec73a7ba2e77d2db48b-Paper.pdf,PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation,"Can Qin, Haoxuan You, Lichen Wang, C.-C. Jay Kuo, Yun Fu",
neurips,https://proceedings.neurips.cc/paper/2019/file/335cd1b90bfa4ee70b39d08a4ae0cf2d-Paper.pdf,Certified Adversarial Robustness with Additive Noise,"Bai Li, Changyou Chen, Wenlin Wang, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2019/file/33b879e7ab79f56af1e88359f9314a10-Paper.pdf,Self-Critical Reasoning for Robust Visual Question Answering,"Jialin Wu, Raymond Mooney",
neurips,https://proceedings.neurips.cc/paper/2019/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf,Optimal Pricing in Repeated Posted-Price Auctions with Different Patience of the Seller and the Buyer,"Arsenii Vanunts, Alexey Drutsa",
neurips,https://proceedings.neurips.cc/paper/2019/file/3416a75f4cea9109507cacd8e2f2aefc-Paper.pdf,Stand-Alone Self-Attention in Vision Models,"Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, Jon Shlens",
neurips,https://proceedings.neurips.cc/paper/2019/file/342285bb2a8cadef22f667eeb6a63732-Paper.pdf,Debiased Bayesian inference for average treatment effects,"Kolyan Ray, Botond Szabo",
neurips,https://proceedings.neurips.cc/paper/2019/file/3430095c577593aad3c39c701712bcfe-Paper.pdf,Globally optimal score-based learning of directed acyclic graphs in high-dimensions,"Bryon Aragam, Arash Amini, Qing Zhou","We prove that
Ω
(
s
log
p
)
Ω
samples suffice to learn a sparse Gaussian directed acyclic graph (DAG) from data, where
s
s
is the maximum Markov blanket size. This improves upon recent results that require
Ω
(
s
4
log
p
)
Ω
samples in the equal variance case. To prove this, we analyze a popular score-based estimator that has been the subject of extensive empirical inquiry in recent years and is known to achieve state-of-the-art results. Furthermore, the approach we study does not require strong assumptions such as faithfulness that existing theory for score-based learning crucially relies on. The resulting estimator is based around a difficult nonconvex optimization problem, and its analysis may be of independent interest given recent interest in nonconvex optimization in machine learning. Our analysis overcomes the drawbacks of existing theoretical analyses, which either fail to guarantee structure consistency in high-dimensions (i.e. learning the correct graph with high probability), or rely on restrictive assumptions. In contrast, we give explicit finite-sample bounds that are valid in the important
p
≫
n
p
regime."
neurips,https://proceedings.neurips.cc/paper/2019/file/34306d99c63613fad5b2a140398c0420-Paper.pdf,GIFT: Learning Transformation-Invariant Dense Visual Descriptors via Group CNNs,"Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, Xiaowei Zhou",
neurips,https://proceedings.neurips.cc/paper/2019/file/348a38cd25abeab0e440f37510e9b1fa-Paper.pdf,Convergence of Adversarial Training in Overparametrized Neural Networks,"Ruiqi Gao, Tianle Cai, Haochuan Li, Cho-Jui Hsieh, Liwei Wang, Jason D. Lee","Neural networks are vulnerable to adversarial examples, i.e. inputs that are imperceptibly perturbed from natural data and yet incorrectly classified by the network. Adversarial training \cite{madry2017towards}, a heuristic form of robust optimization that alternates between minimization and maximization steps, has proven to be among the most successful methods to train networks to be robust against a pre-defined family of perturbations. This paper provides a partial answer to the success of adversarial training, by showing that it converges to a network where the surrogate loss with respect to the the attack algorithm is within
ϵ
ϵ
of the optimal robust loss. Then we show that the optimal robust loss is also close to zero, hence adversarial training finds a robust classifier. The analysis technique leverages recent work on the analysis of neural networks via Neural Tangent Kernel (NTK), combined with motivation from online-learning when the maximization is solved by a heuristic, and the expressiveness of the NTK kernel in the
ℓ
∞
ℓ
-norm. In addition, we also prove that robust interpolation requires more model capacity, supporting the evidence that adversarial training requires wider networks."
neurips,https://proceedings.neurips.cc/paper/2019/file/3493894fa4ea036cfc6433c3e2ee63b0-Paper.pdf,Explicit Disentanglement of Appearance and Perspective in Generative Models,"Nicki Skafte, Søren Hauberg",
neurips,https://proceedings.neurips.cc/paper/2019/file/3501672ebc68a5524629080e3ef60aef-Paper.pdf,Fast and Furious Learning in Zero-Sum Games: Vanishing Regret with Non-Vanishing Step Sizes,"James Bailey, Georgios Piliouras","We show for the first time that it is possible to reconcile in online learning in zero-sum games two seemingly contradictory objectives: vanishing time-average regret and non-vanishing step sizes. This phenomenon, that we coin
fast and furious"" learning in games, sets a new benchmark about what is possible both in max-min optimization as well as in multi-agent systems. Our analysis does not depend on introducing a carefully tailored dynamic. Instead we focus on the most well studied online dynamic, gradient descent. Similarly, we focus on the simplest textbook class of games, two-agent two-strategy zero-sum games, such as Matching Pennies. Even for this simplest of benchmarks the best known bound for total regret, prior to our work, was the trivial one of
O
(
T
)
O
, which is immediately applicable even to a non-learning agent. Based on a tight understanding of the geometry of the non-equilibrating trajectories in the dual space we prove a regret bound of
Θ
(
√
T
)
Θ
matching the well known optimal bound for adaptive step sizes in the online setting. This guarantee holds for all fixed step-sizes without having to know the time horizon in advance and adapt the fixed step-size accordingly.As a corollary, we establish that even with fixed learning rates the time-average of mixed strategies, utilities converge to their exact Nash equilibrium values. We also provide experimental evidence suggesting the stronger regret bound holds for all zero-sum games."
neurips,https://proceedings.neurips.cc/paper/2019/file/351869bde8b9d6ad1e3090bd173f600d-Paper.pdf,Slice-based Learning: A Programming Model for Residual Learning in Critical Data Slices,"Vincent Chen, Sen Wu, Alexander J. Ratner, Jen Weng, Christopher Ré",
neurips,https://proceedings.neurips.cc/paper/2019/file/3557a86db669836730d946052d988e46-Paper.pdf,Nearly Tight Bounds for Robust Proper Learning of Halfspaces with a Margin,"Ilias Diakonikolas, Daniel Kane, Pasin Manurangsi","We study the problem of {\em properly} learning large margin halfspaces in the agnostic PAC model. In more detail, we study the complexity of properly learning
d
d
-dimensional halfspaces on the unit ball within misclassification error
α
⋅
\opt
γ
+
\eps
α
, where
\opt
γ
\opt
is the optimal
γ
γ
-margin error rate and
α
≥
1
α
is the approximation ratio. We give learning algorithms and computational hardness results for this problem, for all values of the approximation ratio
α
≥
1
α
, that are nearly-matching for a range of parameters. Specifically, for the natural setting that
α
α
is any constant bigger than one, we provide an essentially tight complexity characterization. On the positive side, we give an
α
=
1.01
α
-approximate proper learner that uses
O
(
1
/
(
\eps
2
γ
2
)
)
O
samples (which is optimal) and runs in time
\poly
(
d
/
\eps
)
⋅
2
~
O
(
1
/
γ
2
)
\poly
. On the negative side, we show that {\em any} constant factor approximate proper learner has runtime
\poly
(
d
/
\eps
)
⋅
2
(
1
/
γ
)
2
−
o
(
1
)
\poly
, assuming the Exponential Time Hypothesis."
neurips,https://proceedings.neurips.cc/paper/2019/file/358aee4cc897452c00244351e4d91f69-Paper.pdf,Distribution-Independent PAC Learning of Halfspaces with Massart Noise,"Ilias Diakonikolas, Themis Gouleakis, Christos Tzamos","We study the problem of {\em distribution-independent} PAC learning of halfspaces in the presence of Massart noise. Specifically, we are given a set of labeled examples
(
\bx
,
y
)
(
drawn from a distribution
\D
\D
on
\R
d
+
1
\R
such that the marginal distribution on the unlabeled points
\bx
\bx
is arbitrary and the labels
y
y
are generated by an unknown halfspace corrupted with Massart noise at noise rate
η
<
1
/
2
η
. The goal is to find a hypothesis
h
h
that minimizes the misclassification error
\pr
(
\bx
,
y
)
∼
\D
[
h
(
\bx
)
≠
y
]
\pr
. We give a
\poly
(
d
,
1
/
\eps
)
\poly
time algorithm for this problem with misclassification error
η
+
\eps
η
. We also provide evidence that improving on the error guarantee of our algorithm might be computationally hard. Prior to our work, no efficient weak (distribution-independent) learner was known in this model, even for the class of disjunctions. The existence of such an algorithm for halfspaces (or even disjunctions) has been posed as an open question in various works, starting with Sloan (1988), Cohen (1997), and was most recently highlighted in Avrim Blum's FOCS 2003 tutorial."
neurips,https://proceedings.neurips.cc/paper/2019/file/361440528766bbaaaa1901845cf4152b-Paper.pdf,Poisson-Minibatching for Gibbs Sampling with Convergence Rate Guarantees,"Ruqi Zhang, Christopher M. De Sa",
neurips,https://proceedings.neurips.cc/paper/2019/file/363763e5c3dc3a68b399058c34aecf2c-Paper.pdf,Semi-Parametric Dynamic Contextual Pricing,"Virag Shah, Ramesh Johari, Jose Blanchet","Motivated by the application of real-time pricing in e-commerce platforms, we consider the problem of revenue-maximization in a setting where the seller can leverage contextual information describing the customer's history and the product's type to predict her valuation of the product. However, her true valuation is unobservable to the seller, only binary outcome in the form of success-failure of a transaction is observed. Unlike in usual contextual bandit settings, the optimal price/arm given a covariate in our setting is sensitive to the detailed characteristics of the residual uncertainty distribution. We develop a semi-parametric model in which the residual distribution is non-parametric and provide the first algorithm which learns both regression parameters and residual distribution with
~
O
(
√
n
)
O
regret. We empirically test a scalable implementation of our algorithm and observe good performance."
neurips,https://proceedings.neurips.cc/paper/2019/file/36ab62655fa81ce8735ce7cfdaf7c9e8-Paper.pdf,Theoretical evidence for adversarial robustness through randomization,"Rafael Pinot, Laurent Meunier, Alexandre Araujo, Hisashi Kashima, Florian Yger, Cedric Gouy-Pailler, Jamal Atif",
neurips,https://proceedings.neurips.cc/paper/2019/file/36ad8b5f42db492827016448975cc22d-Paper.pdf,On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks,"Sunil Thulasidasan, Gopinath Chennupati, Jeff A. Bilmes, Tanmoy Bhattacharya, Sarah Michalak",
neurips,https://proceedings.neurips.cc/paper/2019/file/36d7534290610d9b7e9abed244dd2f28-Paper.pdf,Thompson Sampling for Multinomial Logit Contextual Bandits,"Min-hwan Oh, Garud Iyengar","We consider a dynamic assortment selection problem where the goal is to offer a sequence of assortments that maximizes the expected cumulative revenue, or alternatively, minimize the expected regret. The feedback here is the item that the user picks from the assortment. The distinguishing feature in this work is that this feedback has a multinomial logistic distribution. The utility of each item is a dynamic function of contextual information of both the item and the user. We propose two Thompson sampling algorithms for this multinomial logit contextual bandit. Our first algorithm maintains a posterior distribution of the true parameter and establishes
~
O
(
d
√
T
)
O
Bayesian regret over
T
T
rounds with
d
d
dimensional context vector. The worst-case computational complexity of this algorithm could be high when the prior distribution is not a conjugate. The second algorithm approximates the posterior by a Gaussian distribution, and uses a new optimistic sampling procedure to address the issues that arise in worst-case regret analysis. This algorithm achieves
~
O
(
d
3
/
2
√
T
)
O
worst-case (frequentist) regret bound. The numerical experiments show that the practical performance of both methods is in line with the theoretical guarantees."
neurips,https://proceedings.neurips.cc/paper/2019/file/36e729ec173b94133d8fa552e4029f8b-Paper.pdf,Symmetry-Based Disentangled Representation Learning requires Interaction with Environments,"Hugo Caselles-Dupré, Michael Garcia Ortiz, David Filliat",
neurips,https://proceedings.neurips.cc/paper/2019/file/36ed197b3f31618fdbadb3df86f804bd-Paper.pdf,Mining GOLD Samples for Conditional GANs,"Sangwoo Mo, Chiheon Kim, Sungwoong Kim, Minsu Cho, Jinwoo Shin",
neurips,https://proceedings.neurips.cc/paper/2019/file/370bfb31abd222b582245b977ea5f25a-Paper.pdf,Few-shot Video-to-Video Synthesis,"Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Bryan Catanzaro, Jan Kautz",
neurips,https://proceedings.neurips.cc/paper/2019/file/373e4c5d8edfa8b74fd4b6791d0cf6dc-Paper.pdf,Unlocking Fairness: a Trade-off Revisited,"Michael Wick, swetasudha panda, Jean-Baptiste Tristan",
neurips,https://proceedings.neurips.cc/paper/2019/file/37693cfc748049e45d87b8c7d8b9aacd-Paper.pdf,Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers,"Liwei Wu, Shuqing Li, Cho-Jui Hsieh, James L. Sharpnack",
neurips,https://proceedings.neurips.cc/paper/2019/file/376c6b9ff3bedbbea56751a84fffc10c-Paper.pdf,An Algorithmic Framework For Differentially Private Data Analysis on Trusted Processors,"Joshua Allen, Bolin Ding, Janardhan Kulkarni, Harsha Nori, Olga Ohrimenko, Sergey Yekhanin",
neurips,https://proceedings.neurips.cc/paper/2019/file/378a063b8fdb1db941e34f4bde584c7d-Paper.pdf,Implicit Generation and Modeling with Energy Based Models,"Yilun Du, Igor Mordatch",
neurips,https://proceedings.neurips.cc/paper/2019/file/37f65c068b7723cd7809ee2d31d7861c-Paper.pdf,Evaluating Protein Transfer Learning with TAPE,"Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, Yun Song",
neurips,https://proceedings.neurips.cc/paper/2019/file/383beaea4aa57dd8202dbff464fee3af-Paper.pdf,Recurrent Space-time Graph Neural Networks,"Andrei Nicolicioiu, Iulia Duta, Marius Leordeanu",
neurips,https://proceedings.neurips.cc/paper/2019/file/384babc3e7faa44cf1ca671b74499c3b-Paper.pdf,Singleshot : a scalable Tucker tensor decomposition,"Abraham Traore, Maxime Berar, Alain Rakotomamonjy",
neurips,https://proceedings.neurips.cc/paper/2019/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf,Secretary Ranking with Minimal Inversions,"Sepehr Assadi, Eric Balkanski, Renato Leme","Our main result is a matching upper and lower bound for the secretary ranking problem. We present an algorithm that ranks n elements with only O(n^{3/2}) inversions in expectation, and show that any algorithm necessarily suffers \Omega(n^{3/2}) inversions when there are n available positions. In terms of techniques, the analysis of our algorithm draws connections to linear probing in the hashing literature, while our lower bound result relies on a general anti-concentration bound for a generic balls and bins sampling process. We also consider the case where the number of positions m can be larger than the number of secretaries n and provide an improved bound by showing a connection of this problem with random binary trees."
neurips,https://proceedings.neurips.cc/paper/2019/file/3891b14b5d8cce2fdd8dcdb4ded28f6d-Paper.pdf,Policy Continuation with Hindsight Inverse Dynamics,"Hao Sun, Zhizhong Li, Xiaotong Liu, Bolei Zhou, Dahua Lin",
neurips,https://proceedings.neurips.cc/paper/2019/file/38ef4b66cb25e92abe4d594acb841471-Paper.pdf,A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off,"Yaniv Blumenfeld, Dar Gilboa, Daniel Soudry","Reducing the precision of weights and activation functions in neural network training, with minimal impact on performance, is essential for the deployment of these models in resource-constrained environments. We apply mean field techniques to networks with quantized activations in order to evaluate the degree to which quantization degrades signal propagation at initialization. We derive initialization schemes which maximize signal propagation in such networks, and suggest why this is helpful for generalization. Building on these results, we obtain a closed form implicit equation for
L
max
L
, the maximal trainable depth (and hence model capacity), given
N
N
, the number of quantization levels in the activation function. Solving this equation numerically, we obtain asymptotically:
L
max
∝
N
1.82
L
."
neurips,https://proceedings.neurips.cc/paper/2019/file/38faae069a1371784081ea9ad9b279d0-Paper.pdf,Exponentially convergent stochastic k-PCA without variance reduction,Cheng Tang,
neurips,https://proceedings.neurips.cc/paper/2019/file/39059724f73a9969845dfe4146c5660e-Paper.pdf,DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction,"Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, Ulrich Neumann",
neurips,https://proceedings.neurips.cc/paper/2019/file/392526094bcba21af9fd4102ce5ed092-Paper.pdf,Personalizing Many Decisions with High-Dimensional Covariates,"Nima Hamidi, Mohsen Bayati, Kapil Gupta",
neurips,https://proceedings.neurips.cc/paper/2019/file/39555391eb0624a439c5131b1bb8a2e0-Paper.pdf,Universal Approximation of Input-Output Maps by Temporal Convolutional Nets,"Joshua Hanson, Maxim Raginsky",
neurips,https://proceedings.neurips.cc/paper/2019/file/39ae2ed11b14a4ccb41d35e9d1ba5d11-Paper.pdf,Equipping Experts/Bandits with Long-term Memory,"Kai Zheng, Haipeng Luo, Ilias Diakonikolas, Liwei Wang","We propose the first black-box approach to obtaining long-term memory guarantees for online learning in the sense of Bousquet and Warmuth, 2002, by reducing the problem to achieving typical switching regret. Specifically, for the classical expert problem with
K
K
actions and
T
T
rounds, using our general framework we develop various algorithms with a regret bound of order
\order
(
√
T
(
S
ln
T
+
n
ln
K
)
)
\order
compared to any sequence of experts with
S
−
1
S
switches among
n
≤
min
{
S
,
K
}
n
distinct experts. In addition, by plugging specific adaptive algorithms into our framework we also achieve the best of both stochastic and adversarial environments simultaneously, which resolves an open problem of Warmuth and Koolen 2014. Furthermore, we extend our results to the sparse multi-armed bandit setting and show both negative and positive results for long-term memory guarantees. As a side result, our lower bound also implies that sparse losses do not help improve the worst-case regret for contextual bandit, a sharp contrast with the non-contextual case."
neurips,https://proceedings.neurips.cc/paper/2019/file/39d929972619274cc9066307f707d002-Paper.pdf,Function-Space Distributions over Kernels,"Gregory Benton, Wesley J. Maddox, Jayson Salkey, Julio Albinati, Andrew Gordon Wilson",
neurips,https://proceedings.neurips.cc/paper/2019/file/39e4973ba3321b80f37d9b55f63ed8b8-Paper.pdf,Fully Neural Network based Model for General Temporal Point Processes,"Takahiro Omi, naonori ueda, Kazuyuki Aihara",
neurips,https://proceedings.neurips.cc/paper/2019/file/3a01fc0853ebeba94fde4d1cc6fb842a-Paper.pdf,Splitting Steepest Descent for Growing Neural Architectures,"Lemeng Wu, Dilin Wang, Qiang Liu","We develop a progressive training approach for neural networks which adaptively grows the network structure by splitting existing neurons to multiple off-springs. By leveraging a functional steepest descent idea, we derive a simple criterion for deciding the best subset of neurons to split and a \emph{splitting gradient} for optimally updating the off-springs. Theoretically, our splitting strategy is a second order functional steepest descent for escaping saddle points in an
\Linfty
\Linfty
-Wasserstein metric space, on which the standard parametric gradient descent is a first-order steepest descent. Our method provides a new computationally efficient approach for optimizing neural network structures, especially for learning lightweight neural architectures in resource-constrained settings."
neurips,https://proceedings.neurips.cc/paper/2019/file/3a029f04d76d32e79367c4b3255dda4d-Paper.pdf,Improving Textual Network Learning with Variational Homophilic Embeddings,"Wenlin Wang, Chenyang Tao, Zhe Gan, Guoyin Wang, Liqun Chen, Xinyuan Zhang, Ruiyi Zhang, Qian Yang, Ricardo Henao, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2019/file/3a066bda8c96b9478bb0512f0a43028c-Paper.pdf,Deep Supervised Summarization: Algorithm and Application to Learning Instructions,"Chengguang Xu, Ehsan Elhamifar",
neurips,https://proceedings.neurips.cc/paper/2019/file/3a0844cee4fcf57de0c71e9ad3035478-Paper.pdf,"Think Globally, Act Locally: A Deep Neural Network Approach to High-Dimensional Time Series Forecasting","Rajat Sen, Hsiang-Fu Yu, Inderjit S. Dhillon",
neurips,https://proceedings.neurips.cc/paper/2019/file/3a1dd98341fafc1dfe9bcf36360e6b84-Paper.pdf,Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso,"Quentin Bertrand, Mathurin Massias, Alexandre Gramfort, Joseph Salmon",
neurips,https://proceedings.neurips.cc/paper/2019/file/3a20f62a0af1aa152670bab3c602feed-Paper.pdf,PAC-Bayes under potentially heavy tails,Matthew Holland,
neurips,https://proceedings.neurips.cc/paper/2019/file/3a24b25a7b092a252166a1641ae953e7-Paper.pdf,Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers,"Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, Greg Yang","Recent works have shown the effectiveness of randomized smoothing as a scalable technique for building neural network-based classifiers that are provably robust to
ℓ
2
ℓ
-norm adversarial perturbations. In this paper, we employ adversarial training to improve the performance of randomized smoothing. We design an adapted attack for smoothed classifiers, and we show how this attack can be used in an adversarial training setting to boost the provable robustness of smoothed classifiers. We demonstrate through extensive experimentation that our method consistently outperforms all existing provably
ℓ
2
ℓ
-robust classifiers by a significant margin on ImageNet and CIFAR-10, establishing the state-of-the-art for provable
ℓ
2
ℓ
-defenses. Moreover, we find that pre-training and semi-supervised learning boost adversarially trained smoothed classifiers even further. Our code and trained models are available at http://github.com/Hadisalman/smoothing-adversarial."
neurips,https://proceedings.neurips.cc/paper/2019/file/3a835d3215755c435ef4fe9965a3f2a0-Paper.pdf,Regression Planning Networks,"Danfei Xu, Roberto Martín-Martín, De-An Huang, Yuke Zhu, Silvio Savarese, Li F. Fei-Fei",
neurips,https://proceedings.neurips.cc/paper/2019/file/3aaa3db6a8983226601cac5dde15a26b-Paper.pdf,Efficient Neural Architecture Transformation Search in Channel-Level for Object Detection,"Junran Peng, Ming Sun, ZHAO-XIANG ZHANG, Tieniu Tan, Junjie Yan","To overcome this obstacle, we introduce a practical neural architecture transformation search(NATS) algorithm for object detection in this paper. Instead of searching and constructing an entire network, NATS explores the architecture space on the base of existing network and reusing its weights."
neurips,https://proceedings.neurips.cc/paper/2019/file/3ab6be46e1d6b21d59a3c3a0b9d0f6ef-Paper.pdf,CXPlain: Causal Explanations for Model Interpretation under Uncertainty,"Patrick Schwab, Walter Karlen",
neurips,https://proceedings.neurips.cc/paper/2019/file/3b220b436e5f3d917a1e649a0dc0281c-Paper.pdf,"Compacting, Picking and Growing for Unforgetting Continual Learning","Ching-Yi Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, Chu-Song Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/3b2acfe2e38102074656ed938abf4ac3-Paper.pdf,Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments,"Vasilis Syrgkanis, Victor Lei, Miruna Oprescu, Maggie Hei, Keith Battocchi, Greg Lewis",
neurips,https://proceedings.neurips.cc/paper/2019/file/3b712de48137572f3849aabd5666a4e3-Paper.pdf,Mapping State Space using Landmarks for Universal Goal Reaching,"Zhiao Huang, Fangchen Liu, Hao Su",
neurips,https://proceedings.neurips.cc/paper/2019/file/3b7d09dac07c9ccd3aae2025cc195250-Paper.pdf,Convergence-Rate-Matching Discretization of Accelerated Optimization Flows Through Opportunistic State-Triggered Control,"Miguel Vaquero, Jorge Cortes",
neurips,https://proceedings.neurips.cc/paper/2019/file/3b92d18aa7a6176dd37d372bc2f1eb71-Paper.pdf,Principal Component Projection and Regression in Nearly Linear Time through Asymmetric SVRG,"Yujia Jin, Aaron Sidford",We achieve our results by applying rational polynomial approximations to reduce the problem to solving asymmetric linear systems which we solve by a variant of SVRG. We corroborate these findings with preliminary empirical experiments.
neurips,https://proceedings.neurips.cc/paper/2019/file/3bd8fdb090f1f5eb66a00c84dbc5ad51-Paper.pdf,Private Stochastic Convex Optimization with Optimal Rates,"Raef Bassily, Vitaly Feldman, Kunal Talwar, Abhradeep Guha Thakurta","We study differentially private (DP) algorithms for stochastic convex optimization (SCO). In this problem the goal is to approximately minimize the population loss given i.i.d.~samples from a distribution over convex and Lipschitz loss functions. A long line of existing work on private convex optimization focuses on the empirical loss and derives asymptotically tight bounds on the excess empirical loss. However a significant gap exists in the known bounds for the population loss. We show that, up to logarithmic factors, the optimal excess population loss for DP algorithms is equal to the larger of the optimal non-private excess population loss, and the optimal excess empirical loss of DP algorithms. This implies that, contrary to intuition based on private ERM, private SCO has asymptotically the same rate of
1
/
√
n
1
as non-private SCO in the parameter regime most common in practice. The best previous result in this setting gives rate of
1
/
n
1
/
4
1
. Our approach builds on existing differentially private algorithms and relies on the analysis of algorithmic stability to ensure generalization."
neurips,https://proceedings.neurips.cc/paper/2019/file/3c0cd9bcd0686e8bc0a9047eae120cc5-Paper.pdf,Complexity of Highly Parallel Non-Smooth Convex Optimization,"Sebastien Bubeck, Qijia Jiang, Yin-Tat Lee, Yuanzhi Li, Aaron Sidford","A landmark result of non-smooth convex optimization is that gradient descent is an optimal algorithm whenever the number of computed gradients is smaller than the dimension
d
d
. In this paper we study the extension of this result to the parallel optimization setting. Namely we consider optimization algorithms interacting with a highly parallel gradient oracle, that is one that can answer
p
o
l
y
(
d
)
p
gradient queries in parallel. We show that in this case gradient descent is optimal only up to
~
O
(
√
d
)
O
rounds of interactions with the oracle. The lower bound improves upon a decades old construction by Nemirovski which proves optimality only up to
d
1
/
3
d
rounds (as recently observed by Balkanski and Singer), and the suboptimality of gradient descent after
√
d
d
rounds was already observed by Duchi, Bartlett and Wainwright. In the latter regime we propose a new method with improved complexity, which we conjecture to be optimal. The analysis of this new method is based upon a generalized version of the recent results on optimal acceleration for highly smooth convex optimization."
neurips,https://proceedings.neurips.cc/paper/2019/file/3c3c139bd8467c1587a41081ad78045e-Paper.pdf,A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning,"Nicolas Carion, Nicolas Usunier, Gabriel Synnaeve, Alessandro Lazaric",
neurips,https://proceedings.neurips.cc/paper/2019/file/3c7417b8df0daf23f39f445e740c7a43-Paper.pdf,Interaction Hard Thresholding: Consistent Sparse Quadratic Regression in Sub-quadratic Time and Space,"Shuo Yang, Yanyao Shen, Sujay Sanghavi","Quadratic regression involves modeling the response as a (generalized) linear function of not only the features
x
j
1
x
but also of quadratic terms
x
j
1
x
j
2
x
. The inclusion of such higher-order “interaction terms"" in regression often provides an easy way to increase accuracy in already-high-dimensional problems. However, this explodes the problem dimension from linear
O
(
p
)
O
to quadratic
O
(
p
2
)
O
, and it is common to look for sparse interactions (typically via heuristics). In this paper, we provide a new algorithm – Interaction Hard Thresholding (IntHT) which is the first one to provably accurately solve this problem in sub-quadratic time and space. It is a variant of Iterative Hard Thresholding; one that uses the special quadratic structure to devise a new way to (approx.) extract the top elements of a
p
2
p
size gradient in sub-
p
2
p
time and space. Our main result is to theoretically prove that, in spite of the many speedup-related approximations, IntHT linearly converges to a consistent estimate under standard high-dimensional sparse recovery assumptions. We also demonstrate its value via synthetic experiments. Moreover, we numerically show that IntHT can be extended to higher-order regression problems, and also theoretically analyze an SVRG variant of IntHT."
neurips,https://proceedings.neurips.cc/paper/2019/file/3c88c1db16b9523b4dcdcd572aa1e16a-Paper.pdf,Differentially Private Distributed Data Summarization under Covariate Shift,"Kanthi Sarpatwar, Karthikeyan Shanmugam, Venkata Sitaramagiridharganesh Ganapavarapu, Ashish Jagmohan, Roman Vaculin","We envision Artificial Intelligence marketplaces to be platforms where consumers, with very less data for a target task, can obtain a relevant model by accessing many private data sources with vast number of data samples. One of the key challenges is to construct a training dataset that matches a target task without compromising on privacy of the data sources. To this end, we consider the following distributed data summarizataion problem. Given K private source datasets denoted by
[
D
i
]
i
∈
[
K
]
[
and a small target validation set
D
v
D
, which may involve a considerable covariate shift with respect to the sources, compute a summary dataset
D
s
⊆
⋃
i
∈
[
K
]
D
i
D
such that its statistical distance from the validation dataset
D
v
D
is minimized. We use the popular Maximum Mean Discrepancy as the measure of statistical distance. The non-private problem has received considerable attention in prior art, for example in prototype selection (Kim et al., NIPS 2016). Our work is the first to obtain strong differential privacy guarantees while ensuring the quality guarantees of the non-private version. We study this problem in a Parsimonious Curator Privacy Model, where a trusted curator coordinates the summarization process while minimizing the amount of private information accessed. Our central result is a novel protocol that (a) ensures the curator does not access more than
O
(
K
1
3
|
D
s
|
+
|
D
v
|
)
O
points (b) has formal privacy guarantees on the leakage of information between the data owners and (c) closely matches the best known non-private greedy algorithm. Our protocol uses two hash functions, one inspired by the Rahimi-Recht random features method and the second leverages state of the art differential privacy mechanisms. We introduce a novel
noiseless'' differentially private auctioning protocol, which may be of independent interest. Apart from theoretical guarantees, we demonstrate the efficacy of our protocol using real-world datasets."
neurips,https://proceedings.neurips.cc/paper/2019/file/3cc697419ea18cc98d525999665cb94a-Paper.pdf,On Fenchel Mini-Max Learning,"Chenyang Tao, Liqun Chen, Shuyang Dai, Junya Chen, Ke Bai, Dong Wang, Jianfeng Feng, Wenlian Lu, Georgiy Bobashev, Lawrence Carin",
neurips,https://proceedings.neurips.cc/paper/2019/file/3ce257b311e5acf849992f5a675188e8-Paper.pdf,Optimizing Generalized Rate Metrics with Three Players,"Harikrishna Narasimhan, Andrew Cotter, Maya Gupta",
neurips,https://proceedings.neurips.cc/paper/2019/file/3ce3bd7d63a2c9c81983cc8e9bd02ae5-Paper.pdf,Stability of Graph Scattering Transforms,"Fernando Gama, Alejandro Ribeiro, Joan Bruna",
neurips,https://proceedings.neurips.cc/paper/2019/file/3cf2559725a9fdfa602ec8c887440f32-Paper.pdf,A Geometric Perspective on Optimal Representations for Reinforcement Learning,"Marc Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel Castro, Nicolas Le Roux, Dale Schuurmans, Tor Lattimore, Clare Lyle",
neurips,https://proceedings.neurips.cc/paper/2019/file/3d779cae2d46cf6a8a99a35ba4167977-Paper.pdf,More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation,"Quanfu Fan, Chun-Fu (Richard) Chen, Hilde Kuehne, Marco Pistoia, David Cox",
neurips,https://proceedings.neurips.cc/paper/2019/file/3d8e03e8b133b16f13a586f0c01b6866-Paper.pdf,Provably Efficient Q-learning with Function Approximation via Distribution Shift Error Checking Oracle,"Simon S. Du, Yuping Luo, Ruosong Wang, Hanrui Zhang","Q-learning with function approximation is one of the most popular methods in reinforcement learning. Though the idea of using function approximation was proposed at least 60 years ago, even in the simplest setup, i.e, approximating Q-functions with linear functions, it is still an open problem how to design a provably efficient algorithm that learns a near-optimal policy. The key challenges are how to efficiently explore the state space and how to decide when to stop exploring in conjunction with the function approximation scheme. The current paper presents a provably efficient algorithm for Q-learning with linear function approximation. Under certain regularity assumptions, our algorithm, Difference Maximization Q-learning, combined with linear function approximation, returns a near-optimal policy using polynomial number of trajectories. Our algorithm introduces a new notion, the Distribution Shift Error Checking (DSEC) oracle. This oracle tests whether there exists a function in the function class that predicts well on a distribution
D
1
D
, but predicts poorly on another distribution
D
2
D
, where
D
1
D
and
D
2
D
are distributions over states induced by two different exploration policies. For the linear function class, this oracle is equivalent to solving a top eigenvalue problem. We believe our algorithmic insights, especially the DSEC oracle, are also useful in designing and analyzing reinforcement learning algorithms with general function approximation."
neurips,https://proceedings.neurips.cc/paper/2019/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf,Learner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints,"Sebastian Tschiatschek, Ahana Ghosh, Luis Haug, Rati Devidze, Adish Singla",
neurips,https://proceedings.neurips.cc/paper/2019/file/3dea6b598a16b334a53145e78701fa87-Paper.pdf,PAC-Bayes Un-Expected Bernstein Inequality,"Zakaria Mhammedi, Peter Grünwald, Benjamin Guedj","We present a new PAC-Bayesian generalization bound. Standard bounds contain a
√
L
n
⋅
\KL
/
n
L
complexity term which dominates unless
L
n
L
, the empirical error of the learning algorithm's randomized predictions, vanishes. We manage to replace
L
n
L
by a term which vanishes in many more situations, essentially whenever the employed learning algorithm is sufficiently stable on the dataset at hand. Our new bound consistently beats state-of-the-art bounds both on a toy example and on UCI datasets (with large enough
n
n
). Theoretically, unlike existing bounds, our new bound can be expected to converge to
0
0
faster whenever a Bernstein/Tsybakov condition holds, thus connecting PAC-Bayesian generalization and {\em excess risk\/} bounds---for the latter it has long been known that faster convergence can be obtained under Bernstein conditions. Our main technical tool is a new concentration inequality which is like Bernstein's but with
X
2
X
taken outside its expectation."
neurips,https://proceedings.neurips.cc/paper/2019/file/3e6260b81898beacda3d16db379ed329-Paper.pdf,Revisiting the Bethe-Hessian: Improved Community Detection in Sparse Heterogeneous Graphs,"Lorenzo Dall'Amico, Romain Couillet, Nicolas Tremblay",
neurips,https://proceedings.neurips.cc/paper/2019/file/3e883840fee4384dd3d2afea5e822517-Paper.pdf,Learning Positive Functions with Pseudo Mirror Descent,"Yingxiang Yang, Haoxiang Wang, Negar Kiyavash, Niao He",
neurips,https://proceedings.neurips.cc/paper/2019/file/3e91970f771a2c473ae36b60d1146068-Paper.pdf,Censored Semi-Bandits: A Framework for Resource Allocation with Censored Feedback,"Arun Verma, Manjesh Hanawal, Arun Rajkumar, Raman Sankaran",
neurips,https://proceedings.neurips.cc/paper/2019/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf,Defending Against Neural Fake News,"Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi","Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like 'Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation."
neurips,https://proceedings.neurips.cc/paper/2019/file/3eb2f1a06667bfb9daba7f7effa0284b-Paper.pdf,Robust and Communication-Efficient Collaborative Learning,"Amirhossein Reisizadeh, Hossein Taheri, Aryan Mokhtari, Hamed Hassani, Ramtin Pedarsani",
neurips,https://proceedings.neurips.cc/paper/2019/file/3eb65004054f5d21fca4087f5658c727-Paper.pdf,A Self Validation Network for Object-Level Human Attention Estimation,"Zehua Zhang, Chen Yu, David Crandall",
neurips,https://proceedings.neurips.cc/paper/2019/file/3eefceb8087e964f89c2d59e8a249915-Paper.pdf,Learning Robust Global Representations by Penalizing Local Predictive Power,"Haohan Wang, Songwei Ge, Zachary Lipton, Eric P. Xing",
neurips,https://proceedings.neurips.cc/paper/2019/file/3ef815416f775098fe977004015c6193-Paper.pdf,Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation,"Mark Bun, Thomas Steinke",
neurips,https://proceedings.neurips.cc/paper/2019/file/3f4366aeb9c157cf9a30c90693eafc55-Paper.pdf,A Regularized Approach to Sparse Optimal Policy in Reinforcement Learning,"Wenhao Yang, Xiang Li, Zhihua Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/3f7bcd0b3ea822683bba8fc530f151bd-Paper.pdf,Dynamic Ensemble Modeling Approach to Nonstationary Neural Decoding in Brain-Computer Interfaces,"Yu Qi, Bin Liu, Yueming Wang, Gang Pan",
neurips,https://proceedings.neurips.cc/paper/2019/file/3fb04953d95a94367bb133f862402bce-Paper.pdf,First-order methods almost always avoid saddle points: The case of vanishing step-sizes,"Ioannis Panageas, Georgios Piliouras, Xiao Wang","In a series of papers [Lee et al 2016], [Panageas and Piliouras 2017], [Lee et al 2019], it was established that some of the most commonly used first order methods almost surely (under random initializations) and with step-size being small enough, avoid strict saddle points, as long as the objective function
f
f
is
C
2
C
and has Lipschitz gradient. The key observation was that first order methods can be studied from a dynamical systems perspective, in which instantiations of Center-Stable manifold theorem allow for a global analysis. The results of the aforementioned papers were limited to the case where the step-size
α
α
is constant, i.e., does not depend on time (and typically bounded from the inverse of the Lipschitz constant of the gradient of
f
f
). It remains an open question whether or not the results still hold when the step-size is time dependent and vanishes with time. In this paper, we resolve this question on the affirmative for gradient descent, mirror descent, manifold descent and proximal point. The main technical challenge is that the induced (from each first order method) dynamical system is time non-homogeneous and the stable manifold theorem is not applicable in its classic form. By exploiting the dynamical systems structure of the aforementioned first order methods, we are able to prove a stable manifold theorem that is applicable to time non-homogeneous dynamical systems and generalize the results in [Lee et al 2019] for time dependent step-sizes."
neurips,https://proceedings.neurips.cc/paper/2019/file/3fd60983292458bf7dee75f12d5e9e05-Paper.pdf,Online Markov Decoding: Lower Bounds and Near-Optimal Approximation Algorithms,"Vikas Garg, Tamar Pichkhadze",
neurips,https://proceedings.neurips.cc/paper/2019/file/3ffebb08d23c609875d7177ee769a3e9-Paper.pdf,Faster Boosting with Smaller Memory,"Julaiti Alafate, Yoav S. Freund",
neurips,https://proceedings.neurips.cc/paper/2019/file/40afd3a37cca05efe623b7509855c73a-Paper.pdf,Modelling the Dynamics of Multiagent Q-Learning in Repeated Symmetric Games: a Mean Field Theoretic Approach,"Shuyue Hu, Chin-wing Leung, Ho-fung Leung",
neurips,https://proceedings.neurips.cc/paper/2019/file/411ae1bf081d1674ca6091f8c59a266f-Paper.pdf,Information-Theoretic Confidence Bounds for Reinforcement Learning,"Xiuyuan Lu, Benjamin Van Roy",
neurips,https://proceedings.neurips.cc/paper/2019/file/412758d043dd247bddea07c7ec558c31-Paper.pdf,Bootstrapping Upper Confidence Bound,"Botao Hao, Yasin Abbasi Yadkori, Zheng Wen, Guang Cheng",
neurips,https://proceedings.neurips.cc/paper/2019/file/415185ea244ea2b2bedeb0449b926802-Paper.pdf,DETOX: A Redundancy-based Framework for Faster and More Robust Gradient Aggregation,"Shashank Rajput, Hongyi Wang, Zachary Charles, Dimitris Papailiopoulos",
neurips,https://proceedings.neurips.cc/paper/2019/file/4158f6d19559955bae372bb00f6204e4-Paper.pdf,Differentially Private Covariance Estimation,"Kareem Amin, Travis Dick, Alex Kulesza, Andres Munoz, Sergei Vassilvitskii","In this work we propose a new epsilon-differentially private algorithm for computing the covariance matrix of a dataset that addresses both of these limitations. We show that it has lower error than existing state-of-the-art approaches, both analytically and empirically. In addition, the algorithm is significantly less complicated than other methods and can be efficiently implemented with rejection sampling."
neurips,https://proceedings.neurips.cc/paper/2019/file/418ef6127e44214882c61e372e866691-Paper.pdf,Meta-Reinforced Synthetic Data for One-Shot Fine-Grained Visual Recognition,"Satoshi Tsutsui, Yanwei Fu, David Crandall",
neurips,https://proceedings.neurips.cc/paper/2019/file/4191ef5f6c1576762869ac49281130c9-Paper.pdf,PHYRE: A New Benchmark for Physical Reasoning,"Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, Ross Girshick",
neurips,https://proceedings.neurips.cc/paper/2019/file/41c576a3bac4220845f9427b002a2a9d-Paper.pdf,Facility Location Problem in Differential Privacy Model Revisited,"Yunus Esencayi, Marco Gaboardi, Shi Li, Di Wang","In this paper we study the facility location problem in the model of differential privacy (DP) with uniform facility cost. Specifically, we first show that under the hierarchically well-separated tree (HST) metrics and the super-set output setting that was introduced in Gupta et. al., there is an
ϵ
ϵ
-DP algorithm that achieves an
O
(
1
ϵ
)
O
(expected multiplicative) approximation ratio; this implies an
O
(
log
n
ϵ
)
O
approximation ratio for the general metric case, where
n
n
is the size of the input metric. These bounds improve the best-known results given by Gupta et. al. In particular, our approximation ratio for HST-metrics is independent of
n
n
, and the ratio for general metrics is independent of the aspect ratio of the input metric. On the negative side, we show that the approximation ratio of any
ϵ
ϵ
-DP algorithm is lower bounded by
Ω
(
1
√
ϵ
)
Ω
, even for instances on HST metrics with uniform facility cost, under the super-set output setting. The lower bound shows that the dependence of the approximation ratio for HST metrics on
ϵ
ϵ
can not be removed or greatly improved. Our novel methods and techniques for both the upper and lower bound may find additional applications."
neurips,https://proceedings.neurips.cc/paper/2019/file/4206e38996fae4028a26d43b24f68d32-Paper.pdf,Provably robust boosted decision stumps and trees against adversarial attacks,"Maksym Andriushchenko, Matthias Hein","The problem of adversarial robustness has been studied extensively for neural networks. However, for boosted decision trees and decision stumps there are almost no results, even though they are widely used in practice (e.g. XGBoost) due to their accuracy, interpretability, and efficiency. We show in this paper that for boosted decision stumps the \textit{exact} min-max robust loss and test error for an
l
∞
l
-attack can be computed in
O
(
T
log
T
)
O
time per input, where
T
T
is the number of decision stumps and the optimal update step of the ensemble can be done in
O
(
n
2
T
log
T
)
O
, where
n
n
is the number of data points. For boosted trees we show how to efficiently calculate and optimize an upper bound on the robust loss, which leads to state-of-the-art robust test error for boosted trees on MNIST (12.5\% for
ϵ
∞
=
0.3
ϵ
), FMNIST (23.2\% for
ϵ
∞
=
0.1
ϵ
), and CIFAR-10 (74.7\% for
ϵ
∞
=
8
/
255
ϵ
). Moreover, the robust test error rates we achieve are competitive to the ones of provably robust convolutional networks. The code of all our experiments is available at \url{http://github.com/max-andr/provably-robust-boosting}."
neurips,https://proceedings.neurips.cc/paper/2019/file/42547f5a44d87da3bc40ee5d09624606-Paper.pdf,Graph-Based Semi-Supervised Learning with Non-ignorable Non-response,"Fan Zhou, Tengfei Li, Haibo Zhou, Hongtu Zhu, Ye Jieping",
neurips,https://proceedings.neurips.cc/paper/2019/file/42a6845a557bef704ad8ac9cb4461d43-Paper.pdf,Latent Ordinary Differential Equations for Irregularly-Sampled Time Series,"Yulia Rubanova, Ricky T. Q. Chen, David K. Duvenaud",
neurips,https://proceedings.neurips.cc/paper/2019/file/42c8938e4cf5777700700e642dc2a8cd-Paper.pdf,On the Correctness and Sample Complexity of Inverse Reinforcement Learning,"Abi Komanduru, Jean Honorio","Inverse reinforcement learning (IRL) is the problem of finding a reward function that generates a given optimal policy for a given Markov Decision Process. This paper looks at an algorithmic-independent geometric analysis of the IRL problem with finite states and actions. A L1-regularized Support Vector Machine formulation of the IRL problem motivated by the geometric analysis is then proposed with the basic objective of the inverse reinforcement problem in mind: to find a reward function that generates a specified optimal policy. The paper further analyzes the proposed formulation of inverse reinforcement learning with
n
n
states and
k
k
actions, and shows a sample complexity of
O
(
d
2
log
(
n
k
)
)
O
for transition probability matrices with at most
d
d
non-zeros per row, for recovering a reward function that generates a policy that satisfies Bellman's optimality condition with respect to the true transition probabilities."
neurips,https://proceedings.neurips.cc/paper/2019/file/43207fd5e34f87c48d584fc5c11befb8-Paper.pdf,A New Distribution on the Simplex with Auto-Encoding Applications,"Andrew Stirn, Tony Jebara, David Knowles",
neurips,https://proceedings.neurips.cc/paper/2019/file/433371e69eb202f8e7bc8ec2c8d48021-Paper.pdf,Model Selection for Contextual Bandits,"Dylan J. Foster, Akshay Krishnamurthy, Haipeng Luo","We introduce the problem of model selection for contextual bandits, where a learner must adapt to the complexity of the optimal policy while balancing exploration and exploitation. Our main result is a new model selection guarantee for linear contextual bandits. We work in the stochastic realizable setting with a sequence of nested linear policy classes of dimension
d
1
<
d
2
<
…
d
, where the
m
⋆
m
-th class contains the optimal policy, and we design an algorithm that achieves
~
O
l
(
T
2
/
3
d
1
/
3
m
⋆
)
O
regret with no prior knowledge of the optimal dimension
d
m
⋆
d
. The algorithm also achieves regret
~
O
(
T
3
/
4
+
√
T
d
m
⋆
)
O
, which is optimal for
d
m
⋆
≥
√
T
d
. This is the first model selection result for contextual bandits with non-vacuous regret for all values of
d
m
⋆
d
, and to the best of our knowledge is the first positive result of this type for any online learning setting with partial information. The core of the algorithm is a new estimator for the gap in the best loss achievable by two linear policy classes, which we show admits a convergence rate faster than the rate required to learn the parameters for either class."
neurips,https://proceedings.neurips.cc/paper/2019/file/438124b4c06f3a5caffab2c07863b617-Paper.pdf,Learning-In-The-Loop Optimization: End-To-End Control And Co-Design Of Soft Robots Through Learned Deep Latent Representations,"Andrew Spielberg, Allan Zhao, Yuanming Hu, Tao Du, Wojciech Matusik, Daniela Rus",
neurips,https://proceedings.neurips.cc/paper/2019/file/43ec517d68b6edd3015b3edc9a11367b-Paper.pdf,FreeAnchor: Learning to Match Anchors for Visual Object Detection,"Xiaosong Zhang, Fang Wan, Chang Liu, Rongrong Ji, Qixiang Ye",
neurips,https://proceedings.neurips.cc/paper/2019/file/44885837c518b06e3f98b41ab8cedc0f-Paper.pdf,Invariance and identifiability issues for word embeddings,"Rachel Carrington, Karthik Bharath, Simon Preston",
neurips,https://proceedings.neurips.cc/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,"Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman",
neurips,https://proceedings.neurips.cc/paper/2019/file/44a2e0804995faf8d2e3b084a1e2db1d-Paper.pdf,PC-Fairness: A Unified Framework for Measuring Causality-based Fairness,"Yongkai Wu, Lu Zhang, Xintao Wu, Hanghang Tong",
neurips,https://proceedings.neurips.cc/paper/2019/file/451ae86722d26a608c2e174b2b2773f1-Paper.pdf,Worst-Case Regret Bounds for Exploration via Randomized Value Functions,Daniel Russo,
neurips,https://proceedings.neurips.cc/paper/2019/file/452bf208bf901322968557227b8f6efe-Paper.pdf,Glyce: Glyph-vectors for Chinese Character Representations,"Yuxian Meng, Wei Wu, Fei Wang, Xiaoya Li, Ping Nie, Fan Yin, Muyu Li, Qinghong Han, Xiaofei Sun, Jiwei Li","In this paper, we address this gap by presenting Glyce, the glyph-vectors for Chinese character representations. We make three major innovations: (1) We use historical Chinese scripts (e.g., bronzeware script, seal script, traditional Chinese, etc) to enrich the pictographic evidence in characters; (2) We design CNN structures (called tianzege-CNN) tailored to Chinese character image processing; and (3) We use image-classification as an auxiliary task in a multi-task learning setup to increase the model's ability to generalize."
neurips,https://proceedings.neurips.cc/paper/2019/file/4559912e7a94a9c32b09d894f2bc3c82-Paper.pdf,Fast and Provable ADMM for Learning with Generative Priors,"Fabian Latorre, Armin eftekhari, Volkan Cevher",
neurips,https://proceedings.neurips.cc/paper/2019/file/455cb2657aaa59e32fad80cb0b65b9dc-Paper.pdf,GRU-ODE-Bayes: Continuous Modeling of Sporadically-Observed Time Series,"Edward De Brouwer, Jaak Simm, Adam Arany, Yves Moreau",
neurips,https://proceedings.neurips.cc/paper/2019/file/456048afb7253926e1fbb7486e699180-Paper.pdf,Stochastic Continuous Greedy ++: When Upper and Lower Bounds Match,"Amin Karbasi, Hamed Hassani, Aryan Mokhtari, Zebang Shen","In this paper, we develop \scg~(\text{SCG}{
+
+
+
}), the first efficient variant of a conditional gradient method for maximizing a continuous submodular function subject to a convex constraint. Concretely, for a monotone and continuous DR-submodular function, \SCGPP achieves a tight
[
(
1
−
1
/
e
)
\OPT
−
ϵ
]
[
solution while using
O
(
1
/
ϵ
2
)
O
stochastic gradients and
O
(
1
/
ϵ
)
O
calls to the linear optimization oracle. The best previously known algorithms either achieve a suboptimal
[
(
1
/
2
)
\OPT
−
ϵ
]
[
solution with
O
(
1
/
ϵ
2
)
O
stochastic gradients or the tight
[
(
1
−
1
/
e
)
\OPT
−
ϵ
]
[
solution with suboptimal
O
(
1
/
ϵ
3
)
O
stochastic gradients. We further provide an information-theoretic lower bound to showcase the necessity of
\OM
(
1
/
ϵ
2
)
\OM
stochastic oracle queries in order to achieve
[
(
1
−
1
/
e
)
\OPT
−
ϵ
]
[
for monotone and DR-submodular functions. This result shows that our proposed \SCGPP enjoys optimality in terms of both approximation guarantee, i.e.,
(
1
−
1
/
e
)
(
approximation factor, and stochastic gradient evaluations, i.e.,
O
(
1
/
ϵ
2
)
O
calls to the stochastic oracle. By using stochastic continuous optimization as an interface, we also show that it is possible to obtain the
[
(
1
−
1
/
e
)
\OPT
−
ϵ
]
[
tight approximation guarantee for maximizing a monotone but stochastic submodular set function subject to a general matroid constraint after at most
O
(
n
2
/
ϵ
2
)
O
calls to the stochastic function value, where
n
n
is the number of elements in the ground set."
neurips,https://proceedings.neurips.cc/paper/2019/file/45d6637b718d0f24a237069fe41b0db4-Paper.pdf,General E(2)-Equivariant Steerable CNNs,"Maurice Weiler, Gabriele Cesa",
neurips,https://proceedings.neurips.cc/paper/2019/file/4625d8e31dad7d1c4c83399a6eb62f0c-Paper.pdf,On the convergence of single-call stochastic extra-gradient methods,"Yu-Guan Hsieh, Franck Iutzeler, Jérôme Malick, Panayotis Mertikopoulos","Variational inequalities have recently attracted considerable interest in machine learning as a flexible paradigm for models that go beyond ordinary loss function minimization (such as generative adversarial networks and related deep learning systems). In this setting, the optimal O(1/t) convergence rate for solving smooth monotone variational inequalities is achieved by the Extra-Gradient (EG) algorithm and its variants. Aiming to alleviate the cost of an extra gradient step per iteration (which can become quite substantial in deep learning), several algorithms have been proposed as surrogates to Extra-Gradient with a single oracle call per iteration. In this paper, we develop a synthetic view of such algorithms, and we complement the existing literature by showing that they retain a
O
(
1
/
t
)
O
ergodic convergence rate in smooth, deterministic problems. Subsequently, beyond the monotone deterministic case, we also show that the last iterate of single-call, stochastic extra-gradient methods still enjoys a
O
(
1
/
t
)
O
local convergence rate to solutions of non-monotone variational inequalities that satisfy a second-order sufficient condition."
neurips,https://proceedings.neurips.cc/paper/2019/file/464074179972cbbd75a39abc6954cd12-Paper.pdf,Learning nonlinear level sets for dimensionality reduction in function approximation,"Guannan Zhang, Jiaxin Zhang, Jacob Hinkle",
neurips,https://proceedings.neurips.cc/paper/2019/file/465636eb4a7ff4b267f3b765d07a02da-Paper.pdf,Regularized Gradient Boosting,"Corinna Cortes, Mehryar Mohri, Dmitry Storcheus",
neurips,https://proceedings.neurips.cc/paper/2019/file/466accbac9a66b805ba50e42ad715740-Paper.pdf,Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models,"Vincent LE GUEN, Nicolas THOME",
neurips,https://proceedings.neurips.cc/paper/2019/file/46922a0880a8f11f8f69cbb52b1396be-Paper.pdf,General Proximal Incremental Aggregated Gradient Algorithms: Better and Novel Results under General Scheme,"Tao Sun, Yuejiao Sun, Dongsheng Li, Qing Liao","In this paper, we propose a general proximal incremental aggregated gradient algorithm, which contains various existing algorithms including the basic incremental aggregated gradient method. Better and new convergence results are proved even with the general scheme. The novel results presented in this paper, which have not appeared in previous literature, include: a general scheme, nonconvex analysis, the sublinear convergence rates of the function values, much larger stepsizes that guarantee the convergence, the convergence when noise exists, the line search strategy of the proximal incremental aggregated gradient algorithm and its convergence."
neurips,https://proceedings.neurips.cc/paper/2019/file/46a4378f835dc8040c8057beb6a2da52-Paper.pdf,Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets,"Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Rong Ge, Sanjeev Arora","We give mathematical explanations for this phenomenon, assuming generic properties (such as dropout stability and noise stability) of well-trained deep nets, which have previously been identified as part of understanding the generalization properties of deep nets. Our explanation holds for realistic multilayer nets, and experiments are presented to verify the theory."
neurips,https://proceedings.neurips.cc/paper/2019/file/46a558d97954d0692411c861cf78ef79-Paper.pdf,Limitations of the empirical Fisher approximation for natural gradient descent,"Frederik Kunstner, Philipp Hennig, Lukas Balles",
neurips,https://proceedings.neurips.cc/paper/2019/file/46c7cb50b373877fb2f8d5c4517bb969-Paper.pdf,"Fast, Provably convergent IRLS Algorithm for p-norm Linear Regression","Deeksha Adil, Richard Peng, Sushant Sachdeva",
neurips,https://proceedings.neurips.cc/paper/2019/file/46d0671dd4117ea366031f87f3aa0093-Paper.pdf,A Model to Search for Synthesizable Molecules,"John Bradshaw, Brooks Paige, Matt J. Kusner, Marwin Segler, José Miguel Hernández-Lobato",
neurips,https://proceedings.neurips.cc/paper/2019/file/46f76a4bda9a9579eab38a8f6eabcda1-Paper.pdf,Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness,"Saeed Mahloujifar, Xiao Zhang, Mohammad Mahmoody, David Evans",
neurips,https://proceedings.neurips.cc/paper/2019/file/471c75ee6643a10934502bdafee198fb-Paper.pdf,Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries,"Fuwen Tan, Paola Cascante-Bonilla, Xiaoxiao Guo, Hui Wu, Song Feng, Vicente Ordonez",
neurips,https://proceedings.neurips.cc/paper/2019/file/473803f0f2ebd77d83ee60daaa61f381-Paper.pdf,Provably Efficient Q-Learning with Low Switching Cost,"Yu Bai, Tengyang Xie, Nan Jiang, Yu-Xiang Wang","We take initial steps in studying PAC-MDP algorithms with limited adaptivity, that is, algorithms that change its exploration policy as infrequently as possible during regret minimization. This is motivated by the difficulty of running fully adaptive algorithms in real-world applications (such as medical domains), and we propose to quantify adaptivity using the notion of \emph{local switching cost}. Our main contribution, Q-Learning with UCB2 exploration, is a model-free algorithm for
H
H
-step episodic MDP that achieves sublinear regret whose local switching cost in
K
K
episodes is
O
(
H
3
S
A
log
K
)
O
, and we provide a lower bound of
Ω
(
H
S
A
)
Ω
on the local switching cost for any no-regret algorithm. Our algorithm can be naturally adapted to the concurrent setting \citep{guo2015concurrent}, which yields nontrivial results that improve upon prior work in certain aspects."
neurips,https://proceedings.neurips.cc/paper/2019/file/475fbefa9ebfba9233364533aafd02a3-Paper.pdf,Fast and Accurate Least-Mean-Squares Solvers,"Alaa Maalouf, Ibrahim Jubran, Dan Feldman","Least-mean squares (LMS) solvers such as Linear / Ridge / Lasso-Regression, SVD and Elastic-Net not only solve fundamental machine learning problems, but are also the building blocks in a variety of other methods, such as decision trees and matrix factorizations. We suggest an algorithm that gets a finite set of
n
n
d
d
-dimensional real vectors and returns a weighted subset of
d
+
1
d
vectors whose sum is \emph{exactly} the same. The proof in Caratheodory's Theorem (1907) computes such a subset in
O
(
n
2
d
2
)
O
time and thus not used in practice. Our algorithm computes this subset in
O
(
n
d
)
O
time, using
O
(
log
n
)
O
calls to Caratheodory's construction on small but ""smart"" subsets. This is based on a novel paradigm of fusion between different data summarization techniques, known as sketches and coresets. As an example application, we show how it can be used to boost the performance of existing LMS solvers, such as those in scikit-learn library, up to x100. Generalization for streaming and distributed (big) data is trivial. Extensive experimental results and complete open source code are also provided."
neurips,https://proceedings.neurips.cc/paper/2019/file/4772c1b987f1f6d8c9d4ef0f3b764f7a-Paper.pdf,Graph Agreement Models for Semi-Supervised Learning,"Otilia Stretcu, Krishnamurthy Viswanathan, Dana Movshovitz-Attias, Emmanouil Platanios, Sujith Ravi, Andrew Tomkins",
neurips,https://proceedings.neurips.cc/paper/2019/file/47d1e990583c9c67424d369f3414728e-Paper.pdf,Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection,"Bingzhe Wu, Shiwan Zhao, Chaochao Chen, Haoyang Xu, Li Wang, Xiaolu Zhang, Guangyu Sun, Jun Zhou",
neurips,https://proceedings.neurips.cc/paper/2019/file/48042b1dae4950fef2bd2aafa0b971a1-Paper.pdf,Large Scale Structure of Neural Network Loss Landscapes,"Stanislav Fort, Stanislaw Jastrzebski","There are many surprising and perhaps counter-intuitive properties of optimization of deep neural networks. We propose and experimentally verify a unified phenomenological model of the loss landscape that incorporates many of them. High dimensionality plays a key role in our model. Our core idea is to model the loss landscape as a set of high dimensional \emph{wedges} that together form a large-scale, inter-connected structure and towards which optimization is drawn. We first show that hyperparameter choices such as learning rate, network width and
L
2
L
regularization, affect the path optimizer takes through the landscape in similar ways, influencing the large scale curvature of the regions the optimizer explores. Finally, we predict and demonstrate new counter-intuitive properties of the loss-landscape. We show an existence of low loss subspaces connecting a set (not only a pair) of solutions, and verify it experimentally. Finally, we analyze recently popular ensembling techniques for deep networks in the light of our model."
neurips,https://proceedings.neurips.cc/paper/2019/file/48237d9f2dea8c74c2a72126cf63d933-Paper.pdf,Model Similarity Mitigates Test Set Overuse,"Horia Mania, John Miller, Ludwig Schmidt, Moritz Hardt, Benjamin Recht",
neurips,https://proceedings.neurips.cc/paper/2019/file/486c825db2f776da72d0b7a791f45b8f-Paper.pdf,Explicit Planning for Efficient Exploration in Reinforcement Learning,"Liangpeng Zhang, Ke Tang, Xin Yao",
neurips,https://proceedings.neurips.cc/paper/2019/file/48aedb8880cab8c45637abc7493ecddd-Paper.pdf,vGraph: A Generative Model for Joint Community Detection and Node Representation Learning,"Fan-Yun Sun, Meng Qu, Jordan Hoffmann, Chin-Wei Huang, Jian Tang",
neurips,https://proceedings.neurips.cc/paper/2019/file/48c8c3963853fff20bd9e8bee9bd4c07-Paper.pdf,Can Unconditional Language Models Recover Arbitrary Sentences?,"Nishant Subramani, Samuel Bowman, Kyunghyun Cho",
neurips,https://proceedings.neurips.cc/paper/2019/file/48e59000d7dfcf6c1d96ce4a603ed738-Paper.pdf,A Kernel Loss for Solving the Bellman Equation,"Yihao Feng, Lihong Li, Qiang Liu",
neurips,https://proceedings.neurips.cc/paper/2019/file/48f7d3043bc03e6c48a6f0ebc0f258a8-Paper.pdf,Covariate-Powered Empirical Bayes Estimation,"Nikolaos Ignatiadis, Stefan Wager",
neurips,https://proceedings.neurips.cc/paper/2019/file/48fbab00052197bc8bd943498b89dd71-Paper.pdf,Tight Sample Complexity of Learning One-hidden-layer Convolutional Neural Networks,"Yuan Cao, Quanquan Gu",
neurips,https://proceedings.neurips.cc/paper/2019/file/490640b43519c77281cb2f8471e61a71-Paper.pdf,Non-asymptotic Analysis of Stochastic Methods for Non-Smooth Non-Convex Regularized Problems,"Yi Xu, Rong Jin, Tianbao Yang",
neurips,https://proceedings.neurips.cc/paper/2019/file/49182f81e6a13cf5eaa496d51fea6406-Paper.pdf,AGEM: Solving Linear Inverse Problems via Deep Priors and Sampling,"Bichuan Guo, Yuxing Han, Jiangtao Wen",
neurips,https://proceedings.neurips.cc/paper/2019/file/49265d2447bc3bbfe9e76306ce40a31f-Paper.pdf,Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks,"Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, Yang Liu",
neurips,https://proceedings.neurips.cc/paper/2019/file/49af6c4e558a7569d80eee2e035e2bd7-Paper.pdf,Probabilistic Watershed: Sampling all spanning forests for seeded segmentation and semi-supervised learning,"Enrique Fita Sanmartin, Sebastian Damrich, Fred A. Hamprecht",
neurips,https://proceedings.neurips.cc/paper/2019/file/49b8b4f95f02e055801da3b4f58e28b7-Paper.pdf,Learning Robust Options by Conditional Value at Risk Optimization,"Takuya Hiraoka, Takahisa Imagawa, Tatsuya Mori, Takashi Onishi, Yoshimasa Tsuruoka",
neurips,https://proceedings.neurips.cc/paper/2019/file/4a1c2f4dcf2bf76b6b278ae40875d536-Paper.pdf,A Generic Acceleration Framework for Stochastic Composite Optimization,"Andrei Kulunchakov, Julien Mairal",
neurips,https://proceedings.neurips.cc/paper/2019/file/4a46fbfca3f1465a27b210f4bdfe6ab3-Paper.pdf,A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation,"Runzhe Yang, Xingyuan Sun, Karthik Narasimhan",
neurips,https://proceedings.neurips.cc/paper/2019/file/4aadd661908b181d059a117f02fbc9ec-Paper.pdf,Communication trade-offs for Local-SGD with large step size,"Aymeric Dieuleveut, Kumar Kshitij Patel","Synchronous mini-batch SGD is state-of-the-art for large-scale distributed machine learning. However, in practice, its convergence is bottlenecked by slow communication rounds between worker nodes. A natural solution to reduce communication is to use the \emph{
local-SGD''} model in which the workers train their model independently and synchronize every once in a while. This algorithm improves the computation-communication trade-off but its convergence is not understood very well. We propose a non-asymptotic error analysis, which enables comparison to \emph{one-shot averaging} i.e., a single communication round among independent workers, and \emph{mini-batch averaging} i.e., communicating at every step. We also provide adaptive lower bounds on the communication frequency for large step-sizes (
t
−
α
t
,
α
∈
(
1
/
2
,
1
)
α
) and show that \emph{Local-SGD} reduces communication by a factor of
O
(
√
T
P
3
/
2
)
O
, with
T
T
the total number of gradients and
P
P
machines."
neurips,https://proceedings.neurips.cc/paper/2019/file/4ab50afd6dcc95fcba76d0fe04295632-Paper.pdf,Towards modular and programmable architecture search,"Renato Negrinho, Matthew Gormley, Geoffrey J. Gordon, Darshan Patil, Nghia Le, Daniel Ferreira",
neurips,https://proceedings.neurips.cc/paper/2019/file/4bbbe6cb5982b9110413c40f3cce680b-Paper.pdf,Large-scale optimal transport map estimation using projection pursuit,"Cheng Meng, Yuan Ke, Jingyi Zhang, Mengrui Zhang, Wenxuan Zhong, Ping Ma",
neurips,https://proceedings.neurips.cc/paper/2019/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf,Understanding Attention and Generalization in Graph Neural Networks,"Boris Knyazev, Graham W. Taylor, Mohamed Amer",
neurips,https://proceedings.neurips.cc/paper/2019/file/4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf,Superposition of many models into one,"Brian Cheung, Alexander Terekhov, Yubei Chen, Pulkit Agrawal, Bruno Olshausen",
neurips,https://proceedings.neurips.cc/paper/2019/file/4cb811134b9d39fc3104bd06ce75abad-Paper.pdf,A Prior of a Googol Gaussians: a Tensor Ring Induced Prior for Generative Models,"Maxim Kuznetsov, Daniil Polykovskiy, Dmitry P. Vetrov, Alex Zhebrak",
neurips,https://proceedings.neurips.cc/paper/2019/file/4d0b954f0bef437c29dfa73fafdf3fa5-Paper.pdf,Beating SGD Saturation with Tail-Averaging and Minibatching,"Nicole Muecke, Gergely Neu, Lorenzo Rosasco",
neurips,https://proceedings.neurips.cc/paper/2019/file/4d5b995358e7798bc7e9d9db83c612a5-Paper.pdf,Extending Stein's unbiased risk estimator to train deep denoisers with correlated pairs of noisy images,"Magauiya Zhussip, Shakarim Soltanayev, Se Young Chun",
neurips,https://proceedings.neurips.cc/paper/2019/file/4dc3ed26a29c9c3df3ec373524377a5b-Paper.pdf,Preference-Based Batch and Sequential Teaching: Towards a Unified View of Models,"Farnam Mansouri, Yuxin Chen, Ara Vartanian, Jerry Zhu, Adish Singla","Algorithmic machine teaching studies the interaction between a teacher and a learner where the teacher selects labeled examples aiming at teaching a target hypothesis. In a quest to lower teaching complexity and to achieve more natural teacher-learner interactions, several teaching models and complexity measures have been proposed for both the batch settings (e.g., worst-case, recursive, preference-based, and non-clashing models) as well as the sequential settings (e.g., local preference-based model). To better understand the connections between these different batch and sequential models, we develop a novel framework which captures the teaching process via preference functions
Σ
Σ
. In our framework, each function
σ
∈
Σ
σ
induces a teacher-learner pair with teaching complexity as
\TD
(
σ
)
\TD
. We show that the above-mentioned teaching models are equivalent to specific types/families of preference functions in our framework. This equivalence, in turn, allows us to study the differences between two important teaching models, namely
σ
σ
functions inducing the strongest batch (i.e., non-clashing) model and
σ
σ
functions inducing a weak sequential (i.e., local preference-based) model. Finally, we identify preference functions inducing a novel family of sequential models with teaching complexity linear in the VC dimension of the hypothesis class: this is in contrast to the best known complexity result for the batch models which is quadratic in the VC dimension."
neurips,https://proceedings.neurips.cc/paper/2019/file/4e38d30e656da5ae9d3a425109ce9e04-Paper.pdf,Value Function in Frequency Domain and the Characteristic Value Iteration Algorithm,Amir-massoud Farahmand,
neurips,https://proceedings.neurips.cc/paper/2019/file/4e87337f366f72daa424dae11df0538c-Paper.pdf,Communication-Efficient Distributed Learning via Lazily Aggregated Quantized Gradients,"Jun Sun, Tianyi Chen, Georgios Giannakis, Zaiyue Yang",
neurips,https://proceedings.neurips.cc/paper/2019/file/4ea06fbc83cdd0a06020c35d50e1e89a-Paper.pdf,Twin Auxilary Classifiers GAN,"Mingming Gong, Yanwu Xu, Chunyuan Li, Kun Zhang, Kayhan Batmanghelich",
neurips,https://proceedings.neurips.cc/paper/2019/file/4ecb679fd35dcfd0f0894c399590be1a-Paper.pdf,Online Prediction of Switching Graph Labelings with Cluster Specialists,"Mark Herbster, James Robinson",
neurips,https://proceedings.neurips.cc/paper/2019/file/4efc9e02abdab6b6166251918570a307-Paper.pdf,AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters,"XIA XIAO, Zigeng Wang, Sanguthevar Rajasekaran",
neurips,https://proceedings.neurips.cc/paper/2019/file/4eff0720836a198b6174eecf02cbfdbf-Paper.pdf,Understanding the Role of Momentum in Stochastic Gradient Methods,"Igor Gitman, Hunter Lang, Pengchuan Zhang, Lin Xiao",
neurips,https://proceedings.neurips.cc/paper/2019/file/4f284803bd0966cc24fa8683a34afc6e-Paper.pdf,DAC: The Double Actor-Critic Architecture for Learning Options,"Shangtong Zhang, Shimon Whiteson",
neurips,https://proceedings.neurips.cc/paper/2019/file/4f398cb9d6bc79ae567298335b51ba8a-Paper.pdf,Safe Exploration for Interactive Machine Learning,"Matteo Turchetta, Felix Berkenkamp, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2019/file/4fc28b7093b135c21c7183ac07e928a6-Paper.pdf,Depth-First Proof-Number Search with Heuristic Edge Cost and Application to Chemical Synthesis Planning,"Akihiro Kishimoto, Beat Buesser, Bei Chen, Adi Botea",
neurips,https://proceedings.neurips.cc/paper/2019/file/4fc848051e4459b8a6afeb210c3664ec-Paper.pdf,Learning from Label Proportions with Generative Adversarial Networks,"Jiabin Liu, Bo Wang, Zhiquan Qi, YingJie Tian, Yong Shi",
neurips,https://proceedings.neurips.cc/paper/2019/file/4fd5aadb85a00525415e3733cb96ed68-Paper.pdf,Sparse High-Dimensional Isotonic Regression,"David Gamarnik, Julia Gaudio",
neurips,https://proceedings.neurips.cc/paper/2019/file/4fdaa19b1f22a4d926fce9bfc7c61fa5-Paper.pdf,Neuropathic Pain Diagnosis Simulator for Causal Discovery Algorithm Evaluation,"Ruibo Tu, Kun Zhang, Bo Bertilson, Hedvig Kjellstrom, Cheng Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/4fe5149039b52765bde64beb9f674940-Paper.pdf,Budgeted Reinforcement Learning in Continuous State Space,"Nicolas Carrara, Edouard Leurent, Romain Laroche, Tanguy Urvoy, Odalric-Ambrym Maillard, Olivier Pietquin",
neurips,https://proceedings.neurips.cc/paper/2019/file/4ff3e350028d0cfcb92c3a87a57585b1-Paper.pdf,Parameter elimination in particle Gibbs sampling,"Anna Wigren, Riccardo Sven Risuleo, Lawrence Murray, Fredrik Lindsten",
neurips,https://proceedings.neurips.cc/paper/2019/file/4ffb0d2ba92f664c2281970110a2e071-Paper.pdf,Towards Optimal Off-Policy Evaluation for Reinforcement Learning with Marginalized Importance Sampling,"Tengyang Xie, Yifei Ma, Yu-Xiang Wang","Motivated by the many real-world applications of reinforcement learning (RL) that require safe-policy iterations, we consider the problem of off-policy evaluation (OPE) --- the problem of evaluating a new policy using the historical data obtained by different behavior policies --- under the model of nonstationary episodic Markov Decision Processes (MDP) with a long horizon and a large action space. Existing importance sampling (IS) methods often suffer from large variance that depends exponentially on the RL horizon
H
H
. To solve this problem, we consider a marginalized importance sampling (MIS) estimator that recursively estimates the state marginal distribution for the target policy at every step. MIS achieves a mean-squared error of
\frac{1}{n} \sum_{t=1}^H\mathbb{E}_{\mu}\left[\frac{d_t^\pi(s_t)^2}{d_t^\mu(s_t)^2} \Var_{\mu}\left[\frac{\pi_t(a_t|s_t)}{\mu_t(a_t|s_t)}\big( V_{t+1}^\pi(s_{t+1}) + r_t\big) \middle| s_t\right]\right] + \tilde{O}(n^{-1.5})
where
μ
μ
and
π
π
are the logging and target policies,
d
μ
t
(
s
t
)
d
and
d
π
t
(
s
t
)
d
are the marginal distribution of the state at
t
t
th step,
H
H
is the horizon,
n
n
is the sample size and
V
π
t
+
1
V
is the value function of the MDP under
π
π
. The result matches the Cramer-Rao lower bound in [Jiang and Li, 2016] up to a multiplicative factor of
H
H
. To the best of our knowledge, this is the first OPE estimation error bound with a polynomial dependence on
H
H
. Besides theory, we show empirical superiority of our method in time-varying, partially observable, and long-horizon RL environments."
neurips,https://proceedings.neurips.cc/paper/2019/file/502cc2c94be1a7c4ca7ef25b8b50bc04-Paper.pdf,Understanding Sparse JL for Feature Hashing,Meena Jagadeesan,
neurips,https://proceedings.neurips.cc/paper/2019/file/50982fb2f2cfa186d335310461dfa2be-Paper.pdf,Planning in entropy-regularized Markov decision processes and games,"Jean-Bastien Grill, Omar Darwiche Domingues, Pierre Menard, Remi Munos, Michal Valko","We propose SmoothCruiser, a new planning algorithm for estimating the value function in entropy-regularized Markov decision processes and two-player games, given a generative model of the SmoothCruiser. SmoothCruiser makes use of the smoothness of the Bellman operator promoted by the regularization to achieve problem-independent sample complexity of order
~
O
(
1
/
ϵ
4
)
O
for a desired accuracy
ϵ
ϵ
, whereas for non-regularized settings there are no known algorithms with guaranteed polynomial sample complexity in the worst case."
neurips,https://proceedings.neurips.cc/paper/2019/file/50a074e6a8da4662ae0a29edde722179-Paper.pdf,Dynamic Local Regret for Non-convex Online Forecasting,"Sergul Aydore, Tianhao Zhu, Dean P. Foster",
neurips,https://proceedings.neurips.cc/paper/2019/file/50c1f44e426560f3f2cdcb3e19e39903-Paper.pdf,NAOMI: Non-Autoregressive Multiresolution Sequence Imputation,"Yukai Liu, Rose Yu, Stephan Zheng, Eric Zhan, Yisong Yue",
neurips,https://proceedings.neurips.cc/paper/2019/file/50d2d2262762648589b1943078712aa6-Paper.pdf,"Write, Execute, Assess: Program Synthesis with a REPL","Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, Armando Solar-Lezama",
neurips,https://proceedings.neurips.cc/paper/2019/file/5103c3584b063c431bd1268e9b5e76fb-Paper.pdf,Conformalized Quantile Regression,"Yaniv Romano, Evan Patterson, Emmanuel Candes",
neurips,https://proceedings.neurips.cc/paper/2019/file/510f2318f324cf07fce24c3a4b89c771-Paper.pdf,Multiagent Evaluation under Incomplete Information,"Mark Rowland, Shayegan Omidshafiei, Karl Tuyls, Julien Perolat, Michal Valko, Georgios Piliouras, Remi Munos","This paper investigates the evaluation of learned multiagent strategies in the incomplete information setting, which plays a critical role in ranking and training of agents. Traditionally, researchers have relied on Elo ratings for this purpose, with recent works also using methods based on Nash equilibria. Unfortunately, Elo is unable to handle intransitive agent interactions, and other techniques are restricted to zero-sum, two-player settings or are limited by the fact that the Nash equilibrium is intractable to compute. Recently, a ranking method called
α
α
-Rank, relying on a new graph-based game-theoretic solution concept, was shown to tractably apply to general games. However, evaluations based on Elo or
α
α
-Rank typically assume noise-free game outcomes, despite the data often being collected from noisy simulations, making this assumption unrealistic in practice. This paper investigates multiagent evaluation in the incomplete information regime, involving general-sum many-player games with noisy outcomes. We derive sample complexity guarantees required to confidently rank agents in this setting. We propose adaptive algorithms for accurate ranking, provide correctness and sample complexity guarantees, then introduce a means of connecting uncertainties in noisy match outcomes to uncertainties in rankings. We evaluate the performance of these approaches in several domains, including Bernoulli games, a soccer meta-game, and Kuhn poker."
neurips,https://proceedings.neurips.cc/paper/2019/file/512c5cad6c37edb98ae91c8a76c3a291-Paper.pdf,SpiderBoost and Momentum: Faster Variance Reduction Algorithms,"Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, Vahid Tarokh",
neurips,https://proceedings.neurips.cc/paper/2019/file/512fc3c5227f637e41437c999a2d3169-Paper.pdf,Mixtape: Breaking the Softmax Bottleneck Efficiently,"Zhilin Yang, Thang Luong, Russ R. Salakhutdinov, Quoc V. Le",
neurips,https://proceedings.neurips.cc/paper/2019/file/51425b752a0b402ed3effc83fc4bbb74-Paper.pdf,High-Dimensional Optimization in Adaptive Random Subspaces,"Jonathan Lacotte, Mert Pilanci, Marco Pavone",
neurips,https://proceedings.neurips.cc/paper/2019/file/516b38afeee70474b04881a633728b15-Paper.pdf,Flexible information routing in neural populations through stochastic comodulation,"Caroline Haimerl, Cristina Savin, Eero Simoncelli",
neurips,https://proceedings.neurips.cc/paper/2019/file/517f24c02e620d5a4dac1db388664a63-Paper.pdf,MarginGAN: Adversarial Training in Semi-Supervised Learning,"Jinhao Dong, Tong Lin",
neurips,https://proceedings.neurips.cc/paper/2019/file/51c68dc084cb0b8467eafad1330bce66-Paper.pdf,Cold Case: The Lost MNIST Digits,"Chhavi Yadav, Leon Bottou",
neurips,https://proceedings.neurips.cc/paper/2019/file/51d92be1c60d1db1d2e5e7a07da55b26-Paper.pdf,RUBi: Reducing Unimodal Biases for Visual Question Answering,"Remi Cadene, Corentin Dancette, Hedi Ben younes, Matthieu Cord, Devi Parikh","We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training."
neurips,https://proceedings.neurips.cc/paper/2019/file/52130c418d4f02c74f74a5bc1f8020b2-Paper.pdf,Text-Based Interactive Recommendation via Constraint-Augmented Reinforcement Learning,"Ruiyi Zhang, Tong Yu, Yilin Shen, Hongxia Jin, Changyou Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/525b8410cc8612283c9ecaf9a319f8ed-Paper.pdf,Learning to Correlate in Multi-Player General-Sum Sequential Games,"Andrea Celli, Alberto Marchesi, Tommaso Bianchi, Nicola Gatti",
neurips,https://proceedings.neurips.cc/paper/2019/file/52d2752b150f9c35ccb6869cbf074e48-Paper.pdf,Learning Sample-Specific Models with Low-Rank Personalized Regression,"Ben Lengerich, Bryon Aragam, Eric P. Xing",
neurips,https://proceedings.neurips.cc/paper/2019/file/532435c44bec236b471a47a88d63513d-Paper.pdf,Learning Reward Machines for Partially Observable Reinforcement Learning,"Rodrigo Toro Icarte, Ethan Waldie, Toryn Klassen, Rick Valenzano, Margarita Castro, Sheila McIlraith",
neurips,https://proceedings.neurips.cc/paper/2019/file/532b7cbe070a3579f424988a040752f2-Paper.pdf,Addressing Sample Complexity in Visual Tasks Using HER and Hallucinatory GANs,"Himanshu Sahni, Toby Buckley, Pieter Abbeel, Ilya Kuzovkin",
neurips,https://proceedings.neurips.cc/paper/2019/file/5352696a9ca3397beb79f116f3a33991-Paper.pdf,Bat-G net: Bat-inspired High-Resolution 3D Image Reconstruction using Ultrasonic Echoes,"Gunpil Hwang, Seohyeon Kim, Hyeon-Min Bae",
neurips,https://proceedings.neurips.cc/paper/2019/file/538a50fb36b97123adc3627cbdb223bf-Paper.pdf,"Procrastinating with Confidence: Near-Optimal, Anytime, Adaptive Algorithm Configuration","Robert Kleinberg, Kevin Leyton-Brown, Brendan Lucier, Devon Graham",
neurips,https://proceedings.neurips.cc/paper/2019/file/53c6de78244e9f528eb3e1cda69699bb-Paper.pdf,Unsupervised Scalable Representation Learning for Multivariate Time Series,"Jean-Yves Franceschi, Aymeric Dieuleveut, Martin Jaggi",
neurips,https://proceedings.neurips.cc/paper/2019/file/53fde96fcc4b4ce72d7739202324cd49-Paper.pdf,Correlated Uncertainty for Learning Dense Correspondences from Noisy Labels,"Natalia Neverova, David Novotny, Andrea Vedaldi",
neurips,https://proceedings.neurips.cc/paper/2019/file/540ae6b0f6ac6e155062f3dd4f0b2b01-Paper.pdf,Total Least Squares Regression in Input Sparsity Time,"Huaian Diao, Zhao Song, David Woodruff, Xin Yang","In the total least squares problem, one is given an
m
×
n
m
matrix
A
A
, and an
m
×
d
m
matrix
B
B
, and one seeks to
correct'' both
A
A
and
B
B
, obtaining matrices
^
A
A
and
^
B
B
, so that there exists an
X
X
satisfying the equation
^
A
X
=
^
B
A
. Typically the problem is overconstrained, meaning that
m
≫
max
(
n
,
d
)
m
. The cost of the solution
^
A
,
^
B
A
is given by
∥
A
−
^
A
∥
2
F
+
∥
B
−
^
B
∥
2
F
‖
. We give an algorithm for finding a solution
X
X
to the linear system
^
A
X
=
^
B
A
for which the cost
∥
A
−
^
A
∥
2
F
+
∥
B
−
^
B
∥
2
F
‖
is at most a multiplicative
(
1
+
ϵ
)
(
factor times the optimal cost, up to an additive error
η
η
that may be an arbitrarily small function of
n
n
. Importantly, our running time is
~
O
(
\nnz
(
A
)
+
\nnz
(
B
)
)
+
\poly
(
n
/
ϵ
)
⋅
d
O
, where for a matrix
C
C
,
\nnz
(
C
)
\nnz
denotes its number of non-zero entries. Importantly, our running time does not directly depend on the large parameter
m
m
. As total least squares regression is known to be solvable via low rank approximation, a natural approach is to invoke fast algorithms for approximate low rank approximation, obtaining matrices
^
A
A
and
^
B
B
from this low rank approximation, and then solving for
X
X
so that
^
A
X
=
^
B
A
. However, existing algorithms do not apply since in total least squares the rank of the low rank approximation needs to be
n
n
, and so the running time of known methods would be at least
m
n
2
m
. In contrast, we are able to achieve a much faster running time for finding
X
X
by never explicitly forming the equation
^
A
X
=
^
B
A
, but instead solving for an
X
X
which is a solution to an implicit such equation. Finally, we generalize our algorithm to the total least squares problem with regularization."
neurips,https://proceedings.neurips.cc/paper/2019/file/5421e013565f7f1afa0cfe8ad87a99ab-Paper.pdf,Bayesian Learning of Sum-Product Networks,"Martin Trapp, Robert Peharz, Hong Ge, Franz Pernkopf, Zoubin Ghahramani",
neurips,https://proceedings.neurips.cc/paper/2019/file/54229abfcfa5649e7003b83dd4755294-Paper.pdf,DeepUSPS: Deep Robust Unsupervised Saliency Prediction via Self-supervision,"Tam Nguyen, Maximilian Dax, Chaithanya Kumar Mummadi, Nhung Ngo, Thi Hoai Phuong Nguyen, Zhongyu Lou, Thomas Brox",
neurips,https://proceedings.neurips.cc/paper/2019/file/5446f217e9504bc593ad9dcf2ec88dda-Paper.pdf,Policy Optimization Provably Converges to Nash Equilibria in Zero-Sum Linear Quadratic Games,"Kaiqing Zhang, Zhuoran Yang, Tamer Basar",
neurips,https://proceedings.neurips.cc/paper/2019/file/5481b2f34a74e427a2818014b8e103b0-Paper.pdf,On the Power and Limitations of Random Features for Understanding Neural Networks,"Gilad Yehudai, Ohad Shamir","Recently, a spate of papers have provided positive theoretical results for training over-parameterized neural networks (where the network size is larger than what is needed to achieve low error). The key insight is that with sufficient over-parameterization, gradient-based methods will implicitly leave some components of the network relatively unchanged, so the optimization dynamics will behave as if those components are essentially fixed at their initial random values. In fact, fixing these \emph{explicitly} leads to the well-known approach of learning with random features (e.g. \citep{rahimi2008random,rahimi2009weighted}). In other words, these techniques imply that we can successfully learn with neural networks, whenever we can successfully learn with random features. In this paper, we formalize the link between existing results and random features, and argue that despite the impressive positive results, random feature approaches are also inherently limited in what they can explain. In particular, we prove that random features cannot be used to learn \emph{even a single ReLU neuron} (over standard Gaussian inputs in
\reals
d
\reals
and
poly
(
d
)
poly
weights), unless the network size (or magnitude of its weights) is exponentially large in
d
d
. Since a single neuron \emph{is} known to be learnable with gradient-based methods, we conclude that we are still far from a satisfying general explanation for the empirical success of neural networks. For completeness we also provide a simple self-contained proof, using a random features technique, that one-hidden-layer neural networks can learn low-degree polynomials."
neurips,https://proceedings.neurips.cc/paper/2019/file/54e36c5ff5f6a1802925ca009f3ebb68-Paper.pdf,Real-Time Reinforcement Learning,"Simon Ramstedt, Chris Pal",
neurips,https://proceedings.neurips.cc/paper/2019/file/54ebdfbbfe6c31c39aaba9a1ee83860a-Paper.pdf,Discriminative Topic Modeling with Logistic LDA,"Iryna Korshunova, Hanchen Xiong, Mateusz Fedoryszak, Lucas Theis",
neurips,https://proceedings.neurips.cc/paper/2019/file/54ee290e80589a2a1225c338a71839f5-Paper.pdf,Streaming Bayesian Inference for Crowdsourced Classification,"Edoardo Manino, Long Tran-Thanh, Nicholas Jennings",
neurips,https://proceedings.neurips.cc/paper/2019/file/55a988dfb00a914717b3000a3374694c-Paper.pdf,Disentangling Influence: Using disentangled representations to audit model predictions,"Charles Marx, Richard Phillips, Sorelle Friedler, Carlos Scheidegger, Suresh Venkatasubramanian",
neurips,https://proceedings.neurips.cc/paper/2019/file/56352739f59643540a3a6e16985f62c7-Paper.pdf,Deep Structured Prediction for Facial Landmark Detection,"Lisha Chen, Hui Su, Qiang Ji",
neurips,https://proceedings.neurips.cc/paper/2019/file/564645fbd0332f066cbd9d083ddd077c-Paper.pdf,Mutually Regressive Point Processes,"Ifigeneia Apostolopoulou, Scott Linderman, Kyle Miller, Artur Dubrawski",
neurips,https://proceedings.neurips.cc/paper/2019/file/567b8f5f423af15818a068235807edc0-Paper.pdf,Demystifying Black-box Models with Symbolic Metamodels,"Ahmed M. Alaa, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2019/file/56a3107cad6611c8337ee36d178ca129-Paper.pdf,SHE: A Fast and Accurate Deep Neural Network for Encrypted Data,"Qian Lou, Lei Jiang","In this paper, we propose a Shift-accumulation-based LHE-enabled deep neural network (SHE) for fast and accurate inferences on encrypted data. We use the binary-operation-friendly leveled-TFHE (LTFHE) encryption scheme to implement ReLU activations and max poolings. We also adopt the logarithmic quantization to accelerate inferences by replacing expensive LTFHE multiplications with cheap LTFHE shifts. We propose a mixed bitwidth accumulator to expedite accumulations. Since the LTFHE ReLU activations, max poolings, shifts and accumulations have small multiplicative depth, SHE can implement much deeper network architectures with more convolutional and activation layers. Our experimental results show SHE achieves the state-of-the-art inference accuracy and reduces the inference latency by 76.21% ~ 94.23% over prior LHECNNs on MNIST and CIFAR-10."
neurips,https://proceedings.neurips.cc/paper/2019/file/56bd37d3a2fda0f2f41925019c81011d-Paper.pdf,Non-Cooperative Inverse Reinforcement Learning,"Xiangyuan Zhang, Kaiqing Zhang, Erik Miehling, Tamer Basar",
neurips,https://proceedings.neurips.cc/paper/2019/file/56c51a39a7c77d8084838cc920585bd0-Paper.pdf,Competitive Gradient Descent,"Florian Schaefer, Anima Anandkumar",
neurips,https://proceedings.neurips.cc/paper/2019/file/56cb94cb34617aeadff1e79b53f38354-Paper.pdf,Learning in Generalized Linear Contextual Bandits with Stochastic Delays,"Zhengyuan Zhou, Renyuan Xu, Jose Blanchet",
neurips,https://proceedings.neurips.cc/paper/2019/file/56f9f88906aebf4ad985aaec7fa01313-Paper.pdf,Arbicon-Net: Arbitrary Continuous Geometric Transformation Networks for Image Registration,"Jianchun Chen, Lingjing Wang, Xiang Li, Yi Fang",
neurips,https://proceedings.neurips.cc/paper/2019/file/571d3a9420bfd9219f65b643d0003bf4-Paper.pdf,On the Calibration of Multiclass Classification with Rejection,"Chenri Ni, Nontawat Charoenphakdee, Junya Honda, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2019/file/5737034557ef5b8c02c0e46513b98f90-Paper.pdf,Point-Voxel CNN for Efficient 3D Deep Learning,"Zhijian Liu, Haotian Tang, Yujun Lin, Song Han",
neurips,https://proceedings.neurips.cc/paper/2019/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Paper.pdf,Importance Weighted Hierarchical Variational Inference,"Artem Sobolev, Dmitry P. Vetrov",
neurips,https://proceedings.neurips.cc/paper/2019/file/573f7f25b7b1eb79a4ec6ba896debefd-Paper.pdf,Fast Convergence of Belief Propagation to Global Optima: Beyond Correlation Decay,Frederic Koehler,
neurips,https://proceedings.neurips.cc/paper/2019/file/576d026223582a390cd323bef4bad026-Paper.pdf,ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization,"Xiangyi Chen, Sijia Liu, Kaidi Xu, Xingguo Li, Xue Lin, Mingyi Hong, David Cox","The adaptive momentum method (AdaMM), which uses past gradients to update descent directions and learning rates simultaneously, has become one of the most popular first-order optimization methods for solving machine learning problems. However, AdaMM is not suited for solving black-box optimization problems, where explicit gradient forms are difficult or infeasible to obtain. In this paper, we propose a zeroth-order AdaMM (ZO-AdaMM) algorithm, that generalizes AdaMM to the gradient-free regime. We show that the convergence rate of ZO-AdaMM for both convex and nonconvex optimization is roughly a factor of
O
(
√
d
)
O
worse than that of the first-order AdaMM algorithm, where
d
d
is problem size. In particular, we provide a deep understanding on why Mahalanobis distance matters in convergence of ZO-AdaMM and other AdaMM-type methods. As a byproduct, our analysis makes the first step toward understanding adaptive learning rate methods for nonconvex constrained optimization.Furthermore, we demonstrate two applications, designing per-image and universal adversarial attacks from black-box neural networks, respectively. We perform extensive experiments on ImageNet and empirically show that ZO-AdaMM converges much faster to a solution of high accuracy compared with
6
6
state-of-the-art ZO optimization methods."
neurips,https://proceedings.neurips.cc/paper/2019/file/57bafb2c2dfeefba931bb03a835b1fa9-Paper.pdf,U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging,"Mathias Perslev, Michael Jensen, Sune Darkner, Poul Jørgen Jennum, Christian Igel",
neurips,https://proceedings.neurips.cc/paper/2019/file/57c0531e13f40b91b3b0f1a30b529a1d-Paper.pdf,Meta-Curvature,"Eunbyung Park, Junier B. Oliva",
neurips,https://proceedings.neurips.cc/paper/2019/file/57db7d68d5335b52d5153a4e01adaa6b-Paper.pdf,Exploration via Hindsight Goal Generation,"Zhizhou Ren, Kefan Dong, Yuan Zhou, Qiang Liu, Jian Peng",
neurips,https://proceedings.neurips.cc/paper/2019/file/582967e09f1b30ca2539968da0a174fa-Paper.pdf,VIREL: A Variational Inference Framework for Reinforcement Learning,"Matthew Fellows, Anuj Mahajan, Tim G. J. Rudner, Shimon Whiteson",
neurips,https://proceedings.neurips.cc/paper/2019/file/5857d68cd9280bc98d079fa912fd6740-Paper.pdf,"What Can ResNet Learn Efficiently, Going Beyond Kernels?","Zeyuan Allen-Zhu, Yuanzhi Li","How can neural networks such as ResNet \emph{efficiently} learn CIFAR-10 with test accuracy more than
96
%
96
, while other methods, especially kernel methods, fall relatively behind? Can we more provide theoretical justifications for this gap? Recently, there is an influential line of work relating neural networks to kernels in the over-parameterized regime, proving they can learn certain concept class that is also learnable by kernels with similar test error. Yet, can neural networks provably learn some concept class \emph{better} than kernels? We answer this positively in the distribution-free setting. We prove neural networks can efficiently learn a notable class of functions, including those defined by three-layer residual networks with smooth activations, without any distributional assumption. At the same time, we prove there are simple functions in this class such that with the same number of training examples, the test error obtained by neural networks can be \emph{much smaller} than \emph{any} kernel method, including neural tangent kernels (NTK). The main intuition is that \emph{multi-layer} neural networks can implicitly perform hierarchal learning using different layers, which reduces the sample complexity comparing to
one-shot'' learning algorithms such as kernel methods. In the end, we also prove a computation complexity advantage of ResNet with respect to other learning methods including linear regression over arbitrary feature mappings."
neurips,https://proceedings.neurips.cc/paper/2019/file/587524833eaf98eb779a387e33768c6a-Paper.pdf,Trajectory of Alternating Direction Method of Multipliers and Adaptive Acceleration,"Clarice Poon, Jingwei Liang",
neurips,https://proceedings.neurips.cc/paper/2019/file/58a2fc6ed39fd083f55d4182bf88826d-Paper.pdf,Reducing Noise in GAN Training with Variance Reduced Extragradient,"Tatjana Chavdarova, Gauthier Gidel, François Fleuret, Simon Lacoste-Julien",
neurips,https://proceedings.neurips.cc/paper/2019/file/58aaee7ae94b52697ad3b9275d46ec7f-Paper.pdf,Focused Quantization for Sparse CNNs,"Yiren Zhao, Xitong Gao, Daniel Bates, Robert Mullins, Cheng-Zhong Xu",
neurips,https://proceedings.neurips.cc/paper/2019/file/58d2d622ed4026cae2e56dffc5818a11-Paper.pdf,Submodular Function Minimization with Noisy Evaluation Oracle,Shinji Ito,"This paper considers submodular function minimization with \textit{noisy evaluation oracles} that return the function value of a submodular objective with zero-mean additive noise. For this problem, we provide an algorithm that returns an
O
(
n
3
/
2
/
√
T
)
O
-additive approximate solution in expectation, where
n
n
and
T
T
stand for the size of the problem and the number of oracle calls, respectively. There is no room for reducing this error bound by a factor smaller than
O
(
1
/
√
n
)
O
. Indeed, we show that any algorithm will suffer additive errors of
Ω
(
n
/
√
T
)
Ω
in the worst case. Further, we consider an extended problem setting with \textit{multiple-point feedback} in which we can get the feedback of
k
k
function values with each oracle call. Under the additional assumption that each noisy oracle is submodular and that
2
≤
k
=
O
(
1
)
2
, we provide an algorithm with an
O
(
n
/
√
T
)
O
-additive error bound as well as a worst-case analysis including a lower bound of
Ω
(
n
/
√
T
)
Ω
, which together imply that the algorithm achieves an optimal error bound up to a constant."
neurips,https://proceedings.neurips.cc/paper/2019/file/596f713f9a7376fe90a62abaaedecc2d-Paper.pdf,Knowledge Extraction with No Observable Data,"Jaemin Yoo, Minyong Cho, Taebum Kim, U Kang",
neurips,https://proceedings.neurips.cc/paper/2019/file/598a90004bace6540f0e2230bdc47c09-Paper.pdf,Global Guarantees for Blind Demodulation with Generative Priors,"Paul Hand, Babhru Joshi","We study a deep learning inspired formulation for the blind demodulation problem, which is the task of recovering two unknown vectors from their entrywise multiplication. We consider the case where the unknown vectors are in the range of known deep generative models,
G
(
1
)
:
R
n
→
R
ℓ
G
and
G
(
2
)
:
R
p
→
R
ℓ
G
. In the case when the networks corresponding to the generative models are expansive, the weight matrices are random and the dimension of the unknown vectors satisfy
ℓ
=
Ω
(
n
2
+
p
2
)
ℓ
, up to log factors, we show that the empirical risk objective has a favorable landscape for optimization. That is, the objective function has a descent direction at every point outside of a small neighborhood around four hyperbolic curves. We also characterize the local maximizers of the empirical risk objective and, hence, show that there does not exist any other stationary points outside of these neighborhood around four hyperbolic curves and the set of local maximizers. We also implement a gradient descent scheme inspired by the geometry of the landscape of the objective function. In order to converge to a global minimizer, this gradient descent scheme exploits the fact that exactly one of the hyperbolic curve corresponds to the global minimizer, and thus points near this hyperbolic curve have a lower objective value than points close to the other spurious hyperbolic curves. We show that this gradient descent scheme can effectively remove distortions synthetically introduced to the MNIST dataset."
neurips,https://proceedings.neurips.cc/paper/2019/file/59b1deff341edb0b76ace57820cef237-Paper.pdf,Neural Jump Stochastic Differential Equations,"Junteng Jia, Austin R. Benson",
neurips,https://proceedings.neurips.cc/paper/2019/file/59bcda7c438bad7d2afffe9e2fed00be-Paper.pdf,"Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning","Nathan Kallus, Masatoshi Uehara",
neurips,https://proceedings.neurips.cc/paper/2019/file/5a0c828364dbf6dd406139dab7b25398-Paper.pdf,Learning about an exponential amount of conditional distributions,"Mohamed Belghazi, Maxime Oquab, David Lopez-Paz",
neurips,https://proceedings.neurips.cc/paper/2019/file/5a142a55461d5fef016acfb927fee0bd-Paper.pdf,Multi-mapping Image-to-Image Translation via Learning Disentanglement,"Xiaoming Yu, Yuanqi Chen, Shan Liu, Thomas Li, Ge Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/5a2afca61e35f45a7dd44ca46e0225f4-Paper.pdf,Computational Mirrors: Blind Inverse Light Transport by Deep Matrix Factorization,"Miika Aittala, Prafull Sharma, Lukas Murmann, Adam Yedidia, Gregory Wornell, Bill Freeman, Fredo Durand",
neurips,https://proceedings.neurips.cc/paper/2019/file/5a38a1eb24d99699159da10e71c45577-Paper.pdf,Explicitly disentangling image content from translation and rotation with spatial-VAE,"Tristan Bepler, Ellen Zhong, Kotaro Kelley, Edward Brignole, Bonnie Berger",
neurips,https://proceedings.neurips.cc/paper/2019/file/5a44a53b7d26bb1e54c05222f186dcfb-Paper.pdf,Imitation-Projected Programmatic Reinforcement Learning,"Abhinav Verma, Hoang Le, Yisong Yue, Swarat Chaudhuri",
neurips,https://proceedings.neurips.cc/paper/2019/file/5ac8bb8a7d745102a978c5f8ccdb61b8-Paper.pdf,The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies,"Basri Ronen, David Jacobs, Yoni Kasten, Shira Kritchman",
neurips,https://proceedings.neurips.cc/paper/2019/file/5acdc9ca5d99ae66afdfe1eea0e3b26b-Paper.pdf,Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem,"Gonzalo Mena, Jonathan Niles-Weed",
neurips,https://proceedings.neurips.cc/paper/2019/file/5ad742cd15633b26fdce1b80f7b39f7c-Paper.pdf,A Game Theoretic Approach to Class-wise Selective Rationalization,"Shiyu Chang, Yang Zhang, Mo Yu, Tommi Jaakkola",
neurips,https://proceedings.neurips.cc/paper/2019/file/5b168fdba5ee5ea262cc2d4c0b457697-Paper.pdf,Scalable Bayesian dynamic covariance modeling with variational Wishart and inverse Wishart processes,"Creighton Heaukulani, Mark van der Wilk",
neurips,https://proceedings.neurips.cc/paper/2019/file/5b4a2146246bc3a3a941f32225bbb792-Paper.pdf,Variational Bayesian Decision-making for Continuous Utilities,"Tomasz Kuśmierczyk, Joseph Sakaya, Arto Klami",
neurips,https://proceedings.neurips.cc/paper/2019/file/5b970a1d9be0fd100063fd6cd688b73e-Paper.pdf,Optimal Sparsity-Sensitive Bounds for Distributed Mean Estimation,"zengfeng Huang, Ziyue Huang, Yilei WANG, Ke Yi",
neurips,https://proceedings.neurips.cc/paper/2019/file/5c48ff18e0a47baaf81d8b8ea51eec92-Paper.pdf,Search on the Replay Buffer: Bridging Planning and Reinforcement Learning,"Ben Eysenbach, Russ R. Salakhutdinov, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2019/file/5c8cb735a1ce65dac514233cbd5576d6-Paper.pdf,Minimal Variance Sampling in Stochastic Gradient Boosting,"Bulat Ibragimov, Gleb Gusev",
neurips,https://proceedings.neurips.cc/paper/2019/file/5ca359ab1e9e3b9c478459944a2d9ca5-Paper.pdf,Transductive Zero-Shot Learning with Visual Structure Constraint,"Ziyu Wan, Dongdong Chen, Yan Li, Xingguang Yan, Junge Zhang, Yizhou Yu, Jing Liao",
neurips,https://proceedings.neurips.cc/paper/2019/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf,Large Scale Markov Decision Processes with Changing Rewards,"Adrian Rivera Cardoso, He Wang, Huan Xu","We consider Markov Decision Processes (MDPs) where the rewards are unknown and may change in an adversarial manner. We provide an algorithm that achieves a regret bound of
O
(
√
τ
(
ln
|
S
|
+
ln
|
A
|
)
T
ln
(
T
)
)
O
, where
S
S
is the state space,
A
A
is the action space,
τ
τ
is the mixing time of the MDP, and
T
T
is the number of periods. The algorithm's computational complexity is polynomial in
|
S
|
|
and
|
A
|
|
. We then consider a setting often encountered in practice, where the state space of the MDP is too large to allow for exact solutions. By approximating the state-action occupancy measures with a linear architecture of dimension
d
≪
|
S
|
d
, we propose a modified algorithm with a computational complexity polynomial in
d
d
and independent of
|
S
|
|
. We also prove a regret bound for this modified algorithm, which to the best of our knowledge, is the first
~
O
(
√
T
)
O
regret bound in the large-scale MDP setting with adversarially changing rewards."
neurips,https://proceedings.neurips.cc/paper/2019/file/5cde6dedeb8892e3794f22db57ada073-Paper.pdf,A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised Learning,"Xuanqing Liu, Si Si, Jerry Zhu, Yang Li, Cho-Jui Hsieh","In this paper, we proposed a general framework for data poisoning attacks to graph-based semi-supervised learning (G-SSL). In this framework, we first unify different tasks, goals and constraints into a single formula for data poisoning attack in G-SSL, then we propose two specialized algorithms to efficiently solve two important cases --- poisoning regression tasks under
ℓ
2
ℓ
-norm constraint and classification tasks under
ℓ
0
ℓ
-norm constraint. In the former case, we transform it into a non-convex trust region problem and show that our gradient-based algorithm with delicate initialization and update scheme finds the (globally) optimal perturbation. For the latter case, although it is an NP-hard integer programming problem, we propose a probabilistic solver that works much better than the classical greedy method. Lastly, we test our framework on real datasets and evaluate the robustness of G-SSL algorithms. For instance, on the MNIST binary classification problem (50000 training data with 50 labeled), flipping two labeled data is enough to make the model perform like random guess (around 50\% error)."
neurips,https://proceedings.neurips.cc/paper/2019/file/5cf21ce30208cfffaa832c6e44bb567d-Paper.pdf,Implicit Regularization for Optimal Sparse Recovery,"Tomas Vaskevicius, Varun Kanade, Patrick Rebeschini","We investigate implicit regularization schemes for gradient descent methods applied to unpenalized least squares regression to solve the problem of reconstructing a sparse signal from an underdetermined system of linear measurements under the restricted isometry assumption. For a given parametrization yielding a non-convex optimization problem, we show that prescribed choices of initialization, step size and stopping time yield a statistically and computationally optimal algorithm that achieves the minimax rate with the same cost required to read the data up to poly-logarithmic factors. Beyond minimax optimality, we show that our algorithm adapts to instance difficulty and yields a dimension-independent rate when the signal-to-noise ratio is high enough. Key to the computational efficiency of our method is an increasing step size scheme that adapts to refined estimates of the true solution. We validate our findings with numerical experiments and compare our algorithm against explicit
ℓ
1
ℓ
penalization. Going from hard instances to easy ones, our algorithm is seen to undergo a phase transition, eventually matching least squares with an oracle knowledge of the true support."
neurips,https://proceedings.neurips.cc/paper/2019/file/5d0d5594d24f0f955548f0fc0ff83d10-Paper.pdf,Residual Flows for Invertible Generative Modeling,"Ricky T. Q. Chen, Jens Behrmann, David K. Duvenaud, Joern-Henrik Jacobsen",
neurips,https://proceedings.neurips.cc/paper/2019/file/5d2c2cee8ab0b9a36bd1ed7196bd6c4a-Paper.pdf,Copula Multi-label Learning,Weiwei Liu,
neurips,https://proceedings.neurips.cc/paper/2019/file/5d4ae76f053f8f2516ad12961ef7fe97-Paper.pdf,Adversarial Training and Robustness for Multiple Perturbations,"Florian Tramer, Dan Boneh","Defenses against adversarial examples, such as adversarial training, are typically tailored to a single perturbation type (e.g., small
ℓ
∞
ℓ
-noise). For other perturbations, these defenses offer no guarantees and, at times, even increase the model's vulnerability. Our aim is to understand the reasons underlying this robustness trade-off, and to train models that are simultaneously robust to multiple perturbation types. We prove that a trade-off in robustness to different types of
ℓ
p
ℓ
-bounded and spatial perturbations must exist in a natural and simple statistical setting. We corroborate our formal analysis by demonstrating similar robustness trade-offs on MNIST and CIFAR10. We propose new multi-perturbation adversarial training schemes, as well as an efficient attack for the
ℓ
1
ℓ
-norm, and use these to show that models trained against multiple attacks fail to achieve robustness competitive with that of models trained on each attack individually. In particular, we find that adversarial training with first-order
ℓ
∞
,
ℓ
1
ℓ
and
ℓ
2
ℓ
attacks on MNIST achieves merely
50
%
50
robust accuracy, partly because of gradient-masking. Finally, we propose affine attacks that linearly interpolate between perturbation types and further degrade the accuracy of adversarially trained models."
neurips,https://proceedings.neurips.cc/paper/2019/file/5dbc8390f17e019d300d5a162c3ce3bc-Paper.pdf,Certainty Equivalence is Efficient for Linear Quadratic Control,"Horia Mania, Stephen Tu, Benjamin Recht",
neurips,https://proceedings.neurips.cc/paper/2019/file/5dcd0ddd3d918c70d380d32bce4e733a-Paper.pdf,Stein Variational Gradient Descent With Matrix-Valued Kernels,"Dilin Wang, Ziyang Tang, Chandrajit Bajaj, Qiang Liu",
neurips,https://proceedings.neurips.cc/paper/2019/file/5dec707028b05bcbd3a1db5640f842c5-Paper.pdf,Differentially Private Bagging: Improved utility and cheaper privacy than subsample-and-aggregate,"James Jordon, Jinsung Yoon, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2019/file/5df0385cba256a135be596dbe28fa7aa-Paper.pdf,Abstraction based Output Range Analysis for Neural Networks,"Pavithra Prabhakar, Zahra Rahimi Afzal",
neurips,https://proceedings.neurips.cc/paper/2019/file/5e2b66750529d8ae895ad2591118466f-Paper.pdf,Paraphrase Generation with Latent Bag of Words,"Yao Fu, Yansong Feng, John P. Cunningham",
neurips,https://proceedings.neurips.cc/paper/2019/file/5e388103a391daabe3de1d76a6739ccd-Paper.pdf,Combinatorial Bandits with Relative Feedback,"Aadirupa Saha, Aditya Gopalan","We consider combinatorial online learning with subset choices when only relative feedback information from subsets is available, instead of bandit or semi-bandit feedback which is absolute. Specifically, we study two regret minimisation problems over subsets of a finite ground set
[
n
]
[
, with subset-wise relative preference information feedback according to the Multinomial logit choice model. In the first setting, the learner can play subsets of size bounded by a maximum size and receives top-
m
m
rank-ordered feedback, while in the second setting the learner can play subsets of a fixed size
k
k
with a full subset ranking observed as feedback. For both settings, we devise instance-dependent and order-optimal regret algorithms with regret
O
(
n
m
ln
T
)
O
and
O
(
n
k
ln
T
)
O
, respectively. We derive fundamental limits on the regret performance of online learning with subset-wise preferences, proving the tightness of our regret guarantees. Our results also show the value of eliciting more general top-
m
m
rank-ordered feedback over single winner feedback (
m
=
1
m
). Our theoretical results are corroborated with empirical evaluations."
neurips,https://proceedings.neurips.cc/paper/2019/file/5e69fda38cda2060819766569fd93aa5-Paper.pdf,Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes,Greg Yang,
neurips,https://proceedings.neurips.cc/paper/2019/file/5ea1649a31336092c05438df996a3e59-Paper.pdf,An Accelerated Decentralized Stochastic Proximal Algorithm for Finite Sums,"Hadrien Hendrikx, Francis Bach, Laurent Massoulié","Modern large-scale finite-sum optimization relies on two key aspects: distribution and stochastic updates. For smooth and strongly convex problems, existing decentralized algorithms are slower than modern accelerated variance-reduced stochastic algorithms when run on a single machine, and are therefore not efficient. Centralized algorithms are fast, but their scaling is limited by global aggregation steps that result in communication bottlenecks. In this work, we propose an efficient \textbf{A}ccelerated \textbf{D}ecentralized stochastic algorithm for \textbf{F}inite \textbf{S}ums named ADFS, which uses local stochastic proximal updates and randomized pairwise communications between nodes. On
n
n
machines, ADFS learns from
n
m
n
samples in the same time it takes optimal algorithms to learn from
m
m
samples on one machine. This scaling holds until a critical network size is reached, which depends on communication delays, on the number of samples
m
m
, and on the network topology. We provide a theoretical analysis based on a novel augmented graph approach combined with a precise evaluation of synchronization times and an extension of the accelerated proximal coordinate gradient algorithm to arbitrary sampling. We illustrate the improvement of ADFS over state-of-the-art decentralized approaches with experiments."
neurips,https://proceedings.neurips.cc/paper/2019/file/5ee5605917626676f6a285fa4c10f7b0-Paper.pdf,Sample Efficient Active Learning of Causal Trees,"Kristjan Greenewald, Dmitriy Katz, Karthikeyan Shanmugam, Sara Magliacane, Murat Kocaoglu, Enric Boix Adsera, Guy Bresler","We consider the problem of experimental design for learning causal graphs that have a tree structure. We propose an adaptive framework that determines the next intervention based on a Bayesian prior updated with the outcomes of previous experiments, focusing on the setting where observational data is cheap (assumed infinite) and interventional data is expensive. While information greedy approaches are popular in active learning, we show that in this setting they can be exponentially suboptimal (in the number of interventions required), and instead propose an algorithm that exploits graph structure in the form of a centrality measure. If infinite interventional data is available, we show that the algorithm requires a number of interventions less than or equal to a factor of 2 times the minimum achievable number. We show that the algorithm and the associated theory can be adapted to the setting where each performed intervention yields finitely many samples. Several extensions are also presented, to the case where a specified set of nodes cannot be intervened on, to the case where
K
K
interventions are scheduled at once, and to the fully adaptive case where each experiment yields only one sample. In the case of finite interventional data, through simulated experiments we show that our algorithms outperform different adaptive baseline algorithms."
neurips,https://proceedings.neurips.cc/paper/2019/file/5f14615696649541a025d3d0f8e0447f-Paper.pdf,Data Cleansing for Models Trained with SGD,"Satoshi Hara, Atsushi Nitanda, Takanori Maehara",
neurips,https://proceedings.neurips.cc/paper/2019/file/5f5d472067f77b5c88f69f1bcfda1e08-Paper.pdf,Universality and individuality in neural dynamics across large populations of recurrent networks,"Niru Maheswaranathan, Alex Williams, Matthew Golub, Surya Ganguli, David Sussillo",
neurips,https://proceedings.neurips.cc/paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf,Generating Diverse High-Fidelity Images with VQ-VAE-2,"Ali Razavi, Aaron van den Oord, Oriol Vinyals",
neurips,https://proceedings.neurips.cc/paper/2019/file/5faf461eff3099671ad63c6f3f094f7f-Paper.pdf,When to Trust Your Model: Model-Based Policy Optimization,"Michael Janner, Justin Fu, Marvin Zhang, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2019/file/5fc34ed307aac159a30d81181c99847e-Paper.pdf,On Making Stochastic Classifiers Deterministic,"Andrew Cotter, Maya Gupta, Harikrishna Narasimhan",
neurips,https://proceedings.neurips.cc/paper/2019/file/5fd0b37cd7dbbb00f97ba6ce92bf5add-Paper.pdf,Blind Super-Resolution Kernel Estimation using an Internal-GAN,"Sefi Bell-Kligler, Assaf Shocher, Michal Irani",
neurips,https://proceedings.neurips.cc/paper/2019/file/6018df1842f7130f1b85a6f8e911b96b-Paper.pdf,Learning to Learn By Self-Critique,"Antreas Antoniou, Amos J. Storkey","Usually a model learns task-specific information from a small training-set (the \emph{support-set}) and subsequently produces predictions on a small unlabelled validation set (\emph{target-set}). The target-set contains additional task-specific information which is not utilized by existing few-shot learning methods. This is a challenge requiring approaches beyond the current methods as at inference time, the target-set contains only input data-points, and so discriminative-based learning cannot be used."
neurips,https://proceedings.neurips.cc/paper/2019/file/6048ff4e8cb07aa60b6777b6f7384d52-Paper.pdf,Learning New Tricks From Old Dogs: Multi-Source Transfer Learning From Pre-Trained Networks,"Joshua Lee, Prasanna Sattigeri, Gregory Wornell",
neurips,https://proceedings.neurips.cc/paper/2019/file/60495b4e033e9f60b32a6607b587aadd-Paper.pdf,Globally Convergent Newton Methods for Ill-conditioned Generalized Self-concordant Losses,"Ulysse Marteau-Ferey, Francis Bach, Alessandro Rudi",
neurips,https://proceedings.neurips.cc/paper/2019/file/606555cf42a6719782a952aa33cfa2cb-Paper.pdf,Is Deeper Better only when Shallow is Good?,"Eran Malach, Shai Shalev-Shwartz",
neurips,https://proceedings.neurips.cc/paper/2019/file/60792d855cd8a912a97711f91a1f155c-Paper.pdf,Variance Reduced Policy Evaluation with Smooth Function Approximation,"Hoi-To Wai, Mingyi Hong, Zhuoran Yang, Zhaoran Wang, Kexin Tang","Policy evaluation with smooth and nonlinear function approximation has shown great potential for reinforcement learning. Compared to linear function approxi- mation, it allows for using a richer class of approximation functions such as the neural networks. Traditional algorithms are based on two timescales stochastic approximation whose convergence rate is often slow. This paper focuses on an offline setting where a trajectory of
m
m
state-action pairs are observed. We formulate the policy evaluation problem as a non-convex primal-dual, finite-sum optimization problem, whose primal sub-problem is non-convex and dual sub-problem is strongly concave. We suggest a single-timescale primal-dual gradient algorithm with variance reduction, and show that it converges to an
ϵ
ϵ
-stationary point using
O
(
m
/
ϵ
)
O
calls (in expectation) to a gradient oracle."
neurips,https://proceedings.neurips.cc/paper/2019/file/6084e82a08cb979cf75ae28aed37ecd4-Paper.pdf,k-Means Clustering of Lines for Big Data,"Yair Marom, Dan Feldman","The input to the \emph{
k
k
-mean for lines} problem is a set
L
L
of
n
n
lines in
R
d
R
, and the goal is to compute a set of
k
k
centers (points) in
R
d
R
that minimizes the sum of squared distances over every line in
L
L
and its nearest center. This is a straightforward generalization of the
k
k
-mean problem where the input is a set of
n
n
points instead of lines. We suggest the first PTAS that computes a
(
1
+
ϵ
)
(
-approximation to this problem in time
O
(
n
log
n
)
O
for any constant approximation error
ϵ
∈
(
0
,
1
)
ϵ
, and constant integers
k
,
d
≥
1
k
. This is by proving that there is always a weighted subset (called coreset) of
d
k
O
(
k
)
log
(
n
)
/
ϵ
2
d
lines in
L
L
that approximates the sum of squared distances from
L
L
to \emph{any} given set of
k
k
points. Using traditional merge-and-reduce technique, this coreset implies results for a streaming set (possibly infinite) of lines to
M
M
machines in one pass (e.g. cloud) using memory, update time and communication that is near-logarithmic in
n
n
, as well as deletion of any line but using linear space. These results generalized for other distance functions such as
k
k
-median (sum of distances) or ignoring farthest
m
m
lines from the given centers to handle outliers. Experimental results on 10 machines on Amazon EC2 cloud show that the algorithm performs well in practice. Open source code for all the algorithms and experiments is also provided."
neurips,https://proceedings.neurips.cc/paper/2019/file/60a6c4002cc7b29142def8871531281a-Paper.pdf,Deep Leakage from Gradients,"Ligeng Zhu, Zhijian Liu, Song Han",
neurips,https://proceedings.neurips.cc/paper/2019/file/60ad83801910ec976590f69f638e0d6d-Paper.pdf,Robustness to Adversarial Perturbations in Learning from Incomplete Data,"Amir Najafi, Shin-ichi Maeda, Masanori Koyama, Takeru Miyato",
neurips,https://proceedings.neurips.cc/paper/2019/file/60cb558c40e4f18479664069d9642d5a-Paper.pdf,Pure Exploration with Multiple Correct Answers,"Rémy Degenne, Wouter M. Koolen",
neurips,https://proceedings.neurips.cc/paper/2019/file/60ce36723c17bbac504f2ef4c8a46995-Paper.pdf,Correlation in Extensive-Form Games: Saddle-Point Formulation and Benchmarks,"Gabriele Farina, Chun Kai Ling, Fei Fang, Tuomas Sandholm",
neurips,https://proceedings.neurips.cc/paper/2019/file/618faa1728eb2ef6e3733645273ab145-Paper.pdf,The Thermodynamic Variational Objective,"Vaden Masrani, Tuan Anh Le, Frank Wood",
neurips,https://proceedings.neurips.cc/paper/2019/file/61b4a64be663682e8cb037d9719ad8cd-Paper.pdf,Sampling Sketches for Concave Sublinear Functions of Frequencies,"Edith Cohen, Ofir Geri","We consider massive distributed datasets that consist of elements modeled as key-value pairs and the task of computing statistics or aggregates where the contribution of each key is weighted by a function of its frequency (sum of values of its elements). This fundamental problem has a wealth of applications in data analytics and machine learning, in particular, with concave sublinear functions of the frequencies that mitigate the disproportionate effect of keys with high frequency. The family of concave sublinear functions includes low frequency moments (
p
≤
1
p
), capping, logarithms, and their compositions. A common approach is to sample keys, ideally, proportionally to their contributions and estimate statistics from the sample. A simple but costly way to do this is by aggregating the data to produce a table of keys and their frequencies, apply our function to the frequency values, and then apply a weighted sampling scheme. Our main contribution is the design of composable sampling sketches that can be tailored to any concave sublinear function of the frequencies. Our sketch structure size is very close to the desired sample size and our samples provide statistical guarantees on the estimation quality that are very close to that of an ideal sample of the same size computed over aggregated data. Finally, we demonstrate experimentally the simplicity and effectiveness of our methods."
neurips,https://proceedings.neurips.cc/paper/2019/file/61f3a6dbc9120ea78ef75544826c814e-Paper.pdf,Solving Interpretable Kernel Dimensionality Reduction,"Chieh Wu, Jared Miller, Yale Chang, Mario Sznaier, Jennifer Dy","Kernel dimensionality reduction (KDR) algorithms find a low dimensional representation of the original data by optimizing kernel dependency measures that are capable of capturing nonlinear relationships. The standard strategy is to first map the data into a high dimensional feature space using kernels prior to a projection onto a low dimensional space. While KDR methods can be easily solved by keeping the most dominant eigenvectors of the kernel matrix, its features are no longer easy to interpret. Alternatively, Interpretable KDR (IKDR) is different in that it projects onto a subspace \textit{before} the kernel feature mapping, therefore, the projection matrix can indicate how the original features linearly combine to form the new features. Unfortunately, the IKDR objective requires a non-convex manifold optimization that is difficult to solve and can no longer be solved by eigendecomposition. Recently, an efficient iterative spectral (eigendecomposition) method (ISM) has been proposed for this objective in the context of alternative clustering. However, ISM only provides theoretical guarantees for the Gaussian kernel. This greatly constrains ISM's usage since any kernel method using ISM is now limited to a single kernel. This work extends the theoretical guarantees of ISM to an entire family of kernels, thereby empowering ISM to solve any kernel method of the same objective. In identifying this family, we prove that each kernel within the family has a surrogate
Φ
Φ
matrix and the optimal projection is formed by its most dominant eigenvectors. With this extension, we establish how a wide range of IKDR applications across different learning paradigms can be solved by ISM. To support reproducible results, the source code is made publicly available on \url{https://github.com/ANONYMIZED}."
neurips,https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf,Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss,"Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, Tengyu Ma",
neurips,https://proceedings.neurips.cc/paper/2019/file/6244b2ba957c48bc64582cf2bcec3d04-Paper.pdf,Multivariate Triangular Quantile Maps for Novelty Detection,"Jingjing Wang, Sun Sun, Yaoliang Yu",
neurips,https://proceedings.neurips.cc/paper/2019/file/624567140fecc40163fed3c45a959a7c-Paper.pdf,Gradient-based Adaptive Markov Chain Monte Carlo,"Michalis Titsias, Petros Dellaportas",
neurips,https://proceedings.neurips.cc/paper/2019/file/62dad6e273d32235ae02b7d321578ee8-Paper.pdf,"Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers","Zeyuan Allen-Zhu, Yuanzhi Li, Yingyu Liang","In this work, we prove that overparameterized neural networks can learn some notable concept classes, including two and three-layer networks with fewer parameters and smooth activations. Moreover, the learning can be simply done by SGD (stochastic gradient descent) or its variants in polynomial time using polynomially many samples. The sample complexity can also be almost independent of the number of parameters in the network."
neurips,https://proceedings.neurips.cc/paper/2019/file/62e0973455fd26eb03e91d5741a4a3bb-Paper.pdf,Online Forecasting of Total-Variation-bounded Sequences,"Dheeraj Baby, Yu-Xiang Wang","We consider the problem of online forecasting of sequences of length
n
n
with total-variation at most
C
n
C
using observations contaminated by independent
σ
σ
-subgaussian noise. We design an
O
(
n
log
n
)
O
-time algorithm that achieves a cumulative square error of
~
O
(
n
1
/
3
C
2
/
3
n
σ
4
/
3
+
C
2
n
)
O
with high probability. We also prove a lower bound that matches the upper bound in all parameters (up to a
log
(
n
)
log
factor). To the best of our knowledge, this is the first **polynomial-time** algorithm that achieves the optimal
O
(
n
1
/
3
)
O
rate in forecasting total variation bounded sequences and the first algorithm that **adapts to unknown**
C
n
C
.Our proof techniques leverage the special localized structure of Haar wavelet basis and the adaptivity to unknown smoothness parameters in the classical wavelet smoothing [Donoho et al., 1998]. We also compare our model to the rich literature of dynamic regret minimization and nonstationary stochastic optimization, where our problem can be treated as a special case. We show that the workhorse in those settings --- online gradient descent and its variants with a fixed restarting schedule --- are instances of a class of **linear forecasters** that require a suboptimal regret of
~
Ω
(
√
n
)
Ω
. This implies that the use of more adaptive algorithms is necessary to obtain the optimal rate."
neurips,https://proceedings.neurips.cc/paper/2019/file/635440afdfc39fe37995fed127d7df4f-Paper.pdf,Approximation Ratios of Graph Neural Networks for Combinatorial Problems,"Ryoma Sato, Makoto Yamada, Hisashi Kashima",
neurips,https://proceedings.neurips.cc/paper/2019/file/6364d3f0f495b6ab9dcf8d3b5c6e0b01-Paper.pdf,Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video,"Jiawang Bian, Zhichao Li, Naiyan Wang, Huangying Zhan, Chunhua Shen, Ming-Ming Cheng, Ian Reid",
neurips,https://proceedings.neurips.cc/paper/2019/file/6395ebd0f4b478145ecfbaf939454fa4-Paper.pdf,Variational Denoising Network: Toward Blind Noise Modeling and Removal,"Zongsheng Yue, Hongwei Yong, Qian Zhao, Deyu Meng, Lei Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/64517d8435994992e682b3e4aa0a0661-Paper.pdf,Multi-task Learning for Aggregated Data using Gaussian Processes,"Fariba Yousefi, Michael T. Smith, Mauricio Álvarez",
neurips,https://proceedings.neurips.cc/paper/2019/file/64c26b2a2dcf068c49894bd07e0e6389-Paper.pdf,Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards,"Alexander Trott, Stephan Zheng, Caiming Xiong, Richard Socher",
neurips,https://proceedings.neurips.cc/paper/2019/file/65184321c340b4d56581ee59b58d9d56-Paper.pdf,Efficient characterization of electrically evoked responses for neural interfaces,"Nishal Shah, Sasidhar Madugula, Pawel Hottowy, Alexander Sher, Alan Litke, Liam Paninski, E.J. Chichilnisky",
neurips,https://proceedings.neurips.cc/paper/2019/file/6562c5c1f33db6e05a082a88cddab5ea-Paper.pdf,The Synthesis of XNOR Recurrent Neural Networks with Stochastic Logic,"Arash Ardakani, Zhengyun Ji, Amir Ardakani, Warren Gross",
neurips,https://proceedings.neurips.cc/paper/2019/file/65699726a3c601b9f31bf04019c8593c-Paper.pdf,HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models,"Sharon Zhou, Mitchell Gordon, Ranjay Krishna, Austin Narcomey, Li F. Fei-Fei, Michael Bernstein","Generative models often use human evaluations to measure the perceived quality of their outputs. Automated metrics are noisy indirect proxies, because they rely on heuristics or pretrained embeddings. However, up until now, direct human evaluation strategies have been ad-hoc, neither standardized nor validated. Our work establishes a gold standard human benchmark for generative realism. We construct Human eYe Perceptual Evaluation (HYPE) a human benchmark that is (1) grounded in psychophysics research in perception, (2) reliable across different sets of randomly sampled outputs from a model, (3) able to produce separable model performances, and (4) efficient in cost and time. We introduce two variants: one that measures visual perception under adaptive time constraints to determine the threshold at which a model's outputs appear real (e.g.
250
250
ms), and the other a less expensive variant that measures human error rate on fake and real images sans time constraints. We test HYPE across six state-of-the-art generative adversarial networks and two sampling techniques on conditional and unconditional image generation using four datasets: CelebA, FFHQ, CIFAR-10, and ImageNet. We find that HYPE can track model improvements across training epochs, and we confirm via bootstrap sampling that HYPE rankings are consistent and replicable."
neurips,https://proceedings.neurips.cc/paper/2019/file/658bbbdef9415ba5e2ff857f1146ba6e-Paper.pdf,McDiarmid-Type Inequalities for Graph-Dependent Variables and Stability Bounds,"Rui (Ray) Zhang, Xingwu Liu, Yuyi Wang, Liwei Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/65a99bb7a3115fdede20da98b08a370f-Paper.pdf,Rapid Convergence of the Unadjusted Langevin Algorithm: Isoperimetry Suffices,"Santosh Vempala, Andre Wibisono","We study the Unadjusted Langevin Algorithm (ULA) for sampling from a probability distribution
ν
=
e
−
f
ν
on
\R
n
\R
. We prove a convergence guarantee in Kullback-Leibler (KL) divergence assuming
ν
ν
satisfies log-Sobolev inequality and
f
f
has bounded Hessian. Notably, we do not assume convexity or bounds on higher derivatives. We also prove convergence guarantees in R\'enyi divergence of order
q
>
1
q
assuming the limit of ULA satisfies either log-Sobolev or Poincar\'e inequality."
neurips,https://proceedings.neurips.cc/paper/2019/file/65b1e92c585fd4c2159d5f33b5030ff2-Paper.pdf,Are sample means in multi-armed bandits positively or negatively biased?,"Jaehyeok Shin, Aaditya Ramdas, Alessandro Rinaldo",
neurips,https://proceedings.neurips.cc/paper/2019/file/65fc52ed8f88c81323a418ca94cec2ed-Paper.pdf,The Landscape of Non-convex Empirical Risk with Degenerate Population Risk,"Shuang Li, Gongguo Tang, Michael B. Wakin",
neurips,https://proceedings.neurips.cc/paper/2019/file/65fc9fb4897a89789352e211ca2d398f-Paper.pdf,Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks,"Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi (Viji) Srinivasan, Xiaodong Cui, Wei Zhang, Kailash Gopalakrishnan",
neurips,https://proceedings.neurips.cc/paper/2019/file/661c1c090ff5831a647202397c61d73c-Paper.pdf,Are deep ResNets provably better than linear predictors?,"Chulhee Yun, Suvrit Sra, Ali Jadbabaie",
neurips,https://proceedings.neurips.cc/paper/2019/file/663772ea088360f95bac3dc7ffb841be-Paper.pdf,E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings,"Yue Wang, Ziyu Jiang, Xiaohan Chen, Pengfei Xu, Yang Zhao, Yingyan Lin, Zhangyang Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/663fd3c5144fd10bd5ca6611a9a5b92d-Paper.pdf,Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels,"Simon S. Du, Kangcheng Hou, Russ R. Salakhutdinov, Barnabas Poczos, Ruosong Wang, Keyulu Xu",
neurips,https://proceedings.neurips.cc/paper/2019/file/6646b06b90bd13dabc11ddba01270d23-Paper.pdf,Privacy-Preserving Q-Learning with Functional Noise in Continuous Spaces,"Baoxiang Wang, Nidhi Hegde",
neurips,https://proceedings.neurips.cc/paper/2019/file/671f0311e2754fcdd37f70a8550379bc-Paper.pdf,Learning Data Manipulation for Augmentation and Weighting,"Zhiting Hu, Bowen Tan, Russ R. Salakhutdinov, Tom M. Mitchell, Eric P. Xing",
neurips,https://proceedings.neurips.cc/paper/2019/file/6754e06e46dfa419d5afe3c9781cecad-Paper.pdf,Hyperparameter Learning via Distributional Transfer,"Ho Chung Law, Peilin Zhao, Leung Sing Chan, Junzhou Huang, Dino Sejdinovic",
neurips,https://proceedings.neurips.cc/paper/2019/file/675f9820626f5bc0afb47b57890b466e-Paper.pdf,Levenshtein Transformer,"Jiatao Gu, Changhan Wang, Junbo Zhao",
neurips,https://proceedings.neurips.cc/paper/2019/file/6766aa2750c19aad2fa1b32f36ed4aee-Paper.pdf,Learning Perceptual Inference by Contrasting,"Chi Zhang, Baoxiong Jia, Feng Gao, Yixin Zhu, HongJing Lu, Song-Chun Zhu",
neurips,https://proceedings.neurips.cc/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf,Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting,"Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, Xifeng Yan",
neurips,https://proceedings.neurips.cc/paper/2019/file/677e09724f0e2df9b6c000b75b5da10d-Paper.pdf,Multi-View Reinforcement Learning,"Minne Li, Lisheng Wu, Jun WANG, Haitham Bou Ammar",
neurips,https://proceedings.neurips.cc/paper/2019/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf,Updates of Equilibrium Prop Match Gradients of Backprop Through Time in an RNN with Static Input,"Maxence Ernoult, Julie Grollier, Damien Querlioz, Yoshua Bengio, Benjamin Scellier",
neurips,https://proceedings.neurips.cc/paper/2019/file/679d8bbd776e0bbf3b044306c5be94ae-Paper.pdf,Toward a Characterization of Loss Functions for Distribution Learning,"Nika Haghtalab, Cameron Musco, Bo Waggoner",
neurips,https://proceedings.neurips.cc/paper/2019/file/67fe0f66449e31fdafdc3505c37d6acb-Paper.pdf,Can SGD Learn Recurrent Neural Networks with Provable Generalization?,"Zeyuan Allen-Zhu, Yuanzhi Li","In this paper, we show using the vanilla stochastic gradient descent (SGD), RNN can actually learn some notable concept class \emph{efficiently}, meaning that both time and sample complexity scale \emph{polynomially} in the input length (or almost polynomially, depending on the concept). This concept class at least includes functions where each output token is generated from inputs of earlier tokens using a smooth two-layer neural network."
neurips,https://proceedings.neurips.cc/paper/2019/file/680390c55bbd9ce416d1d69a9ab4760d-Paper.pdf,Image Captioning: Transforming Objects into Words,"Simao Herdade, Armin Kappeler, Kofi Boakye, Joao Soares",
neurips,https://proceedings.neurips.cc/paper/2019/file/6804c9bca0a615bdb9374d00a9fcba59-Paper.pdf,MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis,"Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de Brébisson, Yoshua Bengio, Aaron C. Courville",
neurips,https://proceedings.neurips.cc/paper/2019/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf,Deliberative Explanations: visualizing network insecurities,"Pei Wang, Nuno Nvasconcelos",
neurips,https://proceedings.neurips.cc/paper/2019/file/6832a7b24bc06775d02b7406880b93fc-Paper.pdf,Uncoupled Regression from Pairwise Comparison Data,"Liyuan Xu, Junya Honda, Gang Niu, Masashi Sugiyama",
neurips,https://proceedings.neurips.cc/paper/2019/file/685217557383cd194b4f10ae4b39eebf-Paper.pdf,No-Regret Learning in Unknown Games with Correlated Payoffs,"Pier Giuseppe Sessa, Ilija Bogunovic, Maryam Kamgarpour, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2019/file/685bfde03eb646c27ed565881917c71c-Paper.pdf,Pareto Multi-Task Learning,"Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, Sam Kwong",
neurips,https://proceedings.neurips.cc/paper/2019/file/6883966fd8f918a4aa29be29d2c386fb-Paper.pdf,Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos,"Yitian Yuan, Lin Ma, Jingwen Wang, Wei Liu, Wenwu Zhu",
neurips,https://proceedings.neurips.cc/paper/2019/file/688f3fe72241429902623b790f15a774-Paper.pdf,Structured Variational Inference in Continuous Cox Process Models,"Virginia Aglietti, Edwin V. Bonilla, Theodoros Damoulas, Sally Cripps",
neurips,https://proceedings.neurips.cc/paper/2019/file/68b1fbe7f16e4ae3024973f12f3cb313-Paper.pdf,Channel Gating Neural Networks,"Weizhe Hua, Yuan Zhou, Christopher M. De Sa, Zhiru Zhang, G. Edward Suh",
neurips,https://proceedings.neurips.cc/paper/2019/file/68d13cf26c4b4f4f932e3eff990093ba-Paper.pdf,Rethinking Generative Mode Coverage: A Pointwise Guaranteed Approach,"Peilin Zhong, Yuchen Mo, Chang Xiao, Pengyu Chen, Changxi Zheng",
neurips,https://proceedings.neurips.cc/paper/2019/file/68d30a9594728bc39aa24be94b319d21-Paper.pdf,Differentially Private Algorithms for Learning Mixtures of Separated Gaussians,"Gautam Kamath, Or Sheffet, Vikrant Singhal, Jonathan Ullman",
neurips,https://proceedings.neurips.cc/paper/2019/file/692baebec3bb4b53d7ebc3b9fabac31b-Paper.pdf,A Domain Agnostic Measure for Monitoring and Evaluating GANs,"Paulina Grnarova, Kfir Y. Levy, Aurelien Lucchi, Nathanael Perraudin, Ian Goodfellow, Thomas Hofmann, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2019/file/6948bd44c91acd2b54ecdd1b132f10fb-Paper.pdf,Enabling hyperparameter optimization in sequential autoencoders for spiking neural data,"Mohammad Reza Keshtkaran, Chethan Pandarinath",
neurips,https://proceedings.neurips.cc/paper/2019/file/6950aa02ae8613af620668146dd11840-Paper.pdf,Grid Saliency for Context Explanations of Semantic Segmentation,"Lukas Hoyer, Mauricio Munoz, Prateek Katiyar, Anna Khoreva, Volker Fischer",
neurips,https://proceedings.neurips.cc/paper/2019/file/69cd21a0e0b7d5f05dc88a0be36950c7-Paper.pdf,Extreme Classification in Log Memory using Count-Min Sketch: A Case Study of Amazon Search with 50M Products,"Tharun Kumar Reddy Medini, Qixuan Huang, Yiqiu Wang, Vijai Mohan, Anshumali Shrivastava","In the last decade, it has been shown that many hard AI tasks, especially in NLP, can be naturally modeled as extreme classification problems leading to improved precision. However, such models are prohibitively expensive to train due to the memory bottleneck in the last layer. For example, a reasonable softmax layer for the dataset of interest in this paper can easily reach well beyond 100 billion parameters (> 400 GB memory). To alleviate this problem, we present Merged-Average Classifiers via Hashing (MACH), a generic
K
K
-classification algorithm where memory provably scales at
O
(
log
K
)
O
without any assumption on the relation between classes. MACH is subtly a count-min sketch structure in disguise, which uses universal hashing to reduce classification with a large number of classes to few embarrassingly parallel and independent classification tasks with a small (constant) number of classes. MACH naturally provides a technique for zero communication model parallelism. We experiment with 6 datasets; some multiclass and some multilabel, and show consistent improvement in precision and recall metrics compared to respective baselines. In particular, we train an end-to -end deep classifier on a private product search dataset sampled from Amazon Search Engine with 70 million queries and 49.46 million documents. MACH outperforms, by a significant margin, the state-of-the-art extreme classification models deployed on commercial search engines: Parabel and dense embedding models. Our largest model has 6.4 billion parameters and trains in less than 35 hrs on a single p3.16x machine. Our training times are 7-10x faster, and our memory footprints are 2-4x smaller than the best baselines. This training time is also significantly lower than the one reported by Google’s mixture of experts (MoE) language model on a comparable model size and hardware."
neurips,https://proceedings.neurips.cc/paper/2019/file/6a10bbd480e4c5573d8f3af73ae0454b-Paper.pdf,Selecting the independent coordinates of manifolds with large aspect ratios,"Yu-Chia Chen, Marina Meila","Many manifold embedding algorithms fail apparently when the data manifold has a large aspect ratio (such as a long, thin strip). Here, we formulate success and failure in terms of finding a smooth embedding, showing also that the problem is pervasive and more complex than previously recognized. Mathematically, success is possible under very broad conditions, provided that embedding is done by carefully selected eigenfunctions of the Laplace-Beltrami operator
Δ
\M
Δ
. Hence, we propose a bicriterial Independent Eigencoordinate Selection (IES) algorithm that selects smooth embeddings with few eigenvectors. The algorithm is grounded in theory, has low computational overhead, and is successful on synthetic and large real data."
neurips,https://proceedings.neurips.cc/paper/2019/file/6a4d5952d4c018a1c1af9fa590a10dda-Paper.pdf,DM2C: Deep Mixed-Modal Clustering,"Yangbangyan Jiang, Qianqian Xu, Zhiyong Yang, Xiaochun Cao, Qingming Huang",
neurips,https://proceedings.neurips.cc/paper/2019/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf,An Improved Analysis of Training Over-parameterized Deep Neural Networks,"Difan Zou, Quanquan Gu","A recent line of research has shown that gradient-based algorithms with random initialization can converge to the global minima of the training loss for over-parameterized (i.e., sufficiently wide) deep neural networks. However, the condition on the width of the neural network to ensure the global convergence is very stringent, which is often a high-degree polynomial in the training sample size
n
n
(e.g.,
O
(
n
24
)
O
). In this paper, we provide an improved analysis of the global convergence of (stochastic) gradient descent for training deep neural networks, which only requires a milder over-parameterization condition than previous work in terms of the training sample size and other problem-dependent parameters. The main technical contributions of our analysis include (a) a tighter gradient lower bound that leads to a faster convergence of the algorithm, and (b) a sharper characterization of the trajectory length of the algorithm. By specializing our result to two-layer (i.e., one-hidden-layer) neural networks, it also provides a milder over-parameterization condition than the best-known result in prior work."
neurips,https://proceedings.neurips.cc/paper/2019/file/6a8018b3a00b69c008601b8becae392b-Paper.pdf,Stochastic Proximal Langevin Algorithm: Potential Splitting and Nonasymptotic Rates,"Adil SALIM, Dmitry Kovalev, Peter Richtarik",
neurips,https://proceedings.neurips.cc/paper/2019/file/6aadca7bd86c4743e6724f9607256126-Paper.pdf,Contextual Bandits with Cross-Learning,"Santiago Balseiro, Negin Golrezaei, Mohammad Mahdian, Vahab Mirrokni, Jon Schneider","In the classical contextual bandits problem, in each round
t
t
, a learner observes some context
c
c
, chooses some action
a
a
to perform, and receives some reward
r
a
,
t
(
c
)
r
. We consider the variant of this problem where in addition to receiving the reward
r
a
,
t
(
c
)
r
, the learner also learns the values of
r
a
,
t
(
c
′
)
r
for all other contexts
c
′
c
; i.e., the rewards that would have been achieved by performing that action under different contexts. This variant arises in several strategic settings, such as learning how to bid in non-truthful repeated auctions (in this setting the context is the decision maker's private valuation for each auction). We call this problem the contextual bandits problem with cross-learning. The best algorithms for the classical contextual bandits problem achieve
~
O
(
√
C
K
T
)
O
regret against all stationary policies, where
C
C
is the number of contexts,
K
K
the number of actions, and
T
T
the number of rounds. We demonstrate algorithms for the contextual bandits problem with cross-learning that remove the dependence on
C
C
and achieve regret
~
O
(
√
K
T
)
O
(when contexts are stochastic with known distribution),
~
O
(
K
1
/
3
T
2
/
3
)
O
(when contexts are stochastic with unknown distribution), and
~
O
(
√
K
T
)
O
(when contexts are adversarial but rewards are stochastic). We simulate our algorithms on real auction data from an ad exchange running first-price auctions (showing that they outperform traditional contextual bandit algorithms)."
neurips,https://proceedings.neurips.cc/paper/2019/file/6add07cf50424b14fdf649da87843d01-Paper.pdf,Fast AutoAugment,"Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, Sungwoong Kim",
neurips,https://proceedings.neurips.cc/paper/2019/file/6aed000af86a084f9cb0264161e29dd3-Paper.pdf,A state-space model for inferring effective connectivity of latent neural dynamics from simultaneous EEG/fMRI,"Tao Tu, John Paisley, Stefan Haufe, Paul Sajda",
neurips,https://proceedings.neurips.cc/paper/2019/file/6b3c49bdba5be0d322334e30c459f8bd-Paper.pdf,A Solvable High-Dimensional Model of GAN,"Chuang Wang, Hong Hu, Yue Lu",
neurips,https://proceedings.neurips.cc/paper/2019/file/6c29793a140a811d0c45ce03c1c93a28-Paper.pdf,On The Classification-Distortion-Perception Tradeoff,"Dong Liu, Haochen Zhang, Zhiwei Xiong",
neurips,https://proceedings.neurips.cc/paper/2019/file/6c442e0e996fa84f344a14927703a8c1-Paper.pdf,Variance Reduction for Matrix Games,"Yair Carmon, Yujia Jin, Aaron Sidford, Kevin Tian",
neurips,https://proceedings.neurips.cc/paper/2019/file/6c468ec5a41d65815de23ec1d08d7951-Paper.pdf,Efficient Forward Architecture Search,"Hanzhang Hu, John Langford, Rich Caruana, Saurajit Mukherjee, Eric J. Horvitz, Debadeepta Dey",
neurips,https://proceedings.neurips.cc/paper/2019/file/6c4bb406b3e7cd5447f7a76fd7008806-Paper.pdf,Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network,"Siqi Wang, Yijie Zeng, Xinwang Liu, En Zhu, Jianping Yin, Chuanfu Xu, Marius Kloft",
neurips,https://proceedings.neurips.cc/paper/2019/file/6c7cd904122e623ce625613d6af337c4-Paper.pdf,"Poincaré Recurrence, Cycles and Spurious Equilibria in Gradient-Descent-Ascent for Non-Convex Non-Concave Zero-Sum Games","Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, Georgios Piliouras",
neurips,https://proceedings.neurips.cc/paper/2019/file/6c7de1f27f7de61a6daddfffbe05c058-Paper.pdf,End-to-End Learning on 3D Protein Structure for Interface Prediction,"Raphael Townshend, Rishi Bedi, Patricia Suriana, Ron Dror",
neurips,https://proceedings.neurips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf,Scalable Global Optimization via Local Bayesian Optimization,"David Eriksson, Michael Pearce, Jacob Gardner, Ryan D. Turner, Matthias Poloczek",
neurips,https://proceedings.neurips.cc/paper/2019/file/6d0f846348a856321729a2f36734d1a7-Paper.pdf,Positional Normalization,"Boyi Li, Felix Wu, Kilian Q. Weinberger, Serge Belongie",
neurips,https://proceedings.neurips.cc/paper/2019/file/6d19c113404cee55b4036fce1a37c058-Paper.pdf,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model,"Atilim Gunes Baydin, Lei Shao, Wahid Bhimji, Lukas Heinrich, Saeid Naderiparizi, Andreas Munk, Jialin Liu, Bradley Gram-Hansen, Gilles Louppe, Lawrence Meadows, Philip Torr, Victor Lee, Kyle Cranmer, Mr. Prabhat, Frank Wood",
neurips,https://proceedings.neurips.cc/paper/2019/file/6d1e481bdcf159961818823e652a7725-Paper.pdf,Online Optimal Control with Linear Dynamics and Predictions: Algorithms and Regret Analysis,"Yingying Li, Xin Chen, Na Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/6d3a2d24eb109dddf78374fe5d0ee067-Paper.pdf,Beyond Vector Spaces: Compact Data Representation as Differentiable Weighted Graphs,"Denis Mazur, Vage Egiazarian, Stanislav Morozov, Artem Babenko",
neurips,https://proceedings.neurips.cc/paper/2019/file/6d9c547cf146054a5a720606a7694467-Paper.pdf,Gradient Information for Representation and Modeling,"Jie Ding, Robert Calderbank, Vahid Tarokh",
neurips,https://proceedings.neurips.cc/paper/2019/file/6da9003b743b65f4c0ccd295cc484e57-Paper.pdf,Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation,"Qiming ZHANG, Jing Zhang, Wei Liu, Dacheng Tao","Unsupervised domain adaptation (UDA) aims to enhance the generalization capability of a certain model from a source domain to a target domain. UDA is of particular significance since no extra effort is devoted to annotating target domain samples. However, the different data distributions in the two domains, or \emph{domain shift/discrepancy}, inevitably compromise the UDA performance. Although there has been a progress in matching the marginal distributions between two domains, the classifier favors the source domain features and makes incorrect predictions on the target domain due to category-agnostic feature alignment. In this paper, we propose a novel category anchor-guided (CAG) UDA model for semantic segmentation, which explicitly enforces category-aware feature alignment to learn shared discriminative features and classifiers simultaneously. First, the category-wise centroids of the source domain features are used as guided anchors to identify the active features in the target domain and also assign them pseudo-labels. Then, we leverage an anchor-based pixel-level distance loss and a discriminative loss to drive the intra-category features closer and the inter-category features further apart, respectively. Finally, we devise a stagewise training mechanism to reduce the error accumulation and adapt the proposed model progressively. Experiments on both the GTA5
→
→
Cityscapes and SYNTHIA
→
→
Cityscapes scenarios demonstrate the superiority of our CAG-UDA model over the state-of-the-art methods. The code is available at \url{https://github.com/RogerZhangzz/CAG\_UDA}."
neurips,https://proceedings.neurips.cc/paper/2019/file/6e0917469214d8fbd8c517dcdc6b8dcf-Paper.pdf,Novel positional encodings to enable tree-based transformers,"Vighnesh Shiv, Chris Quirk",
neurips,https://proceedings.neurips.cc/paper/2019/file/6e2290dbf1e11f39d246e7ce5ac50a1e-Paper.pdf,Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks,"Spencer Frei, Yuan Cao, Quanquan Gu",
neurips,https://proceedings.neurips.cc/paper/2019/file/6e62a992c676f611616097dbea8ea030-Paper.pdf,Scalable Gromov-Wasserstein Learning for Graph Partitioning and Matching,"Hongteng Xu, Dixin Luo, Lawrence Carin","We propose a scalable Gromov-Wasserstein learning (S-GWL) method and establish a novel and theoretically-supported paradigm for large-scale graph analysis. The proposed method is based on the fact that Gromov-Wasserstein discrepancy is a pseudometric on graphs. Given two graphs, the optimal transport associated with their Gromov-Wasserstein discrepancy provides the correspondence between their nodes and achieves graph matching. When one of the graphs is a predefined graph with isolated but self-connected nodes (
i
.
e
.
i
, disconnected graph), the optimal transport indicates the clustering structure of the other graph and achieves graph partitioning. Further, we extend our method to multi-graph partitioning and matching by learning a Gromov-Wasserstein barycenter graph for multiple observed graphs. Our method combines a recursive
K
K
-partition mechanism with a warm-start proximal gradient algorithm, whose time complexity is
O
(
K
(
E
+
V
)
log
K
V
)
O
for graphs with
V
V
nodes and
E
E
edges. To our knowledge, our method is the first attempt to make Gromov-Wasserstein discrepancy applicable to large-scale graph analysis and unify graph partitioning and matching into the same framework. It outperforms state-of-the-art graph partitioning and matching methods, achieving a trade-off between accuracy and efficiency."
neurips,https://proceedings.neurips.cc/paper/2019/file/6e69ebbfad976d4637bb4b39de261bf7-Paper.pdf,Riemannian batch normalization for SPD neural networks,"Daniel Brooks, Olivier Schwander, Frederic Barbaresco, Jean-Yves Schneider, Matthieu Cord",
neurips,https://proceedings.neurips.cc/paper/2019/file/6e79ed05baec2754e25b4eac73a332d2-Paper.pdf,Deep Set Prediction Networks,"Yan Zhang, Jonathon Hare, Adam Prugel-Bennett",
neurips,https://proceedings.neurips.cc/paper/2019/file/6e7d5d259be7bf56ed79029c4e621f44-Paper.pdf,A unified theory for the origin of grid cells through the lens of pattern formation,"Ben Sorscher, Gabriel Mel, Surya Ganguli, Samuel Ocko",
neurips,https://proceedings.neurips.cc/paper/2019/file/6e923226e43cd6fac7cfe1e13ad000ac-Paper.pdf,Functional Adversarial Attacks,"Cassidy Laidlaw, Soheil Feizi",
neurips,https://proceedings.neurips.cc/paper/2019/file/6ea2ef7311b482724a9b7b0bc0dd85c6-Paper.pdf,Memory-oriented Decoder for Light Field Salient Object Detection,"Miao Zhang, Jingjing Li, JI WEI, Yongri Piao, Huchuan Lu",
neurips,https://proceedings.neurips.cc/paper/2019/file/6ea3f1874b188558fafbab78e8c3a968-Paper.pdf,Learning search spaces for Bayesian optimization: Another view of hyperparameter transfer learning,"Valerio Perrone, Huibin Shen, Matthias W. Seeger, Cedric Archambeau, Rodolphe Jenatton",
neurips,https://proceedings.neurips.cc/paper/2019/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf,Image Synthesis with a Single (Robust) Classifier,"Shibani Santurkar, Andrew Ilyas, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry",
neurips,https://proceedings.neurips.cc/paper/2019/file/6f518c31f6baa365f55c38d11cc349d1-Paper.pdf,Learning from Trajectories via Subgoal Discovery,"Sujoy Paul, Jeroen Vanbaar, Amit Roy-Chowdhury",
neurips,https://proceedings.neurips.cc/paper/2019/file/6fb52e71b837628ac16539c1ff911667-Paper.pdf,Unsupervised State Representation Learning in Atari,"Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre Côté, R Devon Hjelm",
neurips,https://proceedings.neurips.cc/paper/2019/file/6fd6b030c6afec018415662d0db43f9d-Paper.pdf,Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Gradient Estimators for Reinforcement Learning,"Gregory Farquhar, Shimon Whiteson, Jakob Foerster",
neurips,https://proceedings.neurips.cc/paper/2019/file/6fe43269967adbb64ec6149852b5cc3e-Paper.pdf,Meta Learning with Relational Information for Short Sequences,"Yujia Xie, Haoming Jiang, Feng Liu, Tuo Zhao, Hongyuan Zha",
neurips,https://proceedings.neurips.cc/paper/2019/file/700fdb2ba62d4554dc268c65add4b16e-Paper.pdf,Private Learning Implies Online Learning: An Efficient Reduction,"Alon Gonen, Elad Hazan, Shay Moran",In this paper we resolve this open question in the context of pure differential privacy. We derive an efficient black-box reduction from differentially private learning to online learning from expert advice.
neurips,https://proceedings.neurips.cc/paper/2019/file/70117ee3c0b15a2950f1e82a215e812b-Paper.pdf,Learning from brains how to regularize machines,"Zhe Li, Wieland Brendel, Edgar Walker, Erick Cobos, Taliah Muhammad, Jacob Reimer, Matthias Bethge, Fabian Sinz, Zachary Pitkow, Andreas Tolias",
neurips,https://proceedings.neurips.cc/paper/2019/file/7012ef0335aa2adbab58bd6d0702ba41-Paper.pdf,Kernel quadrature with DPPs,"Ayoub Belhadji, Rémi Bardenet, Pierre Chainais",
neurips,https://proceedings.neurips.cc/paper/2019/file/702cafa3bb4c9c86e4a3b6834b45aedd-Paper.pdf,A Debiased MDI Feature Importance Measure for Random Forests,"Xiao Li, Yu Wang, Sumanta Basu, Karl Kumbier, Bin Yu",
neurips,https://proceedings.neurips.cc/paper/2019/file/704cddc91e28d1a5517518b2f12bc321-Paper.pdf,Unsupervised Discovery of Temporal Structure in Noisy Data with Dynamical Components Analysis,"David Clark, Jesse Livezey, Kristofer Bouchard",
neurips,https://proceedings.neurips.cc/paper/2019/file/70a32110fff0f26d301e58ebbca9cb9f-Paper.pdf,MintNet: Building Invertible Neural Networks with Masked Convolutions,"Yang Song, Chenlin Meng, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2019/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf,Learning Temporal Pose Estimation from Sparsely-Labeled Videos,"Gedas Bertasius, Christoph Feichtenhofer, Du Tran, Jianbo Shi, Lorenzo Torresani",
neurips,https://proceedings.neurips.cc/paper/2019/file/71560ce98c8250ce57a6a970c9991a5f-Paper.pdf,Learning Generalizable Device Placement Algorithms for Distributed Machine Learning,"ravichandra addanki, Shaileshh Bojja Venkatakrishnan, Shreyan Gupta, Hongzi Mao, Mohammad Alizadeh","Unlike prior approaches that only find a device placement for a specific computation graph, Placeto can learn generalizable device placement policies that can be applied to any graph."
neurips,https://proceedings.neurips.cc/paper/2019/file/717d8b3d60d9eea997b35b02b6a4e867-Paper.pdf,Dynamic Incentive-Aware Learning: Robust Pricing in Contextual Auctions,"Negin Golrezaei, Adel Javanmard, Vahab Mirrokni","Motivated by pricing in ad exchange markets, we consider the problem of robust learning of reserve prices against strategic buyers in repeated contextual second-price auctions. Buyers' valuations \new{for} an item depend on the context that describes the item. However, the seller is not aware of the relationship between the context and buyers' valuations, i.e., buyers' preferences. The seller's goal is to design a learning policy to set reserve prices via observing the past sales data, and her objective is to minimize her regret for revenue, where the regret is computed against a clairvoyant policy that knows buyers' heterogeneous preferences. Given the seller's goal, utility-maximizing buyers have the incentive to bid untruthfully in order to manipulate the seller's learning policy. We propose two learning policies that are robust to such strategic behavior. These policies use the outcomes of the auctions, rather than the submitted bids, to estimate the preferences while controlling the long-term effect of the outcome of each auction on the future reserve prices. The first policy called Contextual Robust Pricing (CORP) is designed for the setting where the market noise distribution is known to the seller and achieves a T-period regret of
O
(
d
log
(
T
d
)
log
(
T
)
)
O
, where
d
d
is the dimension of {the} contextual information. The second policy, which is a variant of the first policy, is called Stable CORP (SCORP). This policy is tailored to the setting where the market noise distribution is unknown to the seller and belongs to an ambiguity set. We show that the SCORP policy has a T-period regret of
O
(
√
d
log
(
T
d
)
T
2
/
3
)
O
."
neurips,https://proceedings.neurips.cc/paper/2019/file/71887f62f073a78511cbac56f8cab53f-Paper.pdf,Optimal Best Markovian Arm Identification with Fixed Confidence,Vrettos Moulos,
neurips,https://proceedings.neurips.cc/paper/2019/file/71ee911dd06428a96c143a0b135041a4-Paper.pdf,On the equivalence between graph isomorphism testing and function approximation with GNNs,"Zhengdao Chen, Soledad Villar, Lei Chen, Joan Bruna","Our work connects these two perspectives and proves their equivalence. We further develop a framework of the representation power of GNNs with the language of sigma-algebra, which incorporates both viewpoints. Using this framework, we compare the expressive power of different classes of GNNs as well as other methods on graphs. In particular, we prove that order-2 Graph G-invariant networks fail to distinguish non-isomorphic regular graphs with the same degree. We then extend them to a new architecture, Ring-GNN, which succeeds in distinguishing these graphs as well as for tasks on real-world datasets."
neurips,https://proceedings.neurips.cc/paper/2019/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf,Information Competing Process for Learning Diversified Representations,"Jie Hu, Rongrong Ji, ShengChuan Zhang, Xiaoshuai Sun, Qixiang Ye, Chia-Wen Lin, Qi Tian",
neurips,https://proceedings.neurips.cc/paper/2019/file/7283518d47a05a09d33779a17adf1707-Paper.pdf,Individual Regret in Cooperative Nonstochastic Multi-Armed Bandits,"Yogev Bar-On, Yishay Mansour","We study agents communicating over an underlying network by exchanging messages, in order to optimize their individual regret in a common nonstochastic multi-armed bandit problem. We derive regret minimization algorithms that guarantee for each agent
v
v
an individual expected regret of
˜
O
(
√
(
1
+
K
∣
∣
N
(
v
)
∣
∣
)
T
)
O
, where
T
T
is the number of time steps,
K
K
is the number of actions and
N
(
v
)
N
is the set of neighbors of agent
v
v
in the communication graph. We present algorithms both for the case that the communication graph is known to all the agents, and for the case that the graph is unknown. When the graph is unknown, each agent knows only the set of its neighbors and an upper bound on the total number of agents. The individual regret between the models differs only by a logarithmic factor. Our work resolves an open problem from [Cesa-Bianchi et al., 2019b]."
neurips,https://proceedings.neurips.cc/paper/2019/file/7298332f04ac004a0ca44cc69ecf6f6b-Paper.pdf,SPoC: Search-based Pseudocode to Code,"Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, Percy S. Liang",
neurips,https://proceedings.neurips.cc/paper/2019/file/72da7fd6d1302c0a159f6436d01e9eb0-Paper.pdf,Distributional Policy Optimization: An Alternative Approach for Continuous Control,"Chen Tessler, Guy Tennenholtz, Shie Mannor",
neurips,https://proceedings.neurips.cc/paper/2019/file/73231e53eeef362c814c8522f5257286-Paper.pdf,Oblivious Sampling Algorithms for Private Data Analysis,"Sajin Sasy, Olga Ohrimenko",
neurips,https://proceedings.neurips.cc/paper/2019/file/7392ea4ca76ad2fb4c9c3b6a5c6e31e3-Paper.pdf,On Relating Explanations and Adversarial Examples,"Alexey Ignatiev, Nina Narodytska, Joao Marques-Silva",
neurips,https://proceedings.neurips.cc/paper/2019/file/73983c01982794632e0270cd0006d407-Paper.pdf,Greedy Sampling for Approximate Clustering in the Presence of Outliers,"Aditya Bhaskara, Sharvaree Vadgama, Hong Xu",
neurips,https://proceedings.neurips.cc/paper/2019/file/73bf6c41e241e28b89d0fb9e0c82f9ce-Paper.pdf,Understanding the Representation Power of Graph Neural Networks in Learning Graph Topology,"Nima Dehmamy, Albert-Laszlo Barabasi, Rose Yu",
neurips,https://proceedings.neurips.cc/paper/2019/file/73c03186765e199c116224b68adc5fa0-Paper.pdf,Single-Model Uncertainties for Deep Learning,"Natasa Tagasovska, David Lopez-Paz",
neurips,https://proceedings.neurips.cc/paper/2019/file/73e0f7487b8e5297182c5a711d20bf26-Paper.pdf,The Fairness of Risk Scores Beyond Classification: Bipartite Ranking and the XAUC Metric,"Nathan Kallus, Angela Zhou",
neurips,https://proceedings.neurips.cc/paper/2019/file/73f104c9fba50050eea11d9d075247cc-Paper.pdf,Robust Principal Component Analysis with Adaptive Neighbors,"Rui Zhang, Hanghang Tong",
neurips,https://proceedings.neurips.cc/paper/2019/file/73fed7fd472e502d8908794430511f4d-Paper.pdf,Wasserstein Weisfeiler-Lehman Graph Kernels,"Matteo Togninalli, Elisabetta Ghisu, Felipe Llinares-López, Bastian Rieck, Karsten Borgwardt",
neurips,https://proceedings.neurips.cc/paper/2019/file/74071a673307ca7459bcf75fbd024e09-Paper.pdf,DATA: Differentiable ArchiTecture Approximation,"Jianlong Chang, xinbang zhang, Yiwen Guo, GAOFENG MENG, SHIMING XIANG, Chunhong Pan",
neurips,https://proceedings.neurips.cc/paper/2019/file/742141ceda6b8f6786609d31c8ef129f-Paper.pdf,Near Neighbor: Who is the Fairest of Them All?,"Sariel Har-Peled, Sepideh Mahabadi","In this work we study a ""fair"" variant of the near neighbor problem. Namely, given a set of
n
n
points
P
P
and a parameter
r
r
, the goal is to preprocess the points, such that given a query point
q
q
, any point in the
r
r
-neighborhood of the query, i.e.,
B
(
q
,
r
)
B
, have the same probability of being reported as the near neighbor. We show that LSH based algorithms can be made fair, without a significant loss in efficiency. Specifically, we show an algorithm that reports a point
p
p
in the
r
r
-neighborhood of a query
q
q
with almost uniform probability. The time to report such a point is proportional to
O
(
\dns
(
q
.
r
)
Q
(
n
,
c
)
)
O
, and its space is
O
(
S
(
n
,
c
)
)
O
, where
Q
(
n
,
c
)
Q
and
S
(
n
,
c
)
S
are the query time and space of an LSH algorithm for
c
c
-approximate near neighbor, and
\dns
(
q
,
r
)
\dns
is a function of the local density around
q
q
. Our approach works more generally for sampling uniformly from a sub-collection of sets of a given collection and can be used in a few other applications. Finally, we run experiments to show performance of our approach on real data."
neurips,https://proceedings.neurips.cc/paper/2019/file/74249bfb363306265299ac4ec44d3cb6-Paper.pdf,"Unsupervised Co-Learning on
G
G
-Manifolds Across Irreducible Representations","Yifeng Fan, Tingran Gao, Zhizhen Jane Zhao","We introduce a novel co-learning paradigm for manifolds naturally admitting an action of a transformation group
G
G
, motivated by recent developments on learning a manifold from attached fibre bundle structures. We utilize a representation theoretic mechanism that canonically associates multiple independent vector bundles over a common base manifold, which provides multiple views for the geometry of the underlying manifold. The consistency across these fibre bundles provide a common base for performing unsupervised manifold co-learning through the redundancy created artificially across irreducible representations of the transformation group. We demonstrate the efficacy of our proposed algorithmic paradigm through drastically improved robust nearest neighbor identification in cryo-electron microscopy image analysis and the clustering accuracy in community detection."
neurips,https://proceedings.neurips.cc/paper/2019/file/743c41a921516b04afde48bb48e28ce6-Paper.pdf,Fast Efficient Hyperparameter Tuning for Policy Gradient Methods,"Supratik Paul, Vitaly Kurin, Shimon Whiteson",
neurips,https://proceedings.neurips.cc/paper/2019/file/74563ba21a90da13dacf2a73e3ddefa7-Paper.pdf,Fast Structured Decoding for Sequence Models,"Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di He, Zi Lin, Zhihong Deng",
neurips,https://proceedings.neurips.cc/paper/2019/file/7486cef2522ee03547cfb970a404a874-Paper.pdf,Efficiently escaping saddle points on manifolds,"Christopher Criscitiello, Nicolas Boumal","Smooth, non-convex optimization problems on Riemannian manifolds occur in machine learning as a result of orthonormality, rank or positivity constraints. First- and second-order necessary optimality conditions state that the Riemannian gradient must be zero, and the Riemannian Hessian must be positive semidefinite. Generalizing Jin et al.'s recent work on perturbed gradient descent (PGD) for optimization on linear spaces [How to Escape Saddle Points Efficiently (2017), Stochastic Gradient Descent Escapes Saddle Points Efficiently (2019)], we study a version of perturbed Riemannian gradient descent (PRGD) to show that necessary optimality conditions can be met approximately with high probability, without evaluating the Hessian. Specifically, for an arbitrary Riemannian manifold
M
M
of dimension
d
d
, a sufficiently smooth (possibly non-convex) objective function
f
f
, and under weak conditions on the retraction chosen to move on the manifold, with high probability, our version of PRGD produces a point with gradient smaller than
ϵ
ϵ
and Hessian within
√
ϵ
ϵ
of being positive semidefinite in
O
(
(
log
d
)
4
/
ϵ
2
)
O
gradient queries. This matches the complexity of PGD in the Euclidean case. Crucially, the dependence on dimension is low, which matters for large-scale applications including PCA and low-rank matrix completion, which both admit natural formulations on manifolds. The key technical idea is to generalize PRGD with a distinction between two types of gradient steps:
steps on the manifold'' and
perturbed steps in a tangent space of the manifold.'' Ultimately, this distinction makes it possible to extend Jin et al.'s analysis seamlessly."
neurips,https://proceedings.neurips.cc/paper/2019/file/748d6b6ed8e13f857ceaa6cfbdca14b8-Paper.pdf,Comparison Against Task Driven Artificial Neural Networks Reveals Functional Properties in Mouse Visual Cortex,"Jianghong Shi, Eric Shea-Brown, Michael Buice",
neurips,https://proceedings.neurips.cc/paper/2019/file/749a8e6c231831ef7756db230b4359c8-Paper.pdf,Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain),"Mariya Toneva, Leila Wehbe",
neurips,https://proceedings.neurips.cc/paper/2019/file/7503cfacd12053d309b6bed5c89de212-Paper.pdf,Adversarial training for free!,"Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S. Davis, Gavin Taylor, Tom Goldstein",
neurips,https://proceedings.neurips.cc/paper/2019/file/7504adad8bb96320eb3afdd4df6e1f60-Paper.pdf,Guided Similarity Separation for Image Retrieval,"Chundi Liu, Guangwei Yu, Maksims Volkovs, Cheng Chang, Himanshu Rai, Junwei Ma, Satya Krishna Gorti",
neurips,https://proceedings.neurips.cc/paper/2019/file/75455e062929d32a333868084286bb68-Paper.pdf,Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks,"Lixin Fan, Kam Woh Ng, Chee Seng Chan",
neurips,https://proceedings.neurips.cc/paper/2019/file/757f843a169cc678064d9530d12a1881-Paper.pdf,Addressing Failure Prediction by Learning Model Confidence,"Charles Corbière, Nicolas THOME, Avner Bar-Hen, Matthieu Cord, Patrick Pérez",
neurips,https://proceedings.neurips.cc/paper/2019/file/75da5036f659fe64b53f3d9b39412967-Paper.pdf,Communication-efficient Distributed SGD with Sketching,"Nikita Ivkin, Daniel Rothchild, Enayat Ullah, Vladimir braverman, Ion Stoica, Raman Arora","Large-scale distributed training of neural networks is often limited by network bandwidth, wherein the communication time overwhelms the local computation time. Motivated by the success of sketching methods in sub-linear/streaming algorithms, we introduce Sketched-SGD, an algorithm for carrying out distributed SGD by communicating sketches instead of full gradients. We show that \ssgd has favorable convergence rates on several classes of functions. When considering all communication -- both of gradients and of updated model weights -- Sketched-SGD reduces the amount of communication required compared to other gradient compression methods from
O
(
d
)
O
or
O
(
W
)
O
to
O
(
log
d
)
O
, where
d
d
is the number of model parameters and
W
W
is the number of workers participating in training. We run experiments on a transformer model, an LSTM, and a residual network, demonstrating up to a 40x reduction in total communication cost with no loss in final model performance. We also show experimentally that Sketched-SGD scales to at least 256 workers without increasing communication cost or degrading model performance."
neurips,https://proceedings.neurips.cc/paper/2019/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf,Multivariate Sparse Coding of Nonstationary Covariances with Gaussian Processes,Rui Li,
neurips,https://proceedings.neurips.cc/paper/2019/file/767d01b4bac1a1e8824c9b9f7cc79a04-Paper.pdf,Exponential Family Estimation via Adversarial Dynamics Embedding,"Bo Dai, Zhen Liu, Hanjun Dai, Niao He, Arthur Gretton, Le Song, Dale Schuurmans",
neurips,https://proceedings.neurips.cc/paper/2019/file/7690dd4db7a92524c684e3191919eb6b-Paper.pdf,Group Retention when Using Machine Learning in Sequential Decision Making: the Interplay between User Dynamics and Fairness,"Xueru Zhang, Mohammadmahdi Khaliligarekani, Cem Tekin, mingyan liu",
neurips,https://proceedings.neurips.cc/paper/2019/file/76d7c0780ceb8fbf964c102ebc16d75f-Paper.pdf,Shallow RNN: Accurate Time-series Classification on Resource Constrained Devices,"Don Dennis, Durmus Alp Emre Acar, Vikram Mandikal, Vinu Sankar Sadasivan, Venkatesh Saligrama, Harsha Vardhan Simhadri, Prateek Jain",
neurips,https://proceedings.neurips.cc/paper/2019/file/770f8e448d07586afbf77bb59f698587-Paper.pdf,Neural Networks with Cheap Differential Operators,"Ricky T. Q. Chen, David K. Duvenaud",
neurips,https://proceedings.neurips.cc/paper/2019/file/7716d0fc31636914783865d34f6cdfd5-Paper.pdf,Towards Understanding the Importance of Shortcut Connections in Residual Networks,"Tianyi Liu, Minshuo Chen, Mo Zhou, Simon S. Du, Enlu Zhou, Tuo Zhao",
neurips,https://proceedings.neurips.cc/paper/2019/file/77cdfc1e11e36a23bb030892ee00b8cf-Paper.pdf,A Polynomial Time Algorithm for Log-Concave Maximum Likelihood via Locally Exponential Families,"Brian Axelrod, Ilias Diakonikolas, Alistair Stewart, Anastasios Sidiropoulos, Gregory Valiant","We consider the problem of computing the maximum likelihood multivariate log-concave distribution for a set of points. Specifically, we present an algorithm which, given
n
n
points in
R
d
R
and an accuracy parameter
\eps
>
0
\eps
, runs in time
\poly
(
n
,
d
,
1
/
\eps
)
,
\poly
and returns a log-concave distribution which, with high probability, has the property that the likelihood of the
n
n
points under the returned distribution is at most an additive
\eps
\eps
less than the maximum likelihood that could be achieved via any log-concave distribution. This is the first computationally efficient (polynomial time) algorithm for this fundamental and practically important task. Our algorithm rests on a novel connection with exponential families: the maximum likelihood log-concave distribution belongs to a class of structured distributions which, while not an exponential family,
locally'' possesses key properties of exponential families. This connection then allows the problem of computing the log-concave maximum likelihood distribution to be formulated as a convex optimization problem, and solved via an approximate first-order method. Efficiently approximating the (sub) gradients of the objective function of this optimization problem is quite delicate, and is the main technical challenge in this work."
neurips,https://proceedings.neurips.cc/paper/2019/file/77d2afcb31f6493e350fca61764efb9a-Paper.pdf,Towards Automatic Concept-based Explanations,"Amirata Ghorbani, James Wexler, James Y. Zou, Been Kim",
neurips,https://proceedings.neurips.cc/paper/2019/file/7813d1590d28a7dd372ad54b5d29d033-Paper.pdf,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs,"Jonas Kubilius, Martin Schrimpf, Kohitij Kar, Rishi Rajalingham, Ha Hong, Najib Majaj, Elias Issa, Pouya Bashivan, Jonathan Prescott-Roy, Kailyn Schmidt, Aran Nayebi, Daniel Bear, Daniel L. Yamins, James J. DiCarlo",
neurips,https://proceedings.neurips.cc/paper/2019/file/78211247db84d96acf4e00092a7fba80-Paper.pdf,Defending Neural Backdoors via Generative Distribution Modeling,"Ximing Qiao, Yukun Yang, Hai Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/785ca71d2c85e3f3774baaf438c5c6eb-Paper.pdf,Correlation clustering with local objectives,"Sanchit Kalhan, Konstantin Makarychev, Timothy Zhou","Recently, Puleo and Milenkovic (ICML '16) initiated the study of the Correlation Clustering framework in which the objectives were more general functions of the disagreements vector. In this paper, we study algorithms for minimizing \ellq norms (q >= 1) of the disagreements vector for both arbitrary and complete graphs. We present the first known algorithm for minimizing the \ellq norm of the disagreements vector on arbitrary graphs and also provide an improved algorithm for minimizing the \ell_q norm (q >= 1) of the disagreements vector on complete graphs. We also study an alternate cluster-wise local objective introduced by Ahmadi, Khuller and Saha (IPCO '19), which aims to minimize the maximum number of disagreements associated with a cluster. We present an improved (2 + \eps) approximation algorithm for this objective."
neurips,https://proceedings.neurips.cc/paper/2019/file/78719f11fa2df9917de3110133506521-Paper.pdf,Logarithmic Regret for Online Control,"Naman Agarwal, Elad Hazan, Karan Singh","We show that the optimal regret in this fundamental setting can be significantly smaller, scaling as polylog(T). This regret bound is achieved by two different efficient iterative methods, online gradient descent and online natural gradient."
neurips,https://proceedings.neurips.cc/paper/2019/file/7876acb66640bad41f1e1371ef30c180-Paper.pdf,Offline Contextual Bayesian Optimization,"Ian Char, Youngseog Chung, Willie Neiswanger, Kirthevasan Kandasamy, Andrew Oakleigh Nelson, Mark Boyer, Egemen Kolemen, Jeff Schneider",
neurips,https://proceedings.neurips.cc/paper/2019/file/7895fc13088ee37f511913bac71fa66f-Paper.pdf,Transfer Anomaly Detection by Inferring Latent Domain Representations,"Atsutoshi Kumagai, Tomoharu Iwata, Yasuhiro Fujiwara",
neurips,https://proceedings.neurips.cc/paper/2019/file/78efce208a5242729d222e7e6e3e565e-Paper.pdf,Uncertainty on Asynchronous Time Event Prediction,"Marin Biloš, Bertrand Charpentier, Stephan Günnemann",
neurips,https://proceedings.neurips.cc/paper/2019/file/78f7d96ea21ccae89a7b581295f34135-Paper.pdf,Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces,"Chuan Guo, Ali Mousavi, Xiang Wu, Daniel N. Holtmann-Rice, Satyen Kale, Sashank Reddi, Sanjiv Kumar",
neurips,https://proceedings.neurips.cc/paper/2019/file/791d3a0048b9c200dceca07f99ddd178-Paper.pdf,Faster width-dependent algorithm for mixed packing and covering LPs,"Digvijay Boob, Saurabh Sawlani, Di Wang","In this paper, we give a faster width-dependent algorithm for mixed packing-covering LPs. Mixed packing-covering LPs are fundamental to combinatorial optimization in computer science and operations research. Our algorithm finds a
1
+
\eps
1
approximate solution in time
O
(
N
w
/
ε
)
O
, where
N
N
is number of nonzero entries in the constraint matrix, and
w
w
is the maximum number of nonzeros in any constraint. This algorithm is faster than Nesterov's smoothing algorithm which requires
O
(
N
√
n
w
/
\eps
)
O
time, where
n
n
is the dimension of the problem. Our work utilizes the framework of area convexity introduced in [Sherman-FOCS'17] to obtain the best dependence on
ε
ε
while breaking the infamous
ℓ
∞
ℓ
barrier to eliminate the factor of
√
n
n
. The current best width-independent algorithm for this problem runs in time
O
(
N
/
\eps
2
)
O
[Young-arXiv-14] and hence has worse running time dependence on
ε
ε
. Many real life instances of mixed packing-covering problems exhibit small width and for such cases, our algorithm can report higher precision results when compared to width-independent algorithms. As a special case of our result, we report a
1
+
ε
1
approximation algorithm for the densest subgraph problem which runs in time
O
(
m
d
/
ε
)
O
, where
m
m
is the number of edges in the graph and
d
d
is the maximum graph degree."
neurips,https://proceedings.neurips.cc/paper/2019/file/7967cc8e3ab559e68cc944c44b1cf3e8-Paper.pdf,Hierarchical Decision Making by Generating and Following Natural Language Instructions,"Hengyuan Hu, Denis Yarats, Qucheng Gong, Yuandong Tian, Mike Lewis",
neurips,https://proceedings.neurips.cc/paper/2019/file/7990ec44fcf3d7a0e5a2add28362213c-Paper.pdf,Structured Prediction with Projection Oracles,Mathieu Blondel,
neurips,https://proceedings.neurips.cc/paper/2019/file/79a3308b13cd31f096d8a4a34f96b66b-Paper.pdf,Sobolev Independence Criterion,"Youssef Mroueh, Tom Sercu, Mattia Rigotti, Inkit Padhi, Cicero Nogueira dos Santos",
neurips,https://proceedings.neurips.cc/paper/2019/file/7a2b33c672ce223b2aa5789171ddde2f-Paper.pdf,Accelerating Rescaled Gradient Descent: Fast Optimization of Smooth Functions,"Ashia C. Wilson, Lester Mackey, Andre Wibisono",
neurips,https://proceedings.neurips.cc/paper/2019/file/7a674153c63cff1ad7f0e261c369ab2c-Paper.pdf,Minimax Optimal Estimation of Approximate Differential Privacy on Neighboring Databases,"Xiyang Liu, Sewoong Oh",
neurips,https://proceedings.neurips.cc/paper/2019/file/7a9a322cbe0d06a98667fdc5160dc6f8-Paper.pdf,Reconciling meta-learning and continual learning with online mixtures of tasks,"Ghassen Jerfel, Erin Grant, Tom Griffiths, Katherine A. Heller",
neurips,https://proceedings.neurips.cc/paper/2019/file/7ac71d433f282034e088473244df8c02-Paper.pdf,Neural Spline Flows,"Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios",
neurips,https://proceedings.neurips.cc/paper/2019/file/7b66b4fd401a271a1c7224027ce111bc-Paper.pdf,Embedding Symbolic Knowledge into Deep Networks,"Yaqi Xie, Ziwei Xu, Mohan S. Kankanhalli, Kuldeep S Meel, Harold Soh",
neurips,https://proceedings.neurips.cc/paper/2019/file/7bb060764a818184ebb1cc0d43d382aa-Paper.pdf,Partitioning Structure Learning for Segmented Linear Regression Trees,"Xiangyu Zheng, Song Xi Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/7bec7e63a493e2d61891b1e4051ef75a-Paper.pdf,Sparse Variational Inference: Bayesian Coresets from Scratch,"Trevor Campbell, Boyan Beronov",
neurips,https://proceedings.neurips.cc/paper/2019/file/7c4bf50b715509a963ce81b168ca674b-Paper.pdf,Policy Evaluation with Latent Confounders via Optimal Balance,"Andrew Bennett, Nathan Kallus",
neurips,https://proceedings.neurips.cc/paper/2019/file/7ca57a9f85a19a6e4b9a248c1daca185-Paper.pdf,Dancing to Music,"Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun Wang, Yu-Ding Lu, Ming-Hsuan Yang, Jan Kautz",
neurips,https://proceedings.neurips.cc/paper/2019/file/7d12b66d3df6af8d429c1a357d8b9e1a-Paper.pdf,Learning Hierarchical Priors in VAEs,"Alexej Klushyn, Nutan Chen, Richard Kurle, Botond Cseke, Patrick van der Smagt",
neurips,https://proceedings.neurips.cc/paper/2019/file/7d265aa7147bd3913fb84c7963a209d1-Paper.pdf,Stochastic Runge-Kutta Accelerates Langevin Monte Carlo and Beyond,"Xuechen Li, Yi Wu, Lester Mackey, Murat A. Erdogdu","Sampling with Markov chain Monte Carlo methods typically amounts to discretizing some continuous-time dynamics with numerical integration. In this paper, we establish the convergence rate of sampling algorithms obtained by discretizing smooth It\^o diffusions exhibiting fast
2
2
-Wasserstein contraction, based on local deviation properties of the integration scheme. In particular, we study a sampling algorithm constructed by discretizing the overdamped Langevin diffusion with the method of stochastic Runge-Kutta. For strongly convex potentials that are smooth up to a certain order, its iterates converge to the target distribution in
2
2
-Wasserstein distance in
~
O
(
d
ϵ
−
2
/
3
)
O
iterations. This improves upon the best-known rate for strongly log-concave sampling based on the overdamped Langevin equation using only the gradient oracle without adjustment. Additionally, we extend our analysis of stochastic Runge-Kutta methods to uniformly dissipative diffusions with possibly non-convex potentials and show they achieve better rates compared to the Euler-Maruyama scheme on the dependence on tolerance
ϵ
ϵ
. Numerical studies show that these algorithms lead to better stability and lower asymptotic errors."
neurips,https://proceedings.neurips.cc/paper/2019/file/7d2be41b1bde6ff8fe45150c37488ebb-Paper.pdf,From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI,"Roman Beliy, Guy Gaziv, Assaf Hoogi, Francesca Strappini, Tal Golan, Michal Irani",
neurips,https://proceedings.neurips.cc/paper/2019/file/7d6044e95a16761171b130dcb476a43e-Paper.pdf,Direct Estimation of Differential Functional Graphical Models,"Boxin Zhao, Y. Samuel Wang, Mladen Kolar",
neurips,https://proceedings.neurips.cc/paper/2019/file/7dd0240cd412efde8bc165e864d3644f-Paper.pdf,Backpropagation-Friendly Eigendecomposition,"Wei Wang, Zheng Dang, Yinlin Hu, Pascal Fua, Mathieu Salzmann",
neurips,https://proceedings.neurips.cc/paper/2019/file/7dd2ae7db7d18ee7c9425e38df1af5e2-Paper.pdf,Reverse KL-Divergence Training of Prior Networks: Improved Uncertainty and Adversarial Robustness,"Andrey Malinin, Mark Gales",
neurips,https://proceedings.neurips.cc/paper/2019/file/7e1cacfb27da22fb243ff2debf4443a0-Paper.pdf,Adversarial Fisher Vectors for Unsupervised Representation Learning,"Shuangfei Zhai, Walter Talbott, Carlos Guestrin, Joshua Susskind",
neurips,https://proceedings.neurips.cc/paper/2019/file/7e3315fe390974fcf25e44a9445bd821-Paper.pdf,Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse,"James Lucas, George Tucker, Roger B. Grosse, Mohammad Norouzi",
neurips,https://proceedings.neurips.cc/paper/2019/file/7e9e346dc5fd268b49bf418523af8679-Paper.pdf,Kernel-Based Approaches for Sequence Modeling: Connections to Neural Methods,"Kevin Liang, Guoyin Wang, Yitong Li, Ricardo Henao, Lawrence Carin","We investigate time-dependent data analysis from the perspective of recurrent kernel machines, from which models with hidden units and gated memory cells arise naturally. By considering dynamic gating of the memory cell, a model closely related to the long short-term memory (LSTM) recurrent neural network is derived. Extending this setup to
n
n
-gram filters, the convolutional neural network (CNN), Gated CNN, and recurrent additive network (RAN) are also recovered as special cases. Our analysis provides a new perspective on the LSTM, while also extending it to
n
n
-gram convolutional filters. Experiments are performed on natural language processing tasks and on analysis of local field potentials (neuroscience). We demonstrate that the variants we derive from kernels perform on par or even better than traditional neural methods. For the neuroscience application, the new models demonstrate significant improvements relative to the prior state of the art."
neurips,https://proceedings.neurips.cc/paper/2019/file/7eacb532570ff6858afd2723755ff790-Paper.pdf,Efficient Symmetric Norm Regression via Linear Sketching,"Zhao Song, Ruosong Wang, Lin Yang, Hongyang Zhang, Peilin Zhong","We provide efficient algorithms for overconstrained linear regression problems with size
n
×
d
n
when the loss function is a symmetric norm (a norm invariant under sign-flips and coordinate-permutations). An important class of symmetric norms are Orlicz norms, where for a function
G
G
and a vector
y
∈
R
n
y
, the corresponding Orlicz norm
∥
y
∥
G
‖
is defined as the unique value
α
α
such that
∑
n
i
=
1
G
(
|
y
i
|
/
α
)
=
1
∑
. When the loss function is an Orlicz norm, our algorithm produces a
(
1
+
ε
)
(
-approximate solution for an arbitrarily small constant
ε
>
0
ε
in input-sparsity time, improving over the previously best-known algorithm which produces a
d
⋅
\polylog
n
d
-approximate solution. When the loss function is a general symmetric norm, our algorithm produces a
√
d
⋅
\polylog
n
⋅
m
m
c
(
ℓ
)
d
-approximate solution in input-sparsity time, where
m
m
c
(
ℓ
)
m
is a quantity related to the symmetric norm under consideration. To the best of our knowledge, this is the first input-sparsity time algorithm with provable guarantees for the general class of symmetric norm regression problem. Our results shed light on resolving the universal sketching problem for linear regression, and the techniques might be of independent interest to numerical linear algebra problems more broadly."
neurips,https://proceedings.neurips.cc/paper/2019/file/7ec3b3cf674f4f1d23e9d30c89426cce-Paper.pdf,Dichotomize and Generalize: PAC-Bayesian Binary Activated Deep Neural Networks,"Gaël Letarte, Pascal Germain, Benjamin Guedj, Francois Laviolette",
neurips,https://proceedings.neurips.cc/paper/2019/file/7eea1f266bfc82028683ad15da46e05e-Paper.pdf,Approximate Feature Collisions in Neural Nets,"Ke Li, Tianhao Zhang, Jitendra Malik",
neurips,https://proceedings.neurips.cc/paper/2019/file/7f018eb7b301a66658931cb8a93fd6e8-Paper.pdf,Characterizing Bias in Classifiers using Generative Models,"Daniel McDuff, Shuang Ma, Yale Song, Ashish Kapoor",
neurips,https://proceedings.neurips.cc/paper/2019/file/7f278ad602c7f47aa76d1bfc90f20263-Paper.pdf,Coresets for Archetypal Analysis,"Sebastian Mair, Ulf Brefeld",
neurips,https://proceedings.neurips.cc/paper/2019/file/7f5fc754c7af0a6370c9bf91314e79f4-Paper.pdf,List-decodable Linear Regression,"Sushrut Karmalkar, Adam Klivans, Pravesh Kothari","For any \alpha < 1, our algorithm takes as input a sample {(xi,yi)}{i \leq n} of n linear equations where \alpha n of the equations satisfy yi = \langle x_i,\ell^\rangle +\zeta for some small noise \zeta and (1-\alpha) n of the equations are {\em arbitrarily} chosen. It outputs a list L of size O(1/\alpha) - a fixed constant - that contains an \ell that is close to \ell^."
neurips,https://proceedings.neurips.cc/paper/2019/file/7f6caf1f0ba788cd7953d817724c2b6e-Paper.pdf,Infra-slow brain dynamics as a marker for cognitive function and decline,"Shagun Ajmera, Shreya Rajagopal, Razi Rehman, Devarajan Sridharan",
neurips,https://proceedings.neurips.cc/paper/2019/file/7fea637fd6d02b8f0adf6f7dc36aed93-Paper.pdf,Fooling Neural Network Interpretations via Adversarial Model Manipulation,"Juyeon Heo, Sunghwan Joo, Taesup Moon",
neurips,https://proceedings.neurips.cc/paper/2019/file/7ffb4e0ece07869880d51662a2234143-Paper.pdf,Maximum Entropy Monte-Carlo Planning,"Chenjun Xiao, Ruitong Huang, Jincheng Mei, Dale Schuurmans, Martin Müller",
neurips,https://proceedings.neurips.cc/paper/2019/file/800b03685c22049f049801f6841861a2-Paper.pdf,Unified Sample-Optimal Property Estimation in Near-Linear Time,"Yi Hao, Alon Orlitsky","We consider the fundamental learning problem of estimating properties of distributions over large domains. Using a novel piecewise-polynomial approximation technique, we derive the first unified methodology for constructing sample- and time-efficient estimators for all sufficiently smooth, symmetric and non-symmetric, additive properties. This technique yields near-linear-time computable estimators whose approximation values are asymptotically optimal and highly-concentrated, resulting in the first: 1) estimators achieving the
O
(
k
/
(
ε
2
log
k
)
)
O
min-max
ε
ε
-error sample complexity for all
k
k
-symbol Lipschitz properties; 2) unified near-optimal differentially private estimators for a variety of properties; 3) unified estimator achieving optimal bias and near-optimal variance for five important properties; 4) near-optimal sample-complexity estimators for several important symmetric properties over both domain sizes and confidence levels."
neurips,https://proceedings.neurips.cc/paper/2019/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf,Unsupervised Keypoint Learning for Guiding Class-Conditional Video Prediction,"Yunji Kim, Seonghyeon Nam, In Cho, Seon Joo Kim",
neurips,https://proceedings.neurips.cc/paper/2019/file/805163a0f0f128e473726ccda5f91bac-Paper.pdf,Statistical Analysis of Nearest Neighbor Methods for Anomaly Detection,"Xiaoyi Gu, Leman Akoglu, Alessandro Rinaldo",
neurips,https://proceedings.neurips.cc/paper/2019/file/80537a945c7aaa788ccfcdf1b99b5d8f-Paper.pdf,Full-Gradient Representation for Neural Network Visualization,"Suraj Srinivas, François Fleuret","We experimentally evaluate the usefulness of FullGrad in explaining model behaviour with two quantitative tests: pixel perturbation and remove-and-retrain. Our experiments reveal that our method explains model behavior correctly, and more comprehensively than other methods in the literature. Visual inspection also reveals that our saliency maps are sharper and more tightly confined to object regions than other methods."
neurips,https://proceedings.neurips.cc/paper/2019/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf,Learnable Tree Filter for Structure-preserving Feature Transform,"Lin Song, Yanwei Li, Zeming Li, Gang Yu, Hongbin Sun, Jian Sun, Nanning Zheng",
neurips,https://proceedings.neurips.cc/paper/2019/file/80a160ff31266be2f93012a2a3eca713-Paper.pdf,The Implicit Metropolis-Hastings Algorithm,"Kirill Neklyudov, Evgenii Egorov, Dmitry P. Vetrov",
neurips,https://proceedings.neurips.cc/paper/2019/file/80a8155eb153025ea1d513d0b2c4b675-Paper.pdf,Optimal Analysis of Subset-Selection Based L_p Low-Rank Approximation,"Chen Dan, Hong Wang, Hongyang Zhang, Yuchen Zhou, Pradeep K. Ravikumar","We show that for the problem of
ℓ
p
ℓ
rank-
k
k
approximation of any given matrix over
R
n
×
m
R
and
C
n
×
m
C
, the algorithm of column subset selection enjoys approximation ratio
(
k
+
1
)
1
/
p
(
for
1
≤
p
≤
2
1
and
(
k
+
1
)
1
−
1
/
p
(
for
p
≥
2
p
. This improves upon the previous
O
(
k
+
1
)
O
bound (Chierichetti et al.,2017) for
p
≥
1
p
. We complement our analysis with lower bounds; these bounds match our upper bounds up to constant 1 when
p
≥
2
p
. At the core of our techniques is an application of Riesz-Thorin interpolation theorem from harmonic analysis, which might be of independent interest to other algorithmic designs and analysis more broadly. Our analysis results in improvements on approximation guarantees of several other algorithms with various time complexity. For example, to make the algorithm of column subset selection computationally efficient, we analyze a polynomial time bi-criteria algorithm which selects
O
(
k
log
m
)
O
number of columns. We show that this algorithm has an approximation ratio of
O
(
(
k
+
1
)
1
/
p
)
O
for
1
≤
p
≤
2
1
and
O
(
(
k
+
1
)
1
−
1
/
p
)
O
for
p
≥
2
p
. This improves over the bound in (Chierichetti et al.,2017) with an
O
(
k
+
1
)
O
approximation ratio. Our bi-criteria algorithm also implies an exact-rank method in polynomial time with a slightly larger approximation ratio."
neurips,https://proceedings.neurips.cc/paper/2019/file/80c0e8c4457441901351e4abbcf8c75c-Paper.pdf,Communication-Efficient Distributed Blockwise Momentum SGD with Error-Feedback,"Shuai Zheng, Ziyue Huang, James Kwok","Communication overhead is a major bottleneck hampering the scalability of distributed machine learning systems. Recently, there has been a surge of interest in using gradient compression to improve the communication efficiency of distributed neural network training. Using 1-bit quantization, signSGD with majority vote achieves a 32x reduction in communication cost. However, its convergence is based on unrealistic assumptions and can diverge in practice. In this paper, we propose a general distributed compressed SGD with Nesterov's momentum. We consider two-way compression, which compresses the gradients both to and from workers. Convergence analysis on nonconvex problems for general gradient compressors is provided. By partitioning the gradient into blocks, a blockwise compressor is introduced such that each gradient block is compressed and transmitted in 1-bit format with a scaling factor, leading to a nearly 32x reduction on communication. Experimental results show that the proposed method converges as fast as full-precision distributed momentum SGD and achieves the same testing accuracy. In particular, on distributed ResNet training with 7 workers on the ImageNet, the proposed algorithm achieves the same testing accuracy as momentum SGD using full-precision gradients, but with
46
%
46
less wall clock time."
neurips,https://proceedings.neurips.cc/paper/2019/file/810dfbbebb17302018ae903e9cb7a483-Paper.pdf,Coresets for Clustering with Fairness Constraints,"Lingxiao Huang, Shaofeng Jiang, Nisheeth Vishnoi",
neurips,https://proceedings.neurips.cc/paper/2019/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf,You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle,"Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, Bin Dong","Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing. However, recent works have shown that deep networks can be vulnerable to adversarial perturbations which raised a serious robustness issue of deep networks. Adversarial training, typically formulated as a robust optimization problem, is an effective way of improving the robustness of deep networks. A major drawback of existing adversarial training algorithms is the computational overhead of the generation of adversarial examples, typically far greater than that of the network training. This leads to unbearable overall computational cost of adversarial training. In this paper, we show that adversarial training can be cast as a discrete time differential game. Through analyzing the Pontryagin’s Maximum Principle (PMP) of the problem, we observe that the adversary update is only coupled with the parameters of the first layer of the network. This inspires us to restrict most of the forward and back propagation within the first layer of the network during adversary updates. This effectively reduces the total number of full forward and backward propagation to only one for each group of adversary updates. Therefore, we refer to this algorithm YOPO (\textbf{Y}ou \textbf{O}nly \textbf{P}ropagate \textbf{O}nce). Numerical experiments demonstrate that YOPO can achieve comparable defense accuracy with \textbf{approximately 1/5
∼
∼
1/4 GPU time} of the projected gradient descent (PGD) algorithm~\cite{kurakin2016adversarial}."
neurips,https://proceedings.neurips.cc/paper/2019/file/8133415ea4647b6345849fb38311cf32-Paper.pdf,On the Hardness of Robust Classification,"Pascale Gourdeau, Varun Kanade, Marta Kwiatkowska, James Worrell","It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks. In this paper we study the feasibility of robust learning from the perspective of computational learning theory, considering both sample and computational complexity. In particular, our definition of robust learnability requires polynomial sample complexity. We start with two negative results. We show that no non-trivial concept class can be robustly learned in the distribution-free setting against an adversary who can perturb just a single input bit. We show moreover that the class of monotone conjunctions cannot be robustly learned under the uniform distribution against an adversary who can perturb
ω
(
log
n
)
ω
input bits. However if the adversary is restricted to perturbing
O
(
log
n
)
O
bits, then the class of monotone conjunctions can be robustly learned with respect to a general class of distributions (that includes the uniform distribution). Finally, we provide a simple proof of the computational hardness of robust learning on the boolean hypercube. Unlike previous results of this nature, our result does not rely on another computational model (e.g. the statistical query model) nor on any hardness assumption other than the existence of a hard learning problem in the PAC framework."
neurips,https://proceedings.neurips.cc/paper/2019/file/816a6db41f0e44644bc65808b6db5ca4-Paper.pdf,Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates,"Carlos Riquelme, Hugo Penedones, Damien Vincent, Hartmut Maennel, Sylvain Gelly, Timothy A. Mann, Andre Barreto, Gergely Neu",
neurips,https://proceedings.neurips.cc/paper/2019/file/81e74d678581a3bb7a720b019f4f1a93-Paper.pdf,Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards,"Siyuan Li, Rui Wang, Minxue Tang, Chongjie Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/82161242827b703e6acf9c726942a1e4-Paper.pdf,Chasing Ghosts: Instruction Following as Bayesian State Tracking,"Peter Anderson, Ayush Shrivastava, Devi Parikh, Dhruv Batra, Stefan Lee",
neurips,https://proceedings.neurips.cc/paper/2019/file/8252831b9fce7a49421e622c14ce0f65-Paper.pdf,Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes,"Junzhe Zhang, Elias Bareinboim",
neurips,https://proceedings.neurips.cc/paper/2019/file/82965d4ed8150294d4330ace00821d77-Paper.pdf,Rethinking the CSC Model for Natural Images,"Dror Simon, Michael Elad",
neurips,https://proceedings.neurips.cc/paper/2019/file/82aa4b0af34c2313a562076992e50aa3-Paper.pdf,Divide and Couple: Using Monte Carlo Variational Objectives for Posterior Approximation,"Justin Domke, Daniel R. Sheldon",
neurips,https://proceedings.neurips.cc/paper/2019/file/82c2559140b95ccda9c6ca4a8b981f1e-Paper.pdf,Numerically Accurate Hyperbolic Embeddings Using Tiling-Based Models,"Tao Yu, Christopher M. De Sa","Hyperbolic embeddings achieve excellent performance when embedding hierarchical data structures like synonym or type hierarchies, but they can be limited by numerical error when ordinary floating-point numbers are used to represent points in hyperbolic space. Standard models such as the Poincar{\'e} disk and the Lorentz model have unbounded numerical error as points get far from the origin. To address this, we propose a new model which uses an integer-based tiling to represent \emph{any} point in hyperbolic space with provably bounded numerical error. This allows us to learn high-precision embeddings without using BigFloats, and enables us to store the resulting embeddings with fewer bits. We evaluate our tiling-based model empirically, and show that it can both compress hyperbolic embeddings (down to
2
%
2
of a Poincar{\'e} embedding on WordNet Nouns) and learn more accurate embeddings on real-world datasets."
neurips,https://proceedings.neurips.cc/paper/2019/file/82edc5c9e21035674d481640448049f3-Paper.pdf,Max-value Entropy Search for Multi-Objective Bayesian Optimization,"Syrine Belakaria, Aryan Deshwal, Janardhan Rao Doppa",
neurips,https://proceedings.neurips.cc/paper/2019/file/831b342d8a83408e5960e9b0c5f31f0c-Paper.pdf,Algorithmic Guarantees for Inverse Imaging with Untrained Network Priors,"Gauri Jagatap, Chinmay Hegde",
neurips,https://proceedings.neurips.cc/paper/2019/file/83462e22a65e7e34975bbf2b639333ec-Paper.pdf,Categorized Bandits,"Matthieu Jedor, Vianney Perchet, Jonathan Louedec",
neurips,https://proceedings.neurips.cc/paper/2019/file/83715fd4755b33f9c3958e1a9ee221e1-Paper.pdf,Curriculum-guided Hindsight Experience Replay,"Meng Fang, Tianyi Zhou, Yali Du, Lei Han, Zhengyou Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/83da7c539e1ab4e759623c38d8737e9e-Paper.pdf,Random Path Selection for Continual Learning,"Jathushan Rajasegaran, Munawar Hayat, Salman H. Khan, Fahad Shahbaz Khan, Ling Shao",
neurips,https://proceedings.neurips.cc/paper/2019/file/83dd3f9f97ef6533766c39d5b2e5e565-Paper.pdf,Learning Multiple Markov Chains via Adaptive Allocation,"Mohammad Sadegh Talebi, Odalric-Ambrym Maillard",
neurips,https://proceedings.neurips.cc/paper/2019/file/8420d359404024567b5aefda1231af24-Paper.pdf,On Single Source Robustness in Deep Fusion Models,"Taewan Kim, Joydeep Ghosh",
neurips,https://proceedings.neurips.cc/paper/2019/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf,GENO -- GENeric Optimization for Classical Machine Learning,"Soeren Laue, Matthias Mitterreiter, Joachim Giesen",
neurips,https://proceedings.neurips.cc/paper/2019/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf,Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift,"Stephan Rabanser, Stephan Günnemann, Zachary Lipton",
neurips,https://proceedings.neurips.cc/paper/2019/file/8471bda5e6201d30893c3582ee131d4d-Paper.pdf,Shadowing Properties of Optimization Algorithms,"Antonio Orvieto, Aurelien Lucchi",
neurips,https://proceedings.neurips.cc/paper/2019/file/84899ae725ba49884f4c85c086f1b340-Paper.pdf,Surrogate Objectives for Batch Policy Optimization in One-step Decision Making,"Minmin Chen, Ramki Gummadi, Chris Harris, Dale Schuurmans",
neurips,https://proceedings.neurips.cc/paper/2019/file/84b20b1f5a0d103f5710bb67a043cd78-Paper.pdf,No-Press Diplomacy: Modeling Multi-Agent Gameplay,"Philip Paquette, Yuchen Lu, SETON STEVEN BOCCO, Max Smith, Satya O.-G., Jonathan K. Kummerfeld, Joelle Pineau, Satinder Singh, Aaron C. Courville",
neurips,https://proceedings.neurips.cc/paper/2019/file/84c2d4860a0fc27bcf854c444fb8b400-Paper.pdf,Bayesian Batch Active Learning as Sparse Subset Approximation,"Robert Pinsler, Jonathan Gordon, Eric Nalisnick, José Miguel Hernández-Lobato",
neurips,https://proceedings.neurips.cc/paper/2019/file/84d2004bf28a2095230e8e14993d398d-Paper.pdf,On the Ineffectiveness of Variance Reduced Optimization for Deep Learning,"Aaron Defazio, Leon Bottou",
neurips,https://proceedings.neurips.cc/paper/2019/file/851300ee84c2b80ed40f51ed26d866fc-Paper.pdf,Putting An End to End-to-End: Gradient-Isolated Learning of Representations,"Sindy Löwe, Peter O'Connor, Bastiaan Veeling",
neurips,https://proceedings.neurips.cc/paper/2019/file/8526e0962a844e4a2f158d831d5fddf7-Paper.pdf,Modular Universal Reparameterization: Deep Multi-task Learning Across Diverse Domains,"Elliot Meyerson, Risto Miikkulainen",
neurips,https://proceedings.neurips.cc/paper/2019/file/85353d3b2f39b9c9b5ee3576578c04b7-Paper.pdf,Decentralized Cooperative Stochastic Bandits,"David Martínez-Rubio, Varun Kanade, Patrick Rebeschini",
neurips,https://proceedings.neurips.cc/paper/2019/file/85422afb467e9456013a2a51d4dff702-Paper.pdf,Powerset Convolutional Neural Networks,"Chris Wendler, Markus Püschel, Dan Alistarh",
neurips,https://proceedings.neurips.cc/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf,Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift,"Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, Jasper Snoek",
neurips,https://proceedings.neurips.cc/paper/2019/file/859b00aec8885efc83d1541b52a1220d-Paper.pdf,"Non-Stationary Markov Decision Processes, a Worst-Case Approach using Model-Based Reinforcement Learning","Erwan Lecarpentier, Emmanuel Rachelson",
neurips,https://proceedings.neurips.cc/paper/2019/file/85f007f8c50dd25f5a45fca73cad64bd-Paper.pdf,Optimal Decision Tree with Noisy Outcomes,"Su Jia, viswanath nagarajan, Fatemeh Navidi, R Ravi",
neurips,https://proceedings.neurips.cc/paper/2019/file/860b37e28ec7ba614f00f9246949561d-Paper.pdf,Generalization Bounds for Neural Networks via Approximate Description Length,"Amit Daniely, Elad Granot","We investigate the sample complexity of networks with bounds on the magnitude of its weights. In particular, we consider the class
\cn
=
{
W
t
∘
ρ
∘
W
t
−
1
∘
ρ
…
∘
ρ
∘
W
1
:
W
1
,
…
,
W
t
−
1
∈
M
d
×
d
,
W
t
∈
M
1
,
d
}
\cn={Wt∘ρ∘Wt−1∘ρ…∘ρ∘W1:W1,…,Wt−1∈Md×d,Wt∈M1,d}
where the spectral norm of each
W
i
W
is bounded by
O
(
1
)
O
, the Frobenius norm is bounded by
R
R
, and
ρ
ρ
is the sigmoid function
e
x
1
+
e
x
e
or the smoothened ReLU function
ln
(
1
+
e
x
)
ln
. We show that for any depth
t
t
, if the inputs are in
[
−
1
,
1
]
d
[
, the sample complexity of
\cn
\cn
is
~
O
(
d
R
2
ϵ
2
)
O
. This bound is optimal up to log-factors, and substantially improves over the previous state of the art of
~
O
(
d
2
R
2
ϵ
2
)
O
, that was established in a recent line of work. We furthermore show that this bound remains valid if instead of considering the magnitude of the
W
i
W
's, we consider the magnitude of
W
i
−
W
0
i
W
, where
W
0
i
W
are some reference matrices, with spectral norm of
O
(
1
)
O
. By taking the
W
0
i
W
to be the matrices in the onset of the training process, we get sample complexity bounds that are sub-linear in the number of parameters, in many {\em typical} regimes of parameters. To establish our results we develop a new technique to analyze the sample complexity of families
\ch
\ch
of predictors. We start by defining a new notion of a randomized approximate description of functions
f
:
\cx
→
\reals
d
f
. We then show that if there is a way to approximately describe functions in a class
\ch
\ch
using
d
d
bits, then
d
ϵ
2
d
examples suffices to guarantee uniform convergence. Namely, that the empirical loss of all the functions in the class is
ϵ
ϵ
-close to the true loss. Finally, we develop a set of tools for calculating the approximate description length of classes of functions that can be presented as a composition of linear function classes and non-linear functions."
neurips,https://proceedings.neurips.cc/paper/2019/file/861578d797aeb0634f77aff3f488cca2-Paper.pdf,Continual Unsupervised Representation Learning,"Dushyant Rao, Francesco Visin, Andrei Rusu, Razvan Pascanu, Yee Whye Teh, Raia Hadsell",
neurips,https://proceedings.neurips.cc/paper/2019/file/866c7ee013c58f01fa153a8d32c9ed57-Paper.pdf,An Inexact Augmented Lagrangian Framework for Nonconvex Optimization with Nonlinear Constraints,"Mehmet Fatih Sahin, Armin eftekhari, Ahmet Alacaoglu, Fabian Latorre, Volkan Cevher","We propose a practical inexact augmented Lagrangian method (iALM) for nonconvex problems with nonlinear constraints. We characterize the total computational complexity of our method subject to a verifiable geometric condition, which is closely related to the Polyak-Lojasiewicz and Mangasarian-Fromowitz conditions. In particular, when a first-order solver is used for the inner iterates, we prove that iALM finds a first-order stationary point with
~
O
(
1
/
ϵ
3
)
O
calls to the first-order oracle. {If, in addition, the problem is smooth and} a second-order solver is used for the inner iterates, iALM finds a second-order stationary point with
~
O
(
1
/
ϵ
5
)
O
calls to the second-order oracle. These complexity results match the known theoretical results in the literature. We also provide strong numerical evidence on large-scale machine learning problems, including the Burer-Monteiro factorization of semidefinite programs, and a novel nonconvex relaxation of the standard basis pursuit template. For these examples, we also show how to verify our geometric condition."
neurips,https://proceedings.neurips.cc/paper/2019/file/8685549650016d9e1d14bf972262450b-Paper.pdf,A Robust Non-Clairvoyant Dynamic Mechanism for Contextual Auctions,"Yuan Deng, Sébastien Lahaie, Vahab Mirrokni",
neurips,https://proceedings.neurips.cc/paper/2019/file/86a1fa88adb5c33bd7a68ac2f9f3f96b-Paper.pdf,Multiple Futures Prediction,"Charlie Tang, Russ R. Salakhutdinov",
neurips,https://proceedings.neurips.cc/paper/2019/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf,Multiview Aggregation for Learning Category-Specific Shape Reconstruction,"Srinath Sridhar, Davis Rempe, Julien Valentin, Bouaziz Sofien, Leonidas J. Guibas",
neurips,https://proceedings.neurips.cc/paper/2019/file/873be0705c80679f2c71fbf4d872df59-Paper.pdf,Reinforcement Learning with Convex Constraints,"Sobhan Miryoosefi, Kianté Brantley, Hal Daume III, Miro Dudik, Robert E. Schapire",
neurips,https://proceedings.neurips.cc/paper/2019/file/8744cf92c88433f8cb04a02e6db69a0d-Paper.pdf,Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel,"Colin Wei, Jason D. Lee, Qiang Liu, Tengyu Ma","Recent works have shown that on sufficiently over-parametrized neural nets, gradient descent with relatively large initialization optimizes a prediction function in the RKHS of the Neural Tangent Kernel (NTK). This analysis leads to global convergence results but does not work when there is a standard
ℓ
2
ℓ
regularizer, which is useful to have in practice. We show that sample efficiency can indeed depend on the presence of the regularizer: we construct a simple distribution in
d
d
dimensions which the optimal regularized neural net learns with
O
(
d
)
O
samples but the NTK requires
Ω
(
d
2
)
Ω
samples to learn. To prove this, we establish two analysis tools: i) for multi-layer feedforward ReLU nets, we show that the global minimizer of a weakly-regularized cross-entropy loss is the max normalized margin solution among all neural nets, which generalizes well; ii) we develop a new technique for proving lower bounds for kernel methods, which relies on showing that the kernel cannot focus on informative features. Motivated by our generalization results, we study whether the regularized global optimum is attainable. We prove that for infinite-width two-layer nets, noisy gradient descent optimizes the regularized neural net loss to a global minimum in polynomial iterations."
neurips,https://proceedings.neurips.cc/paper/2019/file/8767bccb1ff4231a9962e3914f4f1f8f-Paper.pdf,Learning Hawkes Processes from a handful of events,"Farnood Salehi, William Trouleau, Matthias Grossglauser, Patrick Thiran",
neurips,https://proceedings.neurips.cc/paper/2019/file/876e8108f87eb61877c6263228b67256-Paper.pdf,MetaInit: Initializing learning by learning to initialize,"Yann N. Dauphin, Samuel Schoenholz",
neurips,https://proceedings.neurips.cc/paper/2019/file/87784eca6b0dea1dff92478fb786b401-Paper.pdf,"Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence","Aditya Sharad Golatkar, Alessandro Achille, Stefano Soatto",
neurips,https://proceedings.neurips.cc/paper/2019/file/8804f94e16ba5b680e239a554a08f7d2-Paper.pdf,Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation,"Ke Wang, Hang Hua, Xiaojun Wan",
neurips,https://proceedings.neurips.cc/paper/2019/file/885fe656777008c335ac96072a45be15-Paper.pdf,"Accurate, reliable and fast robustness evaluation","Wieland Brendel, Jonas Rauber, Matthias Kümmerer, Ivan Ustyuzhaninov, Matthias Bethge",
neurips,https://proceedings.neurips.cc/paper/2019/file/88855547570f7ff053fff7c54e5148cc-Paper.pdf,"UniXGrad: A Universal, Adaptive Algorithm with Optimal Guarantees for Constrained Optimization","Ali Kavis, Kfir Y. Levy, Francis Bach, Volkan Cevher",
neurips,https://proceedings.neurips.cc/paper/2019/file/88bade49e98db8790df275fcebb37a13-Paper.pdf,From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization,"Krzysztof M. Choromanski, Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Vikas Sindhwani",
neurips,https://proceedings.neurips.cc/paper/2019/file/88fee0421317424e4469f33a48f50cb0-Paper.pdf,Blocking Bandits,"Soumya Basu, Rajat Sen, Sujay Sanghavi, Sanjay Shakkottai","We consider a novel stochastic multi-armed bandit setting, where playing an arm makes it unavailable for a fixed number of time slots thereafter. This models situations where reusing an arm too often is undesirable (e.g. making the same product recommendation repeatedly) or infeasible (e.g. compute job scheduling on machines). We show that with prior knowledge of the rewards and delays of all the arms, the problem of optimizing cumulative reward does not admit any pseudo-polynomial time algorithm (in the number of arms) unless randomized exponential time hypothesis is false, by mapping to the PINWHEEL scheduling problem. Subsequently, we show that a simple greedy algorithm that plays the available arm with the highest reward is asymptotically
(
1
−
1
/
e
)
(
optimal. When the rewards are unknown, we design a UCB based algorithm which is shown to have
c
log
T
+
o
(
log
T
)
c
cumulative regret against the greedy algorithm, leveraging the free exploration of arms due to the unavailability. Finally, when all the delays are equal the problem reduces to Combinatorial Semi-bandits providing us with a lower bound of
c
′
log
T
+
ω
(
log
T
)
c
."
neurips,https://proceedings.neurips.cc/paper/2019/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Paper.pdf,Value Propagation for Decentralized Networked Deep Multi-agent Reinforcement Learning,"Chao Qu, Shie Mannor, Huan Xu, Yuan Qi, Le Song, Junwu Xiong","We consider the networked multi-agent reinforcement learning (MARL) problem in a fully decentralized setting, where agents learn to coordinate to achieve joint success. This problem is widely encountered in many areas including traffic control, distributed control, and smart grids. We assume each agent is located at a node of a communication network and can exchange information only with its neighbors. Using softmax temporal consistency, we derive a primal-dual decentralized optimization method and obtain a principled and data-efficient iterative algorithm named {\em value propagation}. We prove a non-asymptotic convergence rate of
O
(
1
/
T
)
O
with nonlinear function approximation. To the best of our knowledge, it is the first MARL algorithm with a convergence guarantee in the control, off-policy, non-linear function approximation, fully decentralized setting."
neurips,https://proceedings.neurips.cc/paper/2019/file/8a146f1a3da4700cbf03cdc55e2daae6-Paper.pdf,Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller,"Pratyusha Sharma, Deepak Pathak, Abhinav Gupta",
neurips,https://proceedings.neurips.cc/paper/2019/file/8a1ee9f2b7abe6e88d1a479ab6a42c5e-Paper.pdf,L_DMI: A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise,"Yilun Xu, Peng Cao, Yuqing Kong, Yizhou Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/8a50bae297807da9e97722a0b3fd8f27-Paper.pdf,Learning from Bad Data via Generation,"Tianyu Guo, Chang Xu, Boxin Shi, Chao Xu, Dacheng Tao",
neurips,https://proceedings.neurips.cc/paper/2019/file/8a56257ea05c74018291954fc56fc448-Paper.pdf,Connective Cognition Network for Directional Visual Commonsense Reasoning,"Aming Wu, Linchao Zhu, Yahong Han, Yi Yang","Visual commonsense reasoning (VCR) has been introduced to boost research of cognition-level visual understanding, i.e., a thorough understanding of correlated details of the scene plus an inference with related commonsense knowledge. Recent studies on neuroscience have suggested that brain function or cognition can be described as a global and dynamic integration of local neuronal connectivity, which is context-sensitive to specific cognition tasks. Inspired by this idea, towards VCR, we propose a connective cognition network (CCN) to dynamically reorganize the visual neuron connectivity that is contextualized by the meaning of questions and answers. Concretely, we first develop visual neuron connectivity to fully model correlations of visual content. Then, a contextualization process is introduced to fuse the sentence representation with that of visual neurons. Finally, based on the output of contextualized connectivity, we propose directional connectivity to infer answers or rationales. Experimental results on the VCR dataset demonstrate the effectiveness of our method. Particularly, in
Q
→
A
R
Q
mode, our method is around 4\% higher than the state-of-the-art method."
neurips,https://proceedings.neurips.cc/paper/2019/file/8a94ecfa54dcb88a2fa993bfa6388f9e-Paper.pdf,Same-Cluster Querying for Overlapping Clusters,"Wasim Huleihel, Arya Mazumdar, Muriel Medard, Soumyabrata Pal","Overlapping clusters are common in models of many practical data-segmentation applications. Suppose we are given
n
n
elements to be clustered into
k
k
possibly overlapping clusters, and an oracle that can interactively answer queries of the form
do elements
u
u
and
v
v
belong to the same cluster?'' The goal is to recover the clusters with minimum number of such queries. This problem has been of recent interest for the case of disjoint clusters. In this paper, we look at the more practical scenario of overlapping clusters, and provide upper bounds (with algorithms) on the sufficient number of queries. We provide algorithmic results under both arbitrary (worst-case) and statistical modeling assumptions. Our algorithms are parameter free, efficient, and work in the presence of random noise. We also derive information-theoretic lower bounds on the number of queries needed, proving that our algorithms are order optimal. Finally, we test our algorithms over both synthetic and real-world data, showing their practicality and effectiveness."
neurips,https://proceedings.neurips.cc/paper/2019/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf,Discriminator optimal transport,Akinori Tanaka,"Within a broad class of generative adversarial networks, we show that discriminator optimization process increases a lower bound of the dual cost function for the Wasserstein distance between the target distribution
p
p
and the generator distribution
p
G
p
. It implies that the trained discriminator can approximate optimal transport (OT) from
p
G
p
to
p
p
. Based on some experiments and a bit of OT theory, we propose discriminator optimal transport (DOT) scheme to improve generated images. We show that it improves inception score and FID calculated by un-conditional GAN trained by CIFAR-10, STL-10 and a public pre-trained model of conditional GAN trained by ImageNet."
neurips,https://proceedings.neurips.cc/paper/2019/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf,Hierarchical Optimal Transport for Document Representation,"Mikhail Yurochkin, Sebastian Claici, Edward Chien, Farzaneh Mirzazadeh, Justin M. Solomon",
neurips,https://proceedings.neurips.cc/paper/2019/file/8b5c8441a8ff8e151b191c53c1842a38-Paper.pdf,PerspectiveNet: A Scene-consistent Image Generator for New View Synthesis in Real Indoor Environments,"David Novotny, Ben Graham, Jeremy Reizenstein",
neurips,https://proceedings.neurips.cc/paper/2019/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf,Strategizing against No-regret Learners,"Yuan Deng, Jon Schneider, Balasubramanian Sivan",
neurips,https://proceedings.neurips.cc/paper/2019/file/8ba6c657b03fc7c8dd4dff8e45defcd2-Paper.pdf,Sequential Experimental Design for Transductive Linear Bandits,"Tanner Fiez, Lalit Jain, Kevin G. Jamieson, Lillian Ratliff","In this paper we introduce the pure exploration transductive linear bandit problem: given a set of measurement vectors
X
⊂
R
d
X
, a set of items
Z
⊂
R
d
Z
, a fixed confidence
δ
δ
, and an unknown vector
θ
∗
∈
R
d
θ
, the goal is to infer
arg
max
z
∈
Z
z
⊤
θ
∗
arg
with probability
1
−
δ
1
by making as few sequentially chosen noisy measurements of the form
x
⊤
θ
∗
x
as possible. When
X
=
Z
X
, this setting generalizes linear bandits, and when
X
X
is the standard basis vectors and
Z
⊂
{
0
,
1
}
d
Z
, combinatorial bandits. The transductive setting naturally arises when the set of measurement vectors is limited due to factors such as availability or cost. As an example, in drug discovery the compounds and dosages
X
X
a practitioner may be willing to evaluate in the lab in vitro due to cost or safety reasons may differ vastly from those compounds and dosages
Z
Z
that can be safely administered to patients in vivo. Alternatively, in recommender systems for books, the set of books
X
X
a user is queried about may be restricted to known best-sellers even though the goal might be to recommend more esoteric titles
Z
Z
. In this paper, we provide instance-dependent lower bounds for the transductive setting, an algorithm that matches these up to logarithmic factors, and an evaluation. In particular, we present the first non-asymptotic algorithm for linear bandits that nearly achieves the information-theoretic lower bound."
neurips,https://proceedings.neurips.cc/paper/2019/file/8bd39eae38511daad6152e84545e504d-Paper.pdf,End to end learning and optimization on graphs,"Bryan Wilder, Eric Ewing, Bistra Dilkina, Milind Tambe",
neurips,https://proceedings.neurips.cc/paper/2019/file/8c235f89a8143a28a1d6067e959dd858-Paper.pdf,Efficient Meta Learning via Minibatch Proximal Update,"Pan Zhou, Xiaotong Yuan, Huan Xu, Shuicheng Yan, Jiashi Feng",
neurips,https://proceedings.neurips.cc/paper/2019/file/8c66bb19847dd8c21413c5c8c9d68306-Paper.pdf,Triad Constraints for Learning Causal Structure of Latent Variables,"Ruichu Cai, Feng Xie, Clark Glymour, Zhifeng Hao, Kun Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/8ca01ea920679a0fe3728441494041b9-Paper.pdf,Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration,"Meelis Kull, Miquel Perello Nieto, Markus Kängsepp, Telmo Silva Filho, Hao Song, Peter Flach",
neurips,https://proceedings.neurips.cc/paper/2019/file/8cbd005a556ccd4211ce43f309bc0eac-Paper.pdf,Curvilinear Distance Metric Learning,"Shuo Chen, Lei Luo, Jian Yang, Chen Gong, Jun Li, Heng Huang",
neurips,https://proceedings.neurips.cc/paper/2019/file/8cbe9ce23f42628c98f80fa0fac8b19a-Paper.pdf,Sampling Networks and Aggregate Simulation for Online POMDP Planning,"Hao(Jackson) Cui, Roni Khardon",
neurips,https://proceedings.neurips.cc/paper/2019/file/8cd7775f9129da8b5bf787a063d8426e-Paper.pdf,Robust Bi-Tempered Logistic Loss Based on Bregman Divergences,"Ehsan Amid, Manfred K. K. Warmuth, Rohan Anil, Tomer Koren",
neurips,https://proceedings.neurips.cc/paper/2019/file/8ce1a43fb75e779c6b794ba4d255cf6d-Paper.pdf,The Parameterized Complexity of Cascading Portfolio Scheduling,"Eduard Eiben, Robert Ganian, Iyad Kanj, Stefan Szeider",It is known that the computation of an optimal cascading schedule in the offline phase is NP-hard. In this paper we study the parameterized complexity of this problem and establish its fixed-parameter tractability by utilizing structural properties of the success relation between algorithms and test instances.
neurips,https://proceedings.neurips.cc/paper/2019/file/8d1de7457fa769ece8d93a13a59c8552-Paper.pdf,Non-Asymptotic Pure Exploration by Solving Games,"Rémy Degenne, Wouter M. Koolen, Pierre Ménard",
neurips,https://proceedings.neurips.cc/paper/2019/file/8d3369c4c086f236fabf61d614a32818-Paper.pdf,Perceiving the arrow of time in autoregressive motion,"Kristof Meding, Dominik Janzing, Bernhard Schölkopf, Felix A. Wichmann",
neurips,https://proceedings.neurips.cc/paper/2019/file/8d3bba7425e7c98c50f52ca1b52d3735-Paper.pdf,SySCD: A System-Aware Parallel Coordinate Descent Algorithm,"Nikolas Ioannou, Celestine Mendler-Dünner, Thomas Parnell",
neurips,https://proceedings.neurips.cc/paper/2019/file/8d5e957f297893487bd98fa830fa6413-Paper.pdf,Noise-tolerant fair classification,"Alex Lamy, Ziyuan Zhong, Aditya K. Menon, Nakul Verma",
neurips,https://proceedings.neurips.cc/paper/2019/file/8dd291cbea8f231982db0fb1716dfc55-Paper.pdf,Decentralized sketching of low rank matrices,"Rakshith Sharma Srinivasa, Kiryung Lee, Marius Junge, Justin Romberg",
neurips,https://proceedings.neurips.cc/paper/2019/file/8dd48d6a2e2cad213179a3992c0be53c-Paper.pdf,Saccader: Improving Accuracy of Hard Attention Models for Vision,"Gamaleldin Elsayed, Simon Kornblith, Quoc V. Le","Although deep convolutional neural networks achieve state-of-the-art performance across nearly all image classification tasks, their decisions are difficult to interpret. One approach that offers some level of interpretability by design is \textit{hard attention}, which uses only relevant portions of the image. However, training hard attention models with only class label supervision is challenging, and hard attention has proved difficult to scale to complex datasets. Here, we propose a novel hard attention model, which we term Saccader. Key to Saccader is a pretraining step that requires only class labels and provides initial attention locations for policy gradient optimization. Our best models narrow the gap to common ImageNet baselines, achieving
75
%
75
top-1 and
91
%
91
top-5 while attending to less than one-third of the image."
neurips,https://proceedings.neurips.cc/paper/2019/file/8e036cc193d0af59aa9b22821248292b-Paper.pdf,Private Testing of Distributions via Sample Permutations,"Maryam Aliakbarpour, Ilias Diakonikolas, Daniel Kane, Ronitt Rubinfeld",
neurips,https://proceedings.neurips.cc/paper/2019/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf,NeurVPS: Neural Vanishing Point Scanning via Conic Convolution,"Yichao Zhou, Haozhi Qi, Jingwei Huang, Yi Ma",
neurips,https://proceedings.neurips.cc/paper/2019/file/8e987cf1b2f1f6ffa6a43066798b4b7f-Paper.pdf,Estimating Entropy of Distributions in Constant Space,"Jayadev Acharya, Sourbh Bhadane, Piotr Indyk, Ziteng Sun","We consider the task of estimating the entropy of
k
k
-ary distributions from samples in the streaming model, where space is limited. Our main contribution is an algorithm that requires
O
(
k
log
(
1
/
ε
)
2
ε
3
)
O
samples and a constant
O
(
1
)
O
memory words of space and outputs a
±
ε
±
estimate of
H
(
p
)
H
. Without space limitations, the sample complexity has been established as
S
(
k
,
ε
)
=
Θ
(
k
ε
log
k
+
log
2
k
ε
2
)
S
, which is sub-linear in the domain size
k
k
, and the current algorithms that achieve optimal sample complexity also require nearly-linear space in
k
k
. Our algorithm partitions
[
0
,
1
]
[
into intervals and estimates the entropy contribution of probability values in each interval. The intervals are designed to trade bias and variance. Distribution property estimation and testing with limited memory is a largely unexplored research area. We hope our work will motivate research in this field."
neurips,https://proceedings.neurips.cc/paper/2019/file/8efb100a295c0c690931222ff4467bb8-Paper.pdf,Selecting Optimal Decisions via Distributionally Robust Nearest-Neighbor Regression,"Ruidi Chen, Ioannis Paschalidis",
neurips,https://proceedings.neurips.cc/paper/2019/file/8f125da0b3432ed853c0b6f7ee5aaa6b-Paper.pdf,Exploiting Local and Global Structure for Point Cloud Semantic Segmentation with Contextual Point Representations,"Xu Wang, Jingming He, Lin Ma",
neurips,https://proceedings.neurips.cc/paper/2019/file/8f19793b2671094e63a15ab883d50137-Paper.pdf,Heterogeneous Graph Learning for Visual Commonsense Reasoning,"Weijiang Yu, Jingwen Zhou, Weihao Yu, Xiaodan Liang, Nong Xiao",
neurips,https://proceedings.neurips.cc/paper/2019/file/8f1fa0193ca2b5d2fa0695827d8270e9-Paper.pdf,Memory Efficient Adaptive Optimization,"Rohan Anil, Vineet Gupta, Tomer Koren, Yoram Singer",
neurips,https://proceedings.neurips.cc/paper/2019/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf,Conformal Prediction Under Covariate Shift,"Ryan J. Tibshirani, Rina Foygel Barber, Emmanuel Candes, Aaditya Ramdas",
neurips,https://proceedings.neurips.cc/paper/2019/file/8fb5f8be2aa9d6c64a04e3ab9f63feee-Paper.pdf,Adapting Neural Networks for the Estimation of Treatment Effects,"Claudia Shi, David Blei, Victor Veitch",
neurips,https://proceedings.neurips.cc/paper/2019/file/8fc983a91396319d8c394084e2d749d7-Paper.pdf,Solving graph compression via optimal transport,"Vikas Garg, Tommi Jaakkola",
neurips,https://proceedings.neurips.cc/paper/2019/file/8fe04df45a22b63156ebabbb064fcd5e-Paper.pdf,Optimal Sampling and Clustering in the Stochastic Block Model,"Se-Young Yun, Alexandre Proutiere",
neurips,https://proceedings.neurips.cc/paper/2019/file/9001ca429212011f4a4fda6c778cc318-Paper.pdf,Neural Shuffle-Exchange Networks - Sequence Processing in O(n log n) Time,"Karlis Freivalds, Emīls Ozoliņš, Agris Šostaks","We introduce a new Shuffle-Exchange neural network model for sequence to sequence tasks which have O(log n) depth and O(n log n) total complexity. We show that this model is powerful enough to infer efficient algorithms for common algorithmic benchmarks including sorting, addition and multiplication. We evaluate our architecture on the challenging LAMBADA question answering dataset and compare it with the state-of-the-art models which use attention. Our model achieves competitive accuracy and scales to sequences with more than a hundred thousand of elements."
neurips,https://proceedings.neurips.cc/paper/2019/file/90794e3b050f815354e3e29e977a88ab-Paper.pdf,Incremental Scene Synthesis,"Benjamin Planche, Xuejian Rong, Ziyan Wu, Srikrishna Karanam, Harald Kosch, YingLi Tian, Jan Ernst, ANDREAS HUTTER",
neurips,https://proceedings.neurips.cc/paper/2019/file/908075ea2c025c335f4865f7db427062-Paper.pdf,Computing Linear Restrictions of Neural Networks,"Matthew Sotoudeh, Aditya V. Thakur",
neurips,https://proceedings.neurips.cc/paper/2019/file/9087b0efc7c7acd1ef7e153678809c77-Paper.pdf,Markov Random Fields for Collaborative Filtering,Harald Steck,
neurips,https://proceedings.neurips.cc/paper/2019/file/908c9a564a86426585b29f5335b619bc-Paper.pdf,Limiting Extrapolation in Linear Approximate Value Iteration,"Andrea Zanette, Alessandro Lazaric, Mykel J. Kochenderfer, Emma Brunskill",
neurips,https://proceedings.neurips.cc/paper/2019/file/90aef91f0d9e7c3be322bd7bae41617d-Paper.pdf,Regularized Weighted Low Rank Approximation,"Frank Ban, David Woodruff, Richard Zhang","The classical low rank approximation problem is to find a rank
k
k
matrix
U
V
U
(where
U
U
has
k
k
columns and
V
V
has
k
k
rows) that minimizes the Frobenius norm of
A
−
U
V
A
. Although this problem can be solved efficiently, we study an NP-hard variant of this problem that involves weights and regularization. A previous paper of [Razenshteyn et al. '16] derived a polynomial time algorithm for weighted low rank approximation with constant rank. We derive provably sharper guarantees for the regularized version by obtaining parameterized complexity bounds in terms of the statistical dimension rather than the rank, allowing for a rank-independent runtime that can be significantly faster. Our improvement comes from applying sharper matrix concentration bounds, using a novel conditioning technique, and proving structural theorems for regularized low rank problems."
neurips,https://proceedings.neurips.cc/paper/2019/file/90cc440b1b8caa520c562ac4e4bbcb51-Paper.pdf,Structured Graph Learning Via Laplacian Spectral Constraints,"Sandeep Kumar, Jiaxi Ying, Jose Vinicius de Miranda Cardoso, Daniel Palomar",
neurips,https://proceedings.neurips.cc/paper/2019/file/90fd4f88f588ae64038134f1eeaa023f-Paper.pdf,"Lookahead Optimizer: k steps forward, 1 step back","Michael Zhang, James Lucas, Jimmy Ba, Geoffrey E. Hinton",
neurips,https://proceedings.neurips.cc/paper/2019/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf,Finding Friend and Foe in Multi-Agent Games,"Jack Serrino, Max Kleiman-Weiner, David C. Parkes, Josh Tenenbaum",
neurips,https://proceedings.neurips.cc/paper/2019/file/91ba4a4478a66bee9812b0804b6f9d1b-Paper.pdf,Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks,"Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, Quanquan Gu",
neurips,https://proceedings.neurips.cc/paper/2019/file/92262bf907af914b95a0fc33c3f33bf6-Paper.pdf,Self-Supervised Generalisation with Meta Auxiliary Learning,"Shikun Liu, Andrew Davison, Edward Johns",
neurips,https://proceedings.neurips.cc/paper/2019/file/923e325e16617477e457f6a468a2d6df-Paper.pdf,On Robustness of Principal Component Regression,"Anish Agarwal, Devavrat Shah, Dennis Shen, Dogyoon Song",
neurips,https://proceedings.neurips.cc/paper/2019/file/926ffc0ca56636b9e73c565cf994ea5a-Paper.pdf,Data Parameters: A New Family of Parameters for Learning a Differentiable Curriculum,"Shreyas Saxena, Oncel Tuzel, Dennis DeCoste",
neurips,https://proceedings.neurips.cc/paper/2019/file/92af93f73faf3cefc129b6bc55a748a9-Paper.pdf,One-Shot Object Detection with Co-Attention and Co-Excitation,"Ting-I Hsieh, Yi-Chen Lo, Hwann-Tzong Chen, Tyng-Luh Liu",
neurips,https://proceedings.neurips.cc/paper/2019/file/92cf3f7ef90630755b955924254e6ec4-Paper.pdf,"Connections Between Mirror Descent, Thompson Sampling and the Information Ratio","Julian Zimmert, Tor Lattimore","The information-theoretic analysis by Russo and Van Roy [2014] in combination with minimax duality has proved a powerful tool for the analysis of online learning algorithms in full and partial information settings. In most applications there is a tantalising similarity to the classical analysis based on mirror descent. We make a formal connection, showing that the information-theoretic bounds in most applications are derived from existing techniques from online convex optimisation. Besides this, we improve best known regret guarantees for
k
k
-armed adversarial bandits, online linear optimisation on
ℓ
p
ℓ
-balls and bandits with graph feedback."
neurips,https://proceedings.neurips.cc/paper/2019/file/9308b0d6e5898366a4a986bc33f3d3e7-Paper.pdf,Are Anchor Points Really Indispensable in Label-Noise Learning?,"Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, Masashi Sugiyama","In label-noise learning, the \textit{noise transition matrix}, denoting the probabilities that clean labels flip into noisy labels, plays a central role in building \textit{statistically consistent classifiers}. Existing theories have shown that the transition matrix can be learned by exploiting \textit{anchor points} (i.e., data points that belong to a specific class almost surely). However, when there are no anchor points, the transition matrix will be poorly learned, and those previously consistent classifiers will significantly degenerate. In this paper, without employing anchor points, we propose a \textit{transition-revision} (
T
T
-Revision) method to effectively learn transition matrices, leading to better classifiers. Specifically, to learn a transition matrix, we first initialize it by exploiting data points that are similar to anchor points, having high \textit{noisy class posterior probabilities}. Then, we modify the initialized matrix by adding a \textit{slack variable}, which can be learned and validated together with the classifier by using noisy data. Empirical results on benchmark-simulated and real-world label-noise datasets demonstrate that without using exact anchor points, the proposed method is superior to state-of-the-art label-noise learning methods."
neurips,https://proceedings.neurips.cc/paper/2019/file/934b535800b1cba8f96a5d72f72f1611-Paper.pdf,SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models,"Linfeng Zhang, Zhanhong Tan, Jiebo Song, Jingwei Chen, Chenglong Bao, Kaisheng Ma",
neurips,https://proceedings.neurips.cc/paper/2019/file/93db85ed909c13838ff95ccfa94cebd9-Paper.pdf,Multi-Resolution Weak Supervision for Sequential Data,"Paroma Varma, Frederic Sala, Shiori Sagawa, Jason Fries, Daniel Fu, Saelig Khattar, Ashwini Ramamoorthy, Ke Xiao, Kayvon Fatahalian, James Priest, Christopher Ré",
neurips,https://proceedings.neurips.cc/paper/2019/file/940392f5f32a7ade1cc201767cf83e31-Paper.pdf,Smoothing Structured Decomposable Circuits,"Andy Shih, Guy Van den Broeck, Paul Beame, Antoine Amarilli",
neurips,https://proceedings.neurips.cc/paper/2019/file/94130ea17023c4837f0dcdda95034b65-Paper.pdf,Bayesian Joint Estimation of Multiple Graphical Models,"Lingrui Gan, Xinming Yang, Naveen Narisetty, Feng Liang","In this paper, we propose a novel Bayesian group regularization method based on the spike and slab Lasso priors for jointly estimating multiple graphical models. The proposed method can be used to estimate the common sparsity structure underlying the graphical models while capturing potential heterogeneity of the precision matrices corresponding to those models. Our theoretical results show that the proposed method enjoys the optimal rate of convergence in
ℓ
∞
ℓ
norm for estimation consistency and has a strong structure recovery guarantee even when the signal strengths over different graphs are heterogeneous. Through simulation studies and an application to the capital bike-sharing network data, we demonstrate the competitive performance of our method compared to existing alternatives."
neurips,https://proceedings.neurips.cc/paper/2019/file/9426c311e76888b3b2368150cd05f362-Paper.pdf,Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion,"Joan Serrà, Santiago Pascual, Carlos Segura Perales",
neurips,https://proceedings.neurips.cc/paper/2019/file/944a5ae3483ed5c1e10bbccb7942a279-Paper.pdf,Maximum Mean Discrepancy Gradient Flow,"Michael Arbel, Anna Korba, Adil SALIM, Arthur Gretton",
neurips,https://proceedings.neurips.cc/paper/2019/file/947018640bf36a2bb609d3557a285329-Paper.pdf,Causal Confusion in Imitation Learning,"Pim de Haan, Dinesh Jayaraman, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2019/file/94f4ede62112b790c91d5e64fdb09cb8-Paper.pdf,Dimensionality reduction: theoretical perspective on practical measures,"Yair Bartal, Nova Fandina, Ofer Neiman",
neurips,https://proceedings.neurips.cc/paper/2019/file/95192c98732387165bf8e396c0f2dad2-Paper.pdf,MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies,"Xue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2019/file/952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf,Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks,"Aaron Voelker, Ivana Kajić, Chris Eliasmith","We propose a novel memory cell for recurrent neural networks that dynamically maintains information across long windows of time using relatively few resources. The Legendre Memory Unit~(LMU) is mathematically derived to orthogonalize its continuous-time history -- doing so by solving
d
d
coupled ordinary differential equations~(ODEs), whose phase space linearly maps onto sliding windows of time via the Legendre polynomials up to degree
d
−
1
d
. Backpropagation across LMUs outperforms equivalently-sized LSTMs on a chaotic time-series prediction task, improves memory capacity by two orders of magnitude, and significantly reduces training and inference times. LMUs can efficiently handle temporal dependencies spanning
100
,
000
100
time-steps, converge rapidly, and use few internal state-variables to learn complex functions spanning long windows of time -- exceeding state-of-the-art performance among RNNs on permuted sequential MNIST. These results are due to the network's disposition to learn scale-invariant features independently of step size. Backpropagation through the ODE solver allows each layer to adapt its internal time-step, enabling the network to learn task-relevant time-scales. We demonstrate that LMU memory cells can be implemented using
m
m
recurrently-connected Poisson spiking neurons,
O
(
m
)
O
time and memory, with error scaling as
O
(
d
/
√
m
)
O
. We discuss implementations of LMUs on analog and digital neuromorphic hardware."
neurips,https://proceedings.neurips.cc/paper/2019/file/95323660ed2124450caaac2c46b5ed90-Paper.pdf,BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning,"Andreas Kirsch, Joost van Amersfoort, Yarin Gal","We develop BatchBALD, a tractable approximation to the mutual information between a batch of points and model parameters, which we use as an acquisition function to select multiple informative points jointly for the task of deep Bayesian active learning. BatchBALD is a greedy linear-time
1
−
\nicefrac
1
e
1
-approximate algorithm amenable to dynamic programming and efficient caching. We compare BatchBALD to the commonly used approach for batch data acquisition and find that the current approach acquires similar and redundant points, sometimes performing worse than randomly acquiring data. We finish by showing that, using BatchBALD to consider dependencies within an acquisition batch, we achieve new state of the art performance on standard benchmarks, providing substantial data efficiency improvements in batch acquisition."
neurips,https://proceedings.neurips.cc/paper/2019/file/95424358822e753eb993c97ee76a9076-Paper.pdf,Generalized Matrix Means for Semi-Supervised Learning with Multilayer Graphs,"Pedro Mercado, Francesco Tudisco, Matthias Hein",
neurips,https://proceedings.neurips.cc/paper/2019/file/955cb567b6e38f4c6b3f28cc857fc38c-Paper.pdf,Efficiently Estimating Erdos-Renyi Graphs with Node Differential Privacy,"Jonathan Ullman, Adam Sealfon",
neurips,https://proceedings.neurips.cc/paper/2019/file/95688ba636a4720a85b3634acfec8cdd-Paper.pdf,Screening Sinkhorn Algorithm for Regularized Optimal Transport,"Mokhtar Z. Alaya, Maxime Berar, Gilles Gasso, Alain Rakotomamonjy",
neurips,https://proceedings.neurips.cc/paper/2019/file/959ab9a0695c467e7caf75431a872e5c-Paper.pdf,Adaptive Density Estimation for Generative Models,"Thomas Lucas, Konstantin Shmelkov, Karteek Alahari, Cordelia Schmid, Jakob Verbeek",
neurips,https://proceedings.neurips.cc/paper/2019/file/959ef477884b6ac2241b19ee4fb776ae-Paper.pdf,Learning Deep Bilinear Transformation for Fine-grained Image Representation,"Heliang Zheng, Jianlong Fu, Zheng-Jun Zha, Jiebo Luo",
neurips,https://proceedings.neurips.cc/paper/2019/file/95b431e51fc53692913da5263c214162-Paper.pdf,Learning Compositional Neural Programs with Recursive Tree Search and Planning,"Thomas PIERROT, Guillaume Ligner, Scott E. Reed, Olivier Sigaud, Nicolas Perrin, Alexandre Laterre, David Kas, Karim Beguir, Nando de Freitas",
neurips,https://proceedings.neurips.cc/paper/2019/file/95e1533eb1b20a97777749fb94fdb944-Paper.pdf,Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks,"Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, George Pappas",
neurips,https://proceedings.neurips.cc/paper/2019/file/966eaa9527eb956f0dc8788132986707-Paper.pdf,Mo' States Mo' Problems: Emergency Stop Mechanisms from Observation,"Samuel Ainsworth, Matt Barnes, Siddhartha Srinivasa",
neurips,https://proceedings.neurips.cc/paper/2019/file/967c2ae04b169f07e7fa8fdfd110551e-Paper.pdf,Kernelized Bayesian Softmax for Text Generation,"Ning Miao, Hao Zhou, Chengqi Zhao, Wenxian Shi, Lei Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/97008ea27052082be055447be9e85612-Paper.pdf,Bipartite expander Hopfield networks as self-decoding high-capacity error correcting codes,"Rishidev Chaudhuri, Ila Fiete",
neurips,https://proceedings.neurips.cc/paper/2019/file/97108695bd93b6be52fa0334874c8722-Paper.pdf,Distributional Reward Decomposition for Reinforcement Learning,"Zichuan Lin, Li Zhao, Derek Yang, Tao Qin, Tie-Yan Liu, Guangwen Yang",
neurips,https://proceedings.neurips.cc/paper/2019/file/9713faa264b94e2bf346a1bb52587fd8-Paper.pdf,Provably Global Convergence of Actor-Critic: A Case for Linear Quadratic Regulator with Ergodic Cost,"Zhuoran Yang, Yongxin Chen, Mingyi Hong, Zhaoran Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/9715d04413f296eaf3c30c47cec3daa6-Paper.pdf,Fast-rate PAC-Bayes Generalization Bounds via Shifted Rademacher Processes,"Jun Yang, Shengyang Sun, Daniel M. Roy",
neurips,https://proceedings.neurips.cc/paper/2019/file/9718db12cae6be37f7349779007ee589-Paper.pdf,DINGO: Distributed Newton-Type Method for Gradient-Norm Optimization,"Rixon Crane, Fred Roosta",
neurips,https://proceedings.neurips.cc/paper/2019/file/9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf,Deep ReLU Networks Have Surprisingly Few Activation Patterns,"Boris Hanin, David Rolnick",
neurips,https://proceedings.neurips.cc/paper/2019/file/9778d5d219c5080b9a6a17bef029331c-Paper.pdf,Private Hypothesis Selection,"Mark Bun, Gautam Kamath, Thomas Steinke, Steven Z. Wu","We provide a differentially private algorithm for hypothesis selection. Given samples from an unknown probability distribution
P
P
and a set of
m
m
probability distributions
H
H
, the goal is to output, in a
ε
ε
-differentially private manner, a distribution from
H
H
whose total variation distance to
P
P
is comparable to that of the best such distribution (which we denote by
α
α
). The sample complexity of our basic algorithm is
O
(
log
m
α
2
+
log
m
α
ε
)
O
, representing a minimal cost for privacy when compared to the non-private algorithm. We also can handle infinite hypothesis classes
H
H
by relaxing to
(
ε
,
δ
)
(
-differential privacy. We apply our hypothesis selection algorithm to give learning algorithms for a number of natural distribution classes, including Gaussians, product distributions, sums of independent random variables, piecewise polynomials, and mixture classes. Our hypothesis selection procedure allows us to generically convert a cover for a class to a learning algorithm, complementing known learning lower bounds which are in terms of the size of the packing number of the class. As the covering and packing numbers are often closely related, for constant
α
α
, our algorithms achieve the optimal sample complexity for many classes of interest. Finally, we describe an application to private distribution-free PAC learning."
neurips,https://proceedings.neurips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf,ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models,"Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, Boris Katz",
neurips,https://proceedings.neurips.cc/paper/2019/file/97c99dd2a042908aabc0bafc64ddc028-Paper.pdf,Object landmark discovery through unsupervised adaptation,"Enrique Sanchez, Georgios Tzimiropoulos",
neurips,https://proceedings.neurips.cc/paper/2019/file/9872ed9fc22fc182d371c3e9ed316094-Paper.pdf,Block Coordinate Regularization by Denoising,"Yu Sun, Jiaming Liu, Ulugbek Kamilov",
neurips,https://proceedings.neurips.cc/paper/2019/file/98baeb82b676b662e12a7af8ad9212f6-Paper.pdf,Neural Temporal-Difference Learning Converges to Global Optima,"Qi Cai, Zhuoran Yang, Jason D. Lee, Zhaoran Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/98c56bce74669e2e4e7a9fc1caa8c326-Paper.pdf,Learning Nearest Neighbor Graphs from Noisy Distance Samples,"Blake Mason, Ardhendu Tripathy, Robert Nowak",
neurips,https://proceedings.neurips.cc/paper/2019/file/98d8a23fd60826a2a474c5b4f5811707-Paper.pdf,Visual Concept-Metaconcept Learning,"Chi Han, Jiayuan Mao, Chuang Gan, Josh Tenenbaum, Jiajun Wu",
neurips,https://proceedings.neurips.cc/paper/2019/file/98dce83da57b0395e163467c9dae521b-Paper.pdf,The Point Where Reality Meets Fantasy: Mixed Adversarial Generators for Image Splice Detection,"Vladimir V. Kniaz, Vladimir Knyaz, Fabio Remondino",
neurips,https://proceedings.neurips.cc/paper/2019/file/993edc98ca87f7e08494eec37fa836f7-Paper.pdf,Self-Supervised Deep Learning on Point Clouds by Reconstructing Space,"Jonathan Sauder, Bjarne Sievers",
neurips,https://proceedings.neurips.cc/paper/2019/file/99607461cdb9c26e2bd5f31b12dcf27a-Paper.pdf,Outlier-Robust High-Dimensional Sparse Estimation via Iterative Filtering,"Ilias Diakonikolas, Daniel Kane, Sushrut Karmalkar, Eric Price, Alistair Stewart",
neurips,https://proceedings.neurips.cc/paper/2019/file/99a401435dcb65c4008d3ad22c8cdad0-Paper.pdf,ODE2VAE: Deep generative second order ODEs with Bayesian neural networks,"Cagatay Yildiz, Markus Heinonen, Harri Lahdesmaki",
neurips,https://proceedings.neurips.cc/paper/2019/file/99cd3843754d20ec3c5885d805db8a32-Paper.pdf,Cross-Domain Transferability of Adversarial Perturbations,"Muhammad Muzammal Naseer, Salman H. Khan, Muhammad Haris Khan, Fahad Shahbaz Khan, Fatih Porikli","Adversarial examples reveal the blind spots of deep neural networks (DNNs) and represent a major concern for security-critical applications. The transferability of adversarial examples makes real-world attacks possible in black-box settings, where the attacker is forbidden to access the internal parameters of the model. The underlying assumption in most adversary generation methods, whether learning an instance-specific or an instance-agnostic perturbation, is the direct or indirect reliance on the original domain-specific data distribution. In this work, for the first time, we demonstrate the existence of domain-invariant adversaries, thereby showing common adversarial space among different datasets and models. To this end, we propose a framework capable of launching highly transferable attacks that crafts adversarial patterns to mislead networks trained on wholly different domains. For instance, an adversarial function learned on Paintings, Cartoons or Medical images can successfully perturb ImageNet samples to fool the classifier, with success rates as high as
∼
∼
99\% (
ℓ
∞
≤
10
ℓ
). The core of our proposed adversarial function is a generative network that is trained using a relativistic supervisory signal that enables domain-invariant perturbations. Our approach sets the new state-of-the-art for fooling rates, both under the white-box and black-box scenarios. Furthermore, despite being an instance-agnostic perturbation function, our attack outperforms the conventionally much stronger instance-specific attack methods."
neurips,https://proceedings.neurips.cc/paper/2019/file/9a0684d9dad4967ddd09594511de2c52-Paper.pdf,Thresholding Bandit with Optimal Aggregate Regret,"Chao Tao, Saúl Blanco, Jian Peng, Yuan Zhou","We consider the thresholding bandit problem, whose goal is to find arms of mean rewards above a given threshold
θ
θ
, with a fixed budget of
T
T
trials. We introduce LSA, a new, simple and anytime algorithm that aims to minimize the aggregate regret (or the expected number of mis-classified arms). We prove that our algorithm is instance-wise asymptotically optimal. We also provide comprehensive empirical results to demonstrate the algorithm's superior performance over existing algorithms under a variety of different scenarios."
neurips,https://proceedings.neurips.cc/paper/2019/file/9a093d729036a5bd4736e03c5d634501-Paper.pdf,Recovering Bandits,"Ciara Pike-Burke, Steffen Grunewalder",
neurips,https://proceedings.neurips.cc/paper/2019/file/9a0ee0a9e7a42d2d69b8f86b3a0756b1-Paper.pdf,A neurally plausible model for online recognition and postdiction in a dynamical environment,"Li Kevin Wenliang, Maneesh Sahani",
neurips,https://proceedings.neurips.cc/paper/2019/file/9a6a1aaafe73c572b7374828b03a1881-Paper.pdf,Limits of Private Learning with Access to Public Data,"Noga Alon, Raef Bassily, Shay Moran","We consider learning problems where the training set consists of two types of examples: private and public. The goal is to design a learning algorithm that satisfies differential privacy only with respect to the private examples. This setting interpolates between private learning (where all examples are private) and classical learning (where all examples are public). We study the limits of learning in this setting in terms of private and public sample complexities. We show that any hypothesis class of VC-dimension
d
d
can be agnostically learned up to an excess error of
α
α
using only (roughly)
d
/
α
d
public examples and
d
/
α
2
d
private labeled examples. This result holds even when the public examples are unlabeled. This gives a quadratic improvement over the standard
d
/
α
2
d
upper bound on the public sample complexity (where private examples can be ignored altogether if the public examples are labeled). Furthermore, we give a nearly matching lower bound, which we prove via a generic reduction from this setting to the one of private learning without public data."
neurips,https://proceedings.neurips.cc/paper/2019/file/9ac1382fd8fc4b631594aa135d16ad75-Paper.pdf,Optimizing Generalized PageRank Methods for Seed-Expansion Community Detection,"Pan Li, I Chien, Olgica Milenkovic",
neurips,https://proceedings.neurips.cc/paper/2019/file/9ac403da7947a183884c18a67d3aa8de-Paper.pdf,Importance Resampling for Off-policy Prediction,"Matthew Schlegel, Wesley Chung, Daniel Graves, Jian Qian, Martha White",
neurips,https://proceedings.neurips.cc/paper/2019/file/9ad6aaed513b73148b7d49f70afcfb32-Paper.pdf,A Condition Number for Joint Optimization of Cycle-Consistent Networks,"Leonidas J. Guibas, Qixing Huang, Zhenxiao Liang",
neurips,https://proceedings.neurips.cc/paper/2019/file/9b07f50145902e945a1cc629f729c213-Paper.pdf,A Graph Theoretic Additive Approximation of Optimal Transport,"Nathaniel Lahn, Deepika Mulchandani, Sharath Raghvendra","Transportation cost is an attractive similarity measure between probability distributions due to its many useful theoretical properties. However, solving optimal transport exactly can be prohibitively expensive. Therefore, there has been significant effort towards the design of scalable approximation algorithms. Previous combinatorial results [Sharathkumar, Agarwal STOC '12, Agarwal, Sharathkumar STOC '14] have focused primarily on the design of near-linear time multiplicative approximation algorithms. There has also been an effort to design approximate solutions with additive errors [Cuturi NIPS '13, Altschuler \etal\ NIPS '17, Dvurechensky \etal\, ICML '18, Quanrud, SOSA '19] within a time bound that is linear in the size of the cost matrix and polynomial in
C
/
δ
C
; here
C
C
is the largest value in the cost matrix and
δ
δ
is the additive error. We present an adaptation of the classical graph algorithm of Gabow and Tarjan and provide a novel analysis of this algorithm that bounds its execution time by
\BigO
(
n
2
C
δ
+
n
C
2
δ
2
)
\BigO
. Our algorithm is extremely simple and executes, for an arbitrarily small constant
\eps
\eps
, only
⌊
2
C
(
1
−
\eps
)
δ
⌋
+
1
⌊
iterations, where each iteration consists only of a Dijkstra-type search followed by a depth-first search. We also provide empirical results that suggest our algorithm is competitive with respect to a sequential implementation of the Sinkhorn algorithm in execution time. Moreover, our algorithm quickly computes a solution for very small values of
δ
δ
whereas Sinkhorn algorithm slows down due to numerical instability."
neurips,https://proceedings.neurips.cc/paper/2019/file/9b16759a62899465ab21e2e79d2ef75c-Paper.pdf,MaxGap Bandit: Adaptive Algorithms for Approximate Ranking,"Sumeet Katariya, Ardhendu Tripathy, Robert Nowak",
neurips,https://proceedings.neurips.cc/paper/2019/file/9b8b50fb590c590ffbf1295ce92258dc-Paper.pdf,Regret Bounds for Learning State Representations in Reinforcement Learning,"Ronald Ortner, Matteo Pirotta, Alessandro Lazaric, Ronan Fruit, Odalric-Ambrym Maillard",
neurips,https://proceedings.neurips.cc/paper/2019/file/9bb6dee73b8b0ca97466ccb24fff3139-Paper.pdf,Exact Rate-Distortion in Autoencoders via Echo Noise,"Rob Brekelmans, Daniel Moyer, Aram Galstyan, Greg Ver Steeg",
neurips,https://proceedings.neurips.cc/paper/2019/file/9bd5ee6fe55aaeb673025dbcb8f939c1-Paper.pdf,AutoAssist: A Framework to Accelerate Training of Deep Neural Networks,"Jiong Zhang, Hsiang-Fu Yu, Inderjit S. Dhillon",
neurips,https://proceedings.neurips.cc/paper/2019/file/9bdb8b1faffa4b3d41779bb495d79fb9-Paper.pdf,BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling,"Lars Maaløe, Marco Fraccaro, Valentin Liévin, Ole Winther",
neurips,https://proceedings.neurips.cc/paper/2019/file/9be40cee5b0eee1462c82c6964087ff9-Paper.pdf,Multiway clustering via tensor block models,"Miaoyan Wang, Yuchen Zeng",
neurips,https://proceedings.neurips.cc/paper/2019/file/9c19a2aa1d84e04b0bd4bc888792bd1e-Paper.pdf,Bridging Machine Learning and Logical Reasoning by Abductive Learning,"Wang-Zhou Dai, Qiuling Xu, Yang Yu, Zhi-Hua Zhou",
neurips,https://proceedings.neurips.cc/paper/2019/file/9c3b1830513cc3b8fc4b76635d32e692-Paper.pdf,Variational Structured Semantic Inference for Diverse Image Captioning,"Fuhai Chen, Rongrong Ji, Jiayi Ji, Xiaoshuai Sun, Baochang Zhang, Xuri Ge, Yongjian Wu, Feiyue Huang, Yan Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/9c449771d0edc923c2713a7462cefa3b-Paper.pdf,Input-Output Equivalence of Unitary and Contractive RNNs,"Melikasadat Emami, Mojtaba Sahraee Ardakan, Sundeep Rangan, Alyson K. Fletcher",
neurips,https://proceedings.neurips.cc/paper/2019/file/9ca8c9b0996bbf05ae7753d34667a6fd-Paper.pdf,Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization,"Koen Helwegen, James Widdicombe, Lukas Geiger, Zechun Liu, Kwang-Ting Cheng, Roeland Nusselder",
neurips,https://proceedings.neurips.cc/paper/2019/file/9cc138f8dc04cbf16240daa92d8d50e2-Paper.pdf,Nonparametric Regressive Point Processes Based on Conditional Gaussian Processes,"Siqi Liu, Milos Hauskrecht",
neurips,https://proceedings.neurips.cc/paper/2019/file/9cd78264cf2cd821ba651485c111a29a-Paper.pdf,Continuous-time Models for Stochastic Optimization Algorithms,"Antonio Orvieto, Aurelien Lucchi",
neurips,https://proceedings.neurips.cc/paper/2019/file/9ce3c52fc54362e22053399d3181c638-Paper.pdf,Differentiable Convex Optimization Layers,"Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, J. Zico Kolter",
neurips,https://proceedings.neurips.cc/paper/2019/file/9d1827dc5f75b9d65d80e25eb862e676-Paper.pdf,A Zero-Positive Learning Approach for Diagnosing Software Performance Regressions,"Mejbah Alam, Justin Gottschlich, Nesime Tatbul, Javier S. Turek, Tim Mattson, Abdullah Muzahid",
neurips,https://proceedings.neurips.cc/paper/2019/file/9d28de8ff9bb6a3fa41fddfdc28f3bc1-Paper.pdf,Partially Encrypted Deep Learning using Functional Encryption,"Théo Ryffel, David Pointcheval, Francis Bach, Edouard Dufour-Sans, Romain Gay",
neurips,https://proceedings.neurips.cc/paper/2019/file/9d63484abb477c97640154d40595a3bb-Paper.pdf,Graph Transformer Networks,"Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, Hyunwoo J. Kim",
neurips,https://proceedings.neurips.cc/paper/2019/file/9d7099d87947faa8d07a272dd6954b80-Paper.pdf,Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving expressivity with transient dynamics,"Giancarlo Kerg, Kyle Goyette, Maximilian Puelma Touzel, Gauthier Gidel, Eugene Vorontsov, Yoshua Bengio, Guillaume Lajoie",
neurips,https://proceedings.neurips.cc/paper/2019/file/9d8df73a3cfbf3c5b47bc9b50f214aff-Paper.pdf,Large Memory Layers with Product Keys,"Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, Herve Jegou",
neurips,https://proceedings.neurips.cc/paper/2019/file/9e3cfc48eccf81a0d57663e129aef3cb-Paper.pdf,Computing Full Conformal Prediction Set with Approximate Homotopy,"Eugene Ndiaye, Ichiro Takeuchi","If you are predicting the label
y
y
of a new object with
^
y
y
, how confident are you that
y
=
^
y
y
? Conformal prediction methods provide an elegant framework for answering such question by building a
100
(
1
−
α
)
%
100
confidence region without assumptions on the distribution of the data. It is based on a refitting procedure that parses all the possibilities for
y
y
to select the most likely ones. Although providing strong coverage guarantees, conformal set is impractical to compute exactly for many regression problems. We propose efficient algorithms to compute conformal prediction set using approximated solution of (convex) regularized empirical risk minimization. Our approaches rely on a new homotopy continuation technique for tracking the solution path with respect to sequential changes of the observations. We also provide a detailed analysis quantifying its complexity."
neurips,https://proceedings.neurips.cc/paper/2019/file/9e6a921fbc428b5638b3986e365d4f21-Paper.pdf,AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification,"Ronghui You, Zihan Zhang, Ziye Wang, Suyang Dai, Hiroshi Mamitsuka, Shanfeng Zhu",
neurips,https://proceedings.neurips.cc/paper/2019/file/9e82757e9a1c12cb710ad680db11f6f1-Paper.pdf,Policy Learning for Fairness in Ranking,"Ashudeep Singh, Thorsten Joachims",
neurips,https://proceedings.neurips.cc/paper/2019/file/9e984c108157cea74c894b5cf34efc44-Paper.pdf,Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function,"Zihan Zhang, Xiangyang Ji","We present an algorithm based on the \emph{Optimism in the Face of Uncertainty} (OFU) principle which is able to learn Reinforcement Learning (RL) modeled by Markov decision process (MDP) with finite state-action space efficiently. By evaluating the state-pair difference of the optimal bias function
h
∗
h
, the proposed algorithm achieves a regret bound of
~
O
(
√
S
A
T
H
)
O
\footnote{The symbol
~
O
O
means
O
O
with log factors ignored. } for MDP with S states and A actions, in the case that an upper bound
H
H
on the span of
h
∗
h
, i.e.,
s
p
(
h
∗
)
s
is known. This result outperforms the best previous regret bounds
~
O
(
H
S
√
A
T
)
O
\cite{bartlett2009regal} by a factor of
√
S
H
S
. Furthermore, this regret bound matches the lower bound of
Ω
(
√
S
A
T
H
)
Ω
\cite{jaksch2010near} up to a logarithmic factor. As a consequence, we show that there is a near optimal regret bound of
~
O
(
√
D
S
A
T
)
O
for MDPs with finite diameter
D
D
compared to the lower bound of
Ω
(
√
D
S
A
T
)
Ω
\cite{jaksch2010near}."
neurips,https://proceedings.neurips.cc/paper/2019/file/9e9a30b74c49d07d8150c8c83b1ccf07-Paper.pdf,Integer Discrete Flows and Lossless Compression,"Emiel Hoogeboom, Jorn Peters, Rianne van den Berg, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2019/file/9eb53b5052d534ea2619ca03b5649af7-Paper.pdf,Generative Well-intentioned Networks,"Justin Cosentino, Jun Zhu",
neurips,https://proceedings.neurips.cc/paper/2019/file/9ec51f6eb240fb631a35864e13737bca-Paper.pdf,An Embedding Framework for Consistent Polyhedral Surrogates,"Jessica Finocchiaro, Rafael Frongillo, Bo Waggoner",
neurips,https://proceedings.neurips.cc/paper/2019/file/9edda0fd4d983bf975935cfd492fd50b-Paper.pdf,The Normalization Method for Alleviating Pathological Sharpness in Wide Neural Networks,"Ryo Karakida, Shotaro Akaho, Shun-ichi Amari",
neurips,https://proceedings.neurips.cc/paper/2019/file/9f067d8d6df2d4b8c64fb4c084d6c208-Paper.pdf,Re-randomized Densification for One Permutation Hashing and Bin-wise Consistent Weighted Sampling,"Ping Li, Xiaoyun Li, Cun-Hui Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/9f36407ead0629fc166f14dde7970f68-Paper.pdf,Beyond Online Balanced Descent: An Optimal Algorithm for Smoothed Online Optimization,"Gautam Goel, Yiheng Lin, Haoyuan Sun, Adam Wierman","We study online convex optimization in a setting where the learner seeks to minimize the sum of a per-round hitting cost and a movement cost which is incurred when changing decisions between rounds. We prove a new lower bound on the competitive ratio of any online algorithm in the setting where the costs are
m
m
-strongly convex and the movement costs are the squared
ℓ
2
ℓ
norm. This lower bound shows that no algorithm can achieve a competitive ratio that is
o
(
m
−
1
/
2
)
o
as
m
m
tends to zero. No existing algorithms have competitive ratios matching this bound, and we show that the state-of-the-art algorithm, Online Balanced Decent (OBD), has a competitive ratio that is
Ω
(
m
−
2
/
3
)
Ω
. We additionally propose two new algorithms, Greedy OBD (G-OBD) and Regularized OBD (R-OBD) and prove that both algorithms have an
O
(
m
−
1
/
2
)
O
competitive ratio. The result for G-OBD holds when the hitting costs are quasiconvex and the movement costs are the squared
ℓ
2
ℓ
norm, while the result for R-OBD holds when the hitting costs are
m
m
-strongly convex and the movement costs are Bregman Divergences. Further, we show that R-OBD simultaneously achieves constant, dimension-free competitive ratio and sublinear regret when hitting costs are strongly convex."
neurips,https://proceedings.neurips.cc/paper/2019/file/9f396fe44e7c05c16873b05ec425cbad-Paper.pdf,Reconciling λ-Returns with Experience Replay,"Brett Daley, Christopher Amato",
neurips,https://proceedings.neurips.cc/paper/2019/file/9f96f36b7aae3b1ff847c26ac94c604e-Paper.pdf,Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm,"Giulia Luise, Saverio Salzo, Massimiliano Pontil, Carlo Ciliberto",
neurips,https://proceedings.neurips.cc/paper/2019/file/9f9e8cba3700df6a947a8cf91035ab84-Paper.pdf,Finite-Sample Analysis for SARSA with Linear Function Approximation,"Shaofeng Zou, Tengyu Xu, Yingbin Liang",
neurips,https://proceedings.neurips.cc/paper/2019/file/9fe77ac7060e716f2d42631d156825c0-Paper.pdf,Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations,"Fenglin Liu, Yuanxin Liu, Xuancheng Ren, Xiaodong He, Xu Sun",
neurips,https://proceedings.neurips.cc/paper/2019/file/a01a0380ca3c61428c26a231f0e49a09-Paper.pdf,Network Pruning via Transformable Architecture Search,"Xuanyi Dong, Yi Yang",
neurips,https://proceedings.neurips.cc/paper/2019/file/a02ffd91ece5e7efeb46db8f10a74059-Paper.pdf,Regret Minimization for Reinforcement Learning with Vectorial Feedback and Complex Objectives,Wang Chi Cheung,
neurips,https://proceedings.neurips.cc/paper/2019/file/a0872cc5b5ca4cc25076f3d868e1bdf8-Paper.pdf,Online Stochastic Shortest Path with Bandit Feedback and Unknown Transition Function,"Aviv Rosenberg, Yishay Mansour","We consider online learning in episodic loop-free Markov decision processes (MDPs), where the loss function can change arbitrarily between episodes. The transition function is fixed but unknown to the learner, and the learner only observes bandit feedback (not the entire loss function). For this problem we develop no-regret algorithms that perform asymptotically as well as the best stationary policy in hindsight. Assuming that all states are reachable with probability
β
>
0
β
under any policy, we give a regret bound of
~
O
(
L
|
X
|
√
|
A
|
T
/
β
)
O
, where
T
T
is the number of episodes,
X
X
is the state space,
A
A
is the action space, and
L
L
is the length of each episode. When this assumption is removed we give a regret bound of
~
O
(
L
3
/
2
|
X
|
|
A
|
1
/
4
T
3
/
4
)
O
, that holds for an arbitrary transition function. To our knowledge these are the first algorithms that in our setting handle both bandit feedback and an unknown transition function."
neurips,https://proceedings.neurips.cc/paper/2019/file/a0d3973ad100ad83a64c304bb58677dd-Paper.pdf,Selective Sampling-based Scalable Sparse Subspace Clustering,"Shin Matsushima, Maria Brbic",
neurips,https://proceedings.neurips.cc/paper/2019/file/a0dc078ca0d99b5ebb465a9f1cad54ba-Paper.pdf,On the Expressive Power of Deep Polynomial Neural Networks,"Joe Kileel, Matthew Trager, Joan Bruna",
neurips,https://proceedings.neurips.cc/paper/2019/file/a10463df69e52e78372b724471434ec9-Paper.pdf,BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos,"Eleanor Batty, Matthew Whiteway, Shreya Saxena, Dan Biderman, Taiga Abe, Simon Musall, Winthrop Gillis, Jeffrey Markowitz, Anne Churchland, John P. Cunningham, Sandeep R. Datta, Scott Linderman, Liam Paninski",
neurips,https://proceedings.neurips.cc/paper/2019/file/a11da6bd58b95b334f8cd49f00918f16-Paper.pdf,Accurate Layerwise Interpretable Competence Estimation,"Vickram Rajendran, William LeVine","Our contributions allow us to accurately predict the competence of any classification model given any input and error function. We compare our score with state-of-the-art confidence estimators such as model confidence and Trust Score, and show significant improvements in competence prediction over these methods on datasets such as DIGITS, CIFAR10, and CIFAR100."
neurips,https://proceedings.neurips.cc/paper/2019/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf,On the Global Convergence of (Fast) Incremental Expectation Maximization Methods,"Belhal Karimi, Hoi-To Wai, Eric Moulines, Marc Lavielle",
neurips,https://proceedings.neurips.cc/paper/2019/file/a1519de5b5d44b31a01de013b9b51a80-Paper.pdf,Nonconvex Low-Rank Tensor Completion from Noisy Data,"Changxiao Cai, Gen Li, H. Vincent Poor, Yuxin Chen","We study a completion problem of broad practical interest: the reconstruction of a low-rank symmetric tensor from highly incomplete and randomly corrupted observations of its entries. While a variety of prior work has been dedicated to this problem, prior algorithms either are computationally too expensive for large-scale applications, or come with sub-optimal statistical guarantees. Focusing on
incoherent'' and well-conditioned tensors of a constant CP rank, we propose a two-stage nonconvex algorithm --- (vanilla) gradient descent following a rough initialization --- that achieves the best of both worlds. Specifically, the proposed nonconvex algorithm faithfully completes the tensor and retrieves all low-rank tensor factors within nearly linear time, while at the same time enjoying near-optimal statistical guarantees (i.e.~minimal sample complexity and optimal
ℓ
2
ℓ
and
ℓ
∞
ℓ
statistical accuracy). The insights conveyed through our analysis of nonconvex optimization might have implications for other tensor estimation problems."
neurips,https://proceedings.neurips.cc/paper/2019/file/a1a527267c0d33a86382a03c4c721cd2-Paper.pdf,Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning,"Mahmoud Assran, Joshua Romoff, Nicolas Ballas, Joelle Pineau, Michael Rabbat",
neurips,https://proceedings.neurips.cc/paper/2019/file/a1e865a9b1065392ed6035d8ccd072d9-Paper.pdf,Fast and Accurate Stochastic Gradient Estimation,"Beidi Chen, Yingchen Xu, Anshumali Shrivastava",
neurips,https://proceedings.neurips.cc/paper/2019/file/a2186aa7c086b46ad4e8bf81e2a3a19b-Paper.pdf,Learning Disentangled Representations for Recommendation,"Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, Wenwu Zhu",
neurips,https://proceedings.neurips.cc/paper/2019/file/a29d1598024f9e87beab4b98411d48ce-Paper.pdf,Learning Latent Process from High-Dimensional Event Sequences via Efficient Sampling,"Qitian Wu, Zixuan Zhang, Xiaofeng Gao, Junchi Yan, Guihai Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/a2b15837edac15df90721968986f7f8e-Paper.pdf,Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty,"Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, Dawn Song",
neurips,https://proceedings.neurips.cc/paper/2019/file/a2ce8f1706e52936dfad516c23904e3e-Paper.pdf,Space and Time Efficient Kernel Density Estimation in High Dimensions,"Arturs Backurs, Piotr Indyk, Tal Wagner","In this work, we present an improvement to their framework that retains the same query time, while requiring only linear space and linear preprocessing time. We instantiate our framework with the Laplacian and Exponential kernels, two popular kernels which possess the aforementioned property. Our experiments on various datasets verify that our approach attains accuracy and query time similar to Charikar and Siminelakis (2017), with significantly improved space and preprocessing time."
neurips,https://proceedings.neurips.cc/paper/2019/file/a32d7eeaae19821fd9ce317f3ce952a7-Paper.pdf,Random Projections with Asymmetric Quantization,"Xiaoyun Li, Ping Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/a33f5792b2a9a51ddd0111b3ac6e0e76-Paper.pdf,Scalable Deep Generative Relational Model with High-Order Node Dependence,"Xuhui Fan, Bin Li, Caoyuan Li, Scott SIsson, Ling Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/a34bacf839b923770b2c360eefa26748-Paper.pdf,Better Exploration with Optimistic Actor Critic,"Kamil Ciosek, Quan Vuong, Robert Loftin, Katja Hofmann",
neurips,https://proceedings.neurips.cc/paper/2019/file/a35fe7f7fe8217b4369a0af4244d1fca-Paper.pdf,Algorithmic Analysis and Statistical Estimation of SLOPE via Approximate Message Passing,"Zhiqi Bu, Jason Klusowski, Cynthia Rush, Weijie Su","SLOPE is a relatively new convex optimization procedure for high-dimensional linear regression via the sorted
ℓ
1
ℓ
penalty: the larger the rank of the fitted coefficient, the larger the penalty. This non-separable penalty renders many existing techniques invalid or inconclusive in analyzing the SLOPE solution. In this paper, we develop an asymptotically exact characterization of the SLOPE solution under Gaussian random designs through solving the SLOPE problem using approximate message passing (AMP). This algorithmic approach allows us to approximate the SLOPE solution via the much more amenable AMP iterates. Explicitly, we characterize the asymptotic dynamics of the AMP iterates relying on a recently developed state evolution analysis for non-separable penalties, thereby overcoming the difficulty caused by the sorted
ℓ
1
ℓ
penalty. Moreover, we prove that the AMP iterates converge to the SLOPE solution in an asymptotic sense, and numerical simulations show that the convergence is surprisingly fast. Our proof rests on a novel technique that specifically leverages the SLOPE problem. In contrast to prior literature, our work not only yields an asymptotically sharp analysis but also offers an algorithmic, flexible, and constructive approach to understanding the SLOPE problem."
neurips,https://proceedings.neurips.cc/paper/2019/file/a385d7d1e52d89d1a445faa37f5b5307-Paper.pdf,Multi-objects Generation with Amortized Structural Regularization,"Taufik Xu, Chongxuan LI, Jun Zhu, Bo Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/a44ba9086b2b83ccf2baf7c678723449-Paper.pdf,A Family of Robust Stochastic Operators for Reinforcement Learning,"Yingdong Lu, Mark Squillante, Chai Wah Wu",
neurips,https://proceedings.neurips.cc/paper/2019/file/a4613e8d72a61b3b69b32d040f89ad81-Paper.pdf,One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers,"Ari Morcos, Haonan Yu, Michela Paganini, Yuandong Tian",
neurips,https://proceedings.neurips.cc/paper/2019/file/a4d41b834ea903526373a9a1ae2ac66e-Paper.pdf,Learning Distributions Generated by One-Layer ReLU Networks,"Shanshan Wu, Alexandros G. Dimakis, Sujay Sanghavi","We consider the problem of estimating the parameters of a
d
d
-dimensional rectified Gaussian distribution from i.i.d. samples. A rectified Gaussian distribution is defined by passing a standard Gaussian distribution through a one-layer ReLU neural network. We give a simple algorithm to estimate the parameters (i.e., the weight matrix and bias vector of the ReLU neural network) up to an error
\eps
\norm
W
F
\eps
using
˜
O
(
1
/
\eps
2
)
O
samples and
˜
O
(
d
2
/
\eps
2
)
O
time (log factors are ignored for simplicity). This implies that we can estimate the distribution up to
\eps
\eps
in total variation distance using
˜
O
(
κ
2
d
2
/
\eps
2
)
O
samples, where
κ
κ
is the condition number of the covariance matrix. Our only assumption is that the bias vector is non-negative. Without this non-negativity assumption, we show that estimating the bias vector within any error requires the number of samples at least exponential in the infinity norm of the bias vector. Our algorithm is based on the key observation that vector norms and pairwise angles can be estimated separately. We use a recent result on learning from truncated samples. We also prove two sample complexity lower bounds:
Ω
(
1
/
\eps
2
)
Ω
samples are required to estimate the parameters up to error
\eps
\eps
, while
Ω
(
d
/
\eps
2
)
Ω
samples are necessary to estimate the distribution up to
\eps
\eps
in total variation distance. The first lower bound implies that our algorithm is optimal for parameter estimation. Finally, we show an interesting connection between learning a two-layer generative model and non-negative matrix factorization. Experimental results are provided to support our analysis."
neurips,https://proceedings.neurips.cc/paper/2019/file/a4d8e2a7e0d0c102339f97716d2fdfb6-Paper.pdf,Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules,"Niklas Gebauer, Michael Gastegger, Kristof Schütt",
neurips,https://proceedings.neurips.cc/paper/2019/file/a4d92e2cd541fca87e4620aba658316d-Paper.pdf,Quantum Entropy Scoring for Fast Robust Mean Estimation and Improved Outlier Detection,"Yihe Dong, Samuel Hopkins, Jerry Li","We study two problems in high-dimensional robust statistics: \emph{robust mean estimation} and \emph{outlier detection}. In robust mean estimation the goal is to estimate the mean
μ
μ
of a distribution on
R
d
R
given
n
n
independent samples, an
ϵ
ϵ
-fraction of which have been corrupted by a malicious adversary. In outlier detection the goal is to assign an \emph{outlier score} to each element of a data set such that elements more likely to be outliers are assigned higher scores. Our algorithms for both problems are based on a new outlier scoring method we call QUE-scoring based on \emph{quantum entropy regularization}. For robust mean estimation, this yields the first algorithm with optimal error rates and nearly-linear running time
~
O
(
n
d
)
O
in all parameters, improving on the previous fastest running time
~
O
(
min
(
n
d
/
\e
6
,
n
d
2
)
)
O
. For outlier detection, we evaluate the performance of QUE-scoring via extensive experiments on synthetic and real data, and demonstrate that it often performs better than previously proposed algorithms."
neurips,https://proceedings.neurips.cc/paper/2019/file/a4ee59dd868ba016ed2de90d330acb6a-Paper.pdf,Semi-flat minima and saddle points by embedding neural networks to overparameterization,"Kenji Fukumizu, Shoichiro Yamaguchi, Yoh-ichi Mototake, Mirai Tanaka",
neurips,https://proceedings.neurips.cc/paper/2019/file/a501bebf79d570651ff601788ea9d16d-Paper.pdf,Privacy-Preserving Classification of Personal Text Messages with Secure Multi-Party Computation,"Devin Reich, Ariel Todoki, Rafael Dowsley, Martine De Cock, anderson nascimento",
neurips,https://proceedings.neurips.cc/paper/2019/file/a588a6199feff5ba48402883d9b72700-Paper.pdf,Locally Private Gaussian Estimation,"Matthew Joseph, Janardhan Kulkarni, Jieming Mao, Steven Z. Wu",
neurips,https://proceedings.neurips.cc/paper/2019/file/a5b93aaec935a59987f8a5f2280e7cd7-Paper.pdf,Distributed Low-rank Matrix Factorization With Exact Consensus,"Zhihui Zhu, Qiuwei Li, Xinshuo Yang, Gongguo Tang, Michael B. Wakin",
neurips,https://proceedings.neurips.cc/paper/2019/file/a6197a578fe7778e8d49a95ac425bcfc-Paper.pdf,Tensor Monte Carlo: Particle Methods for the GPU era,Laurence Aitchison,"Multi-sample, importance-weighted variational autoencoders (IWAE) give tighter bounds and more accurate uncertainty estimates than variational autoencoders (VAEs) trained with a standard single-sample objective. However, IWAEs scale poorly: as the latent dimensionality grows, they require exponentially many samples to retain the benefits of importance weighting. While sequential Monte-Carlo (SMC) can address this problem, it is prohibitively slow because the resampling step imposes sequential structure which cannot be parallelised, and moreover, resampling is non-differentiable which is problematic when learning approximate posteriors. To address these issues, we developed tensor Monte-Carlo (TMC) which gives exponentially many importance samples by separately drawing
K
K
samples for each of the
n
n
latent variables, then averaging over all
K
n
K
possible combinations. While the sum over exponentially many terms might seem to be intractable, in many cases it can be computed efficiently as a series of tensor inner-products. We show that TMC is superior to IWAE on a generative model with multiple stochastic layers trained on the MNIST handwritten digit database, and we show that TMC can be combined with standard variance reduction techniques."
neurips,https://proceedings.neurips.cc/paper/2019/file/a64a034c3cb8eac64eb46ea474902797-Paper.pdf,Learning Mixtures of Plackett-Luce Models from Structured Partial Orders,"Zhibing Zhao, Lirong Xia","Mixtures of ranking models have been widely used for heterogeneous preferences. However, learning a mixture model is highly nontrivial, especially when the dataset consists of partial orders. In such cases, the parameter of the model may not be even identifiable. In this paper, we focus on three popular structures of partial orders: ranked top-
l
1
l
,
l
2
l
-way, and choice data over a subset of alternatives. We prove that when the dataset consists of combinations of ranked top-
l
1
l
and
l
2
l
-way (or choice data over up to
l
2
l
alternatives), mixture of
k
k
Plackett-Luce models is not identifiable when
l
1
+
l
2
≤
2
k
−
1
l
(
l
2
l
is set to
1
1
when there are no
l
2
l
-way orders). We also prove that under some combinations, including ranked top-
3
3
, ranked top-
2
2
plus
2
2
-way, and choice data over up to
4
4
alternatives, mixtures of two Plackett-Luce models are identifiable. Guided by our theoretical results, we propose efficient generalized method of moments (GMM) algorithms to learn mixtures of two Plackett-Luce models, which are proven consistent. Our experiments demonstrate the efficacy of our algorithms. Moreover, we show that when full rankings are available, learning from different marginal events (partial orders) provides tradeoffs between statistical efficiency and computational efficiency."
neurips,https://proceedings.neurips.cc/paper/2019/file/a660d4563b8f62dd5282319cc643d950-Paper.pdf,Combining Generative and Discriminative Models for Hybrid Inference,"Victor Garcia Satorras, Zeynep Akata, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2019/file/a666587afda6e89aec274a3657558a27-Paper.pdf,Trust Region-Guided Proximal Policy Optimization,"Yuhui Wang, Hao He, Xiaoyang Tan, Yaozhong Gan",
neurips,https://proceedings.neurips.cc/paper/2019/file/a67c8c9a961b4182688768dd9ba015fe-Paper.pdf,Region Mutual Information Loss for Semantic Segmentation,"Shuai Zhao, Yang Wang, Zheng Yang, Deng Cai",
neurips,https://proceedings.neurips.cc/paper/2019/file/a68259547f3d25ab3c0a5c0adb4e3498-Paper.pdf,A Stochastic Composite Gradient Method with Incremental Variance Reduction,"Junyu Zhang, Lin Xiao",
neurips,https://proceedings.neurips.cc/paper/2019/file/a6a767bbb2e3513233f942e0ff24272c-Paper.pdf,An adaptive nearest neighbor rule for classification,"Akshay Balsubramani, Sanjoy Dasgupta, yoav Freund, Shay Moran","We introduce a variant of the
k
k
-nearest neighbor classifier in which
k
k
is chosen adaptively for each query, rather than supplied as a parameter. The choice of
k
k
depends on properties of each neighborhood, and therefore may significantly vary between different points. (For example, the algorithm will use larger
k
k
for predicting the labels of points in noisy regions.) We provide theory and experiments that demonstrate that the algorithm performs comparably to, and sometimes better than,
k
k
-NN with an optimal choice of
k
k
. In particular, we derive bounds on the convergence rates of our classifier that depend on a local quantity we call the
advantage'' which is significantly weaker than the Lipschitz conditions used in previous convergence rate proofs. These generalization bounds hinge on a variant of the seminal Uniform Convergence Theorem due to Vapnik and Chervonenkis; this variant concerns conditional probabilities and may be of independent interest."
neurips,https://proceedings.neurips.cc/paper/2019/file/a6b8deb7798e7532ade2a8934477d3ce-Paper.pdf,Variational Graph Recurrent Neural Networks,"Ehsan Hajiramezanali, Arman Hasanzadeh, Krishna Narayanan, Nick Duffield, Mingyuan Zhou, Xiaoning Qian",
neurips,https://proceedings.neurips.cc/paper/2019/file/a6b964c0bb675116a15ef1325b01ff45-Paper.pdf,Stochastic Bandits with Context Distributions,"Johannes Kirschner, Andreas Krause",
neurips,https://proceedings.neurips.cc/paper/2019/file/a6e4f250fb5c56aaf215a236c64e5b0a-Paper.pdf,Geometry-Aware Neural Rendering,"Joshua Tobin, Wojciech Zaremba, Pieter Abbeel","Understanding the 3-dimensional structure of the world is a core challenge in computer vision and robotics. Neural rendering approaches learn an implicit 3D model by predicting what a camera would see from an arbitrary viewpoint. We extend existing neural rendering to more complex, higher dimensional scenes than previously possible. We propose Epipolar Cross Attention (ECA), an attention mechanism that leverages the geometry of the scene to perform efficient non-local operations, requiring only
O
(
n
)
O
comparisons per spatial dimension instead of
O
(
n
2
)
O
. We introduce three new simulated datasets inspired by real-world robotics and demonstrate that ECA significantly improves the quantitative and qualitative performance of Generative Query Networks (GQN)."
neurips,https://proceedings.neurips.cc/paper/2019/file/a6ea8471c120fe8cc35a2954c9b9c595-Paper.pdf,Training Language GANs from Scratch,"Cyprien de Masson d'Autume, Shakir Mohamed, Mihaela Rosca, Jack Rae",
neurips,https://proceedings.neurips.cc/paper/2019/file/a70145bf8b173e4496b554ce57969e24-Paper.pdf,Generalization Bounds in the Predict-then-Optimize Framework,"Othman El Balghiti, Adam N. Elmachtoub, Paul Grigas, Ambuj Tewari",
neurips,https://proceedings.neurips.cc/paper/2019/file/a724b9124acc7b5058ed75a31a9c2919-Paper.pdf,Almost Horizon-Free Structure-Aware Best Policy Identification with a Generative Model,"Andrea Zanette, Mykel J. Kochenderfer, Emma Brunskill","This paper focuses on the problem of computing an
ϵ
ϵ
-optimal policy in a discounted Markov Decision Process (MDP) provided that we can access the reward and transition function through a generative model. We propose an algorithm that is initially agnostic to the MDP but that can leverage the specific MDP structure, expressed in terms of variances of the rewards and next-state value function, and gaps in the optimal action-value function to reduce the sample complexity needed to find a good policy, precisely highlighting the contribution of each state-action pair to the final sample complexity. A key feature of our analysis is that it removes all horizon dependencies in the sample complexity of suboptimal actions except for the intrinsic scaling of the value function and a constant additive term."
neurips,https://proceedings.neurips.cc/paper/2019/file/a7471fdc77b3435276507cc8f2dc2569-Paper.pdf,On the (In)fidelity and Sensitivity of Explanations,"Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Suggala, David I. Inouye, Pradeep K. Ravikumar",
neurips,https://proceedings.neurips.cc/paper/2019/file/a76c0abe2b7b1b79e70f0073f43c3b44-Paper.pdf,Manifold denoising by Nonlinear Robust Principal Component Analysis,"He Lyu, Ningyu Sha, Shuyang Qin, Ming Yan, Yuying Xie, Rongrong Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/a76da37101dffabe00e5d636c01719b6-Paper.pdf,Foundations of Comparison-Based Hierarchical Clustering,"Debarghya Ghoshdastidar, Michaël Perrot, Ulrike von Luxburg",
neurips,https://proceedings.neurips.cc/paper/2019/file/a78482ce76496fcf49085f2190e675b4-Paper.pdf,On the Accuracy of Influence Functions for Measuring Group Effects,"Pang Wei W. Koh, Kai-Siang Ang, Hubert Teo, Percy S. Liang",
neurips,https://proceedings.neurips.cc/paper/2019/file/a7a3d70c6d17a73140918996d03c014f-Paper.pdf,Neural Similarity Learning,"Weiyang Liu, Zhen Liu, James M. Rehg, Le Song",
neurips,https://proceedings.neurips.cc/paper/2019/file/a7b7e4b27722574c611fe91476a50238-Paper.pdf,Multi-objective Bayesian optimisation with preferences over objectives,"Majid Abdolshah, Alistair Shilton, Santu Rana, Sunil Gupta, Svetha Venkatesh",
neurips,https://proceedings.neurips.cc/paper/2019/file/a8345c3bb9e3896ea538ce77ffaf2c20-Paper.pdf,Global Convergence of Least Squares EM for Demixing Two Log-Concave Densities,"Wei Qian, Yuqian Zhang, Yudong Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/a87c11b9100c608b7f8e98cfa316ff7b-Paper.pdf,The Case for Evaluating Causal Models Using Interventional Measures and Empirical Data,"Amanda Gentzel, Dan Garant, David Jensen",
neurips,https://proceedings.neurips.cc/paper/2019/file/a941493eeea57ede8214fd77d41806bc-Paper.pdf,Spatially Aggregated Gaussian Processes with Multivariate Areal Outputs,"Yusuke Tanaka, Toshiyuki Tanaka, Tomoharu Iwata, Takeshi Kurashima, Maya Okawa, Yasunori Akagi, Hiroyuki Toda",
neurips,https://proceedings.neurips.cc/paper/2019/file/a97da629b098b75c294dffdc3e463904-Paper.pdf,First Exit Time Analysis of Stochastic Gradient Descent Under Heavy-Tailed Gradient Noise,"Thanh Huy Nguyen, Umut Simsekli, Mert Gurbuzbalaban, Gaël RICHARD","Stochastic gradient descent (SGD) has been widely used in machine learning due to its computational efficiency and favorable generalization properties. Recently, it has been empirically demonstrated that the gradient noise in several deep learning settings admits a non-Gaussian, heavy-tailed behavior. This suggests that the gradient noise can be modeled by using
α
α
-stable distributions, a family of heavy-tailed distributions that appear in the generalized central limit theorem. In this context, SGD can be viewed as a discretization of a stochastic differential equation (SDE) driven by a L\'{e}vy motion, and the metastability results for this SDE can then be used for illuminating the behavior of SGD, especially in terms of `preferring wide minima'. While this approach brings a new perspective for analyzing SGD, it is limited in the sense that, due to the time discretization, SGD might admit a significantly different behavior than its continuous-time limit. Intuitively, the behaviors of these two systems are expected to be similar to each other only when the discretization step is sufficiently small; however, to the best of our knowledge, there is no theoretical understanding on how small the step-size should be chosen in order to guarantee that the discretized system inherits the properties of the continuous-time system. In this study, we provide formal theoretical analysis where we derive explicit conditions for the step-size such that the metastability behavior of the discrete-time system is similar to its continuous-time limit. We show that the behaviors of the two systems are indeed similar for small step-sizes and we identify how the error depends on the algorithm and problem parameters. We illustrate our results with simulations on a synthetic model and neural networks."
neurips,https://proceedings.neurips.cc/paper/2019/file/a9986cb066812f440bc2bb6e3c13696c-Paper.pdf,Acceleration via Symplectic Discretization of High-Resolution Differential Equations,"Bin Shi, Simon S. Du, Weijie Su, Michael I. Jordan",
neurips,https://proceedings.neurips.cc/paper/2019/file/a9ad5f2808f68eea468621a04c49efe1-Paper.pdf,Seeing the Wind: Visual Wind Speed Prediction with a Coupled Convolutional and Recurrent Neural Network,"Jennifer Cardona, Michael Howland, John Dabiri",
neurips,https://proceedings.neurips.cc/paper/2019/file/a9be4c2a4041cadbf9d61ae16dd1389e-Paper.pdf,Hyper-Graph-Network Decoders for Block Codes,"Eliya Nachmani, Lior Wolf",
neurips,https://proceedings.neurips.cc/paper/2019/file/a9cc6694dc40736d7a2ec018ea566113-Paper.pdf,Sliced Gromov-Wasserstein,"Vayer Titouan, Rémi Flamary, Nicolas Courty, Romain Tavenard, Laetitia Chapel","Recently used in various machine learning contexts, the Gromov-Wasserstein distance (GW) allows for comparing distributions whose supports do not necessarily lie in the same metric space. However, this Optimal Transport (OT) distance requires solving a complex non convex quadratic program which is most of the time very costly both in time and memory. Contrary to GW, the Wasserstein distance (W) enjoys several properties ({\em e.g.} duality) that permit large scale optimization. Among those, the solution of W on the real line, that only requires sorting discrete samples in 1D, allows defining the Sliced Wasserstein (SW) distance. This paper proposes a new divergence based on GW akin to SW. We first derive a closed form for GW when dealing with 1D distributions, based on a new result for the related quadratic assignment problem. We then define a novel OT discrepancy that can deal with large scale distributions via a slicing approach and we show how it relates to the GW distance while being
O
(
n
log
(
n
)
)
O
to compute. We illustrate the behavior of this so called Sliced Gromov-Wasserstein (SGW) discrepancy in experiments where we demonstrate its ability to tackle similar problems as GW while being several order of magnitudes faster to compute."
neurips,https://proceedings.neurips.cc/paper/2019/file/aa36c88c27650af3b9868b723ae15dfc-Paper.pdf,Sparse Logistic Regression Learns All Discrete Pairwise Graphical Models,"Shanshan Wu, Sujay Sanghavi, Alexandros G. Dimakis","We characterize the effectiveness of a classical algorithm for recovering the Markov graph of a general discrete pairwise graphical model from i.i.d. samples. The algorithm is (appropriately regularized) maximum conditional log-likelihood, which involves solving a convex program for each node; for Ising models this is
ℓ
1
ℓ
-constrained logistic regression, while for more general alphabets an
ℓ
2
,
1
ℓ
group-norm constraint needs to be used. We show that this algorithm can recover any arbitrary discrete pairwise graphical model, and also characterize its sample complexity as a function of model width, alphabet size, edge parameter accuracy, and the number of variables. We show that along every one of these axes, it matches or improves on all existing results and algorithms for this problem. Our analysis applies a sharp generalization error bound for logistic regression when the weight vector has an
ℓ
1
ℓ
(or
ℓ
2
,
1
ℓ
) constraint and the sample vector has an
ℓ
∞
ℓ
(or
ℓ
2
,
∞
ℓ
) constraint. We also show that the proposed convex programs can be efficiently solved in
~
O
(
n
2
)
O
running time (where
n
n
is the number of variables) under the same statistical guarantees. We provide experimental results to support our analysis."
neurips,https://proceedings.neurips.cc/paper/2019/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf,Coordinated hippocampal-entorhinal replay as structural inference,"Talfan Evans, Neil Burgess",
neurips,https://proceedings.neurips.cc/paper/2019/file/aac933717a429f57c6ca58f32975c597-Paper.pdf,A Linearly Convergent Method for Non-Smooth Non-Convex Optimization on the Grassmannian with Applications to Robust Subspace and Dictionary Learning,"Zhihui Zhu, Tianyu Ding, Daniel Robinson, Manolis Tsakiris, René Vidal",
neurips,https://proceedings.neurips.cc/paper/2019/file/aaebdb8bb6b0e73f6c3c54a0ab0c6415-Paper.pdf,Finite-time Analysis of Approximate Policy Iteration for the Linear Quadratic Regulator,"Karl Krauth, Stephen Tu, Benjamin Recht","We study the sample complexity of approximate policy iteration (PI) for the Linear Quadratic Regulator (LQR), building on a recent line of work using LQR as a testbed to understand the limits of reinforcement learning (RL) algorithms on continuous control tasks. Our analysis quantifies the tension between policy improvement and policy evaluation, and suggests that policy evaluation is the dominant factor in terms of sample complexity. Specifically, we show that to obtain a controller that is within
ε
ε
of the optimal LQR controller, each step of policy evaluation requires at most
(
n
+
d
)
3
/
ε
2
(
samples, where
n
n
is the dimension of the state vector and
d
d
is the dimension of the input vector. On the other hand, only
log
(
1
/
ε
)
log
policy improvement steps suffice, resulting in an overall sample complexity of
(
n
+
d
)
3
ε
−
2
log
(
1
/
ε
)
(
. We furthermore build on our analysis and construct a simple adaptive procedure based on
ε
ε
-greedy exploration which relies on approximate PI as a sub-routine and obtains
T
2
/
3
T
regret, improving upon a recent result of Abbasi-Yadkori et al. 2019."
neurips,https://proceedings.neurips.cc/paper/2019/file/ab49ef78e2877bfd2c2bfa738e459bf0-Paper.pdf,The Impact of Regularization on High-dimensional Logistic Regression,"Fariborz Salehi, Ehsan Abbasi, Babak Hassibi","Logistic regression is commonly used for modeling dichotomous outcomes. In the classical setting, where the number of observations is much larger than the number of parameters, properties of the maximum likelihood estimator in logistic regression are well understood. Recently, Sur and Candes~\cite{sur2018modern} have studied logistic regression in the high-dimensional regime, where the number of observations and parameters are comparable, and show, among other things, that the maximum likelihood estimator is biased. In the high-dimensional regime the underlying parameter vector is often structured (sparse, block-sparse, finite-alphabet, etc.) and so in this paper we study regularized logistic regression (RLR), where a convex regularizer that encourages the desired structure is added to the negative of the log-likelihood function. An advantage of RLR is that it allows parameter recovery even for instances where the (unconstrained) maximum likelihood estimate does not exist. We provide a precise analysis of the performance of RLR via the solution of a system of six nonlinear equations, through which any performance metric of interest (mean, mean-squared error, probability of support recovery, etc.) can be explicitly computed. Our results generalize those of Sur and Candes and we provide a detailed study for the cases of
ℓ
2
2
ℓ
-RLR and sparse (
ℓ
1
ℓ
-regularized) logistic regression. In both cases, we obtain explicit expressions for various performance metrics and can find the values of the regularizer parameter that optimizes the desired performance. The theory is validated by extensive numerical simulations across a range of parameter values and problem instances."
neurips,https://proceedings.neurips.cc/paper/2019/file/ab817c9349cf9c4f6877e1894a1faa00-Paper.pdf,Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition,"Jinwoo Choi, Chen Gao, Joseph C. E. Messou, Jia-Bin Huang",
neurips,https://proceedings.neurips.cc/paper/2019/file/aba22f748b1a6dff75bda4fd1ee9fe07-Paper.pdf,(Nearly) Efficient Algorithms for the Graph Matching Problem on Correlated Random Graphs,"Boaz Barak, Chi-Ning Chou, Zhixian Lei, Tselil Schramm, Yueqi Sheng","We consider the graph matching/similarity problem of determining how similar two given graphs
G
0
,
G
1
G
are and recovering the permutation
π
π
on the vertices of
G
1
G
that minimizes the symmetric difference between the edges of
G
0
G
and
π
(
G
1
)
π
. Graph matching/similarity has applications for pattern matching, vision, social network anonymization, malware analysis, and more. We give the first efficient algorithms proven to succeed in the correlated Erdös-Rényi model (Pedarsani and Grossglauser, 2011). Specifically, we give a polynomial time algorithm for the graph similarity/hypothesis testing task which works for every constant level of correlation between the two graphs that can be arbitrarily close to zero. We also give a quasi-polynomial (
n
O
(
log
n
)
n
time) algorithm for the graph matching task of recovering the permutation minimizing the symmetric difference in this model. This is the first algorithm to do so without requiring as additional input a
seed'' of the values of the ground truth permutation on at least
n
Ω
(
1
)
n
vertices. Our algorithms follow a general framework of counting the occurrences of subgraphs from a particular family of graphs allowing for tradeoffs between efficiency and accuracy."
neurips,https://proceedings.neurips.cc/paper/2019/file/abdbeb4d8dbe30df8430a8394b7218ef-Paper.pdf,Cross-sectional Learning of Extremal Dependence among Financial Assets,"Xing Yan, Qi Wu, Wen Zhang","We propose a novel probabilistic model to facilitate the learning of multivariate tail dependence of multiple financial assets. Our method allows one to construct from known random vectors, e.g., standard normal, sophisticated joint heavy-tailed random vectors featuring not only distinct marginal tail heaviness, but also flexible tail dependence structure. The novelty lies in that pairwise tail dependence between any two dimensions is modeled separately from their correlation, and can vary respectively according to its own parameter rather than the correlation parameter, which is an essential advantage over many commonly used methods such as multivariate
t
t
or elliptical distribution. It is also intuitive to interpret, easy to track, and simple to sample comparing to the copula approach. We show its flexible tail dependence structure through simulation. Coupled with a GARCH model to eliminate serial dependence of each individual asset return series, we use this novel method to model and forecast multivariate conditional distribution of stock returns, and obtain notable performance improvements in multi-dimensional coverage tests. Besides, our empirical finding about the asymmetry of tails of the idiosyncratic component as well as the market component is interesting and worth to be well studied in the future."
neurips,https://proceedings.neurips.cc/paper/2019/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf,Invert to Learn to Invert,"Patrick Putzky, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2019/file/ac27b77292582bc293a51055bfc994ee-Paper.pdf,Metamers of neural networks reveal divergence from human perceptual systems,"Jenelle Feather, Alex Durango, Ray Gonzalez, Josh McDermott",
neurips,https://proceedings.neurips.cc/paper/2019/file/ac52c626afc10d4075708ac4c778ddfc-Paper.pdf,Optimal Sparse Decision Trees,"Xiyang Hu, Cynthia Rudin, Margo Seltzer",
neurips,https://proceedings.neurips.cc/paper/2019/file/ac5dab2e99eee9cf9ec672e383691302-Paper.pdf,Distinguishing Distributions When Samples Are Strategically Transformed,"Hanrui Zhang, Yu Cheng, Vincent Conitzer","In this paper, we give necessary and sufficient conditions for when the principal can distinguish between agents of good'' andbad'' types, when the type affects the distribution of samples that the agent has access to. We also study the computational complexity of checking these conditions. Finally, we study how many samples are needed."
neurips,https://proceedings.neurips.cc/paper/2019/file/ac796a52db3f16bbdb6557d3d89d1c5a-Paper.pdf,Positive-Unlabeled Compression on the Cloud,"Yixing Xu, Yunhe Wang, Hanting Chen, Kai Han, Chunjing XU, Dacheng Tao, Chang Xu",
neurips,https://proceedings.neurips.cc/paper/2019/file/aceacd5df18526f1d96ee1b9714e95eb-Paper.pdf,Nonparametric Contextual Bandits in Metric Spaces with Unknown Metric,"Nirandika Wanigasekara, Christina Yu","Consider a nonparametric contextual multi-arm bandit problem where each arm
a
∈
[
K
]
a
is associated to a nonparametric reward function
f
a
:
[
0
,
1
]
→
R
f
mapping from contexts to the expected reward. Suppose that there is a large set of arms, yet there is a simple but unknown structure amongst the arm reward functions, e.g. finite types or smooth with respect to an unknown metric space. We present a novel algorithm which learns data-driven similarities amongst the arms, in order to implement adaptive partitioning of the context-arm space for more efficient learning. We provide regret bounds along with simulations that highlight the algorithm's dependence on the local geometry of the reward functions."
neurips,https://proceedings.neurips.cc/paper/2019/file/ad13a2a07ca4b7642959dc0c4c740ab6-Paper.pdf,Staying up to Date with Online Content Changes Using Reinforcement Learning for Scheduling,"Andrey Kolobov, Yuval Peres, Cheng Lu, Eric J. Horvitz",
neurips,https://proceedings.neurips.cc/paper/2019/file/ad3019b856147c17e82a5bead782d2a8-Paper.pdf,Interlaced Greedy Algorithm for Maximization of Submodular Functions in Nearly Linear Time,Alan Kuhnle,"A deterministic approximation algorithm is presented for the maximization of non-monotone submodular functions over a ground set of size
n
n
subject to cardinality constraint
k
k
; the algorithm is based upon the idea of interlacing two greedy procedures. The algorithm uses interlaced, thresholded greedy procedures to obtain tight ratio
1
/
4
−
ϵ
1
in
O
(
n
ϵ
log
(
k
ϵ
)
)
O
queries of the objective function, which improves upon both the ratio and the quadratic time complexity of the previously fastest deterministic algorithm for this problem. The algorithm is validated in the context of two applications of non-monotone submodular maximization, on which it outperforms the fastest deterministic and randomized algorithms in prior literature."
neurips,https://proceedings.neurips.cc/paper/2019/file/add5aebfcb33a2206b6497d53bc4f309-Paper.pdf,A unified variance-reduced accelerated gradient method for convex optimization,"Guanghui Lan, Zhize Li, Yi Zhou",
neurips,https://proceedings.neurips.cc/paper/2019/file/addfa9b7e234254d26e9c7f2af1005cb-Paper.pdf,SSRGD: Simple Stochastic Recursive Gradient Descent for Escaping Saddle Points,Zhize Li,"We analyze stochastic gradient algorithms for optimizing nonconvex problems. In particular, our goal is to find local minima (second-order stationary points) instead of just finding first-order stationary points which may be some bad unstable saddle points. We show that a simple perturbed version of stochastic recursive gradient descent algorithm (called SSRGD) can find an
(
ϵ
,
δ
)
(
-second-order stationary point with
˜
O
(
√
n
/
ϵ
2
+
√
n
/
δ
4
+
n
/
δ
3
)
O
stochastic gradient complexity for nonconvex finite-sum problems. As a by-product, SSRGD finds an
ϵ
ϵ
-first-order stationary point with
O
(
n
+
√
n
/
ϵ
2
)
O
stochastic gradients. These results are almost optimal since Fang et al. [2018] provided a lower bound
Ω
(
√
n
/
ϵ
2
)
Ω
for finding even just an
ϵ
ϵ
-first-order stationary point. We emphasize that SSRGD algorithm for finding second-order stationary points is as simple as for finding first-order stationary points just by adding a uniform perturbation sometimes, while all other algorithms for finding second-order stationary points with similar gradient complexity need to combine with a negative-curvature search subroutine (e.g., Neon2 [Allen-Zhu and Li, 2018]). Moreover, the simple SSRGD algorithm gets a simpler analysis. Besides, we also extend our results from nonconvex finite-sum problems to nonconvex online (expectation) problems, and prove the corresponding convergence results."
neurips,https://proceedings.neurips.cc/paper/2019/file/adf7ee2dcf142b0e11888e72b43fcb75-Paper.pdf,This Looks Like That: Deep Learning for Interpretable Image Recognition,"Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, Jonathan K. Su",
neurips,https://proceedings.neurips.cc/paper/2019/file/ae2a2db40a12ec0131d48acc1218d2ef-Paper.pdf,Online EXP3 Learning in Adversarial Bandits with Delayed Feedback,"Ilai Bistritz, Zhengyuan Zhou, Xi Chen, Nicholas Bambos, Jose Blanchet",
neurips,https://proceedings.neurips.cc/paper/2019/file/ae31ee951b4d4bfb5518e0fcdc064a83-Paper.pdf,Phase Transitions and Cyclic Phenomena in Bandits with Switching Constraints,"David Simchi-Levi, Yunzong Xu",
neurips,https://proceedings.neurips.cc/paper/2019/file/ae3539867aaeec609a4260c6feb725f4-Paper.pdf,Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning,"Wonjae Kim, Yoonho Lee",
neurips,https://proceedings.neurips.cc/paper/2019/file/ae3f4c649fb55c2ee3ef4d1abdb79ce5-Paper.pdf,Provable Certificates for Adversarial Examples: Fitting a Ball in the Union of Polytopes,"Matt Jordan, Justin Lewis, Alexandros G. Dimakis",
neurips,https://proceedings.neurips.cc/paper/2019/file/ae587cfeea5ac21a8f1c1ea51027fef0-Paper.pdf,Fast Parallel Algorithms for Statistical Subset Selection Problems,"Sharon Qian, Yaron Singer",
neurips,https://proceedings.neurips.cc/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf,On Lazy Training in Differentiable Programming,"Lénaïc Chizat, Edouard Oyallon, Francis Bach",
neurips,https://proceedings.neurips.cc/paper/2019/file/aec851e565646f6835e915293381e20a-Paper.pdf,Estimating Convergence of Markov chains with L-Lag Couplings,"Niloy Biswas, Pierre E. Jacob, Paul Vanetti",
neurips,https://proceedings.neurips.cc/paper/2019/file/aee92f16efd522b9326c25cc3237ac15-Paper.pdf,Efficient Regret Minimization Algorithm for Extensive-Form Correlated Equilibrium,"Gabriele Farina, Chun Kai Ling, Fei Fang, Tuomas Sandholm",
neurips,https://proceedings.neurips.cc/paper/2019/file/af1c25e88a9e818f809f6b5d18ca02e2-Paper.pdf,Using Embeddings to Correct for Unobserved Confounding in Networks,"Victor Veitch, Yixin Wang, David Blei",
neurips,https://proceedings.neurips.cc/paper/2019/file/af3b6a54e9e9338abc54258e3406e485-Paper.pdf,Towards Practical Alternating Least-Squares for CCA,"Zhiqiang Xu, Ping Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/af8d1eb220186400c494db7091e402b0-Paper.pdf,Neural Multisensory Scene Inference,"Jae Hyun Lim, Pedro O. O. Pinheiro, Negar Rostamzadeh, Chris Pal, Sungjin Ahn",
neurips,https://proceedings.neurips.cc/paper/2019/file/af8d9c4e238c63fb074b44eb6aed80ae-Paper.pdf,Emergence of Object Segmentation in Perturbed Generative Models,"Adam Bielski, Paolo Favaro",
neurips,https://proceedings.neurips.cc/paper/2019/file/afe434653a898da20044041262b3ac74-Paper.pdf,Learning Transferable Graph Exploration,"Hanjun Dai, Yujia Li, Chenglong Wang, Rishabh Singh, Po-Sen Huang, Pushmeet Kohli",
neurips,https://proceedings.neurips.cc/paper/2019/file/aff0a6a4521232970b2c1cf539ad0a19-Paper.pdf,On the Optimality of Perturbations in Stochastic and Adversarial Multi-armed Bandit Problems,"Baekjin Kim, Ambuj Tewari",
neurips,https://proceedings.neurips.cc/paper/2019/file/b030afbb3a8af8fb0759241c97466ee4-Paper.pdf,Optimistic Regret Minimization for Extensive-Form Games via Dilated Distance-Generating Functions,"Gabriele Farina, Christian Kroer, Tuomas Sandholm","We study the performance of optimistic regret-minimization algorithms for both minimizing regret in, and computing Nash equilibria of, zero-sum extensive-form games. In order to apply these algorithms to extensive-form games, a distance-generating function is needed. We study the use of the dilated entropy and dilated Euclidean distance functions. For the dilated Euclidean distance function we prove the first explicit bounds on the strong-convexity parameter for general treeplexes. Furthermore, we show that the use of dilated distance-generating functions enable us to decompose the mirror descent algorithm, and its optimistic variant, into local mirror descent algorithms at each information set. This decomposition mirrors the structure of the counterfactual regret minimization framework, and enables important techniques in practice, such as distributed updates and pruning of cold parts of the game tree. Our algorithms provably converge at a rate of
T
−
1
T
, which is superior to prior counterfactual regret minimization algorithms. We experimentally compare to the popular algorithm CFR+, which has a theoretical convergence rate of
T
−
0.5
T
in theory, but is known to often converge at a rate of
T
−
1
T
, or better, in practice. We give an example matrix game where CFR+ experimentally converges at a relatively slow rate of
T
−
0.74
T
, whereas our optimistic methods converge faster than
T
−
1
T
. We go on to show that our fast rate also holds in the Kuhn poker game, which is an extensive-form game. For games with deeper game trees however, we find that CFR+ is still faster. Finally we show that when the goal is minimizing regret, rather than computing a Nash equilibrium, our optimistic methods can outperform CFR+, even in deep game trees."
neurips,https://proceedings.neurips.cc/paper/2019/file/b05b57f6add810d3b7490866d74c0053-Paper.pdf,A Fourier Perspective on Model Robustness in Computer Vision,"Dong Yin, Raphael Gontijo Lopes, Jon Shlens, Ekin Dogus Cubuk, Justin Gilmer",
neurips,https://proceedings.neurips.cc/paper/2019/file/b075703bbe07a50ddcccfaac424bb6d9-Paper.pdf,Two Generator Game: Learning to Sample via Linear Goodness-of-Fit Test,"Lizhong Ding, Mengyang Yu, Li Liu, Fan Zhu, Yong Liu, Yu Li, Ling Shao",
neurips,https://proceedings.neurips.cc/paper/2019/file/b0b183c207f46f0cca7dc63b2604f5cc-Paper.pdf,Fixing Implicit Derivatives: Trust-Region Based Learning of Continuous Energy Functions,"Chris Russell, Matteo Toso, Neill Campbell",
neurips,https://proceedings.neurips.cc/paper/2019/file/b0ba5c44aaf65f6ca34cf116e6d82ebf-Paper.pdf,Correlation Clustering with Adaptive Similarity Queries,"Marco Bressan, Nicolò Cesa-Bianchi, Andrea Paudice, Fabio Vitale","In correlation clustering, we are given
n
n
objects together with a binary similarity score between each pair of them. The goal is to partition the objects into clusters so to minimise the disagreements with the scores. In this work we investigate correlation clustering as an active learning problem: each similarity score can be learned by making a query, and the goal is to minimise both the disagreements and the total number of queries. On the one hand, we describe simple active learning algorithms, which provably achieve an almost optimal trade-off while giving cluster recovery guarantees, and we test them on different datasets. On the other hand, we prove information-theoretical bounds on the number of queries necessary to guarantee a prescribed disagreement bound. These results give a rich characterization of the trade-off between queries and clustering error."
neurips,https://proceedings.neurips.cc/paper/2019/file/b0bef4c9a6e50d43880191492d4fc827-Paper.pdf,Deep imitation learning for molecular inverse problems,Eric Jonas,
neurips,https://proceedings.neurips.cc/paper/2019/file/b0cf188d74589db9b23d5d277238a929-Paper.pdf,Ease-of-Teaching and Language Structure from Emergent Communication,"Fushan Li, Michael Bowling",
neurips,https://proceedings.neurips.cc/paper/2019/file/b139e104214a08ae3f2ebcce149cdf6e-Paper.pdf,Practical Differentially Private Top-k Selection with Pay-what-you-get Composition,"David Durfee, Ryan M. Rogers",
neurips,https://proceedings.neurips.cc/paper/2019/file/b151ce4935a3c2807e1dd9963eda16d8-Paper.pdf,A Communication Efficient Stochastic Multi-Block Alternating Direction Method of Multipliers,Hao Yu,
neurips,https://proceedings.neurips.cc/paper/2019/file/b1b20d09041289e6c3fbb81850c5da54-Paper.pdf,Distributed estimation of the inverse Hessian by determinantal averaging,"Michal Derezinski, Michael W. Mahoney",
neurips,https://proceedings.neurips.cc/paper/2019/file/b1d10e7bafa4421218a51b1e1f1b0ba2-Paper.pdf,muSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking,"Congchao Wang, Yizhi Wang, Yinxue Wang, Chiung-Ting Wu, Guoqiang Yu",
neurips,https://proceedings.neurips.cc/paper/2019/file/b1f62fa99de9f27a048344d55c5ef7a6-Paper.pdf,Invertible Convolutional Flow,"Mahdi Karami, Dale Schuurmans, Jascha Sohl-Dickstein, Laurent Dinh, Daniel Duckworth",
neurips,https://proceedings.neurips.cc/paper/2019/file/b20bb95ab626d93fd976af958fbc61ba-Paper.pdf,Controlling Neural Level Sets,"Matan Atzmon, Niv Haim, Lior Yariv, Ofer Israelov, Haggai Maron, Yaron Lipman","In this paper we present a simple and scalable approach to directly control level sets of a deep neural network. Our method consists of two parts: (i) sampling of the neural level sets, and (ii) relating the samples' positions to the network parameters. The latter is achieved by a sample network that is constructed by adding a single fixed linear layer to the original network. In turn, the sample network can be used to incorporate the level set samples into a loss function of interest."
neurips,https://proceedings.neurips.cc/paper/2019/file/b23f52202479e957b9bada847c1175d7-Paper.pdf,Learning GANs and Ensembles Using Discrepancy,"Ben Adlam, Corinna Cortes, Mehryar Mohri, Ningshan Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/b294504229c668e750dfcc4ea9617f0a-Paper.pdf,Neural Relational Inference with Fast Modular Meta-learning,"Ferran Alet, Erica Weng, Tomás Lozano-Pérez, Leslie Pack Kaelbling",
neurips,https://proceedings.neurips.cc/paper/2019/file/b2ead76dfdc4ae56a2abd1896ec46291-Paper.pdf,Identification of Conditional Causal Effects under Markov Equivalence,"Amin Jaber, Jiji Zhang, Elias Bareinboim",
neurips,https://proceedings.neurips.cc/paper/2019/file/b2eb7349035754953b57a32e2841bda5-Paper.pdf,Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis,"Xihui Liu, Guojun Yin, Jing Shao, Xiaogang Wang, hongsheng Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/b32e8760418e68f23c811a1cfd6bda78-Paper.pdf,"Average Case Column Subset Selection for Entrywise
ℓ
1
ℓ
-Norm Loss","Zhao Song, David Woodruff, Peilin Zhong","We study the column subset selection problem with respect to the entrywise
ℓ
1
ℓ
-norm loss. It is known that in the worst case, to obtain a good rank-
k
k
approximation to a matrix, one needs an arbitrarily large
n
Ω
(
1
)
n
number of columns to obtain a
(
1
+
ϵ
)
(
-approximation to an
n
×
n
n
matrix. Nevertheless, we show that under certain minimal and realistic distributional settings, it is possible to obtain a
(
1
+
ϵ
)
(
-approximation with a nearly linear running time and poly
(
k
/
ϵ
)
+
O
(
k
log
n
)
(
columns. Namely, we show that if the input matrix
A
A
has the form
A
=
B
+
E
A
, where
B
B
is an arbitrary rank-
k
k
matrix, and
E
E
is a matrix with i.i.d. entries drawn from any distribution
μ
μ
for which the
(
1
+
γ
)
(
-th moment exists, for an arbitrarily small constant
γ
>
0
γ
, then it is possible to obtain a
(
1
+
ϵ
)
(
-approximate column subset selection to the entrywise
ℓ
1
ℓ
-norm in nearly linear time. Conversely we show that if the first moment does not exist, then it is not possible to obtain a
(
1
+
ϵ
)
(
-approximate subset selection algorithm even if one chooses any
n
o
(
1
)
n
columns. This is the first algorithm of any kind for achieving a
(
1
+
ϵ
)
(
-approximation for entrywise
ℓ
1
ℓ
-norm loss low rank approximation."
neurips,https://proceedings.neurips.cc/paper/2019/file/b33128cb0089003ddfb5199e1b679652-Paper.pdf,Piecewise Strong Convexity of Neural Networks,Tristan Milne,
neurips,https://proceedings.neurips.cc/paper/2019/file/b3967a0e938dc2a6340e258630febd5a-Paper.pdf,No Pressure! Addressing the Problem of Local Minima in Manifold Learning Algorithms,Max Vladymyrov,
neurips,https://proceedings.neurips.cc/paper/2019/file/b3bbccd6c008e727785cb81b1aa08ac5-Paper.pdf,Approximate Inference Turns Deep Networks into Gaussian Processes,"Mohammad Emtiyaz E. Khan, Alexander Immer, Ehsan Abedi, Maciej Korzepa",
neurips,https://proceedings.neurips.cc/paper/2019/file/b3dd760eb02d2e669c604f6b2f1e803f-Paper.pdf,Elliptical Perturbations for Differential Privacy,"Matthew Reimherr, Jordan Awan","We study elliptical distributions in locally convex vector spaces, and determine conditions when they can or cannot be used to satisfy differential privacy (DP). A requisite condition for a sanitized statistical summary to satisfy DP is that the corresponding privacy mechanism must induce equivalent probability measures for all possible input databases. We show that elliptical distributions with the same dispersion operator,
C
C
, are equivalent if the difference of their means lies in the Cameron-Martin space of
C
C
. In the case of releasing finite-dimensional summaries using elliptical perturbations, we show that the privacy parameter
\ep
\ep
can be computed in terms of a one-dimensional maximization problem. We apply this result to consider multivariate Laplace,
t
t
, Gaussian, and
K
K
-norm noise. Surprisingly, we show that the multivariate Laplace noise does not achieve
\ep
\ep
-DP in any dimension greater than one. Finally, we show that when the dimension of the space is infinite, no elliptical distribution can be used to give
\ep
\ep
-DP; only
(
ϵ
,
δ
)
(
-DP is possible."
neurips,https://proceedings.neurips.cc/paper/2019/file/b4189d9de0fb2b9cce090bd1a15e3420-Paper.pdf,Inherent Tradeoffs in Learning Fair Representations,"Han Zhao, Geoff Gordon",
neurips,https://proceedings.neurips.cc/paper/2019/file/b432f34c5a997c8e7c806a895ecc5e25-Paper.pdf,SGD on Neural Networks Learns Functions of Increasing Complexity,"Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, Haofeng Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/b43a6403c17870707ca3c44984a2da22-Paper.pdf,Online Continuous Submodular Maximization: From Full-Information to Bandit Feedback,"Mingrui Zhang, Lin Chen, Hamed Hassani, Amin Karbasi","In this paper, we propose three online algorithms for submodular maximization. The first one, Mono-Frank-Wolfe, reduces the number of per-function gradient evaluations from
T
1
/
2
T
[Chen2018Online] and
T
3
/
2
T
[chen2018projection] to 1, and achieves a
(
1
−
1
/
e
)
(
-regret bound of
O
(
T
4
/
5
)
O
. The second one, Bandit-Frank-Wolfe, is the first bandit algorithm for continuous DR-submodular maximization, which achieves a
(
1
−
1
/
e
)
(
-regret bound of
O
(
T
8
/
9
)
O
. Finally, we extend Bandit-Frank-Wolfe to a bandit algorithm for discrete submodular maximization, Responsive-Frank-Wolfe, which attains a
(
1
−
1
/
e
)
(
-regret bound of
O
(
T
8
/
9
)
O
in the responsive bandit setting."
neurips,https://proceedings.neurips.cc/paper/2019/file/b49b0133825194a67e0660a5557deb0a-Paper.pdf,Optimistic Distributionally Robust Optimization for Nonparametric Likelihood Approximation,"Viet Anh Nguyen, Soroosh Shafieezadeh Abadeh, Man-Chung Yue, Daniel Kuhn, Wolfram Wiesemann",
neurips,https://proceedings.neurips.cc/paper/2019/file/b49fdab097253cac48e3dc628a49da5e-Paper.pdf,Don't take it lightly: Phasing optical random projections with unknown operators,"Sidharth Gupta, Remi Gribonval, Laurent Daudet, Ivan Dokmanić","In this paper we tackle the problem of recovering the phase of complex linear measurements when only magnitude information is available and we control the input. We are motivated by the recent development of dedicated optics-based hardware for rapid random projections which leverages the propagation of light in random media. A signal of interest
ξ
∈
R
N
ξ
is mixed by a random scattering medium to compute the projection
y
=
A
ξ
y
, with
A
∈
C
M
×
N
A
being a realization of a standard complex Gaussian iid random matrix. Such optics-based matrix multiplications can be much faster and energy-efficient than their CPU or GPU counterparts, yet two difficulties must be resolved: only the intensity
|
y
|
2
|
can be recorded by the camera, and the transmission matrix
A
A
is unknown. We show that even without knowing
A
A
, we can recover the unknown phase of
y
y
for some equivalent transmission matrix with the same distribution as
A
A
. Our method is based on two observations: first, conjugating or changing the phase of any row of
A
A
does not change its distribution; and second, since we control the input we can interfere
ξ
ξ
with arbitrary reference signals. We show how to leverage these observations to cast the measurement phase retrieval problem as a Euclidean distance geometry problem. We demonstrate appealing properties of the proposed algorithm in both numerical simulations and real hardware experiments. Not only does our algorithm accurately recover the missing phase, but it mitigates the effects of quantization and the sensitivity threshold, thus improving the measured magnitudes."
neurips,https://proceedings.neurips.cc/paper/2019/file/b4d168b48157c623fbd095b4a565b5bb-Paper.pdf,Visualizing the PHATE of Neural Networks,"Scott Gigante, Adam S. Charles, Smita Krishnaswamy, Gal Mishne",
neurips,https://proceedings.neurips.cc/paper/2019/file/b51a15f382ac914391a58850ab343b00-Paper.pdf,Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks,"Zhonghui You, Kun Yan, Jinmian Ye, Meng Ma, Ping Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/b522259710151f8cc7870b970b4e0930-Paper.pdf,"Kalman Filter, Sensor Fusion, and Constrained Regression: Equivalences and Insights","Maria Jahja, David Farrow, Roni Rosenfeld, Ryan J. Tibshirani","The Kalman filter (KF) is one of the most widely used tools for data assimilation and sequential estimation. In this work, we show that the state estimates from the KF in a standard linear dynamical system setting are equivalent to those given by the KF in a transformed system, with infinite process noise (i.e., a
flat prior'') and an augmented measurement space. This reformulation---which we refer to as augmented measurement sensor fusion (SF)---is conceptually interesting, because the transformed system here is seemingly static (as there is effectively no process model), but we can still capture the state dynamics inherent to the KF by folding the process model into the measurement space. Further, this reformulation of the KF turns out to be useful in settings in which past states are observed eventually (at some lag). Here, when the measurement noise covariance is estimated by the empirical covariance, we show that the state predictions from SF are equivalent to those from a regression of past states on past measurements, subject to particular linear constraints (reflecting the relationships encoded in the measurement map). This allows us to port standard ideas (say, regularization methods) in regression over to dynamical systems. For example, we can posit multiple candidate process models, fold all of them into the measurement model, transform to the regression perspective, and apply
ℓ
1
ℓ
penalization to perform process model selection. We give various empirical demonstrations, and focus on an application to nowcasting the weekly incidence of influenza in the US."
neurips,https://proceedings.neurips.cc/paper/2019/file/b53477c2821c1bf0da5d40e57b870d35-Paper.pdf,Practical Deep Learning with Bayesian Principles,"Kazuki Osawa, Siddharth Swaroop, Mohammad Emtiyaz E. Khan, Anirudh Jain, Runa Eschenhagen, Richard E. Turner, Rio Yokota",
neurips,https://proceedings.neurips.cc/paper/2019/file/b59307fdacf7b2db12ec4bd5ca1caba8-Paper.pdf,Deep Active Learning with a Neural Architecture Search,"Yonatan Geifman, Ran El-Yaniv",
neurips,https://proceedings.neurips.cc/paper/2019/file/b59a51a3c0bf9c5228fde841714f523a-Paper.pdf,Quality Aware Generative Adversarial Networks,"KANCHARLA PARIMALA, Sumohana Channappayya",
neurips,https://proceedings.neurips.cc/paper/2019/file/b5a1fc2085986034e448d2ccc5bb9703-Paper.pdf,Dual Variational Generation for Low Shot Heterogeneous Face Recognition,"Chaoyou Fu, Xiang Wu, Yibo Hu, Huaibo Huang, Ran He","Heterogeneous Face Recognition (HFR) is a challenging issue because of the large domain discrepancy and a lack of heterogeneous data. This paper considers HFR as a dual generation problem, and proposes a novel Dual Variational Generation (DVG) framework. It generates large-scale new paired heterogeneous images with the same identity from noise, for the sake of reducing the domain gap of HFR. Specifically, we first introduce a dual variational autoencoder to represent a joint distribution of paired heterogeneous images. Then, in order to ensure the identity consistency of the generated paired heterogeneous images, we impose a distribution alignment in the latent space and a pairwise identity preserving in the image space. Moreover, the HFR network reduces the domain discrepancy by constraining the pairwise feature distances between the generated paired heterogeneous images. Extensive experiments on four HFR databases show that our method can significantly improve state-of-the-art results. When using the generated paired images for training, our method gains more than 18\% True Positive Rate improvements over the baseline model when False Positive Rate is at
10
−
5
10
."
neurips,https://proceedings.neurips.cc/paper/2019/file/b5b03f06271f8917685d14cea7c6c50a-Paper.pdf,Off-Policy Evaluation via Off-Policy Classification,"Alexander Irpan, Kanishka Rao, Konstantinos Bousmalis, Chris Harris, Julian Ibarz, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2019/file/b5d3ad899f70013367f24e0b1fa75944-Paper.pdf,Variational Temporal Abstraction,"Taesup Kim, Sungjin Ahn, Yoshua Bengio",
neurips,https://proceedings.neurips.cc/paper/2019/file/b5dc4e5d9b495d0196f61d45b26ef33e-Paper.pdf,Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations,"Vincent Sitzmann, Michael Zollhoefer, Gordon Wetzstein",
neurips,https://proceedings.neurips.cc/paper/2019/file/b6f97e6f0fd175613910d613d574d0cb-Paper.pdf,Control What You Can: Intrinsically Motivated Task-Planning Agent,"Sebastian Blaes, Marin Vlastelica Pogančić, Jiajie Zhu, Georg Martius",
neurips,https://proceedings.neurips.cc/paper/2019/file/b8002139cdde66b87638f7f91d169d96-Paper.pdf,Momentum-Based Variance Reduction in Non-Convex SGD,"Ashok Cutkosky, Francesco Orabona","Variance reduction has emerged in recent years as a strong competitor to stochastic gradient descent in non-convex problems, providing the first algorithms to improve upon the converge rate of stochastic gradient descent for finding first-order critical points. However, variance reduction techniques typically require carefully tuned learning rates and willingness to use excessively large ""mega-batches"" in order to achieve their improved results. We present a new algorithm, STORM, that does not require any batches and makes use of adaptive learning rates, enabling simpler implementation and less hyperparameter tuning. Our technique for removing the batches uses a variant of momentum to achieve variance reduction in non-convex optimization. On smooth losses
F
F
, STORM finds a point
x
x
with
E
[
∥
∇
F
(
x
)
∥
]
≤
O
(
1
/
√
T
+
σ
1
/
3
/
T
1
/
3
)
E
in
T
T
iterations with
σ
2
σ
variance in the gradients, matching the best-known rate but without requiring knowledge of
σ
σ
."
neurips,https://proceedings.neurips.cc/paper/2019/file/b83aac23b9528732c23cc7352950e880-Paper.pdf,Adversarial Self-Defense for Cycle-Consistent GANs,"Dina Bashkirova, Ben Usman, Kate Saenko",
neurips,https://proceedings.neurips.cc/paper/2019/file/b865367fc4c0845c0682bd466e6ebf4c-Paper.pdf,Ultrametric Fitting by Gradient Descent,"Giovanni Chierchia, Benjamin Perret",
neurips,https://proceedings.neurips.cc/paper/2019/file/b86e8d03fe992d1b0e19656875ee557c-Paper.pdf,Expressive power of tensor-network factorizations for probabilistic modeling,"Ivan Glasser, Ryan Sweke, Nicola Pancotti, Jens Eisert, Ignacio Cirac",
neurips,https://proceedings.neurips.cc/paper/2019/file/b87517992f7dce71b674976b280257d2-Paper.pdf,PerspectiveNet: 3D Object Detection from a Single RGB Image via Perspective Points,"Siyuan Huang, Yixin Chen, Tao Yuan, Siyuan Qi, Yixin Zhu, Song-Chun Zhu",
neurips,https://proceedings.neurips.cc/paper/2019/file/b8c8c63d4b8856c7872b225e53a6656c-Paper.pdf,Landmark Ordinal Embedding,"Nikhil Ghosh, Yuxin Chen, Yisong Yue",
neurips,https://proceedings.neurips.cc/paper/2019/file/b91f4f4d36fa98a94ac5584af95594a0-Paper.pdf,On the Value of Target Data in Transfer Learning,"Steve Hanneke, Samory Kpotufe","To this aim, we establish the first minimax-rates in terms of both source and target sample sizes, and show that performance limits are captured by new notions of discrepancy between source and target, which we refer to as transfer exponents."
neurips,https://proceedings.neurips.cc/paper/2019/file/b98a3773ecf715751d3cf0fb6dcba424-Paper.pdf,Machine Teaching of Active Sequential Learners,"Tomi Peltola, Mustafa Mert Çelikok, Pedram Daee, Samuel Kaski",
neurips,https://proceedings.neurips.cc/paper/2019/file/b994697479c5716eda77e8e9713e5f0f-Paper.pdf,Beyond Confidence Regions: Tight Bayesian Ambiguity Sets for Robust MDPs,"Marek Petrik, Reazul Hasan Russel",
neurips,https://proceedings.neurips.cc/paper/2019/file/b9cfe8b6042cf759dc4c0cccb27a6737-Paper.pdf,A General Theory of Equivariant CNNs on Homogeneous Spaces,"Taco S. Cohen, Mario Geiger, Maurice Weiler",
neurips,https://proceedings.neurips.cc/paper/2019/file/ba2f0015122a5955f8b3a50240fb91b2-Paper.pdf,Spatial-Aware Feature Aggregation for Image based Cross-View Geo-Localization,"Yujiao Shi, Liu Liu, Xin Yu, Hongdong Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/ba51e6158bcaf80fd0d834950251e693-Paper.pdf,Leveraging Labeled and Unlabeled Data for Consistent Fair Binary Classification,"Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, Massimiliano Pontil",
neurips,https://proceedings.neurips.cc/paper/2019/file/ba6d843eb4251a4526ce65d1807a9309-Paper.pdf,Tight Dimensionality Reduction for Sketching Low Degree Polynomial Kernels,"Michela Meister, Tamas Sarlos, David Woodruff","We revisit the classic randomized sketch of a tensor product of
q
q
vectors
x
i
∈
R
n
x
. The
i
i
-th coordinate
(
S
x
)
i
(
of the sketch is equal to
∏
q
j
=
1
⟨
u
i
,
j
,
x
j
⟩
/
√
m
∏
, where
u
i
,
j
u
are independent random sign vectors. Kar and Karnick (JMLR, 2012) show that if the sketching dimension
m
=
Ω
(
ϵ
−
2
C
2
Ω
log
(
1
/
δ
)
)
m
, where
C
Ω
C
is a certain property of the point set
Ω
Ω
one wants to sketch, then with probability
1
−
δ
1
,
∥
S
x
∥
2
=
(
1
±
ϵ
)
∥
x
∥
2
‖
for all
x
∈
Ω
x
. However, in their analysis
C
2
Ω
C
can be as large as
Θ
(
n
2
q
)
Θ
, even for a set
Ω
Ω
of
O
(
1
)
O
vectors
x
x
. We give a new analysis of this sketch, providing nearly optimal bounds. Namely, we show an upper bound of
m
=
Θ
(
ϵ
−
2
log
(
n
/
δ
)
+
ϵ
−
1
log
q
(
n
/
δ
)
)
,
m
which by composing with CountSketch, can be improved to
Θ
(
ϵ
−
2
log
(
1
/
(
δ
ϵ
)
)
+
ϵ
−
1
log
q
(
1
/
(
δ
ϵ
)
)
Θ
. For the important case of
q
=
2
q
and
δ
=
1
/
\poly
(
n
)
δ
, this shows that
m
=
Θ
(
ϵ
−
2
log
(
n
)
+
ϵ
−
1
log
2
(
n
)
)
m
, demonstrating that the
ϵ
−
2
ϵ
and
log
2
(
n
)
log
terms do not multiply each other. We also show a nearly matching lower bound of
m
=
Ω
(
\eps
−
2
log
(
1
/
(
δ
)
)
+
\eps
−
1
log
q
(
1
/
(
δ
)
)
)
m
. In a number of applications, one has
|
Ω
|
=
\poly
(
n
)
|
and in this case our bounds are optimal up to a constant factor. This is the first high probability sketch for tensor products that has optimal sketch size and can be implemented in
m
⋅
∑
q
i
=
1
nnz
(
x
i
)
m
time, where
nnz
(
x
i
)
nnz
is the number of non-zero entries of
x
i
x
. Lastly, we empirically compare our sketch to other sketches for tensor products, and give a novel application to compressing neural networks."
neurips,https://proceedings.neurips.cc/paper/2019/file/ba7609ee5789cc4dff171045a693a65f-Paper.pdf,Minimum Stein Discrepancy Estimators,"Alessandro Barp, Francois-Xavier Briol, Andrew Duncan, Mark Girolami, Lester Mackey",
neurips,https://proceedings.neurips.cc/paper/2019/file/bb04af0f7ecaee4aae62035497da1387-Paper.pdf,Provably Powerful Graph Networks,"Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, Yaron Lipman","Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was used to measure the expressive power of graph neural networks (GNN). It was shown that the popular message passing GNN cannot distinguish between graphs that are indistinguishable by the
1
1
-WL test \citep{morris2019,xu2019}. Unfortunately, many simple instances of graphs are indistinguishable by the
1
1
-WL test. In search for more expressive graph learning models we build upon the recent
k
k
-order invariant and equivariant graph neural networks \citep{maron2019} and present two results: First, we show that such
k
k
-order networks can distinguish between non-isomorphic graphs as good as the
k
k
-WL tests, which are provably stronger than the
1
1
-WL test for
k
>
2
k
. This makes these models strictly stronger than message passing models. Unfortunately, the higher expressiveness of these models comes with a computational cost of processing high order tensors. Second, setting our goal at building a provably stronger, \emph{simple} and \emph{scalable} model we show that a reduced
2
2
-order network containing just scaled identity operator, augmented with a single quadratic operation (matrix multiplication) has a provable
3
3
-WL expressive power. Differently put, we suggest a simple model that interleaves applications of standard Multilayer-Perceptron (MLP) applied to the feature dimension and matrix multiplication. We validate this model by presenting state of the art results on popular graph classification and regression tasks. To the best of our knowledge, this is the first practical invariant/equivariant model with guaranteed
3
3
-WL expressiveness, strictly stronger than message passing models."
neurips,https://proceedings.neurips.cc/paper/2019/file/bb1443cc31d7396bf73e7858cea114e1-Paper.pdf,Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning,"Wenjie Shi, Shiji Song, Hui Wu, Ya-Chu Hsu, Cheng Wu, Gao Huang",
neurips,https://proceedings.neurips.cc/paper/2019/file/bb7946e7d85c81a9e69fee1cea4a087c-Paper.pdf,Kernel Stein Tests for Multiple Model Comparison,"Jen Ning Lim, Makoto Yamada, Bernhard Schölkopf, Wittawat Jitkrittum","We address the problem of non-parametric multiple model comparison: given
l
l
candidate models, decide whether each candidate is as good as the best one(s) or worse than it. We propose two statistical tests, each controlling a different notion of decision errors. The first test, building on the post selection inference framework, provably controls the number of best models that are wrongly declared worse (false positive rate). The second test is based on multiple correction, and controls the proportion of the models declared worse but are in fact as good as the best (false discovery rate). We prove that under appropriate conditions the first test can yield a higher true positive rate than the second. Experimental results on toy and real (CelebA, Chicago Crime data) problems show that the two tests have high true positive rates with well-controlled error rates. By contrast, the naive approach of choosing the model with the lowest score without correction leads to more false positives."
neurips,https://proceedings.neurips.cc/paper/2019/file/bb836c01cdc9120a9c984c525e4b1a4a-Paper.pdf,Explanations can be manipulated and geometry is to blame,"Ann-Kathrin Dombrowski, Maximillian Alber, Christopher Anders, Marcel Ackermann, Klaus-Robert Müller, Pan Kessel",
neurips,https://proceedings.neurips.cc/paper/2019/file/bbc12a3a98d8487f58a87d3a3070516e-Paper.pdf,Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks,"Aya Abdelsalam Ismail, Mohamed Gunady, Luiz Pessoa, Hector Corrada Bravo, Soheil Feizi",
neurips,https://proceedings.neurips.cc/paper/2019/file/bbc92a647199b832ec90d7cf57074e9e-Paper.pdf,Paradoxes in Fair Machine Learning,"Paul Goelz, Anson Kahng, Ariel D. Procaccia",
neurips,https://proceedings.neurips.cc/paper/2019/file/bbcbff5c1f1ded46c25d28119a85c6c2-Paper.pdf,Learning Conditional Deformable Templates with Convolutional Networks,"Adrian Dalca, Marianne Rakic, John Guttag, Mert Sabuncu",
neurips,https://proceedings.neurips.cc/paper/2019/file/bbf94b34eb32268ada57a3be5062fe7d-Paper.pdf,Volumetric Correspondence Networks for Optical Flow,"Gengshan Yang, Deva Ramanan",
neurips,https://proceedings.neurips.cc/paper/2019/file/bc047286b224b7bfa73d4cb02de1238d-Paper.pdf,Variance Reduction in Bipartite Experiments through Correlation Clustering,"Jean Pouget-Abadie, Kevin Aydin, Warren Schudy, Kay Brodersen, Vahab Mirrokni",
neurips,https://proceedings.neurips.cc/paper/2019/file/bc1ad6e8f86c42a371aff945535baebb-Paper.pdf,Attribution-Based Confidence Metric For Deep Neural Networks,"Susmit Jha, Sunny Raj, Steven Fernandes, Sumit K. Jha, Somesh Jha, Brian Jalaian, Gunjan Verma, Ananthram Swami",
neurips,https://proceedings.neurips.cc/paper/2019/file/bc3c4a6331a8a9950945a1aa8c95ab8a-Paper.pdf,Are Disentangled Representations Helpful for Abstract Visual Reasoning?,"Sjoerd van Steenkiste, Francesco Locatello, Jürgen Schmidhuber, Olivier Bachem",
neurips,https://proceedings.neurips.cc/paper/2019/file/bc6dc48b743dc5d013b1abaebd2faed2-Paper.pdf,RSN: Randomized Subspace Newton,"Robert Gower, Dmitry Kovalev, Felix Lieder, Peter Richtarik",
neurips,https://proceedings.neurips.cc/paper/2019/file/bc7f621451b4f5df308a8e098112185d-Paper.pdf,Beyond Alternating Updates for Matrix Factorization with Inertial Bregman Proximal Gradient Algorithms,"Mahesh Chandra Mukkamala, Peter Ochs",
neurips,https://proceedings.neurips.cc/paper/2019/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf,Integrating Bayesian and Discriminative Sparse Kernel Machines for Multi-class Active Learning,"Weishi Shi, Qi Yu",
neurips,https://proceedings.neurips.cc/paper/2019/file/bce9abf229ffd7e570818476ee5d7dde-Paper.pdf,Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks,"Yuanzhi Li, Colin Wei, Tengyu Ma",
neurips,https://proceedings.neurips.cc/paper/2019/file/bd33f02c4e28615b5af2d24703e066d5-Paper.pdf,An Algorithm to Learn Polytree Networks with Hidden Nodes,"Firoozeh Sepehr, Donatello Materassi",
neurips,https://proceedings.neurips.cc/paper/2019/file/bd4c9ab730f5513206b999ec0d90d1fb-Paper.pdf,Provable Gradient Variance Guarantees for Black-Box Variational Inference,Justin Domke,
neurips,https://proceedings.neurips.cc/paper/2019/file/bd853b475d59821e100d3d24303d7747-Paper.pdf,LiteEval: A Coarse-to-Fine Framework for Resource Efficient Video Recognition,"Zuxuan Wu, Caiming Xiong, Yu-Gang Jiang, Larry S. Davis",
neurips,https://proceedings.neurips.cc/paper/2019/file/bdb106a0560c4e46ccc488ef010af787-Paper.pdf,Multi-marginal Wasserstein GAN,"Jiezhang Cao, Langyuan Mo, Yifan Zhang, Kui Jia, Chunhua Shen, Mingkui Tan",
neurips,https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf,"PyTorch: An Imperative Style, High-Performance Deep Learning Library","Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala",
neurips,https://proceedings.neurips.cc/paper/2019/file/bdf3fd65c81469f9b74cedd497f2f9ce-Paper.pdf,Learning to Infer Implicit Surfaces without 3D Supervision,"Shichen Liu, Shunsuke Saito, Weikai Chen, Hao Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/be3087e74e9100d4bc4c6268cdbe8456-Paper.pdf,On Sample Complexity Upper and Lower Bounds for Exact Ranking from Noisy Comparisons,"Wenbo Ren, Jia (Kevin) Liu, Ness Shroff","This paper studies the problem of finding the exact ranking from noisy comparisons. A noisy comparison over a set of
m
m
items produces a noisy outcome about the most preferred item, and reveals some information about the ranking. By repeatedly and adaptively choosing items to compare, we want to fully rank the items with a certain confidence, and use as few comparisons as possible. Different from most previous works, in this paper, we have three main novelties: (i) compared to prior works, our upper bounds (algorithms) and lower bounds on the sample complexity (aka number of comparisons) require the minimal assumptions on the instances, and are not restricted to specific models; (ii) we give lower bounds and upper bounds on instances with \textit{unequal} noise levels; and (iii) this paper aims at the \textit{exact} ranking without knowledge on the instances, while most of the previous works either focus on approximate rankings or study exact ranking but require prior knowledge. We first derive lower bounds for pairwise ranking (i.e., compare two items each time), and then propose (nearly) \textit{optimal} pairwise ranking algorithms. We further make extensions to listwise ranking (i.e., comparing multiple items each time). Numerical results also show our improvements against the state of the art."
neurips,https://proceedings.neurips.cc/paper/2019/file/bea6cfd50b4f5e3c735a972cf0eb8450-Paper.pdf,Are Labels Required for Improving Adversarial Robustness?,"Jean-Baptiste Alayrac, Jonathan Uesato, Po-Sen Huang, Alhussein Fawzi, Robert Stanforth, Pushmeet Kohli",
neurips,https://proceedings.neurips.cc/paper/2019/file/beed13602b9b0e6ecb5b568ff5058f07-Paper.pdf,NAT: Neural Architecture Transformer for Accurate and Compact Architectures,"Yong Guo, Yin Zheng, Mingkui Tan, Qi Chen, Jian Chen, Peilin Zhao, Junzhou Huang",
neurips,https://proceedings.neurips.cc/paper/2019/file/bf25356fd2a6e038f1a3a59c26687e80-Paper.pdf,Learning to Self-Train for Semi-Supervised Few-Shot Classification,"Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng, Tat-Seng Chua, Bernt Schiele",
neurips,https://proceedings.neurips.cc/paper/2019/file/bf40d1cbb2ba9fdad19821fc140fa50c-Paper.pdf,Stochastic Frank-Wolfe for Composite Convex Minimization,"Francesco Locatello, Alp Yurtsever, Olivier Fercoq, Volkan Cevher",
neurips,https://proceedings.neurips.cc/paper/2019/file/bf499a12e998d178afd964adf64a60cb-Paper.pdf,Modeling Dynamic Functional Connectivity with Latent Factor Gaussian Processes,"Lingge Li, Dustin Pluta, Babak Shahbaba, Norbert Fortin, Hernando Ombao, Pierre Baldi",
neurips,https://proceedings.neurips.cc/paper/2019/file/c042f4db68f23406c6cecf84a7ebb0fe-Paper.pdf,ETNet: Error Transition Network for Arbitrary Style Transfer,"Chunjin Song, Zhijie Wu, Yang Zhou, Minglun Gong, Hui Huang",
neurips,https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf,Cross-lingual Language Model Pretraining,"Alexis CONNEAU, Guillaume Lample",
neurips,https://proceedings.neurips.cc/paper/2019/file/c055dcc749c2632fd4dd806301f05ba6-Paper.pdf,Icebreaker: Element-wise Efficient Information Acquisition with a Bayesian Deep Latent Gaussian Model,"Wenbo Gong, Sebastian Tschiatschek, Sebastian Nowozin, Richard E. Turner, José Miguel Hernández-Lobato, Cheng Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/c09f9caf5e08836d4673ccdd69bb041e-Paper.pdf,Efficient and Thrifty Voting by Any Means Necessary,"Debmalya Mandal, Ariel D. Procaccia, Nisarg Shah, David Woodruff",
neurips,https://proceedings.neurips.cc/paper/2019/file/c0a62e133894cdce435bcb4a5df1db2d-Paper.pdf,Post training 4-bit quantization of convolutional networks for rapid-deployment,"Ron Banner, Yury Nahshan, Daniel Soudry",
neurips,https://proceedings.neurips.cc/paper/2019/file/c0c783b5fc0d7d808f1d14a6e9c8280d-Paper.pdf,Implicit Regularization in Deep Matrix Factorization,"Sanjeev Arora, Nadav Cohen, Wei Hu, Yuping Luo",
neurips,https://proceedings.neurips.cc/paper/2019/file/c0e19ce0dbabbc0d17a4f8d4324cc8e3-Paper.pdf,Crowdsourcing via Pairwise Co-occurrences: Identifiability and Algorithms,"Shahana Ibrahim, Xiao Fu, Nikolaos Kargas, Kejun Huang",
neurips,https://proceedings.neurips.cc/paper/2019/file/c0e90532fb42ac6de18e25e95db73047-Paper.pdf,Learning low-dimensional state embeddings and metastable clusters from time series data,"Yifan Sun, Yaqi Duan, Hao Gong, Mengdi Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/c1285fcadc52c0d3dc8813fc2c2e2b2a-Paper.pdf,Necessary and Sufficient Geometries for Gradient Methods,"Daniel Levy, John C. Duchi","We study the impact of the constraint set and gradient geometry on the convergence of online and stochastic methods for convex optimization, providing a characterization of the geometries for which stochastic gradient and adaptive gradient methods are (minimax) optimal. In particular, we show that when the constraint set is quadratically convex, diagonally pre-conditioned stochastic gradient methods are minimax optimal. We further provide a converse that shows that when the constraints are not quadratically convex---for example, any
ℓ
p
ℓ
-ball for
p
<
2
p
---the methods are far from optimal. Based on this, we can provide concrete recommendations for when one should use adaptive, mirror or stochastic gradient methods."
neurips,https://proceedings.neurips.cc/paper/2019/file/c133fb1bb634af68c5088f3438848bfd-Paper.pdf,Limitations of Lazy Training of Two-layers Neural Network,"Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, Andrea Montanari",
neurips,https://proceedings.neurips.cc/paper/2019/file/c14a2a57ead18f3532a5a8949382c536-Paper.pdf,Learning Auctions with Robust Incentive Guarantees,"Jacob D. Abernethy, Rachel Cummings, Bhuvesh Kumar, Sam Taggart, Jamie H. Morgenstern",
neurips,https://proceedings.neurips.cc/paper/2019/file/c17028c9b6e0c5deaad29665d582284a-Paper.pdf,Local SGD with Periodic Averaging: Tighter Analysis and Adaptive Synchronization,"Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, Viveck Cadambe","Communication overhead is one of the key challenges that hinders the scalability of distributed optimization algorithms. In this paper, we study local distributed SGD, where data is partitioned among computation nodes, and the computation nodes perform local updates with periodically exchanging the model among the workers to perform averaging. While local SGD is empirically shown to provide promising results, a theoretical understanding of its performance remains open. In this paper, we strengthen convergence analysis for local SGD, and show that local SGD can be far less expensive and applied far more generally than current theory suggests. Specifically, we show that for loss functions that satisfy the Polyak-Kojasiewicz condition,
O
(
(
p
T
)
1
/
3
)
O
rounds of communication suffice to achieve a linear speed up, that is, an error of
O
(
1
/
p
T
)
O
, where
T
T
is the total number of model updates at each worker. This is in contrast with previous work which required higher number of communication rounds, as well as was limited to strongly convex loss functions, for a similar asymptotic performance. We also develop an adaptive synchronization scheme that provides a general condition for linear speed up. Finally, we validate the theory with experimental results, running over AWS EC2 clouds and an internal GPUs cluster."
neurips,https://proceedings.neurips.cc/paper/2019/file/c1a3d34711ab5d85335331ca0e57f067-Paper.pdf,Scalable Bayesian inference of dendritic voltage via spatiotemporal recurrent state space models,"Ruoxi Sun, Scott Linderman, Ian Kinsella, Liam Paninski",
neurips,https://proceedings.neurips.cc/paper/2019/file/c1aeb6517a1c7f33514f7ff69047e74e-Paper.pdf,Constrained Reinforcement Learning Has Zero Duality Gap,"Santiago Paternain, Luiz Chamon, Miguel Calvo-Fullana, Alejandro Ribeiro",
neurips,https://proceedings.neurips.cc/paper/2019/file/c1b70d965ca504aa751ddb62ad69c63f-Paper.pdf,A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning,"Francisco Garcia, Philip S. Thomas",
neurips,https://proceedings.neurips.cc/paper/2019/file/c2073ffa77b5357a498057413bb09d3a-Paper.pdf,Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction,"Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2019/file/c20a7ce2a627ba838cfbff082db35197-Paper.pdf,Learning by Abstraction: The Neural State Machine,"Drew Hudson, Christopher D. Manning",
neurips,https://proceedings.neurips.cc/paper/2019/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf,Unified Language Model Pre-training for Natural Language Understanding and Generation,"Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon",
neurips,https://proceedings.neurips.cc/paper/2019/file/c215b446bcdf956d848a8419c1b5a920-Paper.pdf,Adaptive GNN for Image Analysis and Editing,"Lingyu Liang, LianWen Jin, Yong Xu",
neurips,https://proceedings.neurips.cc/paper/2019/file/c24cd76e1ce41366a4bbe8a49b02a028-Paper.pdf,Metric Learning for Adversarial Robustness,"Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, Baishakhi Ray",
neurips,https://proceedings.neurips.cc/paper/2019/file/c2626d850c80ea07e7511bbae4c76f4b-Paper.pdf,Fine-grained Optimization of Deep Neural Networks,Mete Ozay,"To this end, we pose two problems. First, we aim to obtain weights whose different norms are all upper bounded by a constant number. To achieve these bounds, we propose a two-stage renormalization procedure; (i) normalization of weights according to different norms used in the bounds, and (ii) reparameterization of the normalized weights to set a constant and finite upper bound of their norms. In the second problem, we consider training DNNs with these renormalized weights. To this end, we first propose a strategy to construct joint spaces (manifolds) of weights according to different constraints in DNNs. Next, we propose a fine-grained SGD algorithm (FG-SGD) for optimization on the weight manifolds to train DNNs with assurance of convergence to minima. Experimental analyses show that image classification accuracy of baseline DNNs can be boosted using FG-SGD on collections of manifolds identified by multiple constraints."
neurips,https://proceedings.neurips.cc/paper/2019/file/c26820b8a4c1b3c2aa868d6d57e14a79-Paper.pdf,Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity,"Deepak Pathak, Christopher Lu, Trevor Darrell, Phillip Isola, Alexei A. Efros",
neurips,https://proceedings.neurips.cc/paper/2019/file/c2890d44d06bafb6c7b4aa194857ccbc-Paper.pdf,An adaptive Mirror-Prox method for variational inequalities with singular operators,"Kimon Antonakopoulos, Veronica Belmega, Panayotis Mertikopoulos","Lipschitz continuity is a central requirement for achieving the optimal O(1/T) rate of convergence in monotone, deterministic variational inequalities (a setting that includes convex minimization, convex-concave optimization, nonatomic games, and many other problems). However, in many cases of practical interest, the operator defining the variational inequality may become singular at the boundary of the feasible region, precluding in this way the use of fast gradient methods that attain this rate (such as Nemirovski's mirror-prox algorithm and its variants). To address this issue, we propose a novel smoothness condition which we call Bregman smoothness, and which relates the variation of the operator to that of a suitably chosen Bregman function. Leveraging this condition, we derive an adaptive mirror prox algorithm which attains an O(1/T) rate of convergence in problems with possibly singular operators, without any prior knowledge of the problem's Bregman constant (the Bregman analogue of the Lipschitz constant). We also present an extension of our algorithm to stochastic variational inequalities where the algorithm achieves a
O
(
1
/
√
T
)
O
convergence rate."
neurips,https://proceedings.neurips.cc/paper/2019/file/c2ae5cb2426d96ed19a50b0b7d7c8e11-Paper.pdf,Alleviating Label Switching with Optimal Transport,"Pierre Monteiller, Sebastian Claici, Edward Chien, Farzaneh Mirzazadeh, Justin M. Solomon, Mikhail Yurochkin",
neurips,https://proceedings.neurips.cc/paper/2019/file/c2e06e9a80370952f6ec5463c77cbace-Paper.pdf,Fisher Efficient Inference of Intractable Models,"Song Liu, Takafumi Kanamori, Wittawat Jitkrittum, Yu Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/c3535febaff29fcb7c0d20cbe94391c7-Paper.pdf,Stochastic Gradient Hamiltonian Monte Carlo Methods with Recursive Variance Reduction,"Difan Zou, Pan Xu, Quanquan Gu",
neurips,https://proceedings.neurips.cc/paper/2019/file/c36b1132ac829ece87dda55d77ac06a4-Paper.pdf,Online Learning via the Differential Privacy Lens,"Jacob D. Abernethy, Young Hun Jung, Chansoo Lee, Audra McMillan, Ambuj Tewari",
neurips,https://proceedings.neurips.cc/paper/2019/file/c3d96fbd5b1b45096ff04c04038fff5d-Paper.pdf,Characterization and Learning of Causal Graphs with Latent Variables from Soft Interventions,"Murat Kocaoglu, Amin Jaber, Karthikeyan Shanmugam, Elias Bareinboim",
neurips,https://proceedings.neurips.cc/paper/2019/file/c3e4035af2a1cde9f21e1ae1951ac80b-Paper.pdf,Domes to Drones: Self-Supervised Active Triangulation for 3D Human Pose Reconstruction,"Aleksis Pirinen, Erik Gärtner, Cristian Sminchisescu",
neurips,https://proceedings.neurips.cc/paper/2019/file/c4127b9194fe8562c64dc0f5bf2c93bc-Paper.pdf,SIC-MMAB: Synchronisation Involves Communication in Multiplayer Multi-Armed Bandits,"Etienne Boursier, Vianney Perchet",
neurips,https://proceedings.neurips.cc/paper/2019/file/c429429bf1f2af051f2021dc92a8ebea-Paper.pdf,A Step Toward Quantifying Independently Reproducible Machine Learning Research,Edward Raff,
neurips,https://proceedings.neurips.cc/paper/2019/file/c4414e538a5475ec0244673b7f2f7dbb-Paper.pdf,Latent distance estimation for random geometric graphs,"Ernesto Araya Valdivia, De Castro Yohann","Random geometric graphs are a popular choice for a latent points generative model for networks. Their definition is based on a sample of
n
n
points
X
1
,
X
2
,
⋯
,
X
n
X
on the Euclidean sphere~
S
d
−
1
S
which represents the latent positions of nodes of the network. The connection probabilities between the nodes are determined by an unknown function (referred to as the
link'' function) evaluated at the distance between the latent points. We introduce a spectral estimator of the pairwise distance between latent points and we prove that its rate of convergence is the same as the nonparametric estimation of a function on
S
d
−
1
S
, up to a logarithmic factor. In addition, we provide an efficient spectral algorithm to compute this estimator without any knowledge on the nonparametric link function. As a byproduct, our method can also consistently estimate the dimension
d
d
of the latent space."
neurips,https://proceedings.neurips.cc/paper/2019/file/c46482dd5d39742f0bfd417b492d0e8e-Paper.pdf,Dual Adversarial Semantics-Consistent Network for Generalized Zero-Shot Learning,"Jian Ni, Shanghang Zhang, Haiyong Xie",
neurips,https://proceedings.neurips.cc/paper/2019/file/c4819d06b0ca810d38506453cfaae9d8-Paper.pdf,Manipulating a Learning Defender and Ways to Counteract,"Jiarui Gan, Qingyu Guo, Long Tran-Thanh, Bo An, Michael Wooldridge",
neurips,https://proceedings.neurips.cc/paper/2019/file/c4c42505a03f2e969b4c0a97ee9b34e7-Paper.pdf,Privacy Amplification by Mixing and Diffusion Mechanisms,"Borja Balle, Gilles Barthe, Marco Gaboardi, Joseph Geumlek",
neurips,https://proceedings.neurips.cc/paper/2019/file/c4de8ced6214345614d33fb0b16a8acd-Paper.pdf,Ultra Fast Medoid Identification via Correlated Sequential Halving,"Tavor Baharav, David Tse",
neurips,https://proceedings.neurips.cc/paper/2019/file/c4ef9c39b300931b69a36fb3dbb8d60e-Paper.pdf,On the Inductive Bias of Neural Tangent Kernels,"Alberto Bietti, Julien Mairal",
neurips,https://proceedings.neurips.cc/paper/2019/file/c535e3a7f97daf1c4b1eb03cc8e31623-Paper.pdf,Surround Modulation: A Bio-inspired Connectivity Structure for Convolutional Neural Networks,"Hosein Hasani, Mahdieh Soleymani, Hamid Aghajan",
neurips,https://proceedings.neurips.cc/paper/2019/file/c5c64c10cfd77b16a03aa81f09499f25-Paper.pdf,Rethinking Kernel Methods for Node Representation Learning on Graphs,"Yu Tian, Long Zhao, Xi Peng, Dimitris Metaxas",
neurips,https://proceedings.neurips.cc/paper/2019/file/c5df4f4eabf1cbcfeb50fbbf97c5289f-Paper.pdf,A Necessary and Sufficient Stability Notion for Adaptive Generalization,"Moshe Shenfeld, Katrina Ligett",
neurips,https://proceedings.neurips.cc/paper/2019/file/c61aed648da48aa3893fb3eaadd88a7f-Paper.pdf,Implicit Regularization of Accelerated Methods in Hilbert Spaces,"Nicolò Pagliana, Lorenzo Rosasco",
neurips,https://proceedings.neurips.cc/paper/2019/file/c61f571dbd2fb949d3fe5ae1608dd48b-Paper.pdf,Input Similarity from the Neural Network Perspective,"Guillaume Charpiat, Nicolas Girard, Loris Felardos, Yuliya Tarabalka","We study the mathematical properties of this similarity measure, and show how to estimate sample density with it, in low complexity, enabling new types of statistical analysis for neural networks. We also propose to use it during training, to enforce that examples known to be similar should also be seen as similar by the network."
neurips,https://proceedings.neurips.cc/paper/2019/file/c66dd00e5fc44ba8de89d7713fedcd50-Paper.pdf,Transfer Learning via Minimizing the Performance Gap Between Domains,"Boyu Wang, Jorge Mendez, Mingbo Cai, Eric Eaton",
neurips,https://proceedings.neurips.cc/paper/2019/file/c6bff625bdb0393992c9d4db0c6bbe45-Paper.pdf,Catastrophic Forgetting Meets Negative Transfer: Batch Spectral Shrinkage for Safe Transfer Learning,"Xinyang Chen, Sinan Wang, Bo Fu, Mingsheng Long, Jianmin Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf,ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks,"Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee",
neurips,https://proceedings.neurips.cc/paper/2019/file/c77331e51c5555f8f935d3344c964bd5-Paper.pdf,Efficiently Learning Fourier Sparse Set Functions,"Andisheh Amrollahi, Amir Zandieh, Michael Kapralov, Andreas Krause","Learning set functions is a key challenge arising in many domains, ranging from sketching graphs to black-box optimization with discrete parameters. In this paper we consider the problem of efficiently learning set functions that are defined over a ground set of size
n
n
and that are sparse (say
k
k
-sparse) in the Fourier domain. This is a wide class, that includes graph and hypergraph cut functions, decision trees and more. Our central contribution is the first algorithm that allows learning functions whose Fourier support only contains low degree (say degree
d
=
o
(
n
)
d
) polynomials using
O
(
k
d
log
n
)
O
sample complexity and runtime
O
(
k
n
log
2
k
log
n
log
d
)
O
. This implies that sparse graphs with
k
k
edges can, for the first time, be learned from
O
(
k
log
n
)
O
observations of cut values and in linear time in the number of vertices. Our algorithm can also efficiently learn (sums of) decision trees of small depth. The algorithm exploits techniques from the sparse Fourier transform literature and is easily implementable. Lastly, we also develop an efficient robust version of our algorithm and prove
ℓ
2
/
ℓ
2
ℓ
approximation guarantees without any statistical assumptions on the noise."
neurips,https://proceedings.neurips.cc/paper/2019/file/c82b013313066e0702d58dc70db033ca-Paper.pdf,"Search-Guided, Lightly-Supervised Training of Structured Prediction Energy Networks","Amirmohammad Rooshenas, Dongxu Zhang, Gopal Sharma, Andrew McCallum",
neurips,https://proceedings.neurips.cc/paper/2019/file/c8cc6e90ccbff44c9cee23611711cdc4-Paper.pdf,Planning with Goal-Conditioned Policies,"Soroush Nasiriany, Vitchyr Pong, Steven Lin, Sergey Levine",
neurips,https://proceedings.neurips.cc/paper/2019/file/c8d3a760ebab631565f8509d84b3b3f1-Paper.pdf,Goal-conditioned Imitation Learning,"Yiming Ding, Carlos Florensa, Pieter Abbeel, Mariano Phielipp",
neurips,https://proceedings.neurips.cc/paper/2019/file/c900ced7451da79502d29aa37ebb7b60-Paper.pdf,Superset Technique for Approximate Recovery in One-Bit Compressed Sensing,"Larkin Flodin, Venkata Gandikota, Arya Mazumdar",
neurips,https://proceedings.neurips.cc/paper/2019/file/c91e3483cf4f90057d02aa492d2b25b1-Paper.pdf,Iterative Least Trimmed Squares for Mixed Linear Regression,"Yanyao Shen, Sujay Sanghavi",
neurips,https://proceedings.neurips.cc/paper/2019/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf,Asymptotic Guarantees for Learning Generative Models with the Sliced-Wasserstein Distance,"Kimia Nadjahi, Alain Durmus, Umut Simsekli, Roland Badeau","Minimum expected distance estimation (MEDE) algorithms have been widely used for probabilistic models with intractable likelihood functions and they have become increasingly popular due to their use in implicit generative modeling (e.g.\ Wasserstein generative adversarial networks, Wasserstein autoencoders). Emerging from computational optimal transport, the Sliced-Wasserstein (SW) distance has become a popular choice in MEDE thanks to its simplicity and computational benefits. While several studies have reported empirical success on generative modeling with SW, the theoretical properties of such estimators have not yet been established. In this study, we investigate the asymptotic properties of estimators that are obtained by minimizing SW. We first show that convergence in SW implies weak convergence of probability measures in general Wasserstein spaces. Then we show that estimators obtained by minimizing SW (and also an approximate version of SW) are asymptotically consistent. We finally prove a central limit theorem, which characterizes the asymptotic distribution of the estimators and establish a convergence rate of
√
n
n
, where
n
n
denotes the number of observed data points. We illustrate the validity of our theory on both synthetic data and neural networks."
neurips,https://proceedings.neurips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf,Time-series Generative Adversarial Networks,"Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2019/file/cab070d53bd0d200746fb852a922064a-Paper.pdf,Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup,"Sebastian Goldt, Madhu Advani, Andrew M. Saxe, Florent Krzakala, Lenka Zdeborová",
neurips,https://proceedings.neurips.cc/paper/2019/file/cae82d4350cc23aca7fc9ae38dab38ab-Paper.pdf,Learning Nonsymmetric Determinantal Point Processes,"Mike Gartrell, Victor-Emmanuel Brunel, Elvis Dohmatob, Syrine Krichene",
neurips,https://proceedings.neurips.cc/paper/2019/file/cb12d7f933e7d102c52231bf62b8a678-Paper.pdf,Quantum Embedding of Knowledge for Reasoning,"Dinesh Garg, Shajith Ikbal, Santosh K. Srivastava, Harit Vishwakarma, Hima Karanam, L Venkata Subramaniam",
neurips,https://proceedings.neurips.cc/paper/2019/file/cb3ce9b06932da6faaa7fc70d5b5d2f4-Paper.pdf,Online Normalization for Training Neural Networks,"Vitaliy Chiley, Ilya Sharapov, Atli Kosson, Urs Koster, Ryan Reece, Sofia Samaniego de la Fuente, Vishal Subbiah, Michael James",
neurips,https://proceedings.neurips.cc/paper/2019/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf,Equitable Stable Matchings in Quadratic Time,"Nikolaos Tziavelis, Ioannis Giannakopoulos, Katerina Doka, Nectarios Koziris, Panagiotis Karras",
neurips,https://proceedings.neurips.cc/paper/2019/file/cb79f8fa58b91d3af6c9c991f63962d3-Paper.pdf,Making AI Forget You: Data Deletion in Machine Learning,"Antonio Ginart, Melody Guan, Gregory Valiant, James Y. Zou","Intense recent discussions have focused on how to provide individuals with control over when their data can and cannot be used --- the EU’s Right To Be Forgotten regulation is an example of this effort. In this paper we initiate a framework studying what to do when it is no longer permissible to deploy models derivative from specific user data. In particular, we formulate the problem of efficiently deleting individual data points from trained machine learning models. For many standard ML models, the only way to completely remove an individual's data is to retrain the whole model from scratch on the remaining data, which is often not computationally practical. We investigate algorithmic principles that enable efficient data deletion in ML. For the specific setting of
k
k
-means clustering, we propose two provably deletion efficient algorithms which achieve an average of over
100
×
100
improvement in deletion efficiency across 6 datasets, while producing clusters of comparable statistical quality to a canonical
k
k
-means++ baseline."
neurips,https://proceedings.neurips.cc/paper/2019/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Paper.pdf,A New Defense Against Adversarial Images: Turning a Weakness into a Strength,"Shengyuan Hu, Tao Yu, Chuan Guo, Wei-Lun Chao, Kilian Q. Weinberger",
neurips,https://proceedings.neurips.cc/paper/2019/file/cc02d42b8939768b8a4f1e4d826faa79-Paper.pdf,Hamiltonian descent for composite objectives,"Brendan O'Donoghue, Chris J. Maddison",
neurips,https://proceedings.neurips.cc/paper/2019/file/cc70903297fe1e25537ae50aea186306-Paper.pdf,Game Design for Eliciting Distinguishable Behavior,"Fan Yang, Liu Leqi, Yifan Wu, Zachary Lipton, Pradeep K. Ravikumar, Tom M. Mitchell, William W. Cohen",
neurips,https://proceedings.neurips.cc/paper/2019/file/cc9657884708170e160c8372d92f3535-Paper.pdf,Divergence-Augmented Policy Optimization,"Qing Wang, Yingru Li, Jiechao Xiong, Tong Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/cca289d2a4acd14c1cd9a84ffb41dd29-Paper.pdf,Gaussian-Based Pooling for Convolutional Neural Networks,Takumi Kobayashi,
neurips,https://proceedings.neurips.cc/paper/2019/file/ccce2fab7336b8bc8362d115dec2d5a2-Paper.pdf,Band-Limited Gaussian Processes: The Sinc Kernel,Felipe Tobar,
neurips,https://proceedings.neurips.cc/paper/2019/file/ccdf3864e2fa9089f9eca4fc7a48ea0a-Paper.pdf,Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks,"Sitao Luan, Mingde Zhao, Xiao-Wen Chang, Doina Precup",
neurips,https://proceedings.neurips.cc/paper/2019/file/ccf0304d099baecfbe7ff6844e1f6d91-Paper.pdf,Bayesian Optimization with Unknown Search Space,"Huong Ha, Santu Rana, Sunil Gupta, Thanh Nguyen, Hung Tran-The, Svetha Venkatesh",
neurips,https://proceedings.neurips.cc/paper/2019/file/cd00692c3bfe59267d5ecfac5310286c-Paper.pdf,Towards closing the gap between the theory and practice of SVRG,"Othmane Sebbouh, Nidham Gazagnadou, Samy Jelassi, Francis Bach, Robert Gower","Amongst the very first variance reduced stochastic methods for solving the empirical risk minimization problem was the SVRG method. SVRG is an inner-outer loop based method, where in the outer loop a reference full gradient is evaluated, after which
m
∈
\N
m
steps of an inner loop are executed where the reference gradient is used to build a variance reduced estimate of the current gradient. The simplicity of the SVRG method and its analysis have lead to multiple extensions and variants for even non-convex optimization. Yet there is a significant gap between the parameter settings that the analysis suggests and what is known to work well in practice. Our first contribution is that we take several steps towards closing this gap. In particular, the current analysis shows that
m
m
should be of the order of the condition number so that the resulting method has a favorable complexity. Yet in practice
m
=
n
m
works well regardless of the condition number, where
n
n
is the number of data points. Furthermore, the current analysis shows that the inner iterates have to be reset using averaging after every outer loop. Yet in practice SVRG works best when the inner iterates are updated continuously and not reset. We provide an analysis of these aforementioned practical settings and show that they achieve the same favorable complexity as the original analysis (with slightly better constants). Our second contribution is to provide a more general analysis than had been previously done by using arbitrary sampling, which allows us to analyze virtually all forms of mini-batching through a single theorem. Since our setup and analysis reflect what is done in practice, we are able to set the parameters such as the mini-batch size and step size using our theory in such a way that produces a more efficient algorithm in practice, as we show in extensive numerical experiments."
neurips,https://proceedings.neurips.cc/paper/2019/file/cd474f6341aeffd65f93084d0dae3453-Paper.pdf,A Unifying Framework for Spectrum-Preserving Graph Sparsification and Coarsening,"Gecia Bravo Hermsdorff, Lee Gunderson","How might one
reduce'' a graph? That is, generate a smaller graph that preserves the global structure at the expense of discarding local details? There has been extensive work on both graph sparsification (removing edges) and graph coarsening (merging nodes, often by edge contraction); however, these operations are currently treated separately. Interestingly, for a planar graph, edge deletion corresponds to edge contraction in its planar dual (and more generally, for a graphical matroid and its dual). Moreover, with respect to the dynamics induced by the graph Laplacian (e.g., diffusion), deletion and contraction are physical manifestations of two reciprocal limits: edge weights of
0
0
and
∞
∞
, respectively. In this work, we provide a unifying framework that captures both of these operations, allowing one to simultaneously sparsify and coarsen a graph while preserving its large-scale structure. The limit of infinite edge weight is rarely considered, as many classical notions of graph similarity diverge. However, its algebraic, geometric, and physical interpretations are reflected in the Laplacian pseudoinverse
\mat
L
†
\mat
, which remains finite in this limit. Motivated by this insight, we provide a probabilistic algorithm that reduces graphs while preserving
\mat
L
†
\mat
, using an unbiased procedure that minimizes its variance. We compare our algorithm with several existing sparsification and coarsening algorithms using real-world datasets, and demonstrate that it more accurately preserves the large-scale structure."
neurips,https://proceedings.neurips.cc/paper/2019/file/cd61a580392a70389e27b0bc2b439f49-Paper.pdf,Error Correcting Output Codes Improve Probability Estimation and Adversarial Robustness of Deep Neural Networks,"Gunjan Verma, Ananthram Swami",
neurips,https://proceedings.neurips.cc/paper/2019/file/cd63a3eec3319fd9c84c942a08316e00-Paper.pdf,KerGM: Kernelized Graph Matching,"Zhen Zhang, Yijian Xiang, Lingfei Wu, Bing Xue, Arye Nehorai",
neurips,https://proceedings.neurips.cc/paper/2019/file/cd6b73b67c77edeaff94e24b961119dd-Paper.pdf,On Human-Aligned Risk Minimization,"Liu Leqi, Adarsh Prasad, Pradeep K. Ravikumar",
neurips,https://proceedings.neurips.cc/paper/2019/file/cd9508fdaa5c1390e9cc329001cf1459-Paper.pdf,Robustness Verification of Tree-based Models,"Hongge Chen, Huan Zhang, Si Si, Yang Li, Duane Boning, Cho-Jui Hsieh",
neurips,https://proceedings.neurips.cc/paper/2019/file/ce5193a069bea027a60e06c57a106eb6-Paper.pdf,Provable Non-linear Inductive Matrix Completion,"Kai Zhong, Zhao Song, Prateek Jain, Inderjit S. Dhillon",
neurips,https://proceedings.neurips.cc/paper/2019/file/cf040fc71060367913e81ac1eb050aea-Paper.pdf,STAR-Caps: Capsule Networks with Straight-Through Attentive Routing,"Karim Ahmed, Lorenzo Torresani",
neurips,https://proceedings.neurips.cc/paper/2019/file/cf34645d98a7630e2bcca98b3e29c8f2-Paper.pdf,Self-attention with Functional Time Representation Learning,"Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan",
neurips,https://proceedings.neurips.cc/paper/2019/file/cf67355a3333e6e143439161adc2d82e-Paper.pdf,Multi-label Co-regularization for Semi-supervised Facial Action Unit Recognition,"Xuesong Niu, Hu Han, Shiguang Shan, Xilin Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/cf708fc1decf0337aded484f8f4519ae-Paper.pdf,A Primal Dual Formulation For Deep Learning With Constraints,"Yatin Nandwani, Abhishek Pathak, Mausam, Parag Singla",
neurips,https://proceedings.neurips.cc/paper/2019/file/cf9a242b70f45317ffd281241fa66502-Paper.pdf,DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections,"Ofir Nachum, Yinlam Chow, Bo Dai, Lihong Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/cf9dc5e4e194fc21f397b4cac9cc3ae9-Paper.pdf,Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks,"Yuan Cao, Quanquan Gu","We study the training and generalization of deep neural networks (DNNs) in the over-parameterized regime, where the network width (i.e., number of hidden nodes per layer) is much larger than the number of training data points. We show that, the expected
0
0
-
1
1
loss of a wide enough ReLU network trained with stochastic gradient descent (SGD) and random initialization can be bounded by the training loss of a random feature model induced by the network gradient at initialization, which we call a \textit{neural tangent random feature} (NTRF) model. For data distributions that can be classified by NTRF model with sufficiently small error, our result yields a generalization error bound in the order of
~
O
(
n
−
1
/
2
)
O
that is independent of the network width. Our result is more general and sharper than many existing generalization error bounds for over-parameterized neural networks. In addition, we establish a strong connection between our generalization error bound and the neural tangent kernel (NTK) proposed in recent work."
neurips,https://proceedings.neurips.cc/paper/2019/file/cfcce0621b49c983991ead4c3d4d3b6b-Paper.pdf,Intrinsic dimension of data representations in deep neural networks,"Alessio Ansuini, Alessandro Laio, Jakob H. Macke, Davide Zoccolan",
neurips,https://proceedings.neurips.cc/paper/2019/file/cff34ad343b069ea6920464ad17d4bcf-Paper.pdf,Program Synthesis and Semantic Parsing with Learned Code Idioms,"Eui Chul Shin, Miltiadis Allamanis, Marc Brockschmidt, Alex Polozov",
neurips,https://proceedings.neurips.cc/paper/2019/file/d0010a6f34908640a4a6da2389772a78-Paper.pdf,Data-driven Estimation of Sinusoid Frequencies,"Gautier Izacard, Sreyas Mohan, Carlos Fernandez-Granda",
neurips,https://proceedings.neurips.cc/paper/2019/file/d010396ca8abf6ead8cacc2c2f2f26c7-Paper.pdf,Discovering Neural Wirings,"Mitchell Wortsman, Ali Farhadi, Mohammad Rastegari",
neurips,https://proceedings.neurips.cc/paper/2019/file/d01c25576ff1c53de58e0e6970a2d510-Paper.pdf,Locally Private Learning without Interaction Requires Separation,"Amit Daniely, Vitaly Feldman","We show that the margin complexity of a class of Boolean functions is a lower bound on the complexity of any non-interactive LDP algorithm for distribution-independent PAC learning of the class. In particular, the classes of linear separators and decision lists require exponential number of samples to learn non-interactively even though they can be learned in polynomial time by an interactive LDP algorithm. This gives the first example of a natural problem that is significantly harder to solve without interaction and also resolves an open problem of Kasiviswanathan et al.~(2008). We complement this lower bound with a new efficient learning algorithm whose complexity is polynomial in the margin complexity of the class. Our algorithm is non-interactive on labeled samples but still needs interactive access to unlabeled samples. All of our results also apply to the statistical query model and any model in which the number of bits communicated about each data point is constrained."
neurips,https://proceedings.neurips.cc/paper/2019/file/d03a857a23b5285736c4d55e0bb067c8-Paper.pdf,Fixing the train-test resolution discrepancy,"Hugo Touvron, Andrea Vedaldi, Matthijs Douze, Herve Jegou","We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128x128 images, and 79.8% with one trained at 224x224."
neurips,https://proceedings.neurips.cc/paper/2019/file/d045c59a90d7587d8d671b5f5aec4e7c-Paper.pdf,Quadratic Video Interpolation,"Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, Ming-Hsuan Yang",
neurips,https://proceedings.neurips.cc/paper/2019/file/d04cb95ba2bea9fd2f0daa8945d70f11-Paper.pdf,Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game,"Ngoc-Trung Tran, Viet-Hung Tran, Bao-Ngoc Nguyen, Linxiao Yang, Ngai-Man (Man) Cheung","Self-supervised (SS) learning is a powerful approach for representation learning using unlabeled data. Recently, it has been applied to Generative Adversarial Networks (GAN) training. Specifically, SS tasks were proposed to address the catastrophic forgetting issue in the GAN discriminator. In this work, we perform an in-depth analysis to understand how SS tasks interact with learning of generator. From the analysis, we identify issues of SS tasks which allow a severely mode-collapsed generator to excel the SS tasks. To address the issues, we propose new SS tasks based on a multi-class minimax game. The competition between our proposed SS tasks in the game encourages the generator to learn the data distribution and generate diverse samples. We provide both theoretical and empirical analysis to support that our proposed SS tasks have better convergence property. We conduct experiments to incorporate our proposed SS tasks into two different GAN baseline models. Our approach establishes state-of-the-art FID scores on CIFAR-10, CIFAR-100, STL-10, CelebA, Imagenet
32
×
32
32
and Stacked-MNIST datasets, outperforming existing works by considerable margins in some cases. Our unconditional GAN model approaches performance of conditional GAN without using labeled data. Our code: \url{https://github.com/tntrung/msgan}"
neurips,https://proceedings.neurips.cc/paper/2019/file/d073bb8d0c47f317dd39de9c9f004e9d-Paper.pdf,Learning step sizes for unfolded sparse coding,"Pierre Ablin, Thomas Moreau, Mathurin Massias, Alexandre Gramfort",
neurips,https://proceedings.neurips.cc/paper/2019/file/d0921d442ee91b896ad95059d13df618-Paper.pdf,Efficient Graph Generation with Graph Recurrent Attention Networks,"Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K. Duvenaud, Raquel Urtasun, Richard Zemel",
neurips,https://proceedings.neurips.cc/paper/2019/file/d09bf41544a3365a46c9077ebb5e35c3-Paper.pdf,Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks,"Vineet Kosaraju, Amir Sadeghian, Roberto Martín-Martín, Ian Reid, Hamid Rezatofighi, Silvio Savarese",
neurips,https://proceedings.neurips.cc/paper/2019/file/d0aa518d4d3bfc721aa0b8ab4ef32269-Paper.pdf,Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds,"Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham, Niki Trigoni",
neurips,https://proceedings.neurips.cc/paper/2019/file/d0ac1ed0c5cb9ecbca3d2496ec1ad984-Paper.pdf,Re-examination of the Role of Latent Variables in Sequence Modeling,"Guokun Lai, Zihang Dai, Yiming Yang, Shinjae Yoo",
neurips,https://proceedings.neurips.cc/paper/2019/file/d0f4dae80c3d0277922f8371d5827292-Paper.pdf,Consistency-based Semi-supervised Learning for Object detection,"Jisoo Jeong, Seungeui Lee, Jeesoo Kim, Nojun Kwak",
neurips,https://proceedings.neurips.cc/paper/2019/file/d0f5edad9ac19abed9e235c0fe0aa59f-Paper.pdf,Kernel Truncated Randomized Ridge Regression: Optimal Rates and Low Noise Acceleration,"Kwang-Sung Jun, Ashok Cutkosky, Francesco Orabona",
neurips,https://proceedings.neurips.cc/paper/2019/file/d149231f39b05ae135fa763edb358064-Paper.pdf,Bandits with Feedback Graphs and Switching Costs,"Raman Arora, Teodor Vanislavov Marinov, Mehryar Mohri","We study the adversarial multi-armed bandit problem where the learner is supplied with partial observations modeled by a \emph{feedback graph} and where shifting to a new action incurs a fixed \emph{switching cost}. We give two new algorithms for this problem in the informed setting. Our best algorithm achieves a pseudo-regret of
~
O
(
γ
(
G
)
1
3
T
2
3
)
O
, where
γ
(
G
)
γ
is the domination number of the feedback graph. This significantly improves upon the previous best result for the same problem, which was based on the independence number of
G
G
. We also present matching lower bounds for our result that we describe in detail. Finally, we give a new algorithm with improved policy regret bounds when partial counterfactual feedback is available."
neurips,https://proceedings.neurips.cc/paper/2019/file/d14c2267d848abeb81fd590f371d39bd-Paper.pdf,Exact Combinatorial Optimization with Graph Convolutional Neural Networks,"Maxime Gasse, Didier Chetelat, Nicola Ferroni, Laurent Charlin, Andrea Lodi",
neurips,https://proceedings.neurips.cc/paper/2019/file/d15426b9c324676610fbb01360473ed8-Paper.pdf,Comparing Unsupervised Word Translation Methods Step by Step,"Mareike Hartmann, Yova Kementchedjhieva, Anders Søgaard",
neurips,https://proceedings.neurips.cc/paper/2019/file/d18f655c3fce66ca401d5f38b48c89af-Paper.pdf,"Learn, Imagine and Create: Text-to-Image Generation from Prior Knowledge","Tingting Qiao, Jing Zhang, Duanqing Xu, Dacheng Tao",
neurips,https://proceedings.neurips.cc/paper/2019/file/d1d5923fc822531bbfd9d87d4760914b-Paper.pdf,Compiler Auto-Vectorization with Imitation Learning,"Charith Mendis, Cambridge Yang, Yewen Pu, Dr.Saman Amarasinghe, Michael Carbin",
neurips,https://proceedings.neurips.cc/paper/2019/file/d202ed5bcfa858c15a9f383c3e386ab2-Paper.pdf,"Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification and Local Computations","Debraj Basu, Deepesh Data, Can Karakus, Suhas Diggavi",
neurips,https://proceedings.neurips.cc/paper/2019/file/d240e3d38a8882ecad8633c8f9c78c9b-Paper.pdf,Fast Sparse Group Lasso,"Yasutoshi Ida, Yasuhiro Fujiwara, Hisashi Kashima",
neurips,https://proceedings.neurips.cc/paper/2019/file/d26e5e36c1b0b620407eadabb6c0c5c2-Paper.pdf,Deep Random Splines for Point Process Intensity Estimation of Neural Population Data,"Gabriel Loaiza-Ganem, Sean Perkins, Karen Schroeder, Mark Churchland, John P. Cunningham",
neurips,https://proceedings.neurips.cc/paper/2019/file/d2b15c75c0c389b49c2efbea79cdc946-Paper.pdf,Fast Decomposable Submodular Function Minimization using Constrained Total Variation,"Senanayak Sesh Kumar Karri, Francis Bach, Thomas Pock",
neurips,https://proceedings.neurips.cc/paper/2019/file/d2cdf047a6674cef251d56544a3cf029-Paper.pdf,Deep Signature Transforms,"Patrick Kidger, Patric Bonnier, Imanol Perez Arribas, Cristopher Salvi, Terry Lyons",
neurips,https://proceedings.neurips.cc/paper/2019/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf,ResNets Ensemble via the Feynman-Kac Formalism to Improve Natural and Robust Accuracies,"Bao Wang, Zuoqiang Shi, Stanley Osher","We unify the theory of optimal control of transport equations with the practice of training and testing of ResNets. Based on this unified viewpoint, we propose a simple yet effective ResNets ensemble algorithm to boost the accuracy of the robustly trained model on both clean and adversarial images. The proposed algorithm consists of two components: First, we modify the base ResNets by injecting a variance specified Gaussian noise to the output of each residual mapping. Second, we average over the production of multiple jointly trained modified ResNets to get the final prediction. These two steps give an approximation to the Feynman-Kac formula for representing the solution of a convection-diffusion equation. For the CIFAR10 benchmark, this simple algorithm leads to a robust model with a natural accuracy of {\bf 85.62}\% on clean images and a robust accuracy of
57.94
%
57.94
under the 20 iterations of the IFGSM attack, which outperforms the current state-of-the-art in defending against IFGSM attack on the CIFAR10."
neurips,https://proceedings.neurips.cc/paper/2019/file/d324a0cc02881779dcda44a675fdcaaa-Paper.pdf,Guided Meta-Policy Search,"Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey Levine, Chelsea Finn",
neurips,https://proceedings.neurips.cc/paper/2019/file/d360a502598a4b64b936683b44a5523a-Paper.pdf,Learning elementary structures for 3D shape generation and matching,"Theo Deprelle, Thibault Groueix, Matthew Fisher, Vladimir Kim, Bryan Russell, Mathieu Aubry",
neurips,https://proceedings.neurips.cc/paper/2019/file/d384dec9f5f7a64a36b5c8f03b8a6d92-Paper.pdf,Cross-Modal Learning with Adversarial Samples,"CHAO LI, Shangqian Gao, Cheng Deng, De Xie, Wei Liu",
neurips,https://proceedings.neurips.cc/paper/2019/file/d3aeec875c479e55d1cdeea161842ec6-Paper.pdf,Learning Disentangled Representation for Robust Person Re-identification,"Chanho Eom, Bumsub Ham",
neurips,https://proceedings.neurips.cc/paper/2019/file/d3d80b656929a5bc0fa34381bf42fbdd-Paper.pdf,On Testing for Biases in Peer Review,"Ivan Stelmakh, Nihar Shah, Aarti Singh",
neurips,https://proceedings.neurips.cc/paper/2019/file/d3f93e7766e8e1b7ef66dfdd9a8be93b-Paper.pdf,Learning Deterministic Weighted Automata with Queries and Counterexamples,"Gail Weiss, Yoav Goldberg, Eran Yahav",
neurips,https://proceedings.neurips.cc/paper/2019/file/d3fad7d3634dbfb61018813546edbccb-Paper.pdf,Making the Cut: A Bandit-based Approach to Tiered Interviewing,"Candice Schumann, Zhi Lang, Jeffrey Foster, John Dickerson",
neurips,https://proceedings.neurips.cc/paper/2019/file/d464b5ac99e74462f321c06ccacc4bff-Paper.pdf,Manifold-regression to predict from MEG/EEG brain signals without source modeling,"David Sabbagh, Pierre Ablin, Gael Varoquaux, Alexandre Gramfort, Denis A. Engemann",
neurips,https://proceedings.neurips.cc/paper/2019/file/d47bf0af618a3523a226ed7cada85ce3-Paper.pdf,Reflection Separation using a Pair of Unpolarized and Polarized Images,"Youwei Lyu, Zhaopeng Cui, Si Li, Marc Pollefeys, Boxin Shi",
neurips,https://proceedings.neurips.cc/paper/2019/file/d4a897919a124958e699170b2b1dc8f2-Paper.pdf,Co-Generation with GANs using AIS based HMC,"Tiantian Fang, Alexander Schwing",
neurips,https://proceedings.neurips.cc/paper/2019/file/d4a93297083a23cc099f7bd6a8621131-Paper.pdf,Sim2real transfer learning for 3D human pose estimation: motion to the rescue,"Carl Doersch, Andrew Zisserman",
neurips,https://proceedings.neurips.cc/paper/2019/file/d4cd91e80f36f8f3103617ded9128560-Paper.pdf,Dimension-Free Bounds for Low-Precision Training,"Zheng Li, Christopher M. De Sa","Low-precision training is a promising way of decreasing the time and energy cost of training machine learning models. Previous work has analyzed low-precision training algorithms, such as low-precision stochastic gradient descent, and derived theoretical bounds on their convergence rates. These bounds tend to depend on the dimension of the model
d
d
in that the number of bits needed to achieve a particular error bound increases as
d
d
increases. In this paper, we derive new bounds for low-precision training algorithms that do not contain the dimension
d
d
, which lets us better understand what affects the convergence of these algorithms as parameters scale. Our methods also generalize naturally to let us prove new convergence bounds on low-precision training with other quantization schemes, such as low-precision floating-point computation and logarithmic quantization."
neurips,https://proceedings.neurips.cc/paper/2019/file/d54e99a6c03704e95e6965532dec148b-Paper.pdf,Assessing Disparate Impact of Personalized Interventions: Identifiability and Bounds,"Nathan Kallus, Angela Zhou",
neurips,https://proceedings.neurips.cc/paper/2019/file/d554f7bb7be44a7267068a7df88ddd20-Paper.pdf,Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution,"Thang Vu, Hyunjun Jang, Trung X. Pham, Chang Yoo",
neurips,https://proceedings.neurips.cc/paper/2019/file/d55cbf210f175f4a37916eafe6c04f0d-Paper.pdf,Variational Bayesian Optimal Experimental Design,"Adam Foster, Martin Jankowiak, Elias Bingham, Paul Horsfall, Yee Whye Teh, Thomas Rainforth, Noah Goodman",
neurips,https://proceedings.neurips.cc/paper/2019/file/d55eaf8506f9046a88b8730781830194-Paper.pdf,Flexible Modeling of Diversity with Strongly Log-Concave Distributions,"Joshua Robinson, Suvrit Sra, Stefanie Jegelka",
neurips,https://proceedings.neurips.cc/paper/2019/file/d594b1a945b5d645e59e21f88bd2d83b-Paper.pdf,Neural Machine Translation with Soft Prototype,"Yiren Wang, Yingce Xia, Fei Tian, Fei Gao, Tao Qin, Cheng Xiang Zhai, Tie-Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2019/file/d5a28f81834b6df2b6db6d3e5e2635c7-Paper.pdf,Unsupervised Curricula for Visual Meta-Reinforcement Learning,"Allan Jabri, Kyle Hsu, Abhishek Gupta, Ben Eysenbach, Sergey Levine, Chelsea Finn",
neurips,https://proceedings.neurips.cc/paper/2019/file/d5b3d8dadd770c460b1cde910a711987-Paper.pdf,Improved Regret Bounds for Bandit Combinatorial Optimization,"Shinji Ito, Daisuke Hatano, Hanna Sumita, Kei Takemura, Takuro Fukunaga, Naonori Kakimura, Ken-Ichi Kawarabayashi","\textit{Bandit combinatorial optimization} is a bandit framework in which a player chooses an action within a given finite set
A
⊆
{
0
,
1
}
d
A
and incurs a loss that is the inner product of the chosen action and an unobservable loss vector in
R
d
R
in each round. In this paper, we aim to reveal the property, which makes the bandit combinatorial optimization hard. Recently, Cohen et al.~\citep{cohen2017tight} obtained a lower bound
Ω
(
√
d
k
3
T
/
log
T
)
Ω
of the regret, where
k
k
is the maximum
ℓ
1
ℓ
-norm of action vectors, and
T
T
is the number of rounds. This lower bound was achieved by considering a continuous strongly-correlated distribution of losses. Our main contribution is that we managed to improve this bound by
Ω
(
√
d
k
3
T
)
Ω
through applying a factor of
√
log
T
log
, which can be done by means of strongly-correlated losses with \textit{binary} values. The bound derives better regret bounds for three specific examples of the bandit combinatorial optimization: the multitask bandit, the bandit ranking and the multiple-play bandit. In particular, the bound obtained for the bandit ranking in the present study addresses an open problem raised in \citep{cohen2017tight}. In addition, we demonstrate that the problem becomes easier without considering correlations among entries of loss vectors. In fact, if each entry of loss vectors is an independent random variable, then, one can achieve a regret of
~
O
(
√
d
k
2
T
)
O
, which is
√
k
k
times smaller than the lower bound shown above. The observed results indicated that correlation among losses is the reason for observing a large regret."
neurips,https://proceedings.neurips.cc/paper/2019/file/d60678e8f2ba9c540798ebbde31177e8-Paper.pdf,Doubly-Robust Lasso Bandit,"Gi-Soo Kim, Myunghee Cho Paik","Contextual multi-armed bandit algorithms are widely used in sequential decision tasks such as news article recommendation systems, web page ad placement algorithms, and mobile health. Most of the existing algorithms have regret proportional to a polynomial function of the context dimension,
d
d
. In many applications however, it is often the case that contexts are high-dimensional with only a sparse subset of size
s
0
(
≪
d
)
s
being correlated with the reward. We consider the stochastic linear contextual bandit problem and propose a novel algorithm, namely the Doubly-Robust Lasso Bandit algorithm, which exploits the sparse structure of the regression parameter as in Lasso, while blending the doubly-robust technique used in missing data literature. The high-probability upper bound of the regret incurred by the proposed algorithm does not depend on the number of arms and scales with
l
o
g
(
d
)
l
instead of a polynomial function of
d
d
. The proposed algorithm shows good performance when contexts of different arms are correlated and requires less tuning parameters than existing methods."
neurips,https://proceedings.neurips.cc/paper/2019/file/d60743aab4b625940d39b3b51c3c6a78-Paper.pdf,Recurrent Kernel Networks,"Dexiong Chen, Laurent Jacob, Julien Mairal",
neurips,https://proceedings.neurips.cc/paper/2019/file/d6288499d0083cc34e60a077b7c4b3e1-Paper.pdf,Thinning for Accelerating the Learning of Point Processes,"Tianbo Li, Yiping Ke",
neurips,https://proceedings.neurips.cc/paper/2019/file/d630553e32ae21fb1a6df39c702d2c5c-Paper.pdf,A Universally Optimal Multistage Accelerated Stochastic Gradient Method,"Necdet Serhat Aybat, Alireza Fallah, Mert Gurbuzbalaban, Asuman Ozdaglar",
neurips,https://proceedings.neurips.cc/paper/2019/file/d67d8ab4f4c10bf22aa353e27879133c-Paper.pdf,"Ask not what AI can do, but what AI should do: Towards a framework of task delegability","Brian Lubars, Chenhao Tan",
neurips,https://proceedings.neurips.cc/paper/2019/file/d69768b3da745b77e82cdbddcc8bac98-Paper.pdf,Offline Contextual Bandits with High Probability Fairness Guarantees,"Blossom Metevier, Stephen Giguere, Sarah Brockman, Ari Kobren, Yuriy Brun, Emma Brunskill, Philip S. Thomas",
neurips,https://proceedings.neurips.cc/paper/2019/file/d76d8deea9c19cc9aaf2237d2bf2f785-Paper.pdf,Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting,"Aditya Grover, Jiaming Song, Ashish Kapoor, Kenneth Tran, Alekh Agarwal, Eric J. Horvitz, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2019/file/d77f00766fd3be3f2189c843a6af3fb2-Paper.pdf,LCA: Loss Change Allocation for Neural Network Training,"Janice Lan, Rosanne Liu, Hattie Zhou, Jason Yosinski",
neurips,https://proceedings.neurips.cc/paper/2019/file/d790c9e6c0b5e02c87b375e782ac01bc-Paper.pdf,Adaptive Cross-Modal Few-shot Learning,"Chen Xing, Negar Rostamzadeh, Boris Oreshkin, Pedro O. O. Pinheiro",
neurips,https://proceedings.neurips.cc/paper/2019/file/d7a728a67d909e714c0774e22cb806f2-Paper.pdf,Polynomial Cost of Adaptation for X-Armed Bandits,Hedi Hadiji,
neurips,https://proceedings.neurips.cc/paper/2019/file/d80126524c1e9641333502c664fc6ca1-Paper.pdf,Modelling heterogeneous distributions with an Uncountable Mixture of Asymmetric Laplacians,"Axel Brando, Jose A. Rodriguez, Jordi Vitria, Alberto Rubio Muñoz",
neurips,https://proceedings.neurips.cc/paper/2019/file/d80b7040b773199015de6d3b4293c8ff-Paper.pdf,GNNExplainer: Generating Explanations for Graph Neural Networks,"Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2019/file/d828725179d622a56f951e527a966ed7-Paper.pdf,Missing Not at Random in Matrix Completion: The Effectiveness of Estimating Missingness Probabilities Under a Low Nuclear Norm Assumption,"Wei Ma, George H. Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/d82c8d1619ad8176d665453cfb2e55f0-Paper.pdf,Unsupervised learning of object structure and dynamics from videos,"Matthias Minderer, Chen Sun, Ruben Villegas, Forrester Cole, Kevin P. Murphy, Honglak Lee",
neurips,https://proceedings.neurips.cc/paper/2019/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf,Scalable Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data,"Dominik Linzner, Michael Schmidt, Heinz Koeppl",
neurips,https://proceedings.neurips.cc/paper/2019/file/d840cc5d906c3e9c84374c8919d2074e-Paper.pdf,Cross-channel Communication Networks,"Jianwei Yang, Zhile Ren, Chuang Gan, Hongyuan Zhu, Devi Parikh",
neurips,https://proceedings.neurips.cc/paper/2019/file/d8700cbd38cc9f30cecb34f0c195b137-Paper.pdf,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training,"Haichao Zhang, Jianyu Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/d88518acbcc3d08d1f18da62f9bb26ec-Paper.pdf,Identifying Causal Effects via Context-specific Independence Relations,"Santtu Tikka, Antti Hyttinen, Juha Karvanen",
neurips,https://proceedings.neurips.cc/paper/2019/file/d8c24ca8f23c562a5600876ca2a550ce-Paper.pdf,Differentiable Ranking and Sorting using Optimal Transport,"Marco Cuturi, Olivier Teboul, Jean-Philippe Vert","Sorting is used pervasively in machine learning, either to define elementary algorithms, such as
k
k
-nearest neighbors (
k
k
-NN) rules, or to define test-time metrics, such as top-
k
k
classification accuracy or ranking losses. Sorting is however a poor match for the end-to-end, automatically differentiable pipelines of deep learning. Indeed, sorting procedures output two vectors, neither of which is differentiable: the vector of sorted values is piecewise linear, while the sorting permutation itself (or its inverse, the vector of ranks) has no differentiable properties to speak of, since it is integer-valued. We propose in this paper to replace the usual \texttt{sort} procedure with a differentiable proxy. Our proxy builds upon the fact that sorting can be seen as an optimal assignment problem, one in which the
n
n
values to be sorted are matched to an \emph{auxiliary} probability measure supported on any \emph{increasing} family of
n
n
target values. From this observation, we propose extended rank and sort operators by considering optimal transport (OT) problems (the natural relaxation for assignments) where the auxiliary measure can be any weighted measure supported on
m
m
increasing values, where
m
≠
n
m
. We recover differentiable operators by regularizing these OT problems with an entropic penalty, and solve them by applying Sinkhorn iterations. Using these smoothed rank and sort operators, we propose differentiable proxies for the classification 0/1 loss as well as for the quantile regression loss."
neurips,https://proceedings.neurips.cc/paper/2019/file/d8e1344e27a5b08cdfd5d027d9b8d6de-Paper.pdf,Ordered Memory,"Yikang Shen, Shawn Tan, Arian Hosseini, Zhouhan Lin, Alessandro Sordoni, Aaron C. Courville",
neurips,https://proceedings.neurips.cc/paper/2019/file/d914a6c6d93c8df063b9099a076a488c-Paper.pdf,Approximating the Permanent by Sampling from Adaptive Partitions,"Jonathan Kuck, Tri Dao, Hamid Rezatofighi, Ashish Sabharwal, Stefano Ermon",
neurips,https://proceedings.neurips.cc/paper/2019/file/d921c3c762b1522c475ac8fc0811bb0f-Paper.pdf,Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics,"Niru Maheswaranathan, Alex Williams, Matthew Golub, Surya Ganguli, David Sussillo",
neurips,https://proceedings.neurips.cc/paper/2019/file/d961e9f236177d65d21100592edb0769-Paper.pdf,Quaternion Knowledge Graph Embeddings,"SHUAI ZHANG, Yi Tay, Lina Yao, Qi Liu",
neurips,https://proceedings.neurips.cc/paper/2019/file/d9731321ef4e063ebbee79298fa36f56-Paper.pdf,Initialization of ReLUs for Dynamical Isometry,"Rebekka Burkholz, Alina Dubatovka",
neurips,https://proceedings.neurips.cc/paper/2019/file/d97d404b6119214e4a7018391195240a-Paper.pdf,On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset,"Muhammad Waleed Gondal, Manuel Wuthrich, Djordje Miladinovic, Francesco Locatello, Martin Breidt, Valentin Volchkov, Joel Akpo, Olivier Bachem, Bernhard Schölkopf, Stefan Bauer",
neurips,https://proceedings.neurips.cc/paper/2019/file/d98c1545b7619bd99b817cb3169cdfde-Paper.pdf,Subquadratic High-Dimensional Hierarchical Clustering,"Amir Abboud, Vincent Cohen-Addad, Hussein Houdrouge","We consider the widely-used average-linkage, single-linkage, and Ward's methods for computing hierarchical clusterings of high-dimensional Euclidean inputs. It is easy to show that there is no efficient implementation of these algorithms in high dimensional Euclidean space since it implicitly requires to solve the closest pair problem, a notoriously difficult problem. However, how fast can these algorithms be implemented if we allow approximation? More precisely: these algorithms successively merge the clusters that are at closest average (for average-linkage), minimum distance (for single-linkage), or inducing the least sum-of-square error (for Ward's). We ask whether one could obtain a significant running-time improvement if the algorithm can merge
γ
γ
-approximate closest clusters (namely, clusters that are at distance (average, minimum, or sum-of-square error) at most
γ
γ
times the distance of the closest clusters). We show that one can indeed take advantage of the relaxation and compute the approximate hierarchical clustering tree using
˜
O
(
n
)
O
γ
γ
-approximate nearest neighbor queries. This leads to an algorithm running in time
˜
O
(
n
d
)
+
n
1
+
O
(
1
/
γ
)
O
for
d
d
-dimensional Euclidean space. We then provide experiments showing that these algorithms perform as well as the non-approximate version for classic classification tasks while achieving a significant speed-up."
neurips,https://proceedings.neurips.cc/paper/2019/file/d9fbed9da256e344c1fa46bb46c34c5f-Paper.pdf,PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization,"Thijs Vogels, Sai Praneeth Karimireddy, Martin Jaggi",
neurips,https://proceedings.neurips.cc/paper/2019/file/da54dd5a0398011cdfa50d559c2c0ef8-Paper.pdf,"Distribution oblivious, risk-aware algorithms for multi-armed bandits with unbounded rewards","Anmol Kagrecha, Jayakrishnan Nair, Krishna Jagannathan",
neurips,https://proceedings.neurips.cc/paper/2019/file/da647c549dde572c2c5edc4f5bef039c-Paper.pdf,Multilabel reductions: what is my loss optimising?,"Aditya K. Menon, Ankit Singh Rawat, Sashank Reddi, Sanjiv Kumar","Multilabel classification is a challenging problem arising in applications ranging from information retrieval to image tagging. A popular approach to this problem is to employ a reduction to a suitable series of binary or multiclass problems (e.g., computing a softmax based cross-entropy over the relevant labels). While such methods have seen empirical success, less is understood about how well they approximate two fundamental performance measures: precision@
k
k
and recall@
k
k
. In this paper, we study five commonly used reductions, including the one-versus-all reduction, a reduction to multiclass classification, and normalised versions of the same, wherein the contribution of each instance is normalised by the number of relevant labels. Our main result is a formal justification of each reduction: we explicate their underlying risks, and show they are each consistent with respect to either precision or recall. Further, we show that in general no reduction can be optimal for both measures. We empirically validate our results, demonstrating scenarios where normalised reductions yield recall gains over unnormalised counterparts."
neurips,https://proceedings.neurips.cc/paper/2019/file/dab1263d1e6a88c9ba5e7e294def5e8b-Paper.pdf,A Similarity-preserving Network Trained on Transformed Images Recapitulates Salient Features of the Fly Motion Detection Circuit,"Yanis Bahroun, Dmitri Chklovskii, Anirvan Sengupta",
neurips,https://proceedings.neurips.cc/paper/2019/file/dabd8d2ce74e782c65a973ef76fd540b-Paper.pdf,CNN^{2}: Viewpoint Generalization via a Binocular Vision,"Wei-Da Chen, Shan-Hung (Brandon) Wu",
neurips,https://proceedings.neurips.cc/paper/2019/file/dae3312c4c6c7000a37ecfb7b0aeb0e4-Paper.pdf,Unsupervised Learning of Object Keypoints for Perception and Control,"Tejas D. Kulkarni, Ankush Gupta, Catalin Ionescu, Sebastian Borgeaud, Malcolm Reynolds, Andrew Zisserman, Volodymyr Mnih",
neurips,https://proceedings.neurips.cc/paper/2019/file/daea32adcae6abcb548134fa98f139f9-Paper.pdf,G2SAT: Learning to Generate SAT Formulas,"Jiaxuan You, Haoze Wu, Clark Barrett, Raghuram Ramanujan, Jure Leskovec",
neurips,https://proceedings.neurips.cc/paper/2019/file/db182d2552835bec774847e06406bfa2-Paper.pdf,The Functional Neural Process,"Christos Louizos, Xiahan Shi, Klamer Schutte, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2019/file/db29450c3f5e97f97846693611f98c15-Paper.pdf,Convergent Policy Optimization for Safe Reinforcement Learning,"Ming Yu, Zhuoran Yang, Mladen Kolar, Zhaoran Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/db5cea26ca37aa09e5365f3e7f5dd9eb-Paper.pdf,A Refined Margin Distribution Analysis for Forest Representation Learning,"Shen-Huan Lyu, Liang Yang, Zhi-Hua Zhou","In this paper, we formulate the forest representation learning approach called \textsc{CasDF} as an additive model which boosts the augmented feature instead of the prediction. We substantially improve the upper bound of the generalization gap from
O
(
√
ln
m
/
m
)
O
to
O
(
ln
m
/
m
)
O
, while the margin ratio of the margin standard deviation to the margin mean is sufficiently small. This tighter upper bound inspires us to optimize the ratio. Therefore, we design a margin distribution reweighting approach for deep forest to achieve a small margin ratio by boosting the augmented feature. Experiments confirm the correlation between the margin distribution and generalization performance. We remark that this study offers a novel understanding of \textsc{CasDF} from the perspective of the margin theory and further guides the layer-by-layer forest representation learning."
neurips,https://proceedings.neurips.cc/paper/2019/file/db98dc0dbafde48e8f74c0de001d35e4-Paper.pdf,Diffeomorphic Temporal Alignment Nets,"Ron A. Shapira Weber, Matan Eyal, Nicki Skafte, Oren Shriki, Oren Freifeld",
neurips,https://proceedings.neurips.cc/paper/2019/file/db9ad56c71619aeed9723314d1456037-Paper.pdf,Multi-source Domain Adaptation for Semantic Segmentation,"Sicheng Zhao, Bo Li, Xiangyu Yue, Yang Gu, Pengfei Xu, Runbo Hu, Hua Chai, Kurt Keutzer",
neurips,https://proceedings.neurips.cc/paper/2019/file/dbbf603ff0e99629dda5d75b6f75f966-Paper.pdf,Spectral Modification of Graphs for Improved Spectral Clustering,"Ioannis Koutis, Huong Le","Spectral clustering algorithms provide approximate solutions to hard optimization problems that formulate graph partitioning in terms of the graph conductance. It is well understood that the quality of these approximate solutions is negatively affected by a possibly significant gap between the conductance and the second eigenvalue of the graph. In this paper we show that for \textbf{any} graph
G
G
, there exists a `spectral maximizer' graph
H
H
which is cut-similar to
G
G
, but has eigenvalues that are near the theoretical limit implied by the cut structure of
G
G
. Applying then spectral clustering on
H
H
has the potential to produce improved cuts that also exist in
G
G
due to the cut similarity. This leads to the second contribution of this work: we describe a practical spectral modification algorithm that raises the eigenvalues of the input graph, while preserving its cuts. Combined with spectral clustering on the modified graph, this yields demonstrably improved cuts."
neurips,https://proceedings.neurips.cc/paper/2019/file/dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf,On Exact Computation with an Infinitely Wide Neural Net,"Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Russ R. Salakhutdinov, Ruosong Wang","The current paper gives the first efficient exact algorithm for computing the extension of NTK to convolutional neural nets, which we call Convolutional NTK (CNTK), as well as an efficient GPU implementation of this algorithm. This results in a significant new benchmark for performance of a pure kernel-based method on CIFAR-10, being 10% higher than the methods reported in [Novak et al., 2019], and only 6% lower than the performance of the corresponding finite deep net architecture (once batch normalization etc. are turned off). Theoretically, we also give the first non-asymptotic proof showing that a fully-trained sufficiently wide net is indeed equivalent to the kernel regression predictor using NTK."
neurips,https://proceedings.neurips.cc/paper/2019/file/dbea3d0e2a17c170c412c74273778159-Paper.pdf,Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity,"Chulhee Yun, Suvrit Sra, Ali Jadbabaie","We study finite sample expressivity, i.e., memorization power of ReLU networks. Recent results require
N
N
hidden nodes to memorize/interpolate arbitrary
N
N
data points. In contrast, by exploiting depth, we show that 3-layer ReLU networks with
Ω
(
√
N
)
Ω
hidden nodes can perfectly memorize most datasets with
N
N
points. We also prove that width
Θ
(
√
N
)
Θ
is necessary and sufficient for memorizing
N
N
data points, proving tight bounds on memorization capacity. The sufficiency result can be extended to deeper networks; we show that an
L
L
-layer network with
W
W
parameters in the hidden layers can memorize
N
N
data points if
W
=
Ω
(
N
)
W
. Combined with a recent upper bound
O
(
W
L
log
W
)
O
on VC dimension, our construction is nearly tight for any fixed
L
L
. Subsequently, we analyze memorization capacity of residual networks under a general position assumption; we prove results that substantially reduce the known requirement of
N
N
hidden nodes. Finally, we study the dynamics of stochastic gradient descent (SGD), and show that when initialized near a memorizing global minimum of the empirical risk, SGD quickly finds a nearby point with much smaller empirical risk."
neurips,https://proceedings.neurips.cc/paper/2019/file/dc554706afe4c72a60a25314cbaece80-Paper.pdf,Amortized Bethe Free Energy Minimization for Learning MRFs,"Sam Wiseman, Yoon Kim",
neurips,https://proceedings.neurips.cc/paper/2019/file/dc6a70712a252123c40d2adba6a11d84-Paper.pdf,Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence,"Fengxiang He, Tongliang Liu, Dacheng Tao","Deep neural networks have received dramatic success based on the optimization method of stochastic gradient descent (SGD). However, it is still not clear how to tune hyper-parameters, especially batch size and learning rate, to ensure good generalization. This paper reports both theoretical and empirical evidence of a training strategy that we should control the ratio of batch size to learning rate not too large to achieve a good generalization ability. Specifically, we prove a PAC-Bayes generalization bound for neural networks trained by SGD, which has a positive correlation with the ratio of batch size to learning rate. This correlation builds the theoretical foundation of the training strategy. Furthermore, we conduct a large-scale experiment to verify the correlation and training strategy. We trained 1,600 models based on architectures ResNet-110, and VGG-19 with datasets CIFAR-10 and CIFAR-100 while strictly control unrelated variables. Accuracies on the test sets are collected for the evaluation. Spearman's rank-order correlation coefficients and the corresponding
p
p
values on 164 groups of the collected data demonstrate that the correlation is statistically significant, which fully supports the training strategy."
neurips,https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf,XLNet: Generalized Autoregressive Pretraining for Language Understanding,"Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R. Salakhutdinov, Quoc V. Le",
neurips,https://proceedings.neurips.cc/paper/2019/file/dc87c13749315c7217cdc4ac692e704c-Paper.pdf,Conditional Independence Testing using Generative Adversarial Networks,"Alexis Bellot, Mihaela van der Schaar",
neurips,https://proceedings.neurips.cc/paper/2019/file/dc960c46c38bd16e953d97cdeefdbc68-Paper.pdf,A Tensorized Transformer for Language Modeling,"Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",
neurips,https://proceedings.neurips.cc/paper/2019/file/dca5672ff3444c7e997aa9a2c4eb2094-Paper.pdf,Classification-by-Components: Probabilistic Modeling of Reasoning over a Set of Components,"Sascha Saralajew, Lars Holdijk, Maike Rees, Ebubekir Asan, Thomas Villmann",
neurips,https://proceedings.neurips.cc/paper/2019/file/dd03de08bfdff4d8ab01117276564cc7-Paper.pdf,Recurrent Registration Neural Networks for Deformable Image Registration,"Robin Sandkühler, Simon Andermatt, Grzegorz Bauman, Sylvia Nyilas, Christoph Jud, Philippe C. Cattin",
neurips,https://proceedings.neurips.cc/paper/2019/file/ddb4955263e6c08179393d1beaf18602-Paper.pdf,User-Specified Local Differential Privacy in Unconstrained Adaptive Online Learning,Dirk van der Hoeven,
neurips,https://proceedings.neurips.cc/paper/2019/file/ddf354219aac374f1d40b7e760ee5bb7-Paper.pdf,Learning Representations by Maximizing Mutual Information Across Views,"Philip Bachman, R Devon Hjelm, William Buchwalter",
neurips,https://proceedings.neurips.cc/paper/2019/file/de594ef5c314372edec29b93cab9d72e-Paper.pdf,Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs,"Jian QIAN, Ronan Fruit, Matteo Pirotta, Alessandro Lazaric","The exploration bonus is an effective approach to manage the exploration-exploitation trade-off in Markov Decision Processes (MDPs). While it has been analyzed in infinite-horizon discounted and finite-horizon problems, we focus on designing and analysing the exploration bonus in the more challenging infinite-horizon undiscounted setting. We first introduce SCAL+, a variant of SCAL (Fruit et al. 2018), that uses a suitable exploration bonus to solve any discrete unknown weakly-communicating MDP for which an upper bound
c
c
on the span of the optimal bias function is known. We prove that SCAL+ enjoys the same regret guarantees as SCAL, which relies on the less efficient extended value iteration approach. Furthermore, we leverage the flexibility provided by the exploration bonus scheme to generalize SCAL+ to smooth MDPs with continuous state space and discrete actions. We show that the resulting algorithm (SCCAL+) achieves the same regret bound as UCCRL (Ortner and Ryabko, 2012) while being the first implementable algorithm for this setting."
neurips,https://proceedings.neurips.cc/paper/2019/file/dea184826614d3f4c608731389ed0c74-Paper.pdf,A neurally plausible model learns successor representations in partially observable environments,"Eszter Vértes, Maneesh Sahani",
neurips,https://proceedings.neurips.cc/paper/2019/file/deb54ffb41e085fd7f69a75b6359c989-Paper.pdf,Tight Dimension Independent Lower Bound on the Expected Convergence Rate for Diminishing Step Sizes in SGD,"PHUONG_HA NGUYEN, Lam Nguyen, Marten van Dijk","We study the convergence of Stochastic Gradient Descent (SGD) for strongly convex objective functions. We prove for all
t
t
a lower bound on the expected convergence rate after the
t
t
-th SGD iteration; the lower bound is over all possible sequences of diminishing step sizes. It implies that recently proposed sequences of step sizes at ICML 2018 and ICML 2019 are {\em universally} close to optimal in that the expected convergence rate after {\em each} iteration is within a factor
32
32
of our lower bound. This factor is independent of dimension
d
d
. We offer a framework for comparing with lower bounds in state-of-the-art literature and when applied to SGD for strongly convex objective functions our lower bound is a significant factor
775
⋅
d
775
larger compared to existing work."
neurips,https://proceedings.neurips.cc/paper/2019/file/df0e09d6f25a15a815563df9827f48fa-Paper.pdf,Cost Effective Active Search,"Shali Jiang, Roman Garnett, Benjamin Moseley","We study a special paradigm of active learning, called cost effective active search, where the goal is to find a given number of positive points from a large unlabeled pool with minimum labeling cost. Most existing methods solve this problem heuristically, and few theoretical results have been established. We adopt a principled Bayesian approach for the first time. We first derive the Bayesian optimal policy and establish a strong hardness result: the optimal policy is hard to approximate, with the best-possible approximation ratio lower bounded by
Ω
(
n
0.16
)
Ω
. We then propose an efficient and nonmyopic policy using the negative Poisson binomial distribution. We propose simple and fast approximations for computing its expectation, which serves as an essential role in our proposed policy. We conduct comprehensive experiments on various domains such as drug and materials discovery, and demonstrate that our proposed search procedure is superior to the widely used greedy baseline."
neurips,https://proceedings.neurips.cc/paper/2019/file/df263d996281d984952c07998dc54358-Paper.pdf,Optimal Statistical Rates for Decentralised Non-Parametric Regression with Linear Speed-Up,"Dominic Richards, Patrick Rebeschini",
neurips,https://proceedings.neurips.cc/paper/2019/file/df308fd90635b28d82558cf580c73ed9-Paper.pdf,Modeling Conceptual Understanding in Image Reference Games,"Rodolfo Corona Rodriguez, Stephan Alaniz, Zeynep Akata",
neurips,https://proceedings.neurips.cc/paper/2019/file/dfce06801e1a85d6d06f1fdd4475dacd-Paper.pdf,Inherent Weight Normalization in Stochastic Neural Networks,"Georgios Detorakis, Sourav Dutta, Abhishek Khanna, Matthew Jerry, Suman Datta, Emre Neftci",
neurips,https://proceedings.neurips.cc/paper/2019/file/dffbb6efd376d8dbb22cdf491e481edc-Paper.pdf,Universality in Learning from Linear Measurements,"Ehsan Abbasi, Fariborz Salehi, Babak Hassibi","We study the problem of recovering a structured signal from independently and identically drawn linear measurements. A convex penalty function
f
(
⋅
)
f
is considered which penalizes deviations from the desired structure, and signal recovery is performed by minimizing
f
(
⋅
)
f
subject to the linear measurement constraints. The main question of interest is to determine the minimum number of measurements that is necessary and sufficient for the perfect recovery of the unknown signal with high probability. Our main result states that, under some mild conditions on
f
(
⋅
)
f
and on the distribution from which the linear measurements are drawn, the minimum number of measurements required for perfect recovery depends only on the first and second order statistics of the measurement vectors. As a result, the required of number of measurements can be determining by studying measurement vectors that are Gaussian (and have the same mean vector and covariance matrix) for which a rich literature and comprehensive theory exists. As an application, we show that the minimum number of random quadratic measurements (also known as rank-one projections) required to recover a low rank positive semi-definite matrix is
3
n
r
3
, where
n
n
is the dimension of the matrix and
r
r
is its rank. As a consequence, we settle the long standing open question of determining the minimum number of measurements required for perfect signal recovery in phase retrieval using the celebrated PhaseLift algorithm, and show it to be
3
n
3
."
neurips,https://proceedings.neurips.cc/paper/2019/file/e00406144c1e7e35240afed70f34166a-Paper.pdf,Discrimination in Online Markets: Effects of Social Bias on Learning from Reviews and Policy Design,"Faidra Georgia Monachou, Itai Ashlagi",
neurips,https://proceedings.neurips.cc/paper/2019/file/e025b6279c1b88d3ec0eca6fcb6e6280-Paper.pdf,Structure Learning with Side Information: Sample Complexity,"Saurabh Sihag, Ali Tajer",
neurips,https://proceedings.neurips.cc/paper/2019/file/e046ede63264b10130007afca077877f-Paper.pdf,Discrete Flows: Invertible Generative Models of Discrete Data,"Dustin Tran, Keyon Vafa, Kumar Agrawal, Laurent Dinh, Ben Poole",
neurips,https://proceedings.neurips.cc/paper/2019/file/e077e1a544eec4f0307cf5c3c721d944-Paper.pdf,Disentangled behavioural representations,"Amir Dezfouli, Hassan Ashtiani, Omar Ghattas, Richard Nock, Peter Dayan, Cheng Soon Ong",
neurips,https://proceedings.neurips.cc/paper/2019/file/e0ab531ec312161511493b002f9be2ee-Paper.pdf,A Flexible Generative Framework for Graph-based Semi-supervised Learning,"Jiaqi Ma, Weijing Tang, Ji Zhu, Qiaozhu Mei",
neurips,https://proceedings.neurips.cc/paper/2019/file/e0b0f9051084fd476926501af19e1e96-Paper.pdf,A New Perspective on Pool-Based Active Classification and False-Discovery Control,"Lalit Jain, Kevin G. Jamieson",
neurips,https://proceedings.neurips.cc/paper/2019/file/e0e2b58d64fb37a2527329a5ce093d80-Paper.pdf,Online-Within-Online Meta-Learning,"Giulia Denevi, Dimitris Stamos, Carlo Ciliberto, Massimiliano Pontil",
neurips,https://proceedings.neurips.cc/paper/2019/file/e0eacd983971634327ae1819ea8b6214-Paper.pdf,Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model,"Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris Shallue, Roger B. Grosse",
neurips,https://proceedings.neurips.cc/paper/2019/file/e1054bf2d703bca1e8fe101d3ac5efcd-Paper.pdf,Using Statistics to Automate Stochastic Optimization,"Hunter Lang, Lin Xiao, Pengchuan Zhang",
neurips,https://proceedings.neurips.cc/paper/2019/file/e1dc4bf1f94e87fdfeb2d91ae3dc10ef-Paper.pdf,Margin-Based Generalization Lower Bounds for Boosted Classifiers,"Allan Grønlund, Lior Kamma, Kasper Green Larsen, Alexander Mathiasen, Jelani Nelson","Boosting is one of the most successful ideas in machine learning. The most well-accepted explanations for the low generalization error of boosting algorithms such as AdaBoost stem from margin theory. The study of margins in the context of boosting algorithms was initiated by Schapire, Freund, Bartlett and Lee (1998), and has inspired numerous boosting algorithms and generalization bounds. To date, the strongest known generalization (upper bound) is the
k
k
th margin bound of Gao and Zhou (2013). Despite the numerous generalization upper bounds that have been proved over the last two decades, nothing is known about the tightness of these bounds. In this paper, we give the first margin-based lower bounds on the generalization error of boosted classifiers. Our lower bounds nearly match the
k
k
th margin bound and thus almost settle the generalization performance of boosted classifiers in terms of margins."
neurips,https://proceedings.neurips.cc/paper/2019/file/e205ee2a5de471a70c1fd1b46033a75f-Paper.pdf,D-VAE: A Variational Autoencoder for Directed Acyclic Graphs,"Muhan Zhang, Shali Jiang, Zhicheng Cui, Roman Garnett, Yixin Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/e2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf,"Adversarial Examples Are Not Bugs, They Are Features","Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry",
neurips,https://proceedings.neurips.cc/paper/2019/file/e2c4a40d50b47094f571e40efead3900-Paper.pdf,Characterizing the Exact Behaviors of Temporal Difference Learning Algorithms Using Markov Jump Linear System Theory,"Bin Hu, Usman Syed",
neurips,https://proceedings.neurips.cc/paper/2019/file/e2c61965b5e23b47b77d7c51611b6d7f-Paper.pdf,Deep RGB-D Canonical Correlation Analysis For Sparse Depth Completion,"Yiqi Zhong, Cho-Ying Wu, Suya You, Ulrich Neumann",
neurips,https://proceedings.neurips.cc/paper/2019/file/e2ccf95a7f2e1878fcafc8376649b6e8-Paper.pdf,Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck,"Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek, Cheng Zhang, Sam Devlin, Katja Hofmann",
neurips,https://proceedings.neurips.cc/paper/2019/file/e2db7186375992e729165726762cb4c1-Paper.pdf,Untangling in Invariant Speech Recognition,"Cory Stephenson, Jenelle Feather, Suchismita Padhy, Oguz Elibol, Hanlin Tang, Josh McDermott, SueYeon Chung",
neurips,https://proceedings.neurips.cc/paper/2019/file/e2e14235335d2c0aa5f6855e339233d9-Paper.pdf,Fast structure learning with modular regularization,"Greg Ver Steeg, Hrayr Harutyunyan, Daniel Moyer, Aram Galstyan",
neurips,https://proceedings.neurips.cc/paper/2019/file/e2eabaf96372e20a9e3d4b5f83723a61-Paper.pdf,Graph-based Discriminators: Sample Complexity and Expressiveness,"Roi Livni, Yishay Mansour","A basic question in learning theory is to identify if two distributions are identical when we have access only to examples sampled from the distributions. This basic task is considered, for example, in the context of Generative Adversarial Networks (GANs), where a discriminator is trained to distinguish between a real-life distribution and a synthetic distribution. Classically, we use a hypothesis class
H
H
and claim that the two distributions are distinct if for some
h
∈
H
h
the expected value on the two distributions is (significantly) different. Our starting point is the following fundamental problem: ""is having the hypothesis dependent on more than a single random example beneficial"". To address this challenge we define
k
k
-ary based discriminators, which have a family of Boolean
k
k
-ary functions
\G
\G
. Each function
g
∈
\G
g
naturally defines a hyper-graph, indicating whether a given hyper-edge exists. A function
g
∈
\G
g
distinguishes between two distributions, if the expected value of
g
g
, on a
k
k
-tuple of i.i.d examples, on the two distributions is (significantly) different. We study the expressiveness of families of
k
k
-ary functions, compared to the classical hypothesis class
H
H
, which is
k
=
1
k
. We show a separation in expressiveness of
k
+
1
k
-ary versus
k
k
-ary functions. This demonstrate the great benefit of having
k
≥
2
k
as distinguishers. For
k
≥
2
k
we introduce a notion similar to the VC-dimension, and show that it controls the sample complexity. We proceed and provide upper and lower bounds as a function of our extended notion of VC-dimension."
neurips,https://proceedings.neurips.cc/paper/2019/file/e2f374c3418c50bc30d67d5f7454a5b4-Paper.pdf,Certifiable Robustness to Graph Perturbations,"Aleksandar Bojchevski, Stephan Günnemann",
neurips,https://proceedings.neurips.cc/paper/2019/file/e345fac6bc5c868f0222430c733fa26e-Paper.pdf,Surfing: Iterative Optimization Over Incrementally Trained Deep Networks,"Ganlin Song, Zhou Fan, John Lafferty","We investigate a sequential optimization procedure to minimize the empirical risk functional
f
^
θ
(
x
)
=
1
2
∥
G
^
θ
(
x
)
−
y
∥
2
f
for certain families of deep networks
G
θ
(
x
)
G
. The approach is to optimize a sequence of objective functions that use network parameters obtained during different stages of the training process. When initialized with random parameters
θ
0
θ
, we show that the objective
f
θ
0
(
x
)
f
is
nice'' and easy to optimize with gradient descent. As learning is carried out, we obtain a sequence of generative networks
x
↦
G
θ
t
(
x
)
x
and associated risk functions
f
θ
t
(
x
)
f
, where
t
t
indicates a stage of stochastic gradient descent during training. Since the parameters of the network do not change by very much in each step, the surface evolves slowly and can be incrementally optimized. The algorithm is formalized and analyzed for a family of expansive networks. We call the procedure {\it surfing} since it rides along the peak of the evolving (negative) empirical risk function, starting from a smooth surface at the beginning of learning and ending with a wavy nonconvex surface after learning is complete. Experiments show how surfing can be used to find the global optimum and for compressed sensing even when direct gradient descent on the final learned network fails."
neurips,https://proceedings.neurips.cc/paper/2019/file/e347c51419ffb23ca3fd5050202f9c3d-Paper.pdf,Rates of Convergence for Large-scale Nearest Neighbor Classification,"Xingye Qiao, Jiexin Duan, Guang Cheng",
neurips,https://proceedings.neurips.cc/paper/2019/file/e354fd90b2d5c777bfec87a352a18976-Paper.pdf,Finite-Time Performance Bounds and Adaptive Learning Rate Selection for Two Time-Scale Reinforcement Learning,"Harsh Gupta, R. Srikant, Lei Ying",
neurips,https://proceedings.neurips.cc/paper/2019/file/e3ca0449fa2ea7701a7ac53fb719c51a-Paper.pdf,Pseudo-Extended Markov chain Monte Carlo,"Christopher Nemeth, Fredrik Lindsten, Maurizio Filippone, James Hensman",
neurips,https://proceedings.neurips.cc/paper/2019/file/e41990b122b864f164d2ab96e8322690-Paper.pdf,Hierarchical Optimal Transport for Multimodal Distribution Alignment,"John Lee, Max Dabagia, Eva Dyer, Christopher Rozell",
neurips,https://proceedings.neurips.cc/paper/2019/file/e43739bba7cdb577e9e3e4e42447f5a5-Paper.pdf,Sampled Softmax with Random Fourier Features,"Ankit Singh Rawat, Jiecao Chen, Felix Xinnan X. Yu, Ananda Theertha Suresh, Sanjiv Kumar",
neurips,https://proceedings.neurips.cc/paper/2019/file/e449b9317dad920c0dd5ad0a2a2d5e49-Paper.pdf,Epsilon-Best-Arm Identification in Pay-Per-Reward Multi-Armed Bandits,Sivan Sabato,
neurips,https://proceedings.neurips.cc/paper/2019/file/e44e875c12109e4fa3716c05008048b2-Paper.pdf,On Differentially Private Graph Sparsification and Applications,"Raman Arora, Jalaj Upadhyay",
neurips,https://proceedings.neurips.cc/paper/2019/file/e465ae46b07058f4ab5e96b98f101756-Paper.pdf,On the number of variables to use in principal component regression,"Ji Xu, Daniel J. Hsu","We study least squares linear regression over
N
N
uncorrelated Gaussian features that are selected in order of decreasing variance. When the number of selected features
p
p
is at most the sample size
n
n
, the estimator under consideration coincides with the principal component regression estimator; when
p
>
n
p
, the estimator is the least
ℓ
2
ℓ
norm solution over the selected features. We give an average-case analysis of the out-of-sample prediction error as
p
,
n
,
N
→
∞
p
with
p
/
N
→
α
p
and
n
/
N
→
β
n
, for some constants
α
∈
[
0
,
1
]
α
and
β
∈
(
0
,
1
)
β
. In this average-case setting, the prediction error exhibits a
double descent'' shape as a function of
p
p
. We also establish conditions under which the minimum risk is achieved in the interpolating (
p
>
n
p
) regime."
neurips,https://proceedings.neurips.cc/paper/2019/file/e46bc064f8e92ac2c404b9871b2a4ef2-Paper.pdf,Self-Routing Capsule Networks,"Taeyoung Hahn, Myeongjang Pyeon, Gunhee Kim",
neurips,https://proceedings.neurips.cc/paper/2019/file/e49eb6523da9e1c347bc148ea8ac55d3-Paper.pdf,A Model-Based Reinforcement Learning with Adversarial Training for Online Recommendation,"Xueying Bai, Jian Guan, Hongning Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/e4da3b7fbbce2345d7772b0674a318d5-Paper.pdf,Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation,"Risto Vuorio, Shao-Hua Sun, Hexiang Hu, Joseph J. Lim",
neurips,https://proceedings.neurips.cc/paper/2019/file/e4dd5528f7596dcdf871aa55cfccc53c-Paper.pdf,Predicting the Politics of an Image Using Webly Supervised Data,"Christopher Thomas, Adriana Kovashka",
neurips,https://proceedings.neurips.cc/paper/2019/file/e515df0d202ae52fcebb14295743063b-Paper.pdf,On the Curved Geometry of Accelerated Optimization,Aaron Defazio,
neurips,https://proceedings.neurips.cc/paper/2019/file/e520f70ac3930490458892665cda6620-Paper.pdf,How to Initialize your Network? Robust Initialization for WeightNorm & ResNets,"Devansh Arpit, Víctor Campos, Yoshua Bengio",
neurips,https://proceedings.neurips.cc/paper/2019/file/e52ad5c9f751f599492b4f087ed7ecfc-Paper.pdf,Code Generation as a Dual Task of Code Summarization,"Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, Zhi Jin",
neurips,https://proceedings.neurips.cc/paper/2019/file/e555ebe0ce426f7f9b2bef0706315e0c-Paper.pdf,A Graph Theoretic Framework of Recomputation Algorithms for Memory-Efficient Backpropagation,"Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, Masanori Koyama","Recomputation algorithms collectively refer to a family of methods that aims to reduce the memory consumption of the backpropagation by selectively discarding the intermediate results of the forward propagation and recomputing the discarded results as needed. In this paper, we will propose a novel and efficient recomputation method that can be applied to a wider range of neural nets than previous methods. We use the language of graph theory to formalize the general recomputation problem of minimizing the computational overhead under a fixed memory budget constraint, and provide a dynamic programming solution to the problem. Our method can reduce the peak memory consumption on various benchmark networks by
36
%
∼
81
%
36
, which outperforms the reduction achieved by other methods."
neurips,https://proceedings.neurips.cc/paper/2019/file/e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf,Gradient based sample selection for online continual learning,"Rahaf Aljundi, Min Lin, Baptiste Goujaud, Yoshua Bengio",
neurips,https://proceedings.neurips.cc/paper/2019/file/e57c6b956a6521b28495f2886ca0977a-Paper.pdf,Conditional Structure Generation through Graph Variational Generative Adversarial Nets,"Carl Yang, Peiye Zhuang, Wenhan Shi, Alan Luu, Pan Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf,Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting,"Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, Deyu Meng",
neurips,https://proceedings.neurips.cc/paper/2019/file/e5b294b70c9647dcf804d7baa1903918-Paper.pdf,Thompson Sampling with Information Relaxation Penalties,"Seungki Min, Costis Maglaras, Ciamac C. Moallemi","We develop several novel policies and performance bounds for MAB problems that vary in terms of improving performance and increasing computational complexity between the two endpoints. Our policies can be viewed as natural generalizations of TS that simultaneously incorporate knowledge of the time horizon and explicitly consider the exploration-exploitation trade-off. We prove associated structural results on performance bounds and suboptimality gaps. Numerical experiments suggest that this new class of policies perform well, in particular in settings where the finite time horizon introduces significant exploration-exploitation tension into the problem."
neurips,https://proceedings.neurips.cc/paper/2019/file/e6385d39ec9394f2f3a354d9d2b88eec-Paper.pdf,Oracle-Efficient Algorithms for Online Linear Optimization with Bandit Feedback,"Shinji Ito, Daisuke Hatano, Hanna Sumita, Kei Takemura, Takuro Fukunaga, Naonori Kakimura, Ken-Ichi Kawarabayashi","We propose computationally efficient algorithms for \textit{online linear optimization with bandit feedback}, in which a player chooses an \textit{action vector} from a given (possibly infinite) set
A
⊆
R
d
A
, and then suffers a loss that can be expressed as a linear function in action vectors. Although existing algorithms achieve an optimal regret bound of
~
O
(
√
T
)
O
for
T
T
rounds (ignoring factors of
p
o
l
y
(
d
,
log
T
)
p
), computationally efficient ways of implementing them have not yet been specified, in particular when
|
A
|
|
is not bounded by a polynomial size in
d
d
. A standard way to pursue computational efficiency is to assume that we have an efficient algorithm referred to as \textit{oracle} that solves (offline) linear optimization problems over
A
A
. Under this assumption, the computational efficiency of a bandit algorithm can then be measured in terms of \textit{oracle complexity}, i.e., the number of oracle calls. Our contribution is to propose algorithms that offer optimal regret bounds of
~
O
(
√
T
)
O
as well as low oracle complexity for both \textit{non-stochastic settings} and \textit{stochastic settings}. Our algorithm for non-stochastic settings has an oracle complexity of
~
O
(
T
)
O
and is the first algorithm that achieves both a regret bound of
~
O
(
√
T
)
O
and an oracle complexity of
~
O
(
p
o
l
y
(
T
)
)
O
, given only linear optimization oracles. Our algorithm for stochastic settings calls the oracle only
O
(
p
o
l
y
(
d
,
log
T
)
)
O
times, which is smaller than the current best oracle complexity of
O
(
T
)
O
if
T
T
is sufficiently large."
neurips,https://proceedings.neurips.cc/paper/2019/file/e6872f5bbe75073f8c7cfb93de7f6f3a-Paper.pdf,Constraint-based Causal Structure Learning with Consistent Separating Sets,"Honghao Li, Vincent Cabeli, Nadir Sella, Herve Isambert",
neurips,https://proceedings.neurips.cc/paper/2019/file/e6acf4b0f69f6f6e60e9a815938aa1ff-Paper.pdf,Efficient Deep Approximation of GMMs,"Shirin Jalali, Carl Nuzman, Iraj Saniee","The universal approximation theorem states that any regular function can be approximated closely using a single hidden layer neural network. Some recent work has shown that, for some special functions, the number of nodes in such an approximation could be exponentially reduced with multi-layer neural networks. In this work, we extend this idea to a rich class of functions, namely the discriminant functions that arise in optimal Bayesian classification of Gaussian mixture models (GMMs) in
\mathds
R
n
\mathds
. We show that such functions can be approximated with arbitrary precision using
O
(
n
)
O
nodes in a neural network with two hidden layers (deep neural network), while in contrast, a neural network with a single hidden layer (shallow neural network) would require at least
O
(
exp
(
n
)
)
O
nodes or exponentially large coefficients. Given the universality of the Gaussian distribution in the feature spaces of data, e.g., in speech, image and text, our results shed light on the observed efficiency of deep neural networks in practical classification problems."
neurips,https://proceedings.neurips.cc/paper/2019/file/e6be4c22a5963ab00dfe8f3b695b5332-Paper.pdf,Selecting causal brain features with a single conditional independence test per feature,"Atalanti Mastakouri, Bernhard Schölkopf, Dominik Janzing",
neurips,https://proceedings.neurips.cc/paper/2019/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf,Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update,"Su Young Lee, Choi Sungik, Sae-Young Chung",
neurips,https://proceedings.neurips.cc/paper/2019/file/e6e713296627dff6475085cc6a224464-Paper.pdf,Weakly Supervised Instance Segmentation using the Bounding Box Tightness Prior,"Cheng-Chun Hsu, Kuang-Jui Hsu, Chung-Chi Tsai, Yen-Yu Lin, Yung-Yu Chuang",
neurips,https://proceedings.neurips.cc/paper/2019/file/e721a54a8cf18c8543d44782d9ef681f-Paper.pdf,Copula-like Variational Inference,"Marcel Hirt, Petros Dellaportas, Alain Durmus",
neurips,https://proceedings.neurips.cc/paper/2019/file/e77910ebb93b511588557806310f78f1-Paper.pdf,Towards Hardware-Aware Tractable Learning of Probabilistic Models,"Laura I. Galindez Olascoaga, Wannes Meert, Nimish Shah, Marian Verhelst, Guy Van den Broeck",
neurips,https://proceedings.neurips.cc/paper/2019/file/e7c573c14a09b84f6b7782ce3965f335-Paper.pdf,Two Time-scale Off-Policy TD Learning: Non-asymptotic Analysis over Markovian Samples,"Tengyu Xu, Shaofeng Zou, Yingbin Liang",
neurips,https://proceedings.neurips.cc/paper/2019/file/e833e042f509c996b1b25324d56659fb-Paper.pdf,Incremental Few-Shot Learning with Attention Attractor Networks,"Mengye Ren, Renjie Liao, Ethan Fetaya, Richard Zemel",
neurips,https://proceedings.neurips.cc/paper/2019/file/e88f243bf341ded9b4ced444795c3f17-Paper.pdf,Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations,"Kevin Smith, Lingjie Mei, Shunyu Yao, Jiajun Wu, Elizabeth Spelke, Josh Tenenbaum, Tomer Ullman",
neurips,https://proceedings.neurips.cc/paper/2019/file/e9287a53b94620249766921107fe70a3-Paper.pdf,Outlier Detection and Robust PCA Using a Convex Measure of Innovation,"Mostafa Rahmani, Ping Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/e92e1b476bb5262d793fd40931e0ed53-Paper.pdf,Efficient Convex Relaxations for Streaming PCA,"Raman Arora, Teodor Vanislavov Marinov","We revisit two algorithms, matrix stochastic gradient (MSG) and
ℓ
2
ℓ
-regularized MSG (RMSG), that are instances of stochastic gradient descent (SGD) on a convex relaxation to principal component analysis (PCA). These algorithms have been shown to outperform Oja’s algorithm, empirically, in terms of the iteration complexity, and to have runtime comparable with Oja’s. However, these findings are not supported by existing theoretical results. While the iteration complexity bound for
ℓ
2
ℓ
-RMSG was recently shown to match that of Oja’s algorithm, its theoretical efficiency was left as an open problem. In this work, we give improved bounds on per iteration cost of mini-batched variants of both MSG and
ℓ
2
ℓ
-RMSG and arrive at an algorithm with total computational complexity matching that of Oja's algorithm."
neurips,https://proceedings.neurips.cc/paper/2019/file/e94550c93cd70fe748e6982b3439ad3b-Paper.pdf,Envy-Free Classification,"Maria-Florina F. Balcan, Travis Dick, Ritesh Noothigattu, Ariel D. Procaccia",
neurips,https://proceedings.neurips.cc/paper/2019/file/e94fe9ac8dc10dd8b9a239e6abee2848-Paper.pdf,Deep Model Transferability from Attribution Maps,"Jie Song, Yixin Chen, Xinchao Wang, Chengchao Shen, Mingli Song",
neurips,https://proceedings.neurips.cc/paper/2019/file/e9510081ac30ffa83f10b68cde1cac07-Paper.pdf,Towards Interpretable Reinforcement Learning Using Attention Augmented Agents,"Alexander Mott, Daniel Zoran, Mike Chrzanowski, Daan Wierstra, Danilo Jimenez Rezende",
neurips,https://proceedings.neurips.cc/paper/2019/file/e98741479a7b998f88b8f8c9f0b6b6f1-Paper.pdf,Weight Agnostic Neural Networks,"Adam Gaier, David Ha",Interactive version of this paper at https://weightagnostic.github.io/
neurips,https://proceedings.neurips.cc/paper/2019/file/e9bf14a419d77534105016f5ec122d62-Paper.pdf,DeepWave: A Recurrent Neural-Network for Real-Time Acoustic Imaging,"Matthieu SIMEONI, Sepand Kashani, Paul Hurley, Martin Vetterli",
neurips,https://proceedings.neurips.cc/paper/2019/file/e9fd7c2c6623306db59b6aef5c0d5cac-Paper.pdf,A Linearly Convergent Proximal Gradient Algorithm for Decentralized Optimization,"Sulaiman Alghunaim, Kun Yuan, Ali H. Sayed",
neurips,https://proceedings.neurips.cc/paper/2019/file/ea1818cbe59c23b20f1a10a8aa083a82-Paper.pdf,Meta Architecture Search,"Albert Shaw, Wei Wei, Weiyang Liu, Le Song, Bo Dai",
neurips,https://proceedings.neurips.cc/paper/2019/file/ea4eb49329550caaa1d2044105223721-Paper.pdf,Double Quantization for Communication-Efficient Distributed Optimization,"Yue Yu, Jiaxiang Wu, Longbo Huang",
neurips,https://proceedings.neurips.cc/paper/2019/file/ea6979872125d5acbac6068f186a0359-Paper.pdf,Graph Structured Prediction Energy Networks,"Colin Graber, Alexander Schwing",
neurips,https://proceedings.neurips.cc/paper/2019/file/ea9268cb43f55d1d12380fb6ea5bf572-Paper.pdf,Universal Invariant and Equivariant Graph Neural Networks,"Nicolas Keriven, Gabriel Peyré",
neurips,https://proceedings.neurips.cc/paper/2019/file/eae27d77ca20db309e056e3d2dcd7d69-Paper.pdf,A Primal-Dual link between GANs and Autoencoders,"Hisham Husain, Richard Nock, Robert C. Williamson","Since the introduction of Generative Adversarial Networks (GANs) and Variational Autoencoders (VAE), the literature on generative modelling has witnessed an overwhelming resurgence. The impressive, yet elusive empirical performance of GANs has lead to the rise of many GAN-VAE hybrids, with the hopes of GAN level performance and additional benefits of VAE, such as an encoder for feature reduction, which is not offered by GANs. Recently, the Wasserstein Autoencoder (WAE) was proposed, achieving performance similar to that of GANs, yet it is still unclear whether the two are fundamentally different or can be further improved into a unified model. In this work, we study the
f
f
-GAN and WAE models and make two main discoveries. First, we find that the
f
f
-GAN and WAE objectives partake in a primal-dual relationship and are equivalent under some assumptions, which then allows us to explicate the success of WAE. Second, the equivalence result allows us to, for the first time, prove generalization bounds for Autoencoder models, which is a pertinent problem when it comes to theoretical analyses of generative models. Furthermore, we show that the WAE objective is related to other statistical quantities such as the
f
f
-divergence and in particular, upper bounded by the Wasserstein distance, which then allows us to tap into existing efficient (regularized) optimal transport solvers. Our findings thus present the first primal-dual relationship between GANs and Autoencoder models, comment on generalization abilities and make a step towards unifying these models."
neurips,https://proceedings.neurips.cc/paper/2019/file/eb1e78328c46506b46a4ac4a1e378b91-Paper.pdf,Transfusion: Understanding Transfer Learning for Medical Imaging,"Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, Samy Bengio",
neurips,https://proceedings.neurips.cc/paper/2019/file/eb6dc8aba23375061b6f07b137617096-Paper.pdf,PIDForest: Anomaly Detection via Partial Identification,"Parikshit Gopalan, Vatsal Sharan, Udi Wieder",
neurips,https://proceedings.neurips.cc/paper/2019/file/eb86d510361fc23b59f18c1bc9802cc6-Paper.pdf,The Randomized Midpoint Method for Log-Concave Sampling,"Ruoqi Shen, Yin Tat Lee","Sampling from log-concave distributions is a well researched problem that has many applications in statistics and machine learning. We study the distributions of the form
p
∗
∝
exp
(
−
f
(
x
)
)
p
, where
f
:
R
d
→
R
f
has an
L
L
-Lipschitz gradient and is
m
m
-strongly convex. In our paper, we propose a Markov chain Monte Carlo (MCMC) algorithm based on the underdamped Langevin diffusion (ULD). It can achieve
ϵ
⋅
D
ϵ
error (in 2-Wasserstein distance) in
~
O
(
κ
7
/
6
/
ϵ
1
/
3
+
κ
/
ϵ
2
/
3
)
O
steps, where
D
d
e
f
=
√
d
m
D
is the effective diameter of the problem and
κ
d
e
f
=
L
m
κ
is the condition number. Our algorithm performs significantly faster than the previously best known algorithm for solving this problem, which requires
~
O
(
κ
1.5
/
ϵ
)
O
steps \cite{chen2019optimal,dalalyan2018sampling}. Moreover, our algorithm can be easily parallelized to require only
O
(
κ
log
1
ϵ
)
O
parallel steps. To solve the sampling problem, we propose a new framework to discretize stochastic differential equations. We apply this framework to discretize and simulate ULD, which converges to the target distribution
p
∗
p
. The framework can be used to solve not only the log-concave sampling problem, but any problem that involves simulating (stochastic) differential equations."
neurips,https://proceedings.neurips.cc/paper/2019/file/eb9fc349601c69352c859c1faa287874-Paper.pdf,Face Reconstruction from Voice using Generative Adversarial Networks,"Yandong Wen, Bhiksha Raj, Rita Singh","To address this problem, we propose a simple but effective computational framework based on generative adversarial networks (GANs). The network learns to generate faces from voices by matching the identities of generated faces to those of the speakers, on a training set. We evaluate the performance of the network by leveraging a closely related task - cross-modal matching. The results show that our model is able to generate faces that match several biometric characteristics of the speaker, and results in matching accuracies that are much better than chance. The code is publicly available in https://github.com/cmu-mlsp/reconstructingfacesfrom_voices"
neurips,https://proceedings.neurips.cc/paper/2019/file/eba237eccc24353ccaa4d62013556ac6-Paper.pdf,Using a Logarithmic Mapping to Enable Lower Discount Factors in Reinforcement Learning,"Harm Van Seijen, Mehdi Fatemi, Arash Tavakoli",
neurips,https://proceedings.neurips.cc/paper/2019/file/ebad33b3c9fa1d10327bb55f9e79e2f3-Paper.pdf,PRNet: Self-Supervised Learning for Partial-to-Partial Registration,"Yue Wang, Justin M. Solomon",
neurips,https://proceedings.neurips.cc/paper/2019/file/ebbdfea212e3a756a1fded7b35578525-Paper.pdf,Adversarial Music: Real world Audio Adversary against Wake-word Detection System,"Juncheng Li, Shuhui Qu, Xinjian Li, Joseph Szurley, J. Zico Kolter, Florian Metze",
neurips,https://proceedings.neurips.cc/paper/2019/file/ec04e8ebba7e132043e5b4832e54f070-Paper.pdf,Learning to Optimize in Swarms,"Yue Cao, Tianlong Chen, Zhangyang Wang, Yang Shen",
neurips,https://proceedings.neurips.cc/paper/2019/file/ec1c59141046cd1866bbbcdfb6ae31d4-Paper.pdf,A Little Is Enough: Circumventing Defenses For Distributed Learning,"Gilad Baruch, Moran Baruch, Yoav Goldberg",
neurips,https://proceedings.neurips.cc/paper/2019/file/ecb287ff763c169694f682af52c1f309-Paper.pdf,Statistical Model Aggregation via Parameter Matching,"Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang",
neurips,https://proceedings.neurips.cc/paper/2019/file/ed3d2c21991e3bef5e069713af9fa6ca-Paper.pdf,Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement,"Chao Yang, Xiaojian Ma, Wenbing Huang, Fuchun Sun, Huaping Liu, Junzhou Huang, Chuang Gan",
neurips,https://proceedings.neurips.cc/paper/2019/file/ee0c1616bbc82804b2f4b635d4a055fb-Paper.pdf,Prediction of Spatial Point Processes: Regularized Method with Out-of-Sample Guarantees,"Muhammad Osama, Dave Zachariah, Peter Stoica",
neurips,https://proceedings.neurips.cc/paper/2019/file/ee389847678a3a9d1ce9e4ca69200d06-Paper.pdf,STREETS: A Novel Camera Network Dataset for Traffic Flow,"Corey Snyder, Minh Do",
neurips,https://proceedings.neurips.cc/paper/2019/file/ee39e503b6bedf0c98c388b7e8589aca-Paper.pdf,A Meta-Analysis of Overfitting in Machine Learning,"Rebecca Roelofs, Vaishaal Shankar, Benjamin Recht, Sara Fridovich-Keil, Moritz Hardt, John Miller, Ludwig Schmidt",
neurips,https://proceedings.neurips.cc/paper/2019/file/eea5d933e9dce59c7dd0f6532f9ea81b-Paper.pdf,Projected Stein Variational Newton: A Fast and Scalable Bayesian Inference Method in High Dimensions,"Peng Chen, Keyi Wu, Joshua Chen, Tom O'Leary-Roseberry, Omar Ghattas",
neurips,https://proceedings.neurips.cc/paper/2019/file/eeaebbffb5d29ff62799637fc51adb7b-Paper.pdf,From deep learning to mechanistic understanding in neuroscience: the structure of retinal prediction,"Hidenori Tanaka, Aran Nayebi, Niru Maheswaranathan, Lane McIntosh, Stephen Baccus, Surya Ganguli",
neurips,https://proceedings.neurips.cc/paper/2019/file/f0031c7a91d74015a9addfbc589f3fe5-Paper.pdf,Abstract Reasoning with Distracting Features,"Kecheng Zheng, Zheng-Jun Zha, Wei Wei",
neurips,https://proceedings.neurips.cc/paper/2019/file/f04cd7399b2b0128970efb6d20b5c551-Paper.pdf,Deep Scale-spaces: Equivariance Over Scale,"Daniel Worrall, Max Welling",
neurips,https://proceedings.neurips.cc/paper/2019/file/f06ae085fe74cd78ad5e89496b197fe1-Paper.pdf,Differentially Private Anonymized Histograms,Ananda Theertha Suresh,
neurips,https://proceedings.neurips.cc/paper/2019/file/f0935e4cd5920aa6c7c996a5ee53a70f-Paper.pdf,Generalized Sliced Wasserstein Distances,"Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, Gustavo Rohde",
neurips,https://proceedings.neurips.cc/paper/2019/file/f0d7053396e765bf52de12133cf1afe8-Paper.pdf,"Outlier-robust estimation of a sparse linear model using
ℓ
1
ℓ
-penalized Huber's
M
M
-estimator","Arnak Dalalyan, Philip Thompson","We study the problem of estimating a
p
p
-dimensional
s
s
-sparse vector in a linear model with Gaussian design. In the case where the labels are contaminated by at most
o
o
adversarial outliers, we prove that the
ℓ
1
ℓ
-penalized Huber's
M
M
-estimator based on
n
n
samples attains the optimal rate of convergence
(
s
/
n
)
1
/
2
+
(
o
/
n
)
(
, up to a logarithmic factor. For more general design matrices, our results highlight the importance of two properties: the transfer principle and the incoherence property. These properties with suitable constants are shown to yield the optimal rates of robust estimation with adversarial contamination."
neurips,https://proceedings.neurips.cc/paper/2019/file/f12f2b34a0c3174269c19e21c07dee68-Paper.pdf,Scalable Spike Source Localization in Extracellular Recordings using Amortized Variational Inference,"Cole Hurwitz, Kai Xu, Akash Srivastava, Alessio Buccino, Matthias Hennig",
neurips,https://proceedings.neurips.cc/paper/2019/file/f15eda31a2da646eea513b0f81a5414d-Paper.pdf,Prior-Free Dynamic Auctions with Low Regret Buyers,"Yuan Deng, Jon Schneider, Balasubramanian Sivan","We study the problem of how to repeatedly sell to a buyer running a no-regret, mean-based algorithm. Previous work [Braverman et al., 2018] shows that it is possible to design effective mechanisms in such a setting that extract almost all of the economic surplus, but these mechanisms require the buyer's values each round to be drawn independently and identically from a fixed distribution. In this work, we do away with this assumption and consider the prior-free setting where the buyer's value each round is chosen adversarially (possibly adaptively). We show that even in this prior-free setting, it is possible to extract a
(
1
−
ε
)
(
-approximation of the full economic surplus for any
ε
>
0
ε
. The number of options offered to a buyer in any round scales independently of the number of rounds
T
T
and polynomially in
ε
ε
. We show that this is optimal up to a polynomial factor; any mechanism achieving this approximation factor, even when values are drawn stochastically, requires at least
Ω
(
1
/
ε
)
Ω
options. Finally, we examine what is possible when we constrain our mechanism to a natural auction format where overbidding is dominated. Braverman et al. [2018] show that even when values are drawn from a known stochastic distribution supported on
[
1
/
H
,
1
]
[
, it is impossible in general to extract more than
O
(
log
log
H
/
log
H
)
O
of the economic surplus. We show how to achieve the same approximation factor in the prior-independent setting (where the distribution is unknown to the seller), and an approximation factor of
O
(
1
/
log
H
)
O
in the prior-free setting (where the values are chosen adversarially)."
neurips,https://proceedings.neurips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf,When does label smoothing help?,"Rafael Müller, Simon Kornblith, Geoffrey E. Hinton",
neurips,https://proceedings.neurips.cc/paper/2019/file/f19fec2f129fbdba76493451275c883a-Paper.pdf,A General Framework for Symmetric Property Estimation,"Moses Charikar, Kirankumar Shiragur, Aaron Sidford",
neurips,https://proceedings.neurips.cc/paper/2019/file/f1ea154c843f7cf3677db7ce922a2d17-Paper.pdf,Deep Generative Video Compression,"Salvator Lombardo, JUN HAN, Christopher Schroers, Stephan Mandt",
neurips,https://proceedings.neurips.cc/paper/2019/file/f2201f5191c4e92cc5af043eebfd0946-Paper.pdf,CondConv: Conditionally Parameterized Convolutions for Efficient Inference,"Brandon Yang, Gabriel Bender, Quoc V. Le, Jiquan Ngiam",
neurips,https://proceedings.neurips.cc/paper/2019/file/f24ad6f72d6cc4cb51464f2b29ab69d3-Paper.pdf,Towards a Zero-One Law for Column Subset Selection,"Zhao Song, David Woodruff, Peilin Zhong","There are a number of approximation algorithms for NP-hard versions of low rank approximation, such as finding a rank-
k
k
matrix
B
B
minimizing the sum of absolute values of differences to a given
n
n
-by-
n
n
matrix
A
A
,
min
rank-
k
 B
∥
A
−
B
∥
1
min
, or more generally finding a rank-
k
k
matrix
B
B
which minimizes the sum of
p
p
-th powers of absolute values of differences,
min
rank-
k
 B
∥
A
−
B
∥
p
p
min
. Many of these algorithms are linear time columns subset selection algorithms, returning a subset of
\poly
(
k
log
n
)
\poly
columns whose cost is no more than a
\poly
(
k
)
\poly
factor larger than the cost of the best rank-
k
k
matrix. The above error measures are special cases of the following general entrywise low rank approximation problem: given an arbitrary function
g
:
R
→
R
≥
0
g
, find a rank-
k
k
matrix
B
B
which minimizes
∥
A
−
B
∥
g
=
∑
i
,
j
g
(
A
i
,
j
−
B
i
,
j
)
‖
. A natural question is which functions
g
g
admit efficient approximation algorithms? Indeed, this is a central question of recent work studying generalized low rank models. In this work we give approximation algorithms for {\it every} function
g
g
which is approximately monotone and satisfies an approximate triangle inequality, and we show both of these conditions are necessary. Further, our algorithm is efficient if the function
g
g
admits an efficient approximate regression algorithm. Our approximation algorithms handle functions which are not even scale-invariant, such as the Huber loss function, which we show have very different structural properties than
ℓ
p
ℓ
-norms, e.g., one can show the lack of scale-invariance causes any column subset selection algorithm to provably require a
√
log
n
log
factor larger number of columns than
ℓ
p
ℓ
-norms; nevertheless we design the first efficient column subset selection algorithms for such error measures."
neurips,https://proceedings.neurips.cc/paper/2019/file/f29a179746902e331572c483c45e5086-Paper.pdf,Neural Attribution for Semantic Bug-Localization in Student Programs,"Rahul Gupta, Aditya Kanade, Shirish Shevade",
neurips,https://proceedings.neurips.cc/paper/2019/file/f29e2360ef277f77595dfae0aab78138-Paper.pdf,Theoretical Limits of Pipeline Parallel Optimization and Application to Distributed Deep Learning,"Igor Colin, Ludovic DOS SANTOS, Kevin Scaman","We investigate the theoretical limits of pipeline parallel learning of deep learning architectures, a distributed setup in which the computation is distributed per layer instead of per example. For smooth convex and non-convex objective functions, we provide matching lower and upper complexity bounds and show that a naive pipeline parallelization of Nesterov's accelerated gradient descent is optimal. For non-smooth convex functions, we provide a novel algorithm coined Pipeline Parallel Random Smoothing (PPRS) that is within a
d
1
/
4
d
multiplicative factor of the optimal convergence rate, where
d
d
is the underlying dimension. While the convergence rate still obeys a slow
ε
−
2
ε
convergence rate, the depth-dependent part is accelerated, resulting in a near-linear speed-up and convergence time that only slightly depends on the depth of the deep learning architecture. Finally, we perform an empirical analysis of the non-smooth non-convex case and show that, for difficult and highly non-smooth problems, PPRS outperforms more traditional optimization algorithms such as gradient descent and Nesterov's accelerated gradient descent for problems where the sample size is limited, such as few-shot or adversarial learning."
neurips,https://proceedings.neurips.cc/paper/2019/file/f2d887e01a80e813d9080038decbbabb-Paper.pdf,DppNet: Approximating Determinantal Point Processes with Deep Networks,"Zelda E. Mariet, Yaniv Ovadia, Jasper Snoek",
neurips,https://proceedings.neurips.cc/paper/2019/file/f2e84d98d6dc0c7acd56b40509355666-Paper.pdf,Nonzero-sum Adversarial Hypothesis Testing Games,"Sarath Yasodharan, Patrick Loiseau",
neurips,https://proceedings.neurips.cc/paper/2019/file/f34185c4ca5d58e781d4f14173d41e5d-Paper.pdf,Global Sparse Momentum SGD for Pruning Very Deep Neural Networks,"Xiaohan Ding, guiguang ding, Xiangxin Zhou, Yuchen Guo, Jungong Han, Ji Liu",
neurips,https://proceedings.neurips.cc/paper/2019/file/f3507289cfdc8c9ae93f4098111a13f9-Paper.pdf,Thompson Sampling and Approximate Inference,"My Phan, Yasin Abbasi Yadkori, Justin Domke","We study the effects of approximate inference on the performance of Thompson sampling in the
k
k
-armed bandit problems. Thompson sampling is a successful algorithm for online decision-making but requires posterior inference, which often must be approximated in practice. We show that even small constant inference error (in
α
α
-divergence) can lead to poor performance (linear regret) due to under-exploration (for
α
<
1
α
) or over-exploration (for
α
>
0
α
) by the approximation. While for
α
>
0
α
this is unavoidable, for
α
≤
0
α
the regret can be improved by adding a small amount of forced exploration even when the inference error is a large constant."
neurips,https://proceedings.neurips.cc/paper/2019/file/f35fd567065af297ae65b621e0a21ae9-Paper.pdf,Quantum Wasserstein Generative Adversarial Networks,"Shouvanik Chakrabarti, Huang Yiming, Tongyang Li, Soheil Feizi, Xiaodi Wu",
neurips,https://proceedings.neurips.cc/paper/2019/file/f387624df552cea2f369918c5e1e12bc-Paper.pdf,Deep Learning without Weight Transport,"Mohamed Akrout, Collin Wilson, Peter Humphreys, Timothy Lillicrap, Douglas B. Tweed",
neurips,https://proceedings.neurips.cc/paper/2019/file/f39ae9ff3a81f499230c4126e01f421b-Paper.pdf,Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks,"Gauthier Gidel, Francis Bach, Simon Lacoste-Julien",
neurips,https://proceedings.neurips.cc/paper/2019/file/f3a4ff4839c56a5f460c88cce3666a2b-Paper.pdf,Generative Models for Graph-Based Protein Design,"John Ingraham, Vikas Garg, Regina Barzilay, Tommi Jaakkola",
neurips,https://proceedings.neurips.cc/paper/2019/file/f42a37d114a480b6b57b60ea9a14a9d2-Paper.pdf,Spike-Train Level Backpropagation for Training Deep Recurrent Spiking Neural Networks,"Wenrui Zhang, Peng Li",
neurips,https://proceedings.neurips.cc/paper/2019/file/f471223d1a1614b58a7dc45c9d01df19-Paper.pdf,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,"Derek Yang, Li Zhao, Zichuan Lin, Tao Qin, Jiang Bian, Tie-Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2019/file/f490c742cd8318b8ee6dca10af2a163f-Paper.pdf,Neural Taskonomy: Inferring the Similarity of Task-Derived Representations from Brain Activity,"Aria Wang, Michael Tarr, Leila Wehbe",
neurips,https://proceedings.neurips.cc/paper/2019/file/f4aa0dd960521e045ae2f20621fb4ee9-Paper.pdf,Adaptive Gradient-Based Meta-Learning Methods,"Mikhail Khodak, Maria-Florina F. Balcan, Ameet S. Talwalkar",
neurips,https://proceedings.neurips.cc/paper/2019/file/f4d0e2e7fc057a58f7ca4a391f01940a-Paper.pdf,Compositional generalization through meta sequence-to-sequence learning,Brenden M. Lake,
neurips,https://proceedings.neurips.cc/paper/2019/file/f4dd765c12f2ef67f98f3558c282a9cd-Paper.pdf,Meta-Learning Representations for Continual Learning,"Khurram Javed, Martha White","Moreover, it wasn't clear if the objective we introduced improved over a maml like objective that also learned representations. We added MAML-Rep as a baseline that shows that our method -- which minimizes interference in addition to maximizing fast adaptation -- performs noticeably better."
neurips,https://proceedings.neurips.cc/paper/2019/file/f55cadb97eaff2ba1980e001b0bd9842-Paper.pdf,Massively scalable Sinkhorn distances via the Nyström method,"Jason Altschuler, Francis Bach, Alessandro Rudi, Jonathan Niles-Weed",
neurips,https://proceedings.neurips.cc/paper/2019/file/f56d8183992b6c54c92c16a8519a6e2b-Paper.pdf,Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling,"Ming Hou, Jiajia Tang, Jianhai Zhang, Wanzeng Kong, Qibin Zhao",
neurips,https://proceedings.neurips.cc/paper/2019/file/f5aa4bd09c07d8b2f65bad6c7cd3358f-Paper.pdf,A Composable Specification Language for Reinforcement Learning Tasks,"Kishor Jothimurugan, Rajeev Alur, Osbert Bastani",
neurips,https://proceedings.neurips.cc/paper/2019/file/f5ac21cd0ef1b88e9848571aeb53551a-Paper.pdf,Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer,"Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaakko Lehtinen, Alec Jacobson, Sanja Fidler",
neurips,https://proceedings.neurips.cc/paper/2019/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf,On the Utility of Learning about Humans for Human-AI Coordination,"Micah Carroll, Rohin Shah, Mark K. Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, Anca Dragan",
neurips,https://proceedings.neurips.cc/paper/2019/file/f63f65b503e22cb970527f23c9ad7db1-Paper.pdf,"FastSpeech: Fast, Robust and Controllable Text to Speech","Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2019/file/f69041d874533096748e2d77480c1fea-Paper.pdf,Maximum Expected Hitting Cost of a Markov Decision Process and Informativeness of Rewards,"Falcon Dai, Matthew Walter",
neurips,https://proceedings.neurips.cc/paper/2019/file/f69e505b08403ad2298b9f262659929a-Paper.pdf,Park: An Open Platform for Learning-Augmented Computer Systems,"Hongzi Mao, Parimarjan Negi, Akshay Narayan, Hanrui Wang, Jiacheng Yang, Haonan Wang, Ryan Marcus, ravichandra addanki, Mehrdad Khani Shirkoohi, Songtao He, Vikram Nathan, Frank Cangialosi, Shaileshh Venkatakrishnan, Wei-Hung Weng, Song Han, Tim Kraska, Dr.Mohammad Alizadeh",
neurips,https://proceedings.neurips.cc/paper/2019/file/f6b5f8c32c65fee991049a55dc97d1ce-Paper.pdf,Adaptive Influence Maximization with Myopic Feedback,"Binghui Peng, Wei Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/f6e794a75c5d51de081dbefa224304f9-Paper.pdf,Compression with Flows via Local Bits-Back Coding,"Jonathan Ho, Evan Lohn, Pieter Abbeel",
neurips,https://proceedings.neurips.cc/paper/2019/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf,On Adversarial Mixup Resynthesis,"Christopher Beckham, Sina Honari, Vikas Verma, Alex M. Lamb, Farnoosh Ghadiri, R Devon Hjelm, Yoshua Bengio, Chris Pal",
neurips,https://proceedings.neurips.cc/paper/2019/file/f7177163c833dff4b38fc8d2872f1ec6-Paper.pdf,High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks,"Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru Erhan, Quoc V. Le, Honglak Lee",
neurips,https://proceedings.neurips.cc/paper/2019/file/f7ae58c7f1a1cc4abe9273a0f971ba2a-Paper.pdf,Variational Bayes under Model Misspecification,"Yixin Wang, David Blei",
neurips,https://proceedings.neurips.cc/paper/2019/file/f7fa6aca028e7ff4ef62d75ed025fe76-Paper.pdf,Certifying Geometric Robustness of Neural Networks,"Mislav Balunovic, Maximilian Baader, Gagandeep Singh, Timon Gehr, Martin Vechev",
neurips,https://proceedings.neurips.cc/paper/2019/file/f8037f94e53f17a2cc301033ca86d278-Paper.pdf,Constrained deep neural network architecture search for IoT devices accounting for hardware calibration,"Florian Scheidegger, Luca Benini, Costas Bekas, A. Cristiano I. Malossi",
neurips,https://proceedings.neurips.cc/paper/2019/file/f816dc0acface7498e10496222e9db10-Paper.pdf,MAVEN: Multi-Agent Variational Exploration,"Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, Shimon Whiteson",
neurips,https://proceedings.neurips.cc/paper/2019/file/f82798ec8909d23e55679ee26bb26437-Paper.pdf,The continuous Bernoulli: fixing a pervasive error in variational autoencoders,"Gabriel Loaiza-Ganem, John P. Cunningham",
neurips,https://proceedings.neurips.cc/paper/2019/file/f83630579d055dc5843ae693e7cdafe0-Paper.pdf,Propagating Uncertainty in Reinforcement Learning via Wasserstein Barycenters,"Alberto Maria Metelli, Amarildo Likmeta, Marcello Restelli",
neurips,https://proceedings.neurips.cc/paper/2019/file/f87522788a2be2d171666752f97ddebb-Paper.pdf,DFNets: Spectral CNNs for Graphs with Feedback-Looped Filters,"W. O. K. Asiri Suranga Wijesinghe, Qing Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/f8905bd3df64ace64a68e154ba72f24c-Paper.pdf,Multiclass Learning from Contradictions,"Sauptik Dhar, Vladimir Cherkassky, Mohak Shah","We introduce the notion of learning from contradictions, a.k.a Universum learning, for multiclass problems and propose a novel formulation for multiclass universum SVM (MU-SVM). We show that learning from contradictions (using MU-SVM) incurs lower sample complexity compared to multiclass SVM (M-SVM) by deriving the Natarajan dimension for sample complexity for PAC-learnability of MU-SVM. We also propose an analytic span bound for MU-SVM and demonstrate its utility for model selection resulting in
∼
2
−
4
×
∼
faster computation times than standard resampling techniques. We empirically demonstrate the efficacy of MU- SVM on several real world datasets achieving
>
>
20\% improvement in test accuracies compared to M-SVM. Insights into the underlying behavior of MU-SVM using a histograms-of-projections method are also provided."
neurips,https://proceedings.neurips.cc/paper/2019/file/f8b932c70d0b2e6bf071729a4fa68dfc-Paper.pdf,Multi-relational Poincaré Graph Embeddings,"Ivana Balazevic, Carl Allen, Timothy Hospedales",
neurips,https://proceedings.neurips.cc/paper/2019/file/f8c0c968632845cd133308b1a494967f-Paper.pdf,Verified Uncertainty Calibration,"Ananya Kumar, Percy S. Liang, Tengyu Ma","Applications such as weather forecasting and personalized medicine demand models that output calibrated probability estimates---those representative of the true likelihood of a prediction. Most models are not calibrated out of the box but are recalibrated by post-processing model outputs. We find in this work that popular recalibration methods like Platt scaling and temperature scaling are (i) less calibrated than reported, and (ii) current techniques cannot estimate how miscalibrated they are. An alternative method, histogram binning, has measurable calibration error but is sample inefficient---it requires
O
(
B
/
ϵ
2
)
O
samples, compared to
O
(
1
/
ϵ
2
)
O
for scaling methods, where
B
B
is the number of distinct probabilities the model can output. To get the best of both worlds, we introduce the scaling-binning calibrator, which first fits a parametric function that acts like a baseline for variance reduction and then bins the function values to actually ensure calibration. This requires only
O
(
1
/
ϵ
2
+
B
)
O
samples. We then show that methods used to estimate calibration error are suboptimal---we prove that an alternative estimator introduced in the meteorological community requires fewer samples (
O
(
√
B
)
O
instead of
O
(
B
)
O
). We validate our approach with multiclass calibration experiments on CIFAR-10 and ImageNet, where we obtain a 35\% lower calibration error than histogram binning and, unlike scaling methods, guarantees on true calibration."
neurips,https://proceedings.neurips.cc/paper/2019/file/f8d2e80c1458ea2501f98a2cafadb397-Paper.pdf,Episodic Memory in Lifelong Language Learning,"Cyprien de Masson d'Autume, Sebastian Ruder, Lingpeng Kong, Dani Yogatama",
neurips,https://proceedings.neurips.cc/paper/2019/file/f8e59f4b2fe7c5705bf878bbd494ccdf-Paper.pdf,MetaQuant: Learning to Quantize by Learning to Penetrate Non-differentiable Quantization,"Shangyu Chen, Wenya Wang, Sinno Jialin Pan","Tremendous amount of parameters make deep neural networks impractical to be deployed for edge-device-based real-world applications due to the limit of computational power and storage space. Existing studies have made progress on learning quantized deep models to reduce model size and energy consumption, i.e. converting full-precision weights (
r
r
's) into discrete values (
q
q
's) in a supervised training manner. However, the training process for quantization is non-differentiable, which leads to either infinite or zero gradients (
g
r
g
) w.r.t.
r
r
. To address this problem, most training-based quantization methods use the gradient w.r.t.
q
q
(
g
q
g
) with clipping to approximate
g
r
g
by Straight-Through-Estimator (STE) or manually design their computation. However, these methods only heuristically make training-based quantization applicable, without further analysis on how the approximated gradients can assist training of a quantized network. In this paper, we propose to learn
g
r
g
by a neural network. Specifically, a meta network is trained using
g
q
g
and
r
r
as inputs, and outputs
g
r
g
for subsequent weight updates. The meta network is updated together with the original quantized network. Our proposed method alleviates the problem of non-differentiability, and can be trained in an end-to-end manner. Extensive experiments are conducted with CIFAR10/100 and ImageNet on various deep networks to demonstrate the advantage of our proposed method in terms of a faster convergence rate and better performance. Codes are released at: \texttt{https://github.com/csyhhu/MetaQuant}"
neurips,https://proceedings.neurips.cc/paper/2019/file/f8eb278a8bce873ef365b45e939da38a-Paper.pdf,Normalization Helps Training of Quantized LSTM,"Lu Hou, Jinhua Zhu, James Kwok, Fei Gao, Tao Qin, Tie-Yan Liu",
neurips,https://proceedings.neurips.cc/paper/2019/file/f90f2aca5c640289d0a29417bcb63a37-Paper.pdf,Differentially Private Bayesian Linear Regression,"Garrett Bernstein, Daniel R. Sheldon",
neurips,https://proceedings.neurips.cc/paper/2019/file/f9209b7866c9f69823201c1732cc8645-Paper.pdf,Wasserstein Dependency Measure for Representation Learning,"Sherjil Ozair, Corey Lynch, Yoshua Bengio, Aaron van den Oord, Sergey Levine, Pierre Sermanet",
neurips,https://proceedings.neurips.cc/paper/2019/file/f968fdc88852a4a3a27a81fe3f57bfc5-Paper.pdf,Multi-Agent Common Knowledge Reinforcement Learning,"Christian Schroeder de Witt, Jakob Foerster, Gregory Farquhar, Philip Torr, Wendelin Boehmer, Shimon Whiteson",
neurips,https://proceedings.neurips.cc/paper/2019/file/f9beb1e831faf6aaec2a5cecaf1af293-Paper.pdf,Subspace Detours: Building Transport Plans that are Optimal on Subspace Projections,"Boris Muzellec, Marco Cuturi",
neurips,https://proceedings.neurips.cc/paper/2019/file/f9fd5ec4c141a95257aa99ef1b590672-Paper.pdf,The Broad Optimality of Profile Maximum Likelihood,"Yi Hao, Alon Orlitsky","We study three fundamental statistical-learning problems: distribution estimation, property estimation, and property testing. We establish the profile maximum likelihood (PML) estimator as the first unified sample-optimal approach to a wide range of learning tasks. In particular, for every alphabet size
k
k
and desired accuracy
ε
ε
: \textbf{Distribution estimation} Under
ℓ
1
ℓ
distance, PML yields optimal
Θ
(
k
/
(
ε
2
log
k
)
)
Θ
sample complexity for sorted-distribution estimation, and a PML-based estimator empirically outperforms the Good-Turing estimator on the actual distribution; \textbf{Additive property estimation} For a broad class of additive properties, the PML plug-in estimator uses just four times the sample size required by the best estimator to achieve roughly twice its error, with exponentially higher confidence; \textbf{
α
α
-R\'enyi entropy estimation} For an integer
α
>
1
α
, the PML plug-in estimator has optimal
k
1
−
1
/
α
k
sample complexity; for non-integer
α
>
3
/
4
α
, the PML plug-in estimator has sample complexity lower than the state of the art; \textbf{Identity testing} In testing whether an unknown distribution is equal to or at least
ε
ε
far from a given distribution in
ℓ
1
ℓ
distance, a PML-based tester achieves the optimal sample complexity up to logarithmic factors of
k
k
. With minor modifications, most of these results also hold for a near-linear-time computable variant of PML."
neurips,https://proceedings.neurips.cc/paper/2019/file/fa2e8c4385712f9a1d24c363a2cbe5b8-Paper.pdf,Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers,"Guang-He Lee, Yang Yuan, Shiyu Chang, Tommi Jaakkola","Strong theoretical guarantees of robustness can be given for ensembles of classifiers generated by input randomization. Specifically, an
ℓ
2
ℓ
bounded adversary cannot alter the ensemble prediction generated by an additive isotropic Gaussian noise, where the radius for the adversary depends on both the variance of the distribution as well as the ensemble margin at the point of interest. We build on and considerably expand this work across broad classes of distributions. In particular, we offer adversarial robustness guarantees and associated algorithms for the discrete case where the adversary is
ℓ
0
ℓ
bounded. Moreover, we exemplify how the guarantees can be tightened with specific assumptions about the function class of the classifier such as a decision tree. We empirically illustrate these results with and without functional restrictions across image and molecule datasets."
neurips,https://proceedings.neurips.cc/paper/2019/file/fa3060edb66e6ff4507886f9912e1ab9-Paper.pdf,Exact sampling of determinantal point processes with sublinear time preprocessing,"Michal Derezinski, Daniele Calandriello, Michal Valko",
neurips,https://proceedings.neurips.cc/paper/2019/file/fa3a3c407f82377f55c19c5d403335c7-Paper.pdf,Neural Diffusion Distance for Image Segmentation,"Jian Sun, Zongben Xu",
neurips,https://proceedings.neurips.cc/paper/2019/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf,Experience Replay for Continual Learning,"David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, Gregory Wayne",
neurips,https://proceedings.neurips.cc/paper/2019/file/faad95253aee7437871781018bdf3309-Paper.pdf,Efficient online learning with kernels for adversarial large scale problems,"Rémi Jézéquel, Pierre Gaillard, Alessandro Rudi","We are interested in a framework of online learning with kernels for low-dimensional, but large-scale and potentially adversarial datasets. We study the computational and theoretical performance of online variations of kernel Ridge regression. Despite its simplicity, the algorithm we study is the first to achieve the optimal regret for a wide range of kernels with a per-round complexity of order
n
α
n
with
α
<
2
α
. The algorithm we consider is based on approximating the kernel with the linear span of basis functions. Our contributions are twofold: 1) For the Gaussian kernel, we propose to build the basis beforehand (independently of the data) through Taylor expansion. For
d
d
-dimensional inputs, we provide a (close to) optimal regret of order
O
(
(
log
n
)
d
+
1
)
O
with per-round time complexity and space complexity
O
(
(
log
n
)
2
d
)
O
. This makes the algorithm a suitable choice as soon as
n
≫
e
d
n
which is likely to happen in a scenario with small dimensional and large-scale dataset; 2) For general kernels with low effective dimension, the basis functions are updated sequentially, adapting to the data, by sampling Nyström points. In this case, our algorithm improves the computational trade-off known for online kernel regression."
neurips,https://proceedings.neurips.cc/paper/2019/file/faefec47428cf9a2f0875ba9c2042a81-Paper.pdf,KNG: The K-Norm Gradient Mechanism,"Matthew Reimherr, Jordan Awan",
neurips,https://proceedings.neurips.cc/paper/2019/file/faf02b2358de8933f480a146f4d2d98e-Paper.pdf,On the Downstream Performance of Compressed Word Embeddings,"Avner May, Jian Zhang, Tri Dao, Christopher Ré",
neurips,https://proceedings.neurips.cc/paper/2019/file/fb03a0f2f5d94af4a5c0890fff0ef6e0-Paper.pdf,Primal-Dual Block Generalized Frank-Wolfe,"Qi Lei, JIACHENG ZHUO, Constantine Caramanis, Inderjit S. Dhillon, Alexandros G. Dimakis",
neurips,https://proceedings.neurips.cc/paper/2019/file/fb09f481d40c4d3c0861a46bd2dc52c0-Paper.pdf,Nonparametric Density Estimation & Convergence Rates for GANs under Besov IPM Losses,"Ananya Uppal, Shashank Singh, Barnabas Poczos",
neurips,https://proceedings.neurips.cc/paper/2019/file/fb2fcd534b0ff3bbed73cc51df620323-Paper.pdf,Blended Matching Pursuit,"Cyrille Combettes, Sebastian Pokutta",
neurips,https://proceedings.neurips.cc/paper/2019/file/fb4c835feb0a65cc39739320d7a51c02-Paper.pdf,Efficient Near-Optimal Testing of Community Changes in Balanced Stochastic Block Models,"Aditya Gangrade, Praveen Venkatesh, Bobak Nazer, Venkatesh Saligrama","We propose and analyze the problems of \textit{community goodness-of-fit and two-sample testing} for stochastic block models (SBM), where changes arise due to modification in community memberships of nodes. Motivated by practical applications, we consider the challenging sparse regime, where expected node degrees are constant, and the inter-community mean degree (
b
b
) scales proportionally to intra-community mean degree (
a
a
). Prior work has sharply characterized partial or full community recovery in terms of a
signal-to-noise ratio'' (
S
N
R
S
) based on
a
a
and
b
b
. For both problems, we propose computationally-efficient tests that can succeed far beyond the regime where recovery of community membership is even possible. Overall, for large changes,
s
≫
√
n
s
, we need only
S
N
R
=
O
(
1
)
S
whereas a na\""ive test based on community recovery with
O
(
s
)
O
errors requires
S
N
R
=
Θ
(
log
n
)
S
. Conversely, in the small change regime,
s
≪
√
n
s
, via an information theoretic lower bound, we show that, surprisingly, no algorithm can do better than the na\""ive algorithm that first estimates the community up to
O
(
s
)
O
errors and then detects changes. We validate these phenomena numerically on SBMs and on real-world datasets as well as Markov Random Fields where we only observe node data rather than the existence of links."
neurips,https://proceedings.neurips.cc/paper/2019/file/fbad540b2f3b5638a9be9aa6a4d8e450-Paper.pdf,Who is Afraid of Big Bad Minima? Analysis of gradient-flow in spiked matrix-tensor models,"Stefano Sarao Mannelli, Giulio Biroli, Chiara Cammarota, Florent Krzakala, Lenka Zdeborová",
neurips,https://proceedings.neurips.cc/paper/2019/file/fbefa505c8e8bf6d46f38f5277fed8d6-Paper.pdf,Online Convex Matrix Factorization with Representative Regions,"Jianhao Peng, Olgica Milenkovic, Abhishek Agarwal",
neurips,https://proceedings.neurips.cc/paper/2019/file/fc0de4e0396fff257ea362983c2dda5a-Paper.pdf,Differential Privacy Has Disparate Impact on Model Accuracy,"Eugene Bagdasaryan, Omid Poursaeed, Vitaly Shmatikov","For example, a gender classification model trained using DP-SGD exhibits much lower accuracy for black faces than for white faces. Critically, this gap is bigger in the DP model than in the non-DP model, i.e., if the original model is unfair, the unfairness becomes worse once DP is applied. We demonstrate this effect for a variety of tasks and models, including sentiment analysis of text and image classification. We then explain why DP training mechanisms such as gradient clipping and noise addition have disproportionate effect on the underrepresented and more complex subgroups, resulting in a disparate reduction of model accuracy."
neurips,https://proceedings.neurips.cc/paper/2019/file/fc192b0c0d270dbf41870a63a8c76c2f-Paper.pdf,Fair Algorithms for Clustering,"Suman Bera, Deeparnab Chakrabarty, Nicolas Flores, Maryam Negahbani","We study the problem of finding low-cost {\em fair clusterings} in data where each data point may belong to many protected groups. Our work significantly generalizes the seminal work of Chierichetti \etal (NIPS 2017) as follows. - We allow the user to specify the parameters that define fair representation. More precisely, these parameters define the maximum over- and minimum under-representation of any group in any cluster. - Our clustering algorithm works on any
ℓ
p
ℓ
-norm objective (e.g.
k
k
-means,
k
k
-median, and
k
k
-center). Indeed, our algorithm transforms any vanilla clustering solution into a fair one incurring only a slight loss in quality. - Our algorithm also allows individuals to lie in multiple protected groups. In other words, we do not need the protected groups to partition the data and we can maintain fairness across different groups simultaneously. Our experiments show that on established data sets, our algorithm performs much better in practice than what our theoretical results suggest."
neurips,https://proceedings.neurips.cc/paper/2019/file/fc2c7c47b918d0c2d792a719dfb602ef-Paper.pdf,The Cells Out of Sample (COOS) dataset and benchmarks for measuring out-of-sample generalization of image classifiers,"Alex Lu, Amy Lu, Wiebke Schormann, Marzyeh Ghassemi, David Andrews, Alan Moses",
neurips,https://proceedings.neurips.cc/paper/2019/file/fc2e6a440b94f64831840137698021e1-Paper.pdf,Counting the Optimal Solutions in Graphical Models,"Radu Marinescu, Rina Dechter",
neurips,https://proceedings.neurips.cc/paper/2019/file/fc9812127bf09c7bd29ad6723c683fb5-Paper.pdf,Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems,"Asma Ghandeharioun, Judy Hanwen Shen, Natasha Jaques, Craig Ferguson, Noah Jones, Agata Lapedriza, Rosalind Picard",
neurips,https://proceedings.neurips.cc/paper/2019/file/fc9b003bb003a298c2ad0d05e4342bdc-Paper.pdf,Robust Multi-agent Counterfactual Prediction,"Alexander Peysakhovich, Christian Kroer, Adam Lerer",
neurips,https://proceedings.neurips.cc/paper/2019/file/fccc64972a9468a11f125cadb090e89e-Paper.pdf,On Tractable Computation of Expected Predictions,"Pasha Khosravi, YooJung Choi, Yitao Liang, Antonio Vergari, Guy Van den Broeck",
neurips,https://proceedings.neurips.cc/paper/2019/file/fcdf25d6e191893e705819b177cddea0-Paper.pdf,Stagewise Training Accelerates Convergence of Testing Error Over SGD,"Zhuoning Yuan, Yan Yan, Rong Jin, Tianbao Yang",
neurips,https://proceedings.neurips.cc/paper/2019/file/fcdf698a5d673435e0a5a6f9ffea05ca-Paper.pdf,Specific and Shared Causal Relation Modeling and Mechanism-Based Clustering,"Biwei Huang, Kun Zhang, Pengtao Xie, Mingming Gong, Eric P. Xing, Clark Glymour",
neurips,https://proceedings.neurips.cc/paper/2019/file/fce34b6aef091b6fb2032870279690f8-Paper.pdf,Computational Separations between Sampling and Optimization,Kunal Talwar,
neurips,https://proceedings.neurips.cc/paper/2019/file/fcf55a303b71b84d326fb1d06e332a26-Paper.pdf,Classification Accuracy Score for Conditional Generative Models,"Suman Ravuri, Oriol Vinyals",
neurips,https://proceedings.neurips.cc/paper/2019/file/fd0a5a5e367a0955d81278062ef37429-Paper.pdf,Unsupervised Meta-Learning for Few-Shot Image Classification,"Siavash Khodadadeh, Ladislau Boloni, Mubarak Shah",
neurips,https://proceedings.neurips.cc/paper/2019/file/fd2c5e4680d9a01dba3aada5ece22270-Paper.pdf,Transferable Normalization: Towards Improving Transferability of Deep Neural Networks,"Ximei Wang, Ying Jin, Mingsheng Long, Jianmin Wang, Michael I. Jordan",
neurips,https://proceedings.neurips.cc/paper/2019/file/fd4771e85e1f916f239624486bff502d-Paper.pdf,Semi-Implicit Graph Variational Auto-Encoders,"Arman Hasanzadeh, Ehsan Hajiramezanali, Krishna Narayanan, Nick Duffield, Mingyuan Zhou, Xiaoning Qian",
neurips,https://proceedings.neurips.cc/paper/2019/file/fd95ec8df5dbeea25aa8e6c808bad583-Paper.pdf,Efficient Approximation of Deep ReLU Networks for Functions on Low Dimensional Manifolds,"Minshuo Chen, Haoming Jiang, Wenjing Liao, Tuo Zhao",
neurips,https://proceedings.neurips.cc/paper/2019/file/fdd5b16fc8134339089ef25b3cf0e588-Paper.pdf,GOT: An Optimal Transport framework for Graph comparison,"Hermina Petric Maretic, Mireille El Gheche, Giovanni Chierchia, Pascal Frossard",
neurips,https://proceedings.neurips.cc/paper/2019/file/fe1f9c70bdf347497e1a01b6c486bdb9-Paper.pdf,Multivariate Distributionally Robust Convex Regression under Absolute Error Loss,"Jose Blanchet, Peter W. Glynn, Jun Yan, Zhengqing Zhou",
neurips,https://proceedings.neurips.cc/paper/2019/file/fe4b8556000d0f0cae99daa5c5c5a410-Paper.pdf,A Benchmark for Interpretability Methods in Deep Neural Networks,"Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim",
neurips,https://proceedings.neurips.cc/paper/2019/file/fe5e7cb609bdbe6d62449d61849c38b0-Paper.pdf,Biases for Emergent Communication in Multi-agent Reinforcement Learning,"Tom Eccles, Yoram Bachrach, Guy Lever, Angeliki Lazaridou, Thore Graepel",
neurips,https://proceedings.neurips.cc/paper/2019/file/fe663a72b27bdc613873fbbb512f6f67-Paper.pdf,Zero-shot Knowledge Transfer via Adversarial Belief Matching,"Paul Micaelli, Amos J. Storkey","Performing knowledge transfer from a large teacher network to a smaller student is a popular task in modern deep learning applications. However, due to growing dataset sizes and stricter privacy regulations, it is increasingly common not to have access to the data that was used to train the teacher. We propose a novel method which trains a student to match the predictions of its teacher without using any data or metadata. We achieve this by training an adversarial generator to search for images on which the student poorly matches the teacher, and then using them to train the student. Our resulting student closely approximates its teacher for simple datasets like SVHN, and on CIFAR10 we improve on the state-of-the-art for few-shot distillation (with
100
100
images per class), despite using no data. Finally, we also propose a metric to quantify the degree of belief matching between teacher and student in the vicinity of decision boundaries, and observe a significantly higher match between our zero-shot student and the teacher, than between a student distilled with real data and the teacher. Code is available at: https://github.com/polo5/ZeroShotKnowledgeTransfer"
neurips,https://proceedings.neurips.cc/paper/2019/file/fe73f687e5bc5280214e0486b273a5f9-Paper.pdf,Uniform Error Bounds for Gaussian Process Regression with Application to Safe Control,"Armin Lederer, Jonas Umlauft, Sandra Hirche",
neurips,https://proceedings.neurips.cc/paper/2019/file/fea16e782bc1b1240e4b3c797012e289-Paper.pdf,Leader Stochastic Gradient Descent for Distributed Training of Deep Learning Models,"Yunfei Teng, Wenbo Gao, François Chalus, Anna E. Choromanska, Donald Goldfarb, Adrian Weller",
neurips,https://proceedings.neurips.cc/paper/2019/file/feab05aa91085b7a8012516bc3533958-Paper.pdf,Random deep neural networks are biased towards simple functions,"Giacomo De Palma, Bobak Kiani, Seth Lloyd",
neurips,https://proceedings.neurips.cc/paper/2019/file/febefe1cc5c87748ea02036dbe9e3d67-Paper.pdf,Discrete Object Generation with Reversible Inductive Construction,"Ari Seff, Wenda Zhou, Farhan Damani, Abigail Doyle, Ryan P. Adams",
neurips,https://proceedings.neurips.cc/paper/2019/file/fecc3a370a23d13b1cf91ac3c1e1ca92-Paper.pdf,Adaptively Aligned Image Captioning via Adaptive Attention Time,"Lun Huang, Wenmin Wang, Yaxian Xia, Jie Chen",
neurips,https://proceedings.neurips.cc/paper/2019/file/fface8385abbf94b4593a0ed53a0c70f-Paper.pdf,Fully Dynamic Consistent Facility Location,"Vincent Cohen-Addad, Niklas Oskar D. Hjuler, Nikos Parotsidis, David Saulpic, Chris Schwiegelshohn","We consider classic clustering problems in fully dynamic data streams, where data elements can be both inserted and deleted. In this context, several parameters are of importance: (1) the quality of the solution after each insertion or deletion, (2) the time it takes to update the solution, and (3) how different consecutive solutions are. The question of obtaining efficient algorithms in this context for facility location,
k
k
-median and
k
k
-means has been raised in a recent paper by Hubert-Chan et al. [WWW'18] and also appears as a natural follow-up on the online model with recourse studied by Lattanzi and Vassilvitskii [ICML'17] (i.e.: in insertion-only streams). In this paper, we focus on general metric spaces and mainly on the facility location problem. We give an arguably simple algorithm that maintains a constant factor approximation, with
O
(
n
log
n
)
O
update time, and total recourse
O
(
n
)
O
. This improves over the naive algorithm which consists in recomputing a solution at each time step and that can take up to
O
(
n
2
)
O
update time, and
O
(
n
2
)
O
total recourse. These bounds are nearly optimal: in general metric space, inserting a point take
O
(
n
)
O
times to describe the distances to other points, and we give a simple lower bound of
O
(
n
)
O
for the recourse. Moreover, we generalize this result for the
k
k
-medians and
k
k
-means problems: our algorithm maintains a constant factor approximation in time
˜
O
(
n
+
k
2
)
O
. We complement our analysis with experiments showing that the cost of the solution maintained by our algorithm at any time
t
t
is very close to the cost of a solution obtained by quickly recomputing a solution from scratch at time
t
t
while having a much better running time."
neurips,https://proceedings.neurips.cc/paper/2019/file/ffe10334251de1dc98339d99ae4743ba-Paper.pdf,Efficient Rematerialization for Deep Networks,"Ravi Kumar, Manish Purohit, Zoya Svitkina, Erik Vee, Joshua Wang",
neurips,https://proceedings.neurips.cc/paper/2019/file/ffedf5be3a86e2ee281d54cdc97bc1cf-Paper.pdf,Flow-based Image-to-Image Translation with Feature Disentanglement,"Ruho Kondo, Keisuke Kawano, Satoshi Koide, Takuro Kutsuna",
